Received: from GVXPR01MB11759.eurprd01.prod.exchangelabs.com
 (2603:10a6:150:2e3::12) by AS8PR01MB10319.eurprd01.prod.exchangelabs.com with
 HTTPS; Fri, 26 Sep 2025 15:32:45 +0000
Received: from AS9PR06CA0070.eurprd06.prod.outlook.com (2603:10a6:20b:464::21)
 by GVXPR01MB11759.eurprd01.prod.exchangelabs.com (2603:10a6:150:2e3::12) with
 Microsoft SMTP Server (version=TLS1_2,
 cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id 15.20.9160.9; Fri, 26 Sep
 2025 15:32:42 +0000
Received: from AM4PEPF00025F9C.EURPRD83.prod.outlook.com
 (2603:10a6:20b:464:cafe::6d) by AS9PR06CA0070.outlook.office365.com
 (2603:10a6:20b:464::21) with Microsoft SMTP Server (version=TLS1_3,
 cipher=TLS_AES_256_GCM_SHA384) id 15.20.9160.11 via Frontend Transport; Fri,
 26 Sep 2025 15:32:41 +0000
Authentication-Results: spf=pass (sender IP is 159.112.244.5)
 smtp.mailfrom=mg-d1.substack.com; dkim=pass (signature was verified)
 header.d=mg-d1.substack.com;dmarc=pass action=none
 header.from=substack.com;compauth=pass reason=100
Received-SPF: Pass (protection.outlook.com: domain of mg-d1.substack.com
 designates 159.112.244.5 as permitted sender)
 receiver=protection.outlook.com; client-ip=159.112.244.5;
 helo=m244-5.mailgun.net; pr=C
Received: from m244-5.mailgun.net (159.112.244.5) by
 AM4PEPF00025F9C.mail.protection.outlook.com (10.167.16.11) with Microsoft
 SMTP Server (version=TLS1_2, cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id
 15.20.9182.0 via Frontend Transport; Fri, 26 Sep 2025 15:32:41 +0000
DKIM-Signature: a=rsa-sha256; v=1; c=relaxed/relaxed; d=mg-d1.substack.com; q=dns/txt; s=k1; t=1758900760; x=1758907960;
 h=List-Unsubscribe-Post: List-Unsubscribe: List-Post: List-Id: List-Archive: List-Owner: Reply-To: In-Reply-To: References: sender: sender: Date: Message-Id: To: To: From: From: Subject: Subject: Content-Type: Mime-Version;
 bh=Qb4mTbjXzdaANjfftUK72S+jQNIQPWhPDaMb7ERoVv4=;
 b=Nv8CtZ5YzC26RDeULMukeLl5rJ1kWnLB4VTkaIqBZzEE2jI89PUhhxjKRzB8dMGbNp0XjLdNMQwbb0jz2ejAyxsAqA/WNZE8nup2x5rDKxGYQY8c0bm8L4F4o2ttR2PKF2SlrTufZ/MnnheAwIGOrVQSmASOqN21s9cZjy+AtEs=
X-Mailgun-Sid: WyI1NGVlYSIsImVpdGFuQGVpc2xhdy5jby5pbCIsIjM5YjA4YzYiXQ==
Received: by d7d8ab5b87a16bd9466eb241b49d2617e512afe5621d91c16ab1b922ccdfa28b with HTTP
 id 68d6b2189b5970b439bbce65; Fri, 26 Sep 2025 15:32:39 GMT
X-Mailgun-Sending-Ip: 159.112.244.5
X-Mailgun-Batch-Id: 68d6b217a73000fc33600fe3
Content-Type: multipart/alternative;
 boundary="478d3da90589b4d5b597a56878732fe63b2890310289e5f131c8487c2bf9"
Subject: 9 Papers You Should Know About
From: LLM Watch <xaiguy@substack.com>
To: eitan@eislaw.co.il
X-Mailgun-Tag: post
X-Mailgun-Track-Clicks: false
Message-Id: <20250926153058.3.f90dcaa9950359b6@mg-d1.substack.com>
Date: Fri, 26 Sep 2025 15:30:58 +0000
Feedback-ID: post-174513497:cat-post:pub-1428667:substack
sender: LLM Watch <xaiguy@substack.com>
References: <post-174513497@substack.com>
In-Reply-To: <post-174513497@substack.com>
Reply-To: LLM Watch
 <reply+2vwfh5&5kb93z&&e9879922779dbf93587abb8eb6a557c203237e81dcf97bee9ace8f9f7066a034@mg1.substack.com>
List-Owner: <mailto:xaiguy@substack.com>
List-URL: <https://www.llmwatch.com/>
List-Archive: <https://www.llmwatch.com/archive>
List-Id: <xaiguy.substack.com>
List-Post: <https://www.llmwatch.com/p/9-papers-you-should-know-about-1f5>
List-Unsubscribe: <https://www.llmwatch.com/action/disable_email/disable?token=eyJ1c2VyX2lkIjozMzY0NDgyMjMsInBvc3RfaWQiOjE3NDUxMzQ5NywiaWF0IjoxNzU4OTAwNzU5LCJleHAiOjE3OTA0MzY3NTksImlzcyI6InB1Yi0xNDI4NjY3Iiwic3ViIjoiZGlzYWJsZV9lbWFpbCJ9.s7VG9_g8WH-RVJvDC6DY33clZdlWIZtkmNYquDpdru0&all_sections=true>
List-Unsubscribe-Post: List-Unsubscribe=One-Click
X-Mailgun-Variables: {"category": "post", "email_generated_at": "1758900759187", "is_freemail":
 "true", "post_audience": "everyone", "post_id": "174513497", "post_type":
 "newsletter", "pub_community_enabled": "true", "publication_id": "1428667",
 "subdomain": "xaiguy", "user_id": "336448223"}
Return-Path: bounce+45c996.39b08c6-eitan=eislaw.co.il@mg-d1.substack.com
X-MS-Exchange-Organization-ExpirationStartTime: 26 Sep 2025 15:32:41.7539
 (UTC)
X-MS-Exchange-Organization-ExpirationStartTimeReason: OriginalSubmit
X-MS-Exchange-Organization-ExpirationInterval: 1:00:00:00.0000000
X-MS-Exchange-Organization-ExpirationIntervalReason: OriginalSubmit
X-MS-Exchange-Organization-Network-Message-Id: c474b7cb-0f9d-4315-3e24-08ddfd11eeb4
X-EOPAttributedMessage: 0
X-EOPTenantAttributedMessage: 384c4129-e818-4ea7-8f8b-189d997170d1:0
X-MS-Exchange-Organization-MessageDirectionality: Incoming
X-MS-PublicTrafficType: Email
X-MS-TrafficTypeDiagnostic: AM4PEPF00025F9C:EE_|GVXPR01MB11759:EE_|AS8PR01MB10319:EE_
X-MS-Exchange-Organization-AuthSource: AM4PEPF00025F9C.EURPRD83.prod.outlook.com
X-MS-Exchange-Organization-AuthAs: Anonymous
X-MS-Office365-Filtering-Correlation-Id: c474b7cb-0f9d-4315-3e24-08ddfd11eeb4
X-MS-Exchange-Organization-SCL: 1
X-Microsoft-Antispam: BCL:4;ARA:13230040|12012899012|29132699027|69100299015|1032899013|4022899009|13003099007|2066899003|20203002999003|8096899003;
X-Forefront-Antispam-Report: CIP:159.112.244.5;CTRY:US;LANG:en;SCL:1;SRV:;IPV:NLI;SFV:NSPM;H:m244-5.mailgun.net;PTR:m244-5.mailgun.net;CAT:NONE;SFS:(13230040)(12012899012)(29132699027)(69100299015)(1032899013)(4022899009)(13003099007)(2066899003)(20203002999003)(8096899003);DIR:INB;
X-MS-Exchange-CrossTenant-OriginalArrivalTime: 26 Sep 2025 15:32:41.4263
 (UTC)
X-MS-Exchange-CrossTenant-Network-Message-Id: c474b7cb-0f9d-4315-3e24-08ddfd11eeb4
X-MS-Exchange-CrossTenant-Id: 384c4129-e818-4ea7-8f8b-189d997170d1
X-MS-Exchange-CrossTenant-AuthSource: AM4PEPF00025F9C.EURPRD83.prod.outlook.com
X-MS-Exchange-CrossTenant-AuthAs: Anonymous
X-MS-Exchange-CrossTenant-FromEntityHeader: Internet
X-MS-Exchange-Transport-CrossTenantHeadersStamped: GVXPR01MB11759
X-MS-Exchange-Transport-EndToEndLatency: 00:00:04.0126446
X-MS-Exchange-Processed-By-BccFoldering: 15.20.9160.000
X-Microsoft-Antispam-Mailbox-Delivery:
	ucf:0;jmr:0;auth:0;dest:I;ENG:(910005)(944506478)(944626604)(920097)(930097)(140003);
X-Microsoft-Antispam-Message-Info:
	=?utf-8?B?RmtBckhkRWRsSmVGNDJmb0tHSWhzOXp0dU1RS3Boa2M1N2NLRDhDWTZXdnN3?=
 =?utf-8?B?cW9pNzVhSkUwSkVPYkd2WGZGK0I2RjNkNUdUdkRGWHFVdXNXT3B4UUsxY2Za?=
 =?utf-8?B?cG4rWEhIaFhRbTNxNkt5dkYrY3dKSGRvenpGVWoxaXdTdE9YOXZTUFl4blFZ?=
 =?utf-8?B?R2c2ZDhJRVpyV1BHbWdZUUduZE1MT29uVGRqenhydHVBdmJKVTEvSWQ2cTNv?=
 =?utf-8?B?V0N0Q3JqdGgyTU5uaS8zUkxHTHF5OWppRGpBcXFOQjRhUm9JQ0taZ0tNWDJX?=
 =?utf-8?B?ZkJVcDNlRVJQR2RhWWRLU1R0VEUva0VRZVRuK3ZRM3FqL3dIeFk2SzJjQlZu?=
 =?utf-8?B?M2kyS1RBQm9HSDFjZGRjZWxjcGdJVUY1MlVmU1dINWx4TlZJcDA5SkFjcnNJ?=
 =?utf-8?B?TmFjdmtUZ3JKTUkzRDl4VXVqdTExaVBCM1pBaFlQdGtsWWRtTDd2L0NWSzNZ?=
 =?utf-8?B?Wmx0cjhpS0VJNGxCeTRSK2xDL0dUK1NQZXlHVUxPL3ZVUnRQSnpFVFVDRXVU?=
 =?utf-8?B?TUM0WTI5Y2pka3pJRjNmZkx0WHlnQy91dlVEZmwxZnJLYmRhZ3YydzlYM1Ri?=
 =?utf-8?B?M1ZyTVIzQTVheDJHSi9iL2FyaUpWTnBvV1dTVUNLcVgveU44d1RMNnVwWUtT?=
 =?utf-8?B?VkhCeEZHYm5mc3F6VzNYSncvbDFTV1MzV3hWOWFibEtSdDZjYmpZSFlIYWRN?=
 =?utf-8?B?R0JybHFJTk05Z29QYzRvWkZCL1VLcFdmNHpWVVdNRUl3elh1SDFXVlVpOW5n?=
 =?utf-8?B?aFZWZ3NENlhOTHdiaGlPdG1ndndSQW96anUydmRrdEQ4L3BtdSt2NEFzNVFi?=
 =?utf-8?B?a0ZjVFlXWXlrdjZEQkQwNmRDM2J2QzU3Q3V0ZEFoYzBUTS9wd3M2Y3N4K0FX?=
 =?utf-8?B?WnBBM3VTakR0WG15clJMb2tBaVFtMTUyRm4wdHdVaU1BbnRxSysyYXJzdW50?=
 =?utf-8?B?cFRRaXhaMGlsbWNRaS9rRys0MU12dW9uV3VHZmI2a2pRanNEbXBrRTNxT2ZS?=
 =?utf-8?B?VVArZW5JSnBmc09TQSswUjAwbzRrYUczUnpOR0hYVlJRUmFXVGdUdVJKQjNC?=
 =?utf-8?B?b0ZBOHR2Nkp3TURzY0VZNHR1RWQyWmVVcVpYMDZWOVFZTzVaK3gxSFFCTUZK?=
 =?utf-8?B?dnQvR2FhYlRCMk5WQ05DVEVQNFFtZDZ0UUI3a0l6Qys4MWg4ai85YnJmbXlx?=
 =?utf-8?B?citYWi9Uc0VrZmVHa1E0K2VVcUFmQnU5Q1NBSTk5c0paZTNOVktubXJ6K3Bv?=
 =?utf-8?B?Z1NZVXpMQU91bW8vZXZpN3dlMXZKV0lPd05nYzNtcVZwMm9MYkQ1Vzc4T1N4?=
 =?utf-8?B?SlJtT0E0bW9OeE9Ib29Fc2UwdldYV0RpbzdiL2xwaDZqQW5YUkZ1Zjg5V0d6?=
 =?utf-8?B?L3JBQjAwYTUxby81U3gyUUROZFQ2TmJJZE9wVE90SU1wSEJjN1RkL0FPanhE?=
 =?utf-8?B?L2VhQUg0NTI5eEg0S0FBVXJkclg1OHVzd3VnZ0czQ0xuNWpCOUdsWGRvaTZ1?=
 =?utf-8?B?M24vTDIwcDY5d1ZUVG5uYUE1UzFqV2cycjRLR2c2NVR6blJ1bE00dHZjaXJJ?=
 =?utf-8?B?TThCcUc1T3hmSTd3NUU5aHZNbDc2YS9QK2R0QnQ0U0FXRkNBUDkxTHBlODJT?=
 =?utf-8?B?WFNqajVKZmlWSWxuenAxZy96Z3VUcEcyZEh5alpqMFpjeVpiQ1QrWmZCemtN?=
 =?utf-8?B?NldPcHpMbnd5Mzlja3JnZlNWQ1NSeGdtVUdNc2k1a0Z1cjcyeFRtcmxkZytM?=
 =?utf-8?B?T1RHazlGSmtSdTBaVEFGS1paR29PUDcrMHowNThkVi9uZU04WjBKTHZiSDda?=
 =?utf-8?B?L1R3aU1rbUdsY1ZCclJpZUVnQkhUUzhxWFJkUUpuVXpqWGF6M1JMTGVnTTdM?=
 =?utf-8?B?Wnlia3VISVNqWmIxZkFzRVdPcmgvbTBxRDJ2M3pUWTF6emt2ZzhYUit4UVIy?=
 =?utf-8?B?NHd2eVRScTBQSDVyMG5vOUx4UEc2bDhMZWxvS2NudERZbmVEZXN5UXhMY0F5?=
 =?utf-8?B?NjdudDZDT1prWWFEemIwU2VRRmRyZngvUVR3SzdSbjlRRmNzQ05idWNyZ2RF?=
 =?utf-8?B?VVhhVXpmenZFYk5WdGwzNmZnYUdzcE9QaXdpbkQvV0Vmd3B4eXRuT3MreFJR?=
 =?utf-8?B?RnZSN2tvS1MwQ3R3T2g4bVFLak5vRC90WmhLRmFsWGsyYWZ3UFgxck53UmtW?=
 =?utf-8?B?YXY4NXVvVVZVSHkzYkJKdnVrWTNSQXpoK0FkNHlBSEttdmlFZ1lFVkp6Q3o1?=
 =?utf-8?B?UWIydWVjTGtCb0pEai9naDdmSUh4eHN6WE9JRjJYU0E0ZURpMG52MXprb1Vt?=
 =?utf-8?B?ZjdJdENidUpvVXBPNnR4KzRWc3A0WFl4YlJxV2dRSFlCSnE1VERDYndWZ2pv?=
 =?utf-8?B?T2VWSy9CKzFuNVpqSVZ1bDBRRDB5cGlDRGdZMUJLY1VZOWNwQVJvYUp6UTZ4?=
 =?utf-8?B?RE5nYWJUc1UxNlV1T2c5TFNwY1ZuZ1E4elZoWkZOVy9XMS9pVkZVS3hhWW5L?=
 =?utf-8?B?eXhtanhvMkh1NUVZdk5hdHZlUmZ4UWtneEpPY2kwS2xSaklLOXdPQjlKM1lB?=
 =?utf-8?B?T0ZGaEVCd2N4SnVBa0NpM0I3QlN6VGpZT0k0MWRUcVlpa1BJNklaWHJ4UUpO?=
 =?utf-8?B?M2ZKTThZNGhidWpwTHV6ckJVd1M0YlYwRTI4SWx0S1g1SDNpYWZGVzFTT09j?=
 =?utf-8?B?MWdvSWl2T0c0VCthdDBGVVVaK3JXanpQTUNvbUVMZlRHUUltcytIcGJ6cWEz?=
 =?utf-8?B?Q1pKam9JSEc4VEpRM0xHNkFSYTVkR1h5SjNadzdNY25DekVkajZuR2dWbXY0?=
 =?utf-8?B?UXhiUEtFNU5XbVlocnRFZURLUUQ4aTlMb3U4SGlRSVVJSTNUZkt3ZlJOclBO?=
 =?utf-8?B?OGlmYTRqbkdva1pRNkRaQWhnUzEybzdEK2dsWk9SMWpyM0F5U0I0TExHSXdu?=
 =?utf-8?B?WEtJM1hPRzR1d3FwY2pScWMrSnpWTmgvOWtpQmZvYm8vTm1VVDI2SjdjazNa?=
 =?utf-8?B?U0ZjT0VNRXB0L3hYRlhhYTVKYS9yT1pqKzh5MXNSY041c3pCQ1NUVlN6aEYr?=
 =?utf-8?B?T2swVS8xdk1XMkFRPT0=?=
MIME-Version: 1.0

--478d3da90589b4d5b597a56878732fe63b2890310289e5f131c8487c2bf9
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable

View this post on the web at https://www.llmwatch.com/p/9-papers-you-should=
-know-about-1f5

Welcome, Watchers! This week in LLM Watch, we delve into cutting-edge advan=
ces in large language models (LLMs) and their applications across AI, visio=
n, science, and more. The highlights include:
An open-source evolutionary framework (ShinkaEvolve) that uses LLMs to gene=
rate new algorithms with unprecedented sample-efficiency.
A self-supervised RL paradigm (RLPT) letting LLMs learn reasoning from raw =
pre-training data, yielding up to 8-point gains on benchmarks.
Evidence that generative video models can solve 62 tasks zero-shot =E2=80=
=93 from segmentation to physical reasoning =E2=80=93 hinting at general-pu=
rpose vision models.
A =E2=80=9Cthink-before-you-speak=E2=80=9D training (RLMT) that forces LLMs=
 to generate chain-of-thought, achieving state-of-the-art chat performance =
with just 7K training prompts.
A novel way to train continuous =E2=80=9Csoft=E2=80=9D tokens for reasoning=
 via RL, surpassing discrete chain-of-thought on math tasks while preservin=
g base model skills.
And many more! Feel free to check out the glossary below or jump straight t=
o the paper section.
Members of LLM Watch are invited to participate in the 6th MLOps World | Ge=
nAI [ https://substack.com/redirect/fc7571a6-cbd6-4315-b5ef-f2562964a2ce?j=
=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0 ] Global=
 Summit in Austin Texas. Feat. OpenAI, HuggingFace, and 60+ sessions.
Subscribers can join remotely, for free here. [ https://substack.com/redire=
ct/4d7de022-de02-4941-a69b-28350a82a40f?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPm=
LQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0 ]
Also if you'd like to join (in-person) for practical workshops, use cases, =
food, drink and parties across Austin - use this code for 150$ off!=20
Quick Glossary
Reinforcement Learning on Pre-Training data (RLPT): A training paradigm whe=
re an LLM treats its pre-training corpus as an environment to explore next-=
token =E2=80=9Ctrajectories=E2=80=9D and learn from them via RL, without an=
y human-provided rewards. This approach bypasses the need for human annotat=
ion (unlike RLHF) by deriving rewards directly from predicting actual next =
segments. The result is that models can self-improve their reasoning skills=
 using only raw text data.
Reinforcement Learning with Verifiable Rewards (RLVR): An RL approach that =
uses programmatic or rule-based rewards in domains like math or code where =
correctness can be checked automatically. It boosts reasoning in those veri=
fiable domains, but on open-ended tasks (creative writing, planning, etc.) =
its benefits are limited.
RL with Model-rewarded Thinking (RLMT): A new RL pipeline requiring the mod=
el to generate a lengthy chain-of-thought before giving its final answer. A=
 separate reward model (like those from RLHF) then scores these reasoning t=
races, and the base model is optimized to produce better thought-out answer=
s. This =E2=80=9Cthink then respond=E2=80=9D strategy leads to stronger gen=
eral chat abilities than standard RLHF, as shown by large gains on multiple=
 benchmarks.
Thinking Augmented Pre-Training (TPT): A method to inject =E2=80=9Cthinking=
 trajectories=E2=80=9D (step-by-step reasoning chains) into text training d=
ata to enhance an LLM=E2=80=99s data efficiency. By augmenting text with au=
to-generated rationales, TPT effectively triples the value of each token, h=
elping models learn complex concepts with far fewer examples (e.g. a 3B mod=
el saw >10% higher scores on reasoning tasks).
Continuous Chain-of-Thought (=E2=80=9CSoft=E2=80=9D tokens): Using continuo=
us embeddings instead of discrete tokens for an LLM=E2=80=99s intermediate =
reasoning process. Unlike normal word tokens, these soft tokens can represe=
nt a superposition of many reasoning paths. Recent work shows that an LLM c=
an be trained via RL to utilize hundreds of continuous CoT tokens without a=
ny discrete guide. The result is richer reasoning: models match discrete Co=
T accuracy in one try and exceed it when allowed multiple tries, thanks to =
more diverse solution paths.
ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution
Watching: ShinkaEvolve (paper [ https://substack.com/redirect/adaad010-2777=
-4c7f-a691-bab85ee94bf3?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnP=
G6RxQ4tAjMRMPOw0 ]/code [ https://substack.com/redirect/fad427d8-f322-45f3-=
939d-771afd776a26?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4=
tAjMRMPOw0 ])
What problem does it solve? Modern approaches that use LLMs to evolve code =
(e.g. for solving optimization or programming tasks) are extremely sample-i=
nefficient, often needing thousands of trials to find a good solution. They=
=E2=80=99re also usually closed-source, hindering broad use. This makes LLM=
-driven discovery too costly and inaccessible for most researchers.
How does it solve the problem? ShinkaEvolve is an open-source framework tha=
t treats an LLM as a =E2=80=9Cmutation operator=E2=80=9D to iteratively imp=
rove programs. It introduces three key innovations to slash the number of t=
rials needed: (1) a balanced parent selection strategy that smartly trades =
off exploring new ideas vs. exploiting known good solutions, (2) novelty-ba=
sed rejection sampling to filter out redundant or uncreative mutations (usi=
ng embedding similarity and an LLM judge of novelty), and (3) a bandit algo=
rithm to pick the best LLM from an ensemble for each generation. Together t=
hese ensure ShinkaEvolve spends compute only on the most promising program =
variants, avoiding the =E2=80=9Crandom search=E2=80=9D inefficiencies of pr=
ior methods.
What are the key findings? By making program evolution dramatically more ef=
ficient, ShinkaEvolve achieves success on a wide range of problems that wer=
e previously impractical. It discovered a new state-of-the-art solution to =
a classic 26-circle packing problem using only 150 samples =E2=80=93 a =E2=
=80=9Cmassive leap in efficiency=E2=80=9D over past approaches. It also evo=
lved a high-performing multi-agent strategy for math contests (AIME benchma=
rk) in just 75 generations, outperforming strong human-designed baselines. =
In competitive programming, ShinkaEvolve=E2=80=99s improvements to an AtCod=
er contest agent were so significant that, on one problem, the evolved solu=
tion would have placed 2nd in the competition. Moreover, ShinkaEvolve even =
discovered a better way to train large Mixture-of-Expert LLMs =E2=80=93 fin=
ding a new load-balancing loss that beat the DeepMind =E2=80=9CGlobal LBL=
=E2=80=9D baseline with +1.73% higher task scores and 5.8% less wasted capa=
city. These results show that broad =E2=80=9Copen-ended=E2=80=9D discovery =
is now feasible at modest cost, unlocking an AI-powered co-pilot for scient=
ists and engineers to autonomously search for novel solutions.
What=E2=80=99s next? This work suggests that many hard problems in science =
and engineering (from designing algorithms to discovering new optimizations=
) can be tackled by an LLM-driven evolutionary search in an efficient way. =
By open-sourcing ShinkaEvolve and even providing a Web UI for visualizing t=
he evolutionary runs, the authors aim to democratize this approach. Future =
research will likely build on this by applying the method to new domains (e=
=2Eg. evolving circuits, scientific for=
mulas, or hyperparameters) and by inco=
rporating even more advanced LLMs as they appear. In the long run, techniqu=
es like ShinkaEvolve could serve as automated =E2=80=9Cresearch assistants=
=E2=80=9D, rapidly iterating through ideas and improvements that humans mig=
ht overlook, all while using far fewer trials than brute force methods.
Video Models are Zero-Shot Learners and Reasoners
Watching: Veo 3 Video Model (paper [ https://substack.com/redirect/ac725fce=
-a57a-43ce-a447-e7b27802d7a5?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC=
7BDnPG6RxQ4tAjMRMPOw0 ]/project [ https://substack.com/redirect/3ec29f01-52=
01-4e86-82b0-c2b0655e504e?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BD=
nPG6RxQ4tAjMRMPOw0 ])
What problem does it solve? Large language models surprised the world by di=
splaying emergent zero-shot abilities =E2=80=93 solving tasks they were nev=
er explicitly trained on =E2=80=93 thanks to scale and diverse training dat=
a. This paper asks: can video generative models analogously become general =
problem-solvers in the visual domain? Prior video models were usually evalu=
ated on narrow tasks or short benchmarks, leaving it unclear if they posses=
s broad, human-like visual reasoning capabilities.
How does it solve the problem? The authors take Veo 3, a state-of-the-art g=
enerative video model, and systematically test it on a broad battery of 62 =
tasks spanning classic vision, physical reasoning, and even tool use. Impor=
tantly, they use a minimalist prompt-based approach: they feed the model an=
 initial video frame or image plus a textual instruction (like =E2=80=9Csho=
w the edges=E2=80=9D or =E2=80=9Csolve this maze=E2=80=9D), then let it gen=
erate an 8-second video as the =E2=80=9Canswer=E2=80=9D. No fine-tuning or =
task-specific training =E2=80=93 this is true zero-shot evaluation. To isol=
ate the video model=E2=80=99s own reasoning, they ensure that a standalone =
LLM (Google Gemini 2.5) given just the image can=E2=80=99t solve the task, =
so any success comes from the video generation process itself. Essentially,=
 they=E2=80=99re prompting the video model to think visually step-by-step (=
through the frames it generates) and then checking if the result shows the =
correct solution.
What are the key findings? Remarkably, Veo 3 exhibits a wide range of emerg=
ent skills without any task-specific optimization. It can segment objects, =
detect edges, do image editing (e.g. remove an object), infer physical prop=
erties (like an object=E2=80=99s mass by how it moves), recognize object af=
fordances (how an object can be used), and even simulate tool use =E2=80=93=
 all zero-shot. These perceptual and manipulation abilities enable higher-l=
evel visual reasoning: for example, Veo 3 solves mazes and symmetry puzzles=
 by internally =E2=80=9Cimagining=E2=80=9D the solution path in the video i=
t generates. Quantitatively, across 62 diverse tasks tested, the model achi=
eved high success rates on both low-level vision tasks (e.g. 92% on edge de=
tection, 100% on image de-noising) and more cognitive tasks like physical r=
easoning. Veo 3 also showed clear improvement over its predecessor (Veo 2) =
on these tasks, indicating these capabilities scaled up with model/version =
improvements. All this suggests that video models, given sufficient scale a=
nd training, are following a similar trajectory to LLMs =E2=80=93 becoming =
general-purpose =E2=80=9Cvision foundation models=E2=80=9D that can be prom=
pted to tackle myriad tasks beyond their training.
What=E2=80=99s next? This work implies that future AI may rely on unified m=
ultimodal foundation models: just as one LLM can handle many language tasks=
, one video model could handle many vision tasks. A key next step is to ref=
ine prompting techniques and benchmarks for these video models =E2=80=93 an=
alogous to how prompt engineering and standardized evals drove progress in =
LLMs. Researchers will also explore whether introducing explicit reasoning =
steps (e.g. text-based planning combined with video generation) can further=
 enhance performance on complex tasks. On the application side, a powerful =
zero-shot video reasoner could be transformative: imagine robot assistants =
that reason visually through consequences before acting, or scientific mode=
ls that simulate physics experiments on the fly. Ultimately, the convergenc=
e of capabilities in language and video models hints at a broader trend of =
generalist AI systems, and understanding how and why these abilities emerge=
 (e.g. what in the training data or architecture leads to tool-use simulati=
on?) is an exciting question for fundamental research.
Reinforcement Learning on Pre-Training Data (RLPT)
Watching: RLPT (paper [ https://substack.com/redirect/74b9a360-e267-4b53-b2=
92-065df05c8708?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tA=
jMRMPOw0 ])
What problem does it solve? Scaling up LLMs by feeding them more text has h=
it a bottleneck: we can increase compute easily, but high-quality text data=
 is finite. Moreover, simply predicting the next token (standard training) =
might not teach models to reason through complex dependencies, because the =
model isn=E2=80=99t encouraged to explore beyond the distribution of its da=
ta. Prior methods like RLHF add some signal but require costly human feedba=
ck. In short, we need a way for LLMs to learn more from the data they alrea=
dy have, especially to acquire reasoning skills, without an army of human a=
nnotators.
How does it solve the problem? RLPT turns an LLM=E2=80=99s original pre-tra=
ining corpus into an interactive training playground for reinforcement lear=
ning. It does this by defining a next-segment prediction task as a sequenti=
al decision problem. Concretely, the model reads some text context and then=
 generates the next chunk of text; a separate Generative Reward Model (or a=
n implicit ground-truth signal) gives a reward based on how well that gener=
ation matches the actual continuation in the corpus. The LLM thus treats th=
e authentic text as demonstrations of optimal behavior, exploring different=
 continuations and getting feedback without any human labels. Importantly, =
these rewards come directly from the pre-training data (e.g. matching obser=
ved text), eliminating reliance on hand-crafted rewards or human preference=
 models. In effect, RLPT allows the model to autonomously practice reasonin=
g on unlabeled data: it might deviate from the immediate next token if it f=
inds a longer-term path that yields higher reward (better overall coherence=
 with the text). This training-time exploration is carefully scaled up to b=
illions of tokens so the policy can discover richer reasoning strategies ac=
ross a broad domain of text.
What are the key findings? When applied to a 4-billion parameter base LLM (=
Qwen3-4B), RLPT dramatically improved its performance on multiple challengi=
ng benchmarks. For instance, it boosted the model=E2=80=99s score on MMLU (=
a knowledge exam) by +3.0 points and on MMLU-Pro (an advanced version) by +=
5.1. Gains were even larger on math and logic-heavy tasks: +8.1 on a QA ben=
chmark (GPQA-Diamond) and +6.6 on AIME24 (math competition problems). These=
 are absolute improvements over an already strong base model, achieved with=
out any human-labeled data or task-specific finetuning. Moreover, scaling s=
tudies indicate that as you give RLPT more compute (more training steps), t=
he model keeps improving =E2=80=93 hinting that even bigger gains are possi=
ble with larger budgets. The authors also note that RLPT-trained models exh=
ibit stronger generalizable reasoning: they extend the model=E2=80=99s abil=
ity to handle complex prompts and improve performance of existing verificat=
ion-based RL (RLVR) when used together. In summary, RLPT offers a promising=
 path to break the data scarcity barrier by extracting much more signal fro=
m the text we already have, effectively turning passive pre-training data i=
nto an active learning experience.
What=E2=80=99s next? RLPT=E2=80=99s success with self-supervised rewards pa=
ves the way for more hybrid training regimes. Future LLMs might alternate b=
etween passive reading and active exploration of texts, games, or simulatio=
ns, all without human intervention. One immediate follow-up is to apply RLP=
T to even larger models (e.g. 34B, 70B) and more domains =E2=80=93 does it =
similarly boost reasoning for code, or multimodal data? There=E2=80=99s als=
o room to refine the reward modeling: the current next-segment reward might=
 be enhanced by incorporating logical consistency or factual accuracy metri=
cs derived automatically from the text. If those can be folded into RLPT, m=
odels could self-police their coherence and truthfulness. In the big pictur=
e, RLPT is part of a broader trend of LMs learning to think ahead (plan tok=
ens) rather than just mimic, so we can expect research that combines this w=
ith techniques like tree-of-thought or tool-use during training. All of thi=
s moves toward LLMs that not only absorb internet text, but actively practi=
ce and generalize from it =E2=80=93 much like a student solving problems to=
 better understand the material.
Soft Tokens, Hard Truths
Watching: Continuous CoT (paper [ https://substack.com/redirect/16242107-83=
95-4df8-b129-0bf1a9ec144f?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BD=
nPG6RxQ4tAjMRMPOw0 ])
What problem does it solve? Chain-of-thought prompting (having an LLM gener=
ate step-by-step reasoning) improves performance, but it uses discrete natu=
ral language tokens, which might not be the most efficient internal represe=
ntation for reasoning. Continuous tokens =E2=80=93 essentially vectors that=
 aren=E2=80=99t constrained to the discrete vocabulary =E2=80=93 have theor=
etically much greater expressiveness and can encode multiple ideas at once.=
 In theory, a model that =E2=80=9Cthinks=E2=80=9D in a continuous space cou=
ld explore many reasoning paths in parallel (a superposition) instead of on=
e-by-one. The problem is that actually training an LLM to use continuous, n=
on-language tokens in its reasoning process is very hard: past attempts eit=
her only injected continuous tokens at inference (without training on them)=
, or required distilling from known human-written reasoning chains, which i=
s cumbersome and limited to short chains. No one had shown a scalable way f=
or a model to learn a useful continuous chain-of-thought (CoT) from scratch=
=2E
How does it solve the problem? This work presents the first successful meth=
od to train continuous CoTs via reinforcement learning, without relying on =
any ground-truth human rationales. The idea is to let the model generate =
=E2=80=9Csoft=E2=80=9D tokens (continuous embeddings) between the prompt an=
d the final answer, and use a reward signal to optimize their use. Specific=
ally, they add a small amount of noise to the input embeddings as a form of=
 exploration, and then use policy-gradient RL to reward the model if its fi=
nal answer is correct. Essentially, the model is trying to invent its own i=
nternal language (the continuous tokens and what they represent) that leads=
 to better problem-solving outcomes. By avoiding any supervised training on=
 discrete chains, there=E2=80=99s no human bias limiting what these soft to=
kens can do. Notably, the approach adds minimal computational overhead, so =
they can afford to let the model use hundreds of continuous tokens in the r=
easoning phase during training =E2=80=93 orders of magnitude more =E2=80=9C=
thought capacity=E2=80=9D than prior distillation methods allowed.
What are the key findings? On math reasoning benchmarks, LLMs trained with =
this continuous CoT technique achieved performance on par with or better th=
an those using traditional discrete chain-of-thoughts. For example, on GSM8=
K math problems, a Llama-7B model with continuous CoT matched the accuracy =
of the same model with standard (discrete) CoT when considering the single =
best answer (pass@1). However, when allowed to sample multiple answers (pas=
s@32), the continuous-CoT model outperformed the discrete CoT model, indica=
ting it found a more diverse set of reasoning paths leading to correct answ=
ers. This demonstrates one big advantage of continuous tokens =E2=80=93 the=
y can capture a richer variety of solutions, which pays off when you can tr=
y multiple outputs. Interestingly, the authors found the best strategy was =
a hybrid: train with continuous tokens, but use discrete tokens at inferenc=
e. In other words, let the model think in vectors during training to gain t=
he benefits, but at deployment it can just output normal text rationale if =
needed =E2=80=93 the training still improved its latent reasoning ability. =
Moreover, continuous CoT training caused less interference with the model=
=E2=80=99s other capabilities: the model retained its accuracy on unrelated=
 tasks better than a model trained on discrete CoT, meaning this approach i=
s a =E2=80=9Csofter=E2=80=9D touch that avoids overfitting to the reasoning=
 data. All told, this is a proof-of-concept that LLMs can develop their own=
 non-human-readable thought vectors that yield real problem-solving gains.
What=E2=80=99s next? Training LLMs to think in vectors opens up many resear=
ch directions. One immediate question is how to interpret or visualize thes=
e learned continuous tokens =E2=80=93 do they correspond to human-like conc=
epts, or something entirely alien yet effective? There=E2=80=99s also poten=
tial to extend continuous CoT to multimodal reasoning (imagine an LLM that =
internally represents an image with =E2=80=9Csoft visual tokens=E2=80=9D wh=
ile reasoning). The success with reinforcement learning here may inspire us=
ing other reward signals to shape continuous thoughts, such as logical cons=
istency checks or factual verification as rewards to produce even more reli=
able reasoning. In practice, we might see hybrid systems where models do he=
avy-duty reasoning in continuous space and then distill the outcome into a =
concise explanation for humans. The fact that the =E2=80=9Csoft=E2=80=9D mo=
del=E2=80=99s final answers can be executed in standard form means adoption=
 is easy =E2=80=93 e.g. a math tutor LLM could silently use continuous CoT =
to figure out a tough proof, then present the answer in neat natural langua=
ge. Overall, this work lays groundwork for more efficient, diverse reasonin=
g in LLMs, potentially overcoming some limits of discrete token thinking th=
at chain-of-thought still had.
Thinking Augmented Pre-Training (TPT)
Watching: TPT (paper [ https://substack.com/redirect/08889166-c48a-416d-aa6=
5-437ce07d02dc?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAj=
MRMPOw0 ])
What problem does it solve? High-quality training data for LLMs is limited,=
 and some complex patterns in language are hard for a model to learn just b=
y next-word prediction. Often, the reason behind a statement or the chain o=
f logic connecting sentences is not explicitly in the text =E2=80=93 it=E2=
=80=99s implicit or assumed. This makes certain =E2=80=9Chigh-quality token=
s=E2=80=9D (like a step in a math proof, or a hidden logical connection in =
code) effectively very difficult to learn. The result is data inefficiency:=
 even with billions of words, the model might still struggle with multi-ste=
p reasoning or harder comprehension, because it never sees the intermediate=
 thinking. The challenge addressed here is how to make better use of the da=
ta by making the hidden reasoning explicit.
How does it solve the problem? TPT tackles this by augmenting the pre-train=
ing corpus with =E2=80=9Cthinking trajectories=E2=80=9D =E2=80=93 essential=
ly generating step-by-step reasoning or explanatory content and inserting i=
t alongside the original text. For example, if the original text says =E2=
=80=9CThe student solved the problem and got the answer 42,=E2=80=9D a thin=
king trajectory might include the steps the student took to solve it. These=
 trajectories are created automatically (likely using prompting on a strong=
 LLM or heuristics) for a wide range of tasks and domains, and then interwo=
ven with the original data for training. By doing so, TPT increases the eff=
ective data volume (since we add new tokens) and, crucially, makes complex =
tokens easier to learn by breaking down their underlying rationale. The met=
hod is =E2=80=9Cuniversal=E2=80=9D =E2=80=93 they apply it in various setti=
ngs: pre-training from scratch on limited data, augmenting an already large=
 corpus, or even mid-training an open-source model to further improve it. I=
n each case, the presence of explicit reasoning chains helps the model gene=
ralize better from the same amount of original text.
What are the key findings? Across model sizes and training setups, TPT deli=
vered substantial performance boosts, indicating a huge win in data efficie=
ncy. Notably, the authors report that TPT improves the data efficiency of p=
re-training by a factor of 3. In practical terms, this means an LLM trained=
 on 100B tokens with TPT augmentation could achieve comparable or better re=
sults than a model trained on 300B tokens of standard data. For a 3B-parame=
ter model, they saw over +10% improvement on multiple challenging reasoning=
 benchmarks just by incorporating thinking trajectories during training. La=
rger models and different families (they tested both decoder-only and other=
s) all benefited, suggesting TPT is robust. Importantly, these gains aren=
=E2=80=99t limited to niche tasks =E2=80=93 the paper notes improvements =
=E2=80=9Cacross various model sizes and families=E2=80=9D on general NLP be=
nchmarks. This implies the method injects a broad understanding or skill, r=
ather than overfitting to specific problems. By explicitly including reason=
ing, the model is better at tasks requiring step-by-step logic, math word p=
roblems, complex QA, etc., while also not hurting performance on standard l=
anguage tasks. Essentially, TPT shows that more thinking per token is as go=
od as (or better than) just more tokens =E2=80=93 a significant result for =
efficient training.
What=E2=80=99s next? TPT=E2=80=99s approach aligns with a growing trend of =
making LLM training more intentional or structured. Future research might e=
xplore automating the generation of thinking trajectories even further =E2=
=80=93 perhaps using one LLM to generate and another to verify or refine th=
e reasoning before using it for training. There=E2=80=99s also the possibil=
ity of extending this to other modalities: for instance, augmenting image c=
aptions with chains of visual reasoning, or code with chains of program log=
ic, to similarly boost learning. In terms of immediate practical impact, co=
mpanies training models could adopt TPT to reach high performance with less=
 data (or get better results with the same data), which is economically app=
ealing. One could also combine TPT with RLPT (from paper #4 above): first a=
ugment data with reasoning (TPT), then let the model explore that data via =
RL =E2=80=93 potentially a very powerful combo for self-improving AI. Final=
ly, TPT prompts us to consider the quality of data over quantity; by focusi=
ng on the =E2=80=9Chidden=E2=80=9D information in text and making it explic=
it, we might uncover new levels of LLM capability without needing an order-=
of-magnitude more data.
SimpleFold: Folding Proteins is Simpler than You Think
Watching: SimpleFold (paper [ https://substack.com/redirect/898f3f21-5f17-4=
50c-af9b-6194c15866d7?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6=
RxQ4tAjMRMPOw0 ]/code [ https://substack.com/redirect/cfd03e05-605b-4850-8d=
96-328b40a4a01d?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tA=
jMRMPOw0 ])
What problem does it solve? Recent breakthroughs in protein folding (like A=
lphaFold) rely on very complex model architectures tailored to capturing pr=
otein-specific geometry =E2=80=93 e.g. triangle attention modules, pairwise=
 distance matrices, multiple bespoke loss terms, etc. While extremely succe=
ssful, these specialized designs are computationally heavy and depart signi=
ficantly from the =E2=80=9Cstandard=E2=80=9D architectures used in NLP or v=
ision. This raises an intriguing question: do we really need all that domai=
n-specific complexity, or could a much simpler, more generic model fold pro=
teins with similar accuracy? In other words, is protein folding fundamental=
ly simpler than current models make it seem?
How does it solve the problem? SimpleFold is a bold attempt to strip protei=
n folding models down to basics. It uses a general-purpose Transformer arch=
itecture with no special protein-specific blocks. Instead of the customary =
tricks (triangular updates, separate 2D pair representation of amino acids,=
 etc.), it relies on standard self-attention layers (augmented by some adap=
tive gating layers) and trains them end-to-end on protein structure data. T=
he key insight is to cast protein folding as a generative modeling problem:=
 SimpleFold is trained with a flow-matching objective, which is related to =
diffusion models or normalizing flows, guiding it to incrementally refine a=
 random structure into the correct folded structure. They include a minor a=
dditional loss term to encourage correct structural predictions (so it=E2=
=80=99s not purely generic, but almost). They then scale this model up to 3=
 billion parameters and train on ~9 million protein structures (including a=
 large set of distilled/predicted ones plus experimentally solved ones). Es=
sentially, SimpleFold treats protein coordinates like a data sequence and l=
earns to =E2=80=9Cflow=E2=80=9D them into the correct shape using a Transfo=
rmer, without explicitly encoding protein biophysics knowledge.
What are the key findings? SimpleFold-3B=E2=80=99s performance rivals state=
-of-the-art specialized models on standard protein folding benchmarks. It a=
chieves competitive accuracy in predicting 3D structures, showing that a va=
nilla Transformer can indeed learn the complex dependencies needed for fold=
ing. Moreover, SimpleFold exhibits strengths where deterministic models oft=
en struggle: because it=E2=80=99s generative, it can naturally produce an e=
nsemble of different probable structures. The paper notes strong performanc=
e in ensemble prediction =E2=80=93 it can sample multiple foldings and capt=
ure alternative conformations, which is typically hard for models like Alph=
aFold that give one answer. Another practical benefit is efficiency: with i=
ts simpler architecture, SimpleFold is easier to deploy and runs faster on =
standard hardware (no specialized ops needed). The success of SimpleFold ef=
fectively challenges the notion that we need highly domain-specific designs=
 for protein folding. It opens the door to using more off-the-shelf AI comp=
onents in scientific domains. In short, the paper demonstrates that much of=
 protein folding can be learned by a generic sequence model, which is a sur=
prising and encouraging finding.
What=E2=80=99s next? SimpleFold=E2=80=99s approach could spark a re-evaluat=
ion of how we design models for scientific problems. If a plain Transformer=
 works for protein structures, perhaps other tasks (molecular property pred=
iction, DNA folding, etc.) can also shift to simpler architectures with the=
 right training approach. Future work might integrate SimpleFold with downs=
tream tasks =E2=80=93 for example, coupling it with drug binding prediction=
, where its generative ensemble ability could explore multiple protein conf=
ormations. The use of a flow-matching objective also hints at connections t=
o diffusion models; one could imagine a diffusion-based folding model that =
further improves accuracy or captures dynamics by simulating folding as a t=
ime series. Additionally, because SimpleFold is closer to standard AI model=
s, it could potentially benefit from transfer learning: e.g., initialize wi=
th a language model=E2=80=99s weights or vice versa, to inject some cross-d=
omain knowledge (there=E2=80=99s early speculation that some language featu=
res help in protein sequences). The big takeaway is that simplicity can som=
etimes achieve the same ends as complexity =E2=80=93 a valuable lesson that=
 might lead researchers to try more =E2=80=9Cminimalist=E2=80=9D baselines =
in domains dominated by hand-engineered networks. As this trend continues, =
we may see a convergence where the same core model type underpins progress =
in both science (protein folding, chemistry) and general AI, differing most=
ly in training data rather than architecture.
LLMs4All: A Review on Large Language Models for Research and Applications i=
n Academic Disciplines
Watching: LLMs4All Survey (paper [ https://substack.com/redirect/103298ce-2=
766-4247-8904-28d4aa80ae10?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7B=
DnPG6RxQ4tAjMRMPOw0 ])
What problem does it solve? The impact of LLMs is not confined to computer =
science =E2=80=93 they are rapidly permeating every academic field from his=
tory to biology. However, knowledge of how to effectively use LLMs in these=
 diverse disciplines is scattered. Researchers in, say, law or chemistry mi=
ght not be up-to-date on the latest LLM techniques relevant to their field.=
 This paper addresses the need for a comprehensive survey that brings toget=
her the state-of-the-art LLM applications, opportunities, and challenges ac=
ross the full spectrum of academic research areas.
How does it solve the problem? LLMs4All serves as an extensive review and g=
uide, spanning three broad domains of academia and detailing how LLMs are b=
eing applied in each. The authors categorize fields into: (1) Arts, Humanit=
ies, and Law (like history, philosophy, political science, architecture, le=
gal studies), (2) Economics and Business (finance, marketing, management, e=
tc.), and (3) Science and Engineering (mathematics, physics, biology, chemi=
stry, earth sciences, computer science, etc.). For each area, the survey ou=
tlines current use cases of LLMs in research and practice =E2=80=93 for ins=
tance, assisting historical text analysis, aiding legal document summarizat=
ion, generating hypotheses in scientific research =E2=80=93 with examples o=
f state-of-the-art models or systems in that domain. It also discusses how =
LLM capabilities (like text generation, reasoning, coding, multilingual und=
erstanding) are tailored or fine-tuned to meet discipline-specific needs. B=
eyond applications, the review addresses key limitations and challenges in =
each field: data privacy in medicine, factual accuracy in history, ethical =
concerns in law (like bias or fairness), etc., as well as open research que=
stions and future directions for integrating LLMs. In effect, LLMs4All acts=
 as a bridge between the AI frontier and domain experts, summarizing =E2=80=
=9Cwhat LLM can do for X field=E2=80=9D in one place.
What are the key findings? The survey=E2=80=99s primary contribution is qua=
litative synthesis, but it offers important insights and observations. One =
overarching finding is that virtually no discipline is untouched =E2=80=93 =
from using GPT-4 to draft legal contracts to employing generative models fo=
r experimental design in chemistry, academia is experiencing a wave of LLM-=
driven innovation. However, the review notes that the maturity varies: some=
 fields (like computer science and law) already have numerous LLM applicati=
ons, while others (say, philosophy or the arts) are still exploring initial=
 use cases. Common challenges emerge across disciplines, such as concerns o=
ver LLMs generating plausible-sounding but incorrect information (hallucina=
tions) which could mislead non-expert users in fields like medicine or fina=
nce. The paper highlights that interdisciplinary collaboration is key =E2=
=80=93 e.g. combining an LLM with domain-specific knowledge bases or models=
 yields better results (as seen in tools for scientific discovery that use =
LLMs plus chemistry rules). A positive finding is that LLMs are acting as a=
 democratizing force in research: they enable individuals without advanced =
technical training to leverage AI for their domain problems (for example, h=
istorians using GPT to translate and summarize ancient texts). The survey a=
lso compiles best practices and ethical guidelines that are emerging as com=
munities grapple with responsible LLM use (such as disclosure policies in a=
cademic writing if AI was used). Altogether, LLMs4All provides a roadmap fo=
r researchers in each field to understand the current landscape and to iden=
tify how they might use LLMs in their own work.
Why does it matter? As generative AI becomes as fundamental as data analysi=
s, having a clear view of its role in each discipline is crucial. This surv=
ey will help educators and policymakers as well =E2=80=93 for instance, uni=
versity departments can refer to it when updating curricula to include AI l=
iteracy relevant to their field. By documenting limitations and future dire=
ctions, the paper also points AI researchers towards important unsolved pro=
blems (like improving factuality for scientific Q&A, or aligning LLMs with =
legal reasoning). One likely outcome is that the survey will spur cross-dis=
ciplinary collaborations: a biologist reading about LLM use in chemistry mi=
ght team up with AI experts to apply similar techniques to biology. It also=
 emphasizes that despite the hype, current LLMs have serious shortcomings i=
n specialized domains =E2=80=93 thus tempering expectations and encouraging=
 more research on reliability, which is a recurring theme. In summary, LLMs=
4All matters because it catalogues the transformation happening at the inte=
rsection of AI and every other field, ensuring that knowledge is shared bro=
adly rather than siloed, and helping to steer the next phase of research wh=
ere AI truly becomes an every-discipline tool.
Language Models that Think, Chat Better
Watching: RLMT (paper [ https://substack.com/redirect/3b0bd9ae-8207-46a9-ac=
a2-52bd861e7121?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tA=
jMRMPOw0 ]/code [ https://substack.com/redirect/edcdd08a-b6d0-44b5-8ef0-a86=
145f37ea8?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPO=
w0 ])
What problem does it solve? Reinforcement learning from human feedback (RLH=
F) has become a standard way to finetune chat LLMs to be more helpful and s=
afe. However, RLHF optimizes only the final answer the model gives, not the=
 reasoning process behind it =E2=80=93 the model might produce some hidden =
chain-of-thought internally, but the reward model only judges the end respo=
nse. This can lead to answers that sound good but aren=E2=80=99t deeply rea=
soned. There=E2=80=99s another approach, RL with verifiable rewards (RLVR),=
 which forces the model to output checkable working (like a math proof or c=
ode test) and rewards correctness. RLVR yields better reasoning but only wo=
rks in domains with objective checks. The open problem addressed here is: C=
an we get the generality of RLHF while also encouraging the model to actual=
ly think through problems?
How does it solve the problem? The solution is a new training paradigm call=
ed RLMT (Reinforcement Learning with Model-rewarded Thinking). In RLMT, dur=
ing training the model is required to generate a long chain-of-thought (CoT=
) before its final answer. This CoT might be a detailed outline, step-by-st=
ep reasoning, or intermediate =E2=80=9Cthoughts.=E2=80=9D A reward model (t=
he same kind used in RLHF, pretrained on human preference data) then scores=
 not just the final answer but the combination of the CoT and the answer. E=
ssentially, the model is rewarded for producing helpful reasoning that lead=
s to a good answer. They implement this by using diverse real-world prompts=
 (open-ended tasks like writing an essay, planning a meal, answering a comp=
lex question) and apply policy gradient methods (PPO, DPO, etc.) to optimiz=
e the LLM=E2=80=99s policy to output better thought+answer pairs. They do t=
his across 40 separate training runs on two base models (Llama-3.1 8B and Q=
wen-7B) under different settings to ensure the approach is robust. Importan=
tly, they also explore training from scratch with RLMT (R1-Zero) =E2=80=93 =
meaning they take a base model with no supervised fine-tuning and directly =
apply RLMT, to see if they can skip the usual instruction-tuning phase.
What are the key findings? RLMT-trained models consistently outperform stan=
dard RLHF-trained models on a wide array of evaluations. For instance, on t=
hree different open-ended chat benchmarks (AlpacaEval2, WildBench, Arena Ha=
rd), the RLMT models scored 3=E2=80=937 points higher than equivalent RLHF =
models =E2=80=93 a sizable jump in quality. They also saw general ability i=
mprovements, like +1=E2=80=933 point gains on creative writing tasks and kn=
owledge quizzes. Perhaps most impressively, their best RLMT-tuned 8B model =
actually surpassed the performance of GPT-4 (open variant) on those chat be=
nchmarks and creative tasks, and even approached Claude 2=E2=80=99s level o=
n one benchmark. This is a remarkable result given the model is an order of=
 magnitude smaller than GPT-4. Another striking finding: an 8B Llama traine=
d with only 7,000 RLMT prompts (no supervised fine-tune at all) outperforme=
d the official Llama-3.1-8B that had been instruction-tuned on 25 million e=
xamples. In other words, a few thousand carefully chosen scenarios with thi=
nk-before-answer optimization beat a massive conventional training =E2=80=
=93 that speaks to how powerfully efficient RLMT is. Qualitatively, the aut=
hors observed the RLMT models produce more structured and thoughtful respon=
ses (e.g. making lists, reasoning out loud, considering alternatives) and f=
ewer failure modes like going off-topic. The results strongly suggest that =
rewarding the thinking process leads to measurably better chat performance =
than rewarding final answers alone.
What=E2=80=99s next? This work may prompt a paradigm shift in how we train =
conversational agents. Rather than treating chain-of-thought as just an opt=
ional byproduct, it might become standard to explicitly train LLMs to artic=
ulate their reasoning. Future directions include combining RLMT with human-=
in-the-loop: e.g., having human feedback not only on answers but on interme=
diate thoughts to further refine the reward model for reasoning quality (be=
yond what the existing preference model can do). Also, applying RLMT to lar=
ger models (the paper did 8B; doing this at 34B or 70B could yield even mor=
e powerful models, potentially surpassing even larger closed models in some=
 areas). Another consideration is real-world deployment: RLMT models, by ex=
plaining more, might be more interpretable, which is a boon for safety =E2=
=80=93 but it also means they might divulge their =E2=80=9Cthoughts=E2=80=
=9D even when not prompted to, which could be tuned as needed. Finally, thi=
s research calls for understanding why RLMT is so effective: does the rewar=
d model indirectly favor certain structures in CoT that align better with h=
uman preferences, or does the act of generating a longer context help the m=
odel avoid mistakes? Answering these questions could further improve traini=
ng. All in all, Language Models that think truly do chat better, and we can=
 expect the next generation of AI assistants to be much more explicit in th=
eir reasoning as a result of techniques like this.
SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines
Watching: SciReasoner (paper [ https://substack.com/redirect/bdb6da27-4887-=
4443-8af6-b3677a6f354c?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG=
6RxQ4tAjMRMPOw0 ]/code [ https://substack.com/redirect/92489bd1-1226-40aa-8=
199-2a2a18a3cc8b?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4t=
AjMRMPOw0 ])
What problem does it solve? Science-focused LLMs to date have mostly been s=
pecialists =E2=80=93 models fine-tuned for a particular domain (like chemis=
try, or a math theorem solver). Real scientific research, however, often sp=
ans multiple disciplines and data formats (imagine linking a biology findin=
g to a chemistry theory, with equations and text). There is a need for a fo=
undation model for scientific reasoning that can understand natural languag=
e questions, but also handle formulas, sequences (DNA, protein sequences), =
tables of properties, and more in a unified way. In short, the goal is to c=
reate an AI scientist that isn=E2=80=99t siloed to one field, but has a bro=
ad, cross-disciplinary reasoning ability with the various representations s=
cience uses.
How does it solve the problem? SciReasoner is built through an extensive mu=
lti-stage training process to align language with diverse scientific repres=
entations. First, it is pre-trained on a 206 billion token corpus that incl=
udes not just scientific text from many fields, but also purely symbolic se=
quences and mixed sequence-text data. This means it sees things like amino =
acid sequences, chemical SMILES strings, math equations, alongside explanat=
ory text. After this large pre-training, they perform supervised fine-tunin=
g on 40 million science-related instructions, covering an enormous range of=
 tasks (the model supports 103 different scientific tasks). These tasks fal=
l into families: (i) translating between text and scientific formats (e.g.,=
 =E2=80=9Cdescribe this molecule structure in words=E2=80=9D and vice versa=
), (ii) extracting knowledge from text or figures, (iii) predicting propert=
ies (given a compound, predict melting point, etc.), (iv) classifying prope=
rties (e.g. classify a star as red dwarf or not from data), and (v) generat=
ing or designing sequences (like proposing a DNA sequence with certain prop=
erties). After supervised tuning, they apply an =E2=80=9Cannealed cold-star=
t=E2=80=9D bootstrapping to specifically teach the model long-form chain-of=
-thought reasoning for scientific problems. This likely involves prompting =
the model to generate step-by-step solutions for complex questions and usin=
g those as additional training data (gradually increasing the complexity, h=
ence =E2=80=9Cannealed=E2=80=9D). Finally, they use reinforcement learning =
with custom reward shaping for scientific reasoning. This last step probabl=
y gives the model feedback on intermediate steps (like units consistency, e=
quation correctness, logical coherence) to firmly instill deliberate, rigor=
ous reasoning habits. All training artifacts (model weights, instruction da=
ta, evaluation code) are released openly, making SciReasoner a community re=
source.
What are the key findings? SciReasoner emerges as a single model capable of=
 handling tasks that previously would require an ensemble of separate tools=
=2E Compared to specialist models or ba=
selines, SciReasoner shows broader ins=
truction coverage, better cross-domain generalization, and higher fidelity =
in its outputs. For example, it can take a chemistry problem described in t=
ext and output a step-by-step solution with equations, or translate a genom=
ic sequence into a likely function description =E2=80=93 tasks bridging lan=
guage and formal data =E2=80=93 with notable accuracy. The paper indicates =
that training on multiple disciplines together actually led to improved tra=
nsfer learning: solving tasks in one domain improved its performance in oth=
ers, because it learned general scientific reasoning strategies. This cross=
-pollination strengthened the model=E2=80=99s reliability; e.g., the rigor =
it learned from physics equations helped it avoid errors in, say, accountin=
g calculations. In evaluations, SciReasoner performed on par with or better=
 than domain-specific models on many benchmarks, despite not being an exper=
t of any single field. And on challenges that require mixing knowledge (lik=
e a question involving both biology and chemistry), it had a clear advantag=
e. Essentially, SciReasoner lays a groundwork: it demonstrates that one mod=
el can be a competent physicist, chemist, biologist, and more at the same t=
ime, and that this union actually makes each facet stronger. This is a step=
 toward AI that can reason across the entirety of science. The open-sourcin=
g of the model and its data is also a major outcome =E2=80=93 it provides t=
he community with a powerful base to finetune further or to benchmark for s=
cientific QA, hypothesis generation, etc., accelerating research in scienti=
fic AI.
What=E2=80=99s next? SciReasoner opens up a host of new possibilities. In t=
he near term, researchers might build on it to create specialized agents =
=E2=80=93 for instance, a robot scientist that uses SciReasoner to generate=
 experiments, then executes them in a lab simulation. The training techniqu=
es (like the massive multi-format pre-training and the careful staged align=
ment) could be applied to other domains requiring multi-representation reas=
oning, such as economics (mixing text with spreadsheets and formulas) or so=
cial sciences (mixing text and statistical data). Another likely direction =
is scaling: SciReasoner-8B is impressive, but imagine a 70B model trained s=
imilarly =E2=80=93 it could potentially approach expert human level in many=
 fields. There will also be work on evaluation: the model covers 103 tasks,=
 but how do we thoroughly verify its reasoning quality and factual accuracy=
 in each? New interdisciplinary benchmarks may arise from this. Finally, Sc=
iReasoner=E2=80=99s release fosters a culture of open scientific AI =E2=80=
=93 as more researchers use and improve it, we could see a virtuous cycle l=
eading to an =E2=80=9CAI Scientific Assistant=E2=80=9D that any researcher =
can use to boost their work, much like a powerful but wide-ranging colleagu=
e who=E2=80=99s read every textbook. The long-term vision is an AI that can=
 cross-pollinate insights between disciplines (say, use a physics principle=
 to solve a biology problem), and SciReasoner is a foundational step in tha=
t direction, illustrating the value of broad training for broad thinking in=
 AI.
=E2=9D=A4=EF=B8=8F If you enjoyed this article, give it a like and share it=
 with your peers.
Thanks for reading LLM Watch! Subscribe for free to receive new posts and s=
upport my work

Unsubscribe https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly93d3cubGxtd2F=
0Y2guY29tL2FjdGlvbi9kaXNhYmxlX2VtYWlsP3Rva2VuPWV5SjFjMlZ5WDJsa0lqb3pNelkwTk=
RneU1qTXNJbkJ2YzNSZmFXUWlPakUzTkRVeE16UTVOeXdpYVdGMElqb3hOelU0T1RBd056VTVMQ=
0psZUhBaU9qRTNPVEEwTXpZM05Ua3NJbWx6Y3lJNkluQjFZaTB4TkRJNE5qWTNJaXdpYzNWaUlq=
b2laR2x6WVdKc1pWOWxiV0ZwYkNKOS5zN1ZHOV9nOFdILVJWSnZEQzZEWTMzY2xaZGxXSVp0a21=
OWXF1RHBkcnUwIiwicCI6MTc0NTEzNDk3LCJzIjoxNDI4NjY3LCJmIjp0cnVlLCJ1IjozMzY0ND=
gyMjMsImlhdCI6MTc1ODkwMDc1OSwiZXhwIjoyMDc0NDc2NzU5LCJpc3MiOiJwdWItMCIsInN1Y=
iI6ImxpbmstcmVkaXJlY3QifQ.1vZLZ751gF3eZarHuC9koEM_I3XGLSDMlMugcDUsnHw?
--478d3da90589b4d5b597a56878732fe63b2890310289e5f131c8487c2bf9
Content-Type: text/html; charset="utf-8"
Content-Transfer-Encoding: quoted-printable

<html style=3D"scrollbar-width: thin;scrollbar-color: rgb(219,219,219)rgb(2=
55,255,255);"><head>
<meta http-equiv=3D"Content-Type" content=3D"text/html; charset=3Dutf-8"><t=
itle>9 Papers You Should Know About</title><style>
@media (max-width: 1024px) {
  .typography .pullquote-align-left,
  .typography.editor .pullquote-align-left,
  .typography .pullquote-align-right,
  .typography.editor .pullquote-align-right,
  .typography .pullquote-align-wide,
  .typography.editor .pullquote-align-wide,
  .typography .pullquote-align-center,
  .typography.editor .pullquote-align-center {
    float: none;
    margin: 0 auto;
    width: 100%;
    max-width: 100%;
  }
}
@media all and (-ms-high-contrast: none), (-ms-high-contrast: active) {
  .typography .markup table.image-wrapper img,
  .typography.editor .markup table.image-wrapper img,
  .typography .markup table.kindle-wrapper img,
  .typography.editor .markup table.kindle-wrapper img {
    max-width: 550px;
  }
}
@media (min-width: 1024px) {
  .typography:not(:has(#toc)) .captioned-image-container figure:has(> a.ima=
ge2-offset-left),
  .typography.editor:not(:has(#toc)) .captioned-image-container figure:has(=
> a.image2-offset-left) {
    margin-left: var(--image-offset-margin);
  }
  .typography:not(:has(#toc)) .captioned-image-container figure:has(> a.ima=
ge2-offset-right),
  .typography.editor:not(:has(#toc)) .captioned-image-container figure:has(=
> a.image2-offset-right) {
    margin-right: var(--image-offset-margin);
  }
}
@media (min-width: 1300px) {
  .typography .captioned-image-container figure:has(> a.image2-offset-left)=
,
  .typography.editor .captioned-image-container figure:has(> a.image2-offse=
t-left) {
    margin-left: var(--image-offset-margin);
  }
  .typography .captioned-image-container figure:has(> a.image2-offset-right=
),
  .typography.editor .captioned-image-container figure:has(> a.image2-offse=
t-right) {
    margin-right: var(--image-offset-margin);
  }
}
@media (max-width: 1024px) {
  .typography,
  .typography.editor {
    /* Disable offset on mobile/tablet */
  }
  .typography .captioned-image-container figure:has(> a.image2-align-left),
  .typography.editor .captioned-image-container figure:has(> a.image2-align=
-left),
  .typography .captioned-image-container figure:has(> a.image2-align-right)=
,
  .typography.editor .captioned-image-container figure:has(> a.image2-align=
-right) {
    float: none;
    margin: 1em auto;
    max-width: 100%;
    width: auto;
    padding: 0;
  }
  .typography .captioned-image-container figure:has(> a.image2-align-left.t=
hefp),
  .typography.editor .captioned-image-container figure:has(> a.image2-align=
-left.thefp),
  .typography .captioned-image-container figure:has(> a.image2-align-right.=
thefp),
  .typography.editor .captioned-image-container figure:has(> a.image2-align=
-right.thefp) {
    margin: 1em auto;
  }
  .typography .captioned-image-container figure:has(> a.image2-offset-left)=
,
  .typography.editor .captioned-image-container figure:has(> a.image2-offse=
t-left),
  .typography .captioned-image-container figure:has(> a.image2-offset-right=
),
  .typography.editor .captioned-image-container figure:has(> a.image2-offse=
t-right) {
    margin: 1em auto;
  }
  .typography .captioned-image-container figure:has(> a.image2-align-left) =
.image2-inset,
  .typography.editor .captioned-image-container figure:has(> a.image2-align=
-left) .image2-inset,
  .typography .captioned-image-container figure:has(> a.image2-align-right)=
 .image2-inset,
  .typography.editor .captioned-image-container figure:has(> a.image2-align=
-right) .image2-inset {
    display: block;
    justify-content: initial;
  }
}
@media (max-width: 768px) {
  .typography .markup div.sponsorship-campaign-embed,
  .typography.editor .markup div.sponsorship-campaign-embed {
    margin-top: 24px;
    margin-bottom: 24px;
  }
  .typography .markup div.sponsorship-campaign-embed:first-child,
  .typography.editor .markup div.sponsorship-campaign-embed:first-child {
    margin-top: 0px;
  }
}
@media screen and (max-width: 650px) {
  .typography .markup div.youtube-overlay,
  .typography.editor .markup div.youtube-overlay,
  .typography .markup div.vimeo-overlay,
  .typography.editor .markup div.vimeo-overlay {
    display: none !important;
  }
}
@media screen and (max-width: 370px) {
  .typography .markup div.tiktok-wrap,
  .typography.editor .markup div.tiktok-wrap {
    width: calc(95vw - 32px);
    height: calc((95vw - 32px - 2px) / 0.485714);
  }
}
@media screen and (max-width: 650px) {
  .typography .markup div.embedded-publication-wrap .embedded-publication.s=
how-subscribe,
  .typography.editor .markup div.embedded-publication-wrap .embedded-public=
ation.show-subscribe {
    padding: 24px;
  }
}
@media screen and (max-width: 650px) {
  .typography .markup div.subscription-widget-wrap .subscription-widget.sho=
w-subscribe,
  .typography.editor .markup div.subscription-widget-wrap .subscription-wid=
get.show-subscribe,
  .typography .markup div.subscription-widget-wrap-editor .subscription-wid=
get.show-subscribe,
  .typography.editor .markup div.subscription-widget-wrap-editor .subscript=
ion-widget.show-subscribe,
  .typography .markup div.captioned-button-wrap .subscription-widget.show-s=
ubscribe,
  .typography.editor .markup div.captioned-button-wrap .subscription-widget=
.show-subscribe {
    padding: 0px 24px;
  }
}
@media screen and (max-width: 650px) {
  .typography .markup div.subscription-widget-wrap .subscription-widget.sho=
w-subscribe .subscription-widget-subscribe .button,
  .typography.editor .markup div.subscription-widget-wrap .subscription-wid=
get.show-subscribe .subscription-widget-subscribe .button,
  .typography .markup div.subscription-widget-wrap-editor .subscription-wid=
get.show-subscribe .subscription-widget-subscribe .button,
  .typography.editor .markup div.subscription-widget-wrap-editor .subscript=
ion-widget.show-subscribe .subscription-widget-subscribe .button,
  .typography .markup div.captioned-button-wrap .subscription-widget.show-s=
ubscribe .subscription-widget-subscribe .button,
  .typography.editor .markup div.captioned-button-wrap .subscription-widget=
.show-subscribe .subscription-widget-subscribe .button {
    padding: 10px 12px;
    min-width: 110px;
  }
}
@media (max-width: 650px) {
  .typography .markup .tweet,
  .typography.editor .markup .tweet {
    padding: 12px;
  }
}
@media (max-width: 650px) {
  .typography .markup .tweet .tweet-text,
  .typography.editor .markup .tweet .tweet-text {
    font-size: 14px;
    line-height: 20px;
  }
}
@media (max-width: 650px) {
  .typography .markup .tweet .tweet-photos-container.two,
  .typography.editor .markup .tweet .tweet-photos-container.two,
  .typography .markup .tweet .tweet-photos-container.three,
  .typography.editor .markup .tweet .tweet-photos-container.three,
  .typography .markup .tweet .tweet-photos-container.four,
  .typography.editor .markup .tweet .tweet-photos-container.four {
    height: 200px;
  }
}
@media (max-width: 650px) {
  .typography .markup .tweet a.expanded-link .expanded-link-img,
  .typography.editor .markup .tweet a.expanded-link .expanded-link-img {
    max-height: 180px;
  }
}
@media (max-width: 650px) {
  .typography .markup .tweet a.expanded-link .expanded-link-description,
  .typography.editor .markup .tweet a.expanded-link .expanded-link-descript=
ion {
    display: none;
  }
}
@media screen and (max-width: 650px) {
  .typography .markup .apple-podcast-container,
  .typography.editor .markup .apple-podcast-container {
    width: unset;
  }
}
@media (max-width: 420px) {
  .typography .markup .install-substack-app-embed img.install-substack-app-=
embed-img,
  .typography.editor .markup .install-substack-app-embed img.install-substa=
ck-app-embed-img {
    margin: 0 auto 16px auto;
  }
}
@media (max-width: 420px) {
  .typography .markup .install-substack-app-embed .install-substack-app-emb=
ed-text,
  .typography.editor .markup .install-substack-app-embed .install-substack-=
app-embed-text {
    margin: 0 0 12px 0;
    max-width: 100%;
    width: auto;
    text-align: center;
  }
}
@media (max-width: 420px) {
  .typography .markup .install-substack-app-embed .install-substack-app-emb=
ed-link,
  .typography.editor .markup .install-substack-app-embed .install-substack-=
app-embed-link {
    display: flex;
    justify-content: center;
  }
}
@media screen and (min-width: 481px) {
  .share-button-container {
    height: 38px;
  }
}
@media screen and (min-width: 481px) {
  .share-button-container a.comment {
    height: 38px;
    line-height: 38px;
    padding-right: 10px;
  }
}
@media screen and (max-width: 480px) {
  .share-button-container .separator {
    display: block;
    margin: 0;
    height: 8px;
    border-left: none;
  }
}
@media screen and (max-width: 480px) {
  .share-button-container a.share.first img {
    padding-left: 0;
  }
}
@media screen and (min-width: 481px) {
  .share-button-container a.mobile {
    display: none !important;
  }
}
@media screen and (min-width: 541px) {
  .settings-add-pub-modal-wrapper .container .add-recommending-pub-modal-co=
ntainer {
    padding: 36px;
    height: 680px;
  }
}
@media screen and (min-width: 541px) {
  .settings-add-pub-modal-wrapper .container .add-recommending-pub-modal-co=
ntainer .footer {
    position: absolute;
    bottom: 36px;
    margin: 0px;
  }
}
@media screen and (max-width: 650px) {
  .header-anchor-parent {
    display: none;
  }
}
@media screen and (max-width: 768px) {
  .post {
    padding: 16px 0 0 0;
  }
}
@media screen and (max-width: 650px) {
  .post .post-header .post-label {
    margin-top: 8px;
  }
}
@media screen and (max-width: 650px) {
  .post .post-header .meta-author-wrap.alternative-meta .meta-right-column =
.post-meta {
    margin-top: 6px;
  }
}
@media screen and (max-width: 650px) {
  .post .footer-facepile-container {
    height: 64px;
    padding: 0 16px;
    display: flex;
    align-items: center;
    justify-content: flex-start;
    width: 100%;
  }
}
@media screen and (max-width: 650px) {
  .post .post-footer.use-separators {
    justify-content: center;
  }
}
@media screen and (max-width: 650px) {
  .post .post-footer.next-prev {
    height: 64px;
    justify-content: space-between;
    box-sizing: border-box;
  }
}
@media screen and (max-width: 650px) {
  .post-contributor-footer .post-contributor-bio-table {
    display: block;
  }
  .post-contributor-footer .post-contributor-bio-table-row {
    display: flex;
    flex-direction: row;
  }
  .post-contributor-footer .post-contributor-bio-userhead-cell,
  .post-contributor-footer .post-contributor-bio-body-cell {
    display: block;
  }
  .post-contributor-footer .post-contributor-bio-body-cell {
    flex-grow: 1;
  }
  .post-contributor-footer .post-contributor-bio-body-table {
    display: block;
  }
  .post-contributor-footer .post-contributor-bio-body-table-row {
    display: block;
  }
  .post-contributor-footer .post-contributor-bio-copy-cell,
  .post-contributor-footer .post-contributor-bio-controls-cell {
    display: block;
  }
  .post-contributor-footer .post-contributor-bio-copy-cell {
    margin: 0 0 16px 0;
  }
  .post-contributor-footer .post-contributor-bio-controls-cell {
    width: auto;
  }
  .post-contributor-footer .post-contributor-bio-controls {
    margin: auto;
  }
  .post-contributor-footer .post-contributor-bio-controls .button.primary {
    width: 100%;
  }
  .post-contributor-footer .post-contributor-bio-text {
    font-size: 14px;
  }
}
@media screen and (min-width: 768px) {
  .post-silhouette {
    padding: 32px 0;
  }
}
@media screen and (max-width: 650px) {
  .post-silhouette .post-silhouette-title {
    margin-top: 10.44225025px;
    height: 120px;
  }
}
@media screen and (max-width: 650px) {
  .post-silhouette .post-silhouette-meta {
    width: 75%;
  }
}
@media screen and (max-width: 650px) {
  .post-silhouette .post-silhouette-meta.with-byline-image {
    margin: 20px 0;
  }
}
@media screen and (max-width: 650px) {
  .use-theme-bg .post-meta.alternative-meta .post-meta-item,
  .post-meta.alternative-meta .post-meta-item {
    padding-right: 16px;
  }
}
@media screen and (max-width: 370px) {
  .use-theme-bg .post-meta.alternative-meta .post-meta-item,
  .post-meta.alternative-meta .post-meta-item {
    font-size: 14px;
  }
}
@media screen and (max-width: 650px) {
  .use-theme-bg .post-meta.alternative-meta .post-meta-item.guest-author-pu=
blication,
  .post-meta.alternative-meta .post-meta-item.guest-author-publication {
    display: none;
  }
}
@media screen and (max-width: 370px) {
  .post-meta .post-meta-item .post-meta-button {
    height: 36px !important;
    /* important to override in-line height style on emails */
  }
  .post-meta .post-meta-item .post-meta-button .meta-button-label {
    display: none;
  }
  .post-meta .post-meta-item .post-meta-button > svg {
    margin-right: 0;
  }
}
@media screen and (max-width: 370px) {
  .post-meta .post-meta-item {
    font-size: 12px;
  }
}
@media screen and (max-width: 650px) {
  .post .floating-subscribe-button {
    bottom: 20px;
    right: 20px;
  }
}
@media (max-width: 1024px) {
  body .pullquote-align-left,
  body .pullquote-align-right,
  body .pullquote-align-wide,
  body .pullquote-align-center {
    float: none;
    margin: 0 auto;
    width: 100%;
    max-width: 100%;
  }
}
@media all and (-ms-high-contrast: none), (-ms-high-contrast: active) {
  body .markup table.image-wrapper img,
  body .markup table.kindle-wrapper img {
    max-width: 550px;
  }
}
@media (min-width: 1024px) {
  body:not(:has(#toc)) .captioned-image-container figure:has(> a.image2-off=
set-left) {
    margin-left: var(--image-offset-margin);
  }
  body:not(:has(#toc)) .captioned-image-container figure:has(> a.image2-off=
set-right) {
    margin-right: var(--image-offset-margin);
  }
}
@media (min-width: 1300px) {
  body .captioned-image-container figure:has(> a.image2-offset-left) {
    margin-left: var(--image-offset-margin);
  }
  body .captioned-image-container figure:has(> a.image2-offset-right) {
    margin-right: var(--image-offset-margin);
  }
}
@media (max-width: 1024px) {
  body {
    /* Disable offset on mobile/tablet */
  }
  body .captioned-image-container figure:has(> a.image2-align-left),
  body .captioned-image-container figure:has(> a.image2-align-right) {
    float: none;
    margin: 1em auto;
    max-width: 100%;
    width: auto;
    padding: 0;
  }
  body .captioned-image-container figure:has(> a.image2-align-left.thefp),
  body .captioned-image-container figure:has(> a.image2-align-right.thefp) =
{
    margin: 1em auto;
  }
  body .captioned-image-container figure:has(> a.image2-offset-left),
  body .captioned-image-container figure:has(> a.image2-offset-right) {
    margin: 1em auto;
  }
  body .captioned-image-container figure:has(> a.image2-align-left) .image2=
-inset,
  body .captioned-image-container figure:has(> a.image2-align-right) .image=
2-inset {
    display: block;
    justify-content: initial;
  }
}
@media (max-width: 768px) {
  body .markup div.sponsorship-campaign-embed {
    margin-top: 24px;
    margin-bottom: 24px;
  }
  body .markup div.sponsorship-campaign-embed:first-child {
    margin-top: 0px;
  }
}
@media screen and (max-width: 650px) {
  body .markup div.youtube-overlay,
  body .markup div.vimeo-overlay {
    display: none !important;
  }
}
@media screen and (max-width: 370px) {
  body .markup div.tiktok-wrap {
    width: calc(95vw - 32px);
    height: calc((95vw - 32px - 2px) / 0.485714);
  }
}
@media screen and (max-width: 650px) {
  body .markup div.embedded-publication-wrap .embedded-publication.show-sub=
scribe {
    padding: 24px;
  }
}
@media screen and (max-width: 650px) {
  body .markup div.subscription-widget-wrap .subscription-widget.show-subsc=
ribe,
  body .markup div.subscription-widget-wrap-editor .subscription-widget.sho=
w-subscribe,
  body .markup div.captioned-button-wrap .subscription-widget.show-subscrib=
e {
    padding: 0px 24px;
  }
}
@media screen and (max-width: 650px) {
  body .markup div.subscription-widget-wrap .subscription-widget.show-subsc=
ribe .subscription-widget-subscribe .button,
  body .markup div.subscription-widget-wrap-editor .subscription-widget.sho=
w-subscribe .subscription-widget-subscribe .button,
  body .markup div.captioned-button-wrap .subscription-widget.show-subscrib=
e .subscription-widget-subscribe .button {
    padding: 10px 12px;
    min-width: 110px;
  }
}
@media (max-width: 650px) {
  body .markup .tweet {
    padding: 12px;
  }
}
@media (max-width: 650px) {
  body .markup .tweet .tweet-text {
    font-size: 14px;
    line-height: 20px;
  }
}
@media (max-width: 650px) {
  body .markup .tweet .tweet-photos-container.two,
  body .markup .tweet .tweet-photos-container.three,
  body .markup .tweet .tweet-photos-container.four {
    height: 200px;
  }
}
@media (max-width: 650px) {
  body .markup .tweet a.expanded-link .expanded-link-img {
    max-height: 180px;
  }
}
@media (max-width: 650px) {
  body .markup .tweet a.expanded-link .expanded-link-description {
    display: none;
  }
}
@media screen and (max-width: 650px) {
  body .markup .apple-podcast-container {
    width: unset;
  }
}
@media (max-width: 420px) {
  body .markup .install-substack-app-embed img.install-substack-app-embed-i=
mg {
    margin: 0 auto 16px auto;
  }
}
@media (max-width: 420px) {
  body .markup .install-substack-app-embed .install-substack-app-embed-text=
 {
    margin: 0 0 12px 0;
    max-width: 100%;
    width: auto;
    text-align: center;
  }
}
@media (max-width: 420px) {
  body .markup .install-substack-app-embed .install-substack-app-embed-link=
 {
    display: flex;
    justify-content: center;
  }
}
@media screen and (min-width: 500px) {
  body .header a.logo {
    width: 42px;
    height: 42px;
    border-radius: 12px;
  }
}
@media screen and (max-width: 420px) {
  body .subscription-receipt table:first-of-type .subscription-amount .subs=
cription-discount {
    width: 72px !important;
  }
}
@media screen and (min-width: 481px) {
  body .share-button-container {
    height: auto;
  }
}
@media screen and (max-width: 480px) {
  body .share-button-container .separator {
    display: block !important;
    margin: 0 !important;
    height: 8px !important;
    border-left: none !important;
  }
}
@media screen and (max-width: 650px) {
  .digest .item .post-meta-item.audience {
    display: none;
  }
}
@media screen and (min-width: 500px) {
  .digest-publication .logo img {
    width: 42px;
    height: 42px;
    border-radius: 8px;
  }
}
@media screen and (max-width: 650px) {
  .comments-page .container .comment-list .collapsed-reply {
    margin-left: calc(10 + 32px - 24px);
  }
}
@media screen and (max-width: 650px) {
  .comment > .comment-list {
    padding-left: 24px;
  }
}
@media screen and (max-width: 650px) {
  .finish-magic-login-modal .modal-content .container {
    padding: 24px 0;
  }
}
@media (max-width: 650px) {
  .reader2-text-b3 {
    line-height: 24px;
  }
}
@media screen and (max-width: 650px) {
  .reader2-text-h4 {
    line-height: 24px;
  }
}
@media screen and (min-width: 541px) {
  .user-profile-modal {
    padding-left: 12px;
    padding-right: 12px;
  }
}
@media screen and (max-width: 650px) {
  .subscribe-widget form.form .sideBySideWrap button.rightButton {
    padding: 10px 12px;
  }
}
@media screen and (min-width: 541px) {
  .pub-icon:hover .logo-hover,
  .feed-item-icon:hover .logo-hover {
    display: block;
  }
}
@media screen and (max-width: 650px) {
  .post-ufi.single-full-width-button .post-ufi-button-wrapper {
    width: 100%;
    padding: 16px;
  }
  .post-ufi.single-full-width-button .post-ufi-button-wrapper:empty {
    display: none;
  }
  .post-ufi.single-full-width-button .post-ufi-button {
    width: 100%;
    justify-content: center;
  }
}
@media screen and (max-width: 768px) {
  .file-embed-wrapper {
    padding: 0;
  }
}
@media screen and (max-width: 768px) {
  .file-embed-wrapper-editor {
    padding: 0;
  }
}
@media screen and (max-width: 768px) {
  .file-embed-wrapper-editor:active {
    padding: 0;
  }
}
@media only screen and (max-width: 650px) {
  .file-embed-button.wide,
  .file-embed-error-button.wide {
    display: none;
  }
}
@media only screen and (min-width: 630px) {
  .file-embed-button.narrow,
  .file-embed-error-button.narrow {
    display: none;
  }
}
@media screen and (min-width: 541px) {
  .audio-player-wrapper .audio-player {
    min-width: 500px;
  }
}
@media screen and (max-width: 650px) {
  .audio-player-wrapper .audio-player .audio-player-progress {
    border-left-width: 16px;
    border-right-width: 16px;
  }
}
@media screen and (max-width: 650px) {
  .audio-player-wrapper .audio-player .audio-player-progress .audio-player-=
progress-bar .audio-player-progress-bar-popup {
    top: -54px;
  }
}
@media screen and (max-width: 650px) {
  .audio-player-wrapper-fancy .audio-player .audio-player-progress {
    border-left-width: 16px;
    border-right-width: 16px;
  }
}
@media screen and (max-width: 650px) {
  .audio-player-wrapper-fancy .audio-player .audio-player-progress .audio-p=
layer-progress-bar .audio-player-progress-bar-popup {
    top: -54px;
  }
}
@media (min-width: 250px) {
  .audio-player-wrapper-fancy .audio-player {
    padding: 32px;
  }
  .audio-player-wrapper-fancy .audio-player .btn-group {
    display: flex;
  }
  .audio-player-wrapper-fancy .audio-player .btn-group .button:last-of-type=
 {
    display: block;
  }
}
@media (min-width: 300px) {
  .audio-player-wrapper-fancy .audio-player .btn-group {
    display: block;
  }
  .audio-player-wrapper-fancy .audio-player .btn-group .button:first-of-typ=
e {
    display: block;
  }
}
@media (min-width: 350px) {
  .audio-player-wrapper-fancy .audio-player .audio-player-substack-logo {
    display: block;
  }
  .audio-player-wrapper-fancy .audio-player .audio-player-title {
    margin-top: 16px;
  }
  .audio-player-wrapper-fancy .audio-player .audio-player-hero-image-contai=
ner {
    padding-top: 15%;
    width: 15%;
    display: block;
  }
  .audio-player-wrapper-fancy .audio-player .btn-group .button:first-of-typ=
e {
    display: block;
  }
  .audio-player-wrapper-fancy .audio-player .audio-player-substack-logo {
    display: block;
  }
}
@media (min-width: 350px) {
  .audio-player-wrapper-fancy .audio-player .audio-player-hero-image-contai=
ner {
    padding-top: 25%;
    width: 25%;
    display: block;
  }
  .audio-player-wrapper-fancy .audio-player .btn-group {
    display: flex;
  }
  .audio-player-wrapper-fancy .audio-player .btn-group .button:first-of-typ=
e {
    display: block;
  }
}
@media (min-width: 400px) {
  .audio-player-wrapper-fancy .audio-player .audio-player-hero-image-contai=
ner {
    padding-top: 40%;
    width: 40%;
  }
}
@media (max-width: 400px) {
  .audio-player-wrapper-fancy .audio-player .btn-group {
    margin-top: 12px;
  }
  .audio-player-wrapper-fancy .audio-player .btn-group .button {
    font-size: 13px;
    padding: 6px 12px;
    height: auto;
    margin-top: 10px;
  }
}
@media (min-width: 600px) {
  .audio-player-wrapper-fancy .audio-player .audio-player-hero-image-contai=
ner {
    padding-top: 55%;
    width: 55%;
  }
}
@media (max-width: 650px) {
  .poll-editor-modal {
    min-width: calc(100% - 20px);
  }
}
@media (max-width: 750px) {
  .poll-embed .poll-anchor-target .poll-anchor-copy-button {
    left: 8px;
    top: 45px;
  }
}</style></head><body class=3D"email-body" style=3D"font-kerning: auto;--im=
age-offset-margin: -120px;"><img src=3D"https://eotrx.substackcdn.com/open?=
token=3DeyJtIjoiPDIwMjUwOTI2MTUzMDU4LjMuZjkwZGNhYTk5NTAzNTliNkBtZy1kMS5zdWJ=
zdGFjay5jb20-IiwidSI6MzM2NDQ4MjIzLCJyIjoiZWl0YW5AZWlzbGF3LmNvLmlsIiwiZCI6Im=
1nLWQxLnN1YnN0YWNrLmNvbSIsInAiOjE3NDUxMzQ5NywidCI6Im5ld3NsZXR0ZXIiLCJhIjoiZ=
XZlcnlvbmUiLCJzIjoxNDI4NjY3LCJjIjoicG9zdCIsImYiOnRydWUsInBvc2l0aW9uIjoidG9w=
IiwiaWF0IjoxNzU4OTAwNzU5LCJleHAiOjE3NjE0OTI3NTksImlzcyI6InB1Yi0wIiwic3ViIjo=
iZW8ifQ.0H9xadNFwKeZxrlDxTGO7C3L19AHKRvwpuoIa6hyjqc" alt=3D"" width=3D"1" h=
eight=3D"1" border=3D"0" style=3D"height:1px !important;width:1px !importan=
t;border-width:0 !important;margin-top:0 !important;margin-bottom:0 !import=
ant;margin-right:0 !important;margin-left:0 !important;padding-top:0 !impor=
tant;padding-bottom:0 !important;padding-right:0 !important;padding-left:0 =
!important;"><div class=3D"preview" style=3D"display:none;font-size:1px;col=
or:#333333;line-height:1px;max-height:0px;max-width:0px;opacity:0;overflow:=
hidden;">Get ahead of the curve with LLM Watch</div><div class=3D"preview" =
style=3D"display:none;font-size:1px;color:#333333;line-height:1px;max-heigh=
t:0px;max-width:0px;opacity:0;overflow:hidden;">=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD</div><table class=3D"email=
-body-container" role=3D"presentation" width=3D"100%" border=3D"0" cellspac=
ing=3D"0" cellpadding=3D"0"><tbody><tr><td></td><td class=3D"content" width=
=3D"550"></td><td></td></tr><tr><td></td><td class=3D"content" width=3D"550=
" align=3D"left"><div style=3D"font-size: 16px;line-height: 26px;max-width:=
 550px;width: 100%;margin: 0 auto;overflow-wrap: break-word;"><table role=
=3D"presentation" width=3D"100%" border=3D"0" cellspacing=3D"0" cellpadding=
=3D"0"><tbody><tr><td align=3D"right" style=3D"height:20px;"><table role=3D=
"presentation" width=3D"auto" border=3D"0" cellspacing=3D"0" cellpadding=3D=
"0"><tbody><tr><td style=3D"vertical-align:middle;"><span class=3D"pencraft=
 pc-reset reset-IxiVJZ tw-font-body tw-text-ssm tw-text-substack-secondary"=
 style=3D"font-family: SF Pro Text, -apple-system, system-ui, BlinkMacSyste=
mFont, Inter, Segoe UI, Roboto, Helvetica, Arial, sans-serif, Apple Color E=
moji, Segoe UI Emoji, Segoe UI Symbol;font-size: 13px;color: unset;list-sty=
le: none;text-decoration: unset;margin: 0;"><div class=3D"pencraft pc-reset=
 align-right-VJbKw5 size-12-mmZ61m reset-IxiVJZ" style=3D"list-style: none;=
color: unset;text-align: right;font-size: 12px;line-height: 16px;text-decor=
ation: unset;margin: 0;"><span class=3D"pencraft pc-reset reset-IxiVJZ" tra=
nslated=3D"" style=3D"list-style: none;color: unset;text-decoration: unset;=
margin: 0;">Forwarded this email? <a class=3D"pencraft pc-reset decoration-=
underline-ClTkYc reset-IxiVJZ" href=3D"https://substack.com/redirect/2/eyJl=
IjoiaHR0cHM6Ly93d3cubGxtd2F0Y2guY29tL3N1YnNjcmliZT91dG1fc291cmNlPWVtYWlsJnV=
0bV9jYW1wYWlnbj1lbWFpbC1zdWJzY3JpYmUmcj01a2I5M3ombmV4dD1odHRwcyUzQSUyRiUyRn=
d3dy5sbG13YXRjaC5jb20lMkZwJTJGOS1wYXBlcnMteW91LXNob3VsZC1rbm93LWFib3V0LTFmN=
SIsInAiOjE3NDUxMzQ5NywicyI6MTQyODY2NywiZiI6dHJ1ZSwidSI6MzM2NDQ4MjIzLCJpYXQi=
OjE3NTg5MDA3NTksImV4cCI6MjA3NDQ3Njc1OSwiaXNzIjoicHViLTAiLCJzdWIiOiJsaW5rLXJ=
lZGlyZWN0In0.XVWE79wHyg4GcP_lnJosj5CTbJZxSeuRbuzEr28jU6o?" style=3D"list-st=
yle: none;color: unset;text-decoration: unset;margin: 0;-webkit-text-decora=
tion-line: underline;text-decoration-line: underline;">Subscribe here</a> f=
or more</span></div></span></td></tr></tbody></table></td></tr></tbody></ta=
ble><table class=3D"header graphic-header" role=3D"presentation" style=3D"b=
order-spacing: 0;padding: 16px 0 32px;"><tbody><tr><td align=3D"center" sty=
le=3D"text-align: center;padding: 0;"><a href=3D"https://substack.com/redir=
ect/2/eyJlIjoiaHR0cHM6Ly93d3cubGxtd2F0Y2guY29tL3AvOS1wYXBlcnMteW91LXNob3VsZ=
C1rbm93LWFib3V0LTFmNT91dG1fY2FtcGFpZ249ZW1haWwtaGFsZi1wb3N0JnI9NWtiOTN6JnRv=
a2VuPWV5SjFjMlZ5WDJsa0lqb3pNelkwTkRneU1qTXNJbkJ2YzNSZmFXUWlPakUzTkRVeE16UTV=
OeXdpYVdGMElqb3hOelU0T1RBd056VTVMQ0psZUhBaU9qRTNOakUwT1RJM05Ua3NJbWx6Y3lJNk=
luQjFZaTB4TkRJNE5qWTNJaXdpYzNWaUlqb2ljRzl6ZEMxeVpXRmpkR2x2YmlKOS5VSUUxSzRCe=
GdWZDdmeFZmSkhtdldBZnk4cW1XaVVPMjBFNGEwclJPR1FzIiwicCI6MTc0NTEzNDk3LCJzIjox=
NDI4NjY3LCJmIjp0cnVlLCJ1IjozMzY0NDgyMjMsImlhdCI6MTc1ODkwMDc1OSwiZXhwIjoyMDc=
0NDc2NzU5LCJpc3MiOiJwdWItMCIsInN1YiI6ImxpbmstcmVkaXJlY3QifQ.a61kgi5muohQb8Z=
WobrdVS3rYtOV9E0HPn6-G36V9oI?"><img class=3D"header-image" role=3D"presenta=
tion" width=3D"550" height=3D"165" src=3D"https://substackcdn.com/image/fet=
ch/$s_!Zt7H!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3=
A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F869583e6-3f=
79-490c-888a-07a2b002fe94_2000x600.png" style=3D"border: none !important;ve=
rtical-align: middle;max-width: 550px;display: block;margin: 0 auto;height:=
 auto;width: 100%;"></a></td></tr></tbody></table><div class=3D"post typogr=
aphy" dir=3D"auto" style=3D"--image-offset-margin: -120px;padding: 32px 0 0=
 0;font-size: 16px;line-height: 26px;"><div class=3D"post-header" role=3D"r=
egion" aria-label=3D"Post header" style=3D"font-size: 16px;line-height: 26p=
x;"><h1 class=3D"post-title published title-X77sOw" dir=3D"auto" style=3D"d=
irection: auto;text-align: start;unicode-bidi: isolate;color: rgb(54,55,55)=
;font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-syst=
em,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple C=
olor Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-fo=
nt-smoothing: antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appe=
arance: optimizelegibility;-moz-appearance: optimizelegibility;appearance: =
optimizelegibility;margin: 0;line-height: 36px;font-size: 32px;"><a href=3D=
"https://substack.com/app-link/post?publication_id=3D1428667&amp;post_id=3D=
174513497&amp;utm_source=3Dpost-email-title&amp;utm_campaign=3Demail-post-t=
itle&amp;isFreemail=3Dtrue&amp;r=3D5kb93z&amp;token=3DeyJ1c2VyX2lkIjozMzY0N=
DgyMjMsInBvc3RfaWQiOjE3NDUxMzQ5NywiaWF0IjoxNzU4OTAwNzU5LCJleHAiOjE3NjE0OTI3=
NTksImlzcyI6InB1Yi0xNDI4NjY3Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.UIE1K4BxgVd7fx=
VfJHmvWAfy8qmWiUO20E4a0rROGQs" style=3D"color: rgb(54,55,55);text-decoratio=
n: none;">9 Papers You Should Know About</a></h1><h3 class=3D"subtitle subt=
itle-HEEcLo" dir=3D"auto" style=3D"direction: auto;text-align: start;unicod=
e-bidi: isolate;font-family: 'SF Pro Display',-apple-system-headline,system=
-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans=
-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: =
normal;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing: antiali=
ased;-webkit-appearance: optimizelegibility;-moz-appearance: optimizelegibi=
lity;appearance: optimizelegibility;margin: 4px 0 0;color: #777777;line-hei=
ght: 24px;font-size: 18px;margin-top: 12px;">Get ahead of the curve with LL=
M Watch</h3><table class=3D"post-meta custom" cellpadding=3D"0" cellspacing=
=3D"0" style=3D"margin: 1em 0;height: 20px;align-items: center;"><tbody><tr=
><td class=3D"post-meta-item post-date" title=3D"2025-09-26T15:31:03.906Z" =
style=3D"position: relative;padding: 0px 12px 0px 0;height: 14px;font-size:=
 14px;font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Ro=
boto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe=
 UI Symbol';color: #777777;text-decoration: none;margin-right: 0;padding-ri=
ght: 0;white-space: nowrap;font-weight: 400;padding-top: 0;padding-bottom: =
0;"><div class=3D"pencraft pc-reset color-secondary-ls1g8s line-height-20-t=
4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-upperc=
ase-yKDgcq reset-IxiVJZ meta-EgzBVA" style=3D"list-style: none;font-size: 1=
1px;line-height: 20px;text-decoration: unset;color: rgb(119,119,119);margin=
: 0;font-family: 'SF Compact',-apple-system,system-ui,-apple-system,BlinkMa=
cSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji=
','Segoe UI Emoji','Segoe UI Symbol';font-weight: 500;text-transform: upper=
case;letter-spacing: .2px;"><time datetime=3D"2025-09-26T15:31:03.906Z">Sep=
 26</time></div></td></tr></tbody></table><table class=3D"email-ufi-2-top" =
role=3D"presentation" width=3D"100%" border=3D"0" cellspacing=3D"0" cellpad=
ding=3D"0" style=3D"border-top: 1px solid rgb(0,0,0,.1);border-bottom: 1px =
solid rgb(0,0,0,.1);min-width: 100%;"><tbody><tr height=3D"16"><td height=
=3D"16" style=3D"font-size:0px;line-height:0;">&nbsp;</td></tr><tr><td><tab=
le role=3D"presentation" width=3D"100%" border=3D"0" cellspacing=3D"0" cell=
padding=3D"0"><tbody><tr><td><table role=3D"presentation" width=3D"auto" bo=
rder=3D"0" cellspacing=3D"0" cellpadding=3D"0"><tbody><tr><td style=3D"vert=
ical-align:middle;"><table role=3D"presentation" width=3D"38" border=3D"0" =
cellspacing=3D"0" cellpadding=3D"0"><tbody><tr><td align=3D"center"><a clas=
s=3D"email-icon-button" href=3D"https://substack.com/app-link/post?publicat=
ion_id=3D1428667&amp;post_id=3D174513497&amp;utm_source=3Dsubstack&amp;isFr=
eemail=3Dtrue&amp;submitLike=3Dtrue&amp;token=3DeyJ1c2VyX2lkIjozMzY0NDgyMjM=
sInBvc3RfaWQiOjE3NDUxMzQ5NywicmVhY3Rpb24iOiLinaQiLCJpYXQiOjE3NTg5MDA3NTksIm=
V4cCI6MTc2MTQ5Mjc1OSwiaXNzIjoicHViLTE0Mjg2NjciLCJzdWIiOiJyZWFjdGlvbiJ9.M6fT=
XptMkeSixoEYv2ZL7os6gegvjkVfcMMVOPLYiSk&amp;utm_medium=3Demail&amp;utm_camp=
aign=3Demail-reaction&amp;r=3D5kb93z" style=3D"font-family: system-ui,-appl=
e-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'A=
pple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';display: inline-block;=
font-weight: 500;border: 1px solid rgb(0,0,0,.1);border-radius: 9999px;text=
-transform: uppercase;font-size: 12px;line-height: 1;padding: 9px 0;text-de=
coration: none;color: rgb(119,119,119);min-width: 38px;box-sizing: border-b=
ox;width: 38px"><img class=3D"icon" src=3D"https://substackcdn.com/image/fe=
tch/$s_!PeVs!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%=
2F%2Fsubstack.com%2Ficon%2FLucideHeart%3Fv%3D4%26height%3D36%26fill%3Dnone%=
26stroke%3D%2523808080%26strokeWidth%3D2" width=3D"18" height=3D"18" style=
=3D"border: none;vertical-align: middle;max-width: 18px" alt=3D""></a></td>=
</tr></tbody></table></td><td width=3D"8" style=3D"min-width: 8px"></td><td=
 style=3D"vertical-align:middle;"><table role=3D"presentation" width=3D"38"=
 border=3D"0" cellspacing=3D"0" cellpadding=3D"0"><tbody><tr><td align=3D"c=
enter"><a class=3D"email-icon-button" href=3D"https://substack.com/app-link=
/post?publication_id=3D1428667&amp;post_id=3D174513497&amp;utm_source=3Dsub=
stack&amp;utm_medium=3Demail&amp;isFreemail=3Dtrue&amp;comments=3Dtrue&amp;=
token=3DeyJ1c2VyX2lkIjozMzY0NDgyMjMsInBvc3RfaWQiOjE3NDUxMzQ5NywiaWF0IjoxNzU=
4OTAwNzU5LCJleHAiOjE3NjE0OTI3NTksImlzcyI6InB1Yi0xNDI4NjY3Iiwic3ViIjoicG9zdC=
1yZWFjdGlvbiJ9.UIE1K4BxgVd7fxVfJHmvWAfy8qmWiUO20E4a0rROGQs&amp;r=3D5kb93z&a=
mp;utm_campaign=3Demail-half-magic-comments&amp;action=3Dpost-comment&amp;u=
tm_source=3Dsubstack&amp;utm_medium=3Demail" style=3D"font-family: system-u=
i,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-s=
erif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';display: inline=
-block;font-weight: 500;border: 1px solid rgb(0,0,0,.1);border-radius: 9999=
px;text-transform: uppercase;font-size: 12px;line-height: 1;padding: 9px 0;=
text-decoration: none;color: rgb(119,119,119);min-width: 38px;box-sizing: b=
order-box;width: 38px"><img class=3D"icon" src=3D"https://substackcdn.com/i=
mage/fetch/$s_!x1tS!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/ht=
tps%3A%2F%2Fsubstack.com%2Ficon%2FLucideComments%3Fv%3D4%26height%3D36%26fi=
ll%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2" width=3D"18" height=3D"=
18" style=3D"border: none;vertical-align: middle;max-width: 18px" alt=3D"">=
</a></td></tr></tbody></table></td><td width=3D"8" style=3D"min-width: 8px"=
></td><td style=3D"vertical-align:middle;"><table role=3D"presentation" wid=
th=3D"38" border=3D"0" cellspacing=3D"0" cellpadding=3D"0"><tbody><tr><td a=
lign=3D"center"><a class=3D"email-icon-button" href=3D"https://substack.com=
/app-link/post?publication_id=3D1428667&amp;post_id=3D174513497&amp;utm_sou=
rce=3Dsubstack&amp;utm_medium=3Demail&amp;utm_content=3Dshare&amp;utm_campa=
ign=3Demail-share&amp;action=3Dshare&amp;triggerShare=3Dtrue&amp;isFreemail=
=3Dtrue&amp;r=3D5kb93z&amp;token=3DeyJ1c2VyX2lkIjozMzY0NDgyMjMsInBvc3RfaWQi=
OjE3NDUxMzQ5NywiaWF0IjoxNzU4OTAwNzU5LCJleHAiOjE3NjE0OTI3NTksImlzcyI6InB1Yi0=
xNDI4NjY3Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.UIE1K4BxgVd7fxVfJHmvWAfy8qmWiUO20=
E4a0rROGQs" style=3D"font-family: system-ui,-apple-system,BlinkMacSystemFon=
t,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe U=
I Emoji','Segoe UI Symbol';display: inline-block;font-weight: 500;border: 1=
px solid rgb(0,0,0,.1);border-radius: 9999px;text-transform: uppercase;font=
-size: 12px;line-height: 1;padding: 9px 0;text-decoration: none;color: rgb(=
119,119,119);min-width: 38px;box-sizing: border-box;width: 38px"><img class=
=3D"icon" src=3D"https://substackcdn.com/image/fetch/$s_!_L14!,w_36,c_scale=
,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2=
FLucideShare2%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26=
strokeWidth%3D2" width=3D"18" height=3D"18" style=3D"border: none;vertical-=
align: middle;max-width: 18px" alt=3D""></a></td></tr></tbody></table></td>=
<td width=3D"8" style=3D"min-width: 8px"></td><td style=3D"vertical-align:m=
iddle;"><table role=3D"presentation" width=3D"38" border=3D"0" cellspacing=
=3D"0" cellpadding=3D"0"><tbody><tr><td align=3D"center"><a class=3D"email-=
icon-button" href=3D"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9vcG=
VuLnN1YnN0YWNrLmNvbS9wdWIveGFpZ3V5L3AvOS1wYXBlcnMteW91LXNob3VsZC1rbm93LWFib=
3V0LTFmNT91dG1fc291cmNlPXN1YnN0YWNrJnV0bV9tZWRpdW09ZW1haWwmdXRtX2NhbXBhaWdu=
PWVtYWlsLXJlc3RhY2stY29tbWVudCZhY3Rpb249cmVzdGFjay1jb21tZW50JnI9NWtiOTN6JnR=
va2VuPWV5SjFjMlZ5WDJsa0lqb3pNelkwTkRneU1qTXNJbkJ2YzNSZmFXUWlPakUzTkRVeE16UT=
VOeXdpYVdGMElqb3hOelU0T1RBd056VTVMQ0psZUhBaU9qRTNOakUwT1RJM05Ua3NJbWx6Y3lJN=
kluQjFZaTB4TkRJNE5qWTNJaXdpYzNWaUlqb2ljRzl6ZEMxeVpXRmpkR2x2YmlKOS5VSUUxSzRC=
eGdWZDdmeFZmSkhtdldBZnk4cW1XaVVPMjBFNGEwclJPR1FzIiwicCI6MTc0NTEzNDk3LCJzIjo=
xNDI4NjY3LCJmIjp0cnVlLCJ1IjozMzY0NDgyMjMsImlhdCI6MTc1ODkwMDc1OSwiZXhwIjoyMD=
c0NDc2NzU5LCJpc3MiOiJwdWItMCIsInN1YiI6ImxpbmstcmVkaXJlY3QifQ.qJd-zqJN3nvZNT=
mSYwqClH7Ztt8JBNirzf9q_GF3rkI?&amp;utm_source=3Dsubstack&amp;utm_medium=3De=
mail" style=3D"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Seg=
oe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoj=
i','Segoe UI Symbol';display: inline-block;font-weight: 500;border: 1px sol=
id rgb(0,0,0,.1);border-radius: 9999px;text-transform: uppercase;font-size:=
 12px;line-height: 1;padding: 9px 0;text-decoration: none;color: rgb(119,11=
9,119);min-width: 38px;box-sizing: border-box;width: 38px"><img class=3D"ic=
on" src=3D"https://substackcdn.com/image/fetch/$s_!5EGt!,w_36,c_scale,f_png=
,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FNoteF=
orwardIcon%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26str=
okeWidth%3D2" width=3D"18" height=3D"18" style=3D"border: none;vertical-ali=
gn: middle;max-width: 18px" alt=3D""></a></td></tr></tbody></table></td></t=
r></tbody></table></td><td align=3D"right"><table role=3D"presentation" wid=
th=3D"auto" border=3D"0" cellspacing=3D"0" cellpadding=3D"0"><tbody><tr><td=
 style=3D"vertical-align:middle;"><table role=3D"presentation" width=3D"aut=
o" border=3D"0" cellspacing=3D"0" cellpadding=3D"0"><tbody><tr><td align=3D=
"center"><a class=3D"email-button-outline" href=3D"https://open.substack.co=
m/pub/xaiguy/p/9-papers-you-should-know-about-1f5?utm_source=3Demail&amp;re=
direct=3Dapp-store&amp;utm_campaign=3Demail-read-in-app" style=3D"font-fami=
ly: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,=
Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';dis=
play: inline-block;font-weight: 500;border: 1px solid rgb(0,0,0,.1);border-=
radius: 9999px;text-transform: uppercase;font-size: 12px;line-height: 12px;=
padding: 9px 14px;text-decoration: none;color: rgb(119,119,119);"><div clas=
s=3D"email-button-spacer" style=3D"font-size: 16px;line-height: 26px;displa=
y: inline-block;vertical-align: middle;max-width: 0;min-height: 18px;"></di=
v><span class=3D"email-button-text" style=3D"vertical-align: middle;margin-=
right: 4px">READ IN APP</span><img class=3D"icon text-icon" src=3D"https://=
substackcdn.com/image/fetch/$s_!ET-_!,w_36,c_scale,f_png,q_auto:good,fl_pro=
gressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideArrowUpRight%3Fv%3=
D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2" wi=
dth=3D"18" height=3D"18" style=3D"min-width: 18px;min-height: 18px;border: =
none;vertical-align: middle;margin-right: 0;margin-left: 0;max-width: 18px"=
 alt=3D""></a></td></tr></tbody></table></td></tr></tbody></table></td></tr=
></tbody></table></td></tr><tr height=3D"16"><td height=3D"16" style=3D"fon=
t-size:0px;line-height:0;">&nbsp;</td></tr></tbody></table></div></div><div=
 class=3D"post typography" dir=3D"auto" style=3D"--image-offset-margin: -12=
0px;padding: 32px 0 0 0;font-size: 16px;line-height: 26px;"><div class=3D"b=
ody markup" dir=3D"auto" style=3D"text-align: initial;font-size: 16px;line-=
height: 26px;width: 100%;word-break: break-word;margin-bottom: 16px;"><p st=
yle=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size:=
 16px;margin-top: 0;">Welcome, Watchers! This week in LLM Watch, we delve i=
nto cutting-edge advances in large language models (LLMs) and their applica=
tions across AI, vision, science, and more. The highlights include:</p><ol =
style=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;"><p =
style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing=
: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><span>An </span>=
<strong>open-source evolutionary framework (ShinkaEvolve)</strong><span> th=
at uses LLMs to generate new algorithms with unprecedented sample-efficienc=
y.</span></p></li><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rg=
b(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;paddi=
ng-left: 4px;font-size: 16px;margin: 0;"><span>A </span><strong>self-superv=
ised RL paradigm (RLPT)</strong><span> letting LLMs learn reasoning from ra=
w pre-training data, yielding up to 8-point gains on benchmarks.</span></p>=
</li><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);l=
ine-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;=
font-size: 16px;margin: 0;"><span>Evidence that </span><strong>generative v=
ideo models</strong><span> can solve </span><em>62 tasks</em><span> zero-sh=
ot =E2=80=93 from segmentation to physical reasoning =E2=80=93 hinting at g=
eneral-purpose vision models.</span></p></li><li style=3D"margin: 8px 0 0 3=
2px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;b=
ox-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><span>A=
 </span><strong>=E2=80=9Cthink-before-you-speak=E2=80=9D training (RLMT)</s=
trong><span> that forces LLMs to generate chain-of-thought, achieving state=
-of-the-art chat performance with just 7K training prompts.</span></p></li>=
<li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line-h=
eight: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-=
size: 16px;margin: 0;"><span>A novel way to </span><strong>train continuous=
 =E2=80=9Csoft=E2=80=9D tokens</strong><span> for reasoning via RL, surpass=
ing discrete chain-of-thought on math tasks while preserving base model ski=
lls.</span></p></li></ol><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55=
);line-height: 26px;font-size: 16px;">And many more! Feel free to check out=
 the glossary below or jump straight to the paper section.</p><div style=3D=
"font-size: 16px;line-height: 26px;"><hr style=3D"margin: 32px 0;padding: 0=
;height: 1px;background: #313131;border: none;"></div><div class=3D"pullquo=
te" style=3D"font-size: 16px;line-height: 26px;border-top: 1px solid #31313=
1;border-bottom: 1px solid #313131;margin: 32px auto;text-align: center;"><=
div class=3D"captioned-image-container-static" style=3D"font-size: 16px;lin=
e-height: 26px;margin: 32px auto;margin-top: 0;"><figure style=3D"width: 10=
0%;margin: 0 auto;"><table class=3D"image-wrapper" width=3D"100%" border=3D=
"0" cellspacing=3D"0" cellpadding=3D"0" data-component-name=3D"Image2ToDOMS=
tatic" style=3D"mso-padding-alt: 1em 0 1.6em;"><tbody><tr><td style=3D"text=
-align: center;"></td><td class=3D"content" align=3D"left" width=3D"1456" s=
tyle=3D"text-align: center;"><a class=3D"image-link" target=3D"_blank" href=
=3D"https://substack.com/redirect/6c60c95c-ca15-4b48-bf36-4a55ad789dac?j=3D=
eyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" s=
tyle=3D"position: relative;flex-direction: column;align-items: center;paddi=
ng: 0;width: auto;height: auto;border: none;text-decoration: none;display: =
block;margin: 0;margin-top: 0;margin-bottom: 0;"><img class=3D"wide-image" =
data-attrs=3D"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazona=
ws.com/public/images/76fe99a4-2986-4dca-9584-ebb063a903b9_1920x1080.png&quo=
t;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageS=
ize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWi=
dth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;unnamed (2).jpg=
&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&=
quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedir=
ect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;=
offset&quot;:false}" alt=3D"unnamed (2).jpg" title=3D"unnamed (2).jpg" widt=
h=3D"550" height=3D"309.375" src=3D"https://substackcdn.com/image/fetch/$s_=
!eOTA!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2=
Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76fe99a4-2986-4dc=
a-9584-ebb063a903b9_1920x1080.png" style=3D"border: none !important;vertica=
l-align: middle;display: block;-ms-interpolation-mode: bicubic;height: auto=
;margin-bottom: 0;width: auto !important;max-width: 100% !important;margin:=
 0 auto;"></a></td><td style=3D"text-align: center;"></td></tr></tbody></ta=
ble></figure></div><p style=3D"color: rgb(54,55,55);line-height: 26px;font-=
size: 16px;margin: 32px 0;text-align: center;font-weight: 500;font-style: i=
talic;"><span>Members of LLM Watch are invited to participate in the 6th </=
span><a href=3D"https://substack.com/redirect/fc7571a6-cbd6-4315-b5ef-f2562=
964a2ce?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0=
" rel=3D"" style=3D"color: rgb(54,55,55);text-decoration: underline;">MLOps=
 World | GenAI</a><span> Global Summit in Austin Texas. Feat. OpenAI, Huggi=
ngFace, and 60+ sessions.</span></p><p style=3D"color: rgb(54,55,55);line-h=
eight: 26px;font-size: 16px;margin: 32px 0;text-align: center;font-weight: =
500;font-style: italic;"><a href=3D"https://substack.com/redirect/4d7de022-=
de02-4941-a69b-28350a82a40f?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7=
BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,55);text-decoratio=
n: underline;">Subscribers can join remotely, for free here.</a><br><br><sp=
an>Also if you'd like to join (in-person) for practical workshops, use case=
s, food, drink and parties across Austin - use this code for 150$ off! </sp=
an></p><p class=3D"button-wrapper" data-attrs=3D"{&quot;url&quot;:&quot;htt=
ps://www.eventbrite.ca/e/6th-ann-mlops-world-genai-summit-2025-tickets-1291=
752639919?discount=3DLLMWATCH&quot;,&quot;text&quot;:&quot;$150 discount&qu=
ot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name=3D=
"ButtonCreateButton" style=3D"color: rgb(54,55,55);line-height: 26px;font-s=
ize: 16px;margin: 32px 0;font-weight: 500;font-style: italic;text-align: ce=
nter;cursor: pointer;border-radius: 4px;"><a class=3D"button primary" href=
=3D"https://substack.com/redirect/699308b5-8916-4948-a068-19f9beb0edb2?j=3D=
eyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" s=
tyle=3D"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',=
Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Seg=
oe UI Symbol';display: inline-block;box-sizing: border-box;cursor: pointer;=
border: none;border-radius: 8px;font-size: 14px;line-height: 20px;font-weig=
ht: 600;text-align: center;margin: 0;opacity: 1;outline: none;white-space: =
nowrap;color: #363737 !important;text-decoration: none !important;backgroun=
d-color: #ffca4b;padding: 12px 20px;height: auto;"><span style=3D"color: #3=
63737;text-decoration: none;">$150 discount</span></a></p></div><h2 class=
=3D"header-anchor-post" style=3D"position: relative;font-family: 'SF Pro Di=
splay',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'S=
egoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Em=
oji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoothing: antialiase=
d;-moz-osx-font-smoothing: antialiased;-webkit-appearance: optimizelegibili=
ty;-moz-appearance: optimizelegibility;appearance: optimizelegibility;margi=
n: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-size: 1.62=
5em;"><strong>Quick Glossary</strong></h2><blockquote style=3D"border-left:=
 4px solid #ffca4b;margin: 20px 0;padding: 0;"><p style=3D"margin: 0 0 20px=
 0;color: rgb(54,55,55);margin-left: 20px;line-height: 26px;font-size: 16px=
;"><strong>Reinforcement Learning on Pre-Training data (RLPT):</strong><spa=
n> A training paradigm where an LLM treats its pre-training corpus as an en=
vironment to explore next-token =E2=80=9Ctrajectories=E2=80=9D and learn fr=
om them via RL, without any human-provided rewards. This approach bypasses =
the need for human annotation (unlike RLHF) by deriving rewards directly fr=
om predicting actual next segments. The result is that models can </span><e=
m>self-improve</em><span> their reasoning skills using only raw text data.<=
/span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);margin-left: =
20px;line-height: 26px;font-size: 16px;"><strong>Reinforcement Learning wit=
h Verifiable Rewards (RLVR):</strong><span> An RL approach that uses progra=
mmatic or rule-based rewards in domains like math or code where correctness=
 can be checked automatically. It boosts reasoning in those </span><em>veri=
fiable</em><span> domains, but on open-ended tasks (creative writing, plann=
ing, etc.) its benefits are limited.</span></p><p style=3D"margin: 0 0 20px=
 0;color: rgb(54,55,55);margin-left: 20px;line-height: 26px;font-size: 16px=
;"><strong>RL with Model-rewarded Thinking (RLMT):</strong><span> A new RL =
pipeline requiring the model to generate a lengthy chain-of-thought before =
giving its final answer. A separate reward model (like those from RLHF) the=
n scores these reasoning traces, and the base model is optimized to produce=
 better thought-out answers. This =E2=80=9Cthink then respond=E2=80=9D stra=
tegy leads to stronger general chat abilities than standard RLHF, as shown =
by large gains on multiple benchmarks.</span></p><p style=3D"margin: 0 0 20=
px 0;color: rgb(54,55,55);margin-left: 20px;line-height: 26px;font-size: 16=
px;"><strong>Thinking Augmented Pre-Training (TPT):</strong><span> A method=
 to inject </span><em>=E2=80=9Cthinking trajectories=E2=80=9D</em><span> (s=
tep-by-step reasoning chains) into text training data to enhance an LLM=E2=
=80=99s data efficiency. By augmenting text with auto-generated rationales,=
 TPT effectively triples the value of each token, helping models learn comp=
lex concepts with far fewer examples (e.g. a 3B model saw &gt;10% higher sc=
ores on reasoning tasks).</span></p><p style=3D"margin: 0 0 20px 0;color: r=
gb(54,55,55);margin-left: 20px;line-height: 26px;font-size: 16px;"><strong>=
Continuous Chain-of-Thought (=E2=80=9CSoft=E2=80=9D tokens):</strong><span>=
 Using </span><em>continuous embeddings</em><span> instead of discrete toke=
ns for an LLM=E2=80=99s intermediate reasoning process. Unlike normal word =
tokens, these </span><strong>soft tokens</strong><span> can represent a sup=
erposition of many reasoning paths. Recent work shows that an LLM can be tr=
ained via RL to utilize hundreds of continuous CoT tokens without any discr=
ete guide. The result is richer reasoning: models match discrete CoT accura=
cy in one try and </span><em>exceed it</em><span> when allowed multiple tri=
es, thanks to more diverse solution paths.</span></p></blockquote><div styl=
e=3D"font-size: 16px;line-height: 26px;"><hr style=3D"margin: 32px 0;paddin=
g: 0;height: 1px;background: #313131;border: none;"></div><h2 class=3D"head=
er-anchor-post" style=3D"position: relative;font-family: 'SF Pro Display',-=
apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI'=
,Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Se=
goe UI Symbol';font-weight: bold;-webkit-font-smoothing: antialiased;-moz-o=
sx-font-smoothing: antialiased;-webkit-appearance: optimizelegibility;-moz-=
appearance: optimizelegibility;appearance: optimizelegibility;margin: 1em 0=
 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-size: 1.625em;"><s=
trong>ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evoluti=
on</strong></h2><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-he=
ight: 26px;font-size: 16px;"><strong>Watching:</strong><span> ShinkaEvolve =
(</span><a href=3D"https://substack.com/redirect/adaad010-2777-4c7f-a691-ba=
b85ee94bf3?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMP=
Ow0" rel=3D"" style=3D"color: rgb(54,55,55);text-decoration: underline;">pa=
per</a><span>/</span><a href=3D"https://substack.com/redirect/fad427d8-f322=
-45f3-939d-771afd776a26?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnP=
G6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,55);text-decoration: u=
nderline;">code</a><span>)</span></p><div class=3D"captioned-image-containe=
r-static" style=3D"font-size: 16px;line-height: 26px;margin: 32px auto;"><f=
igure style=3D"width: 100%;margin: 0 auto;"><table class=3D"image-wrapper" =
width=3D"100%" border=3D"0" cellspacing=3D"0" cellpadding=3D"0" data-compon=
ent-name=3D"Image2ToDOMStatic" style=3D"mso-padding-alt: 1em 0 1.6em;"><tbo=
dy><tr><td style=3D"text-align: center;"></td><td class=3D"content" align=
=3D"left" width=3D"1139" style=3D"text-align: center;"><a class=3D"image-li=
nk" target=3D"_blank" href=3D"https://substack.com/redirect/1442e549-715e-4=
0e4-b5b9-401fd22ca366?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6=
RxQ4tAjMRMPOw0" rel=3D"" style=3D"position: relative;flex-direction: column=
;align-items: center;padding: 0;width: auto;height: auto;border: none;text-=
decoration: none;display: block;margin: 0;"><img class=3D"wide-image" data-=
attrs=3D"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.co=
m/public/images/9f7587be-cbe4-44b5-8547-e564b552af51_1139x528.png&quot;,&qu=
ot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&qu=
ot;:null,&quot;height&quot;:528,&quot;width&quot;:1139,&quot;resizeWidth&qu=
ot;:null,&quot;bytes&quot;:167610,&quot;alt&quot;:null,&quot;title&quot;:nu=
ll,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;below=
TheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:=
&quot;https://www.llmwatch.com/i/174513497?img=3Dhttps%3A%2F%2Fsubstack-pos=
t-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f7587be-cbe4-44b5-8547-e564b5=
52af51_1139x528.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:=
null,&quot;offset&quot;:false}" alt=3D"" width=3D"550" height=3D"254.960491=
6593503" src=3D"https://substackcdn.com/image/fetch/$s_!vBu5!,w_1100,c_limi=
t,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media=
.s3.amazonaws.com%2Fpublic%2Fimages%2F9f7587be-cbe4-44b5-8547-e564b552af51_=
1139x528.png" style=3D"border: none !important;vertical-align: middle;displ=
ay: block;-ms-interpolation-mode: bicubic;height: auto;margin-bottom: 0;wid=
th: auto !important;max-width: 100% !important;margin: 0 auto;"></a></td><t=
d style=3D"text-align: center;"></td></tr></tbody></table></figure></div><p=
 style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-si=
ze: 16px;"><strong>What problem does it solve?</strong><span> Modern approa=
ches that use LLMs to evolve code (e.g. for solving optimization or program=
ming tasks) are extremely </span><em>sample-inefficient</em><span>, often n=
eeding thousands of trials to find a good solution. They=E2=80=99re also us=
ually closed-source, hindering broad use. This makes LLM-driven discovery t=
oo costly and inaccessible for most researchers.</span></p><p style=3D"marg=
in: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><st=
rong>How does it solve the problem?</strong><span> </span><strong>ShinkaEvo=
lve</strong><span> is an open-source framework that treats an LLM as a =E2=
=80=9Cmutation operator=E2=80=9D to iteratively improve programs. It introd=
uces three key innovations to slash the number of trials needed: (1) a </sp=
an><strong>balanced parent selection</strong><span> strategy that smartly t=
rades off exploring new ideas vs. exploiting known good solutions, (2) </sp=
an><strong>novelty-based rejection sampling</strong><span> to filter out re=
dundant or uncreative mutations (using embedding similarity and an LLM judg=
e of novelty), and (3) a </span><strong>bandit algorithm to pick the best L=
LM</strong><span> from an ensemble for each generation. Together these ensu=
re ShinkaEvolve spends compute only on the most promising program variants,=
 avoiding the =E2=80=9Crandom search=E2=80=9D inefficiencies of prior metho=
ds.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-heig=
ht: 26px;font-size: 16px;"><strong>What are the key findings?</strong><span=
> By making program evolution dramatically more efficient, ShinkaEvolve ach=
ieves success on a wide range of problems that were previously impractical.=
 It discovered a new </span><em>state-of-the-art solution</em><span> to a c=
lassic </span><strong>26-circle packing</strong><span> problem using only <=
/span><strong>150 samples</strong><span> =E2=80=93 a =E2=80=9Cmassive leap =
in efficiency=E2=80=9D over past approaches. It also evolved a high-perform=
ing multi-agent strategy for math contests (AIME benchmark) in just 75 gene=
rations, outperforming strong human-designed baselines. In competitive prog=
ramming, ShinkaEvolve=E2=80=99s improvements to an AtCoder contest agent we=
re so significant that, on one problem, the evolved solution would have pla=
ced </span><strong>2nd in the competition</strong><span>. Moreover, ShinkaE=
volve even discovered a better way to train large Mixture-of-Expert LLMs =
=E2=80=93 finding a new load-balancing loss that beat the DeepMind =E2=80=
=9CGlobal LBL=E2=80=9D baseline with </span><strong>+1.73% higher task scor=
es and 5.8% less wasted capacity</strong><span>. These results show that br=
oad </span><em>=E2=80=9Copen-ended=E2=80=9D</em><span> discovery is now fea=
sible at modest cost, unlocking an </span><strong>AI-powered co-pilot for s=
cientists and engineers</strong><span> to autonomously search for novel sol=
utions.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-=
height: 26px;font-size: 16px;"><strong>What=E2=80=99s next?</strong><span> =
This work suggests that many hard problems in science and engineering (from=
 designing algorithms to discovering new optimizations) can be tackled by a=
n </span><em>LLM-driven evolutionary search</em><span> in an efficient way.=
 By open-sourcing ShinkaEvolve and even providing a Web UI for visualizing =
the evolutionary runs, the authors aim to democratize this approach. Future=
 research will likely build on this by applying the method to new domains (=
e.g. evolving circuits, scientific formulas, or hyperparameters) and by inc=
orporating even more advanced LLMs as they appear. In the long run, techniq=
ues like ShinkaEvolve could serve as </span><strong>automated =E2=80=9Crese=
arch assistants=E2=80=9D</strong><span>, rapidly iterating through ideas an=
d improvements that humans might overlook, all while using far fewer trials=
 than brute force methods.</span></p><div style=3D"font-size: 16px;line-hei=
ght: 26px;"><hr style=3D"margin: 32px 0;padding: 0;height: 1px;background: =
#313131;border: none;"></div><h2 class=3D"header-anchor-post" style=3D"posi=
tion: relative;font-family: 'SF Pro Display',-apple-system-headline,system-=
ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-=
serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: b=
old;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing: antialiase=
d;-webkit-appearance: optimizelegibility;-moz-appearance: optimizelegibilit=
y;appearance: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,5=
5);line-height: 1.16em;font-size: 1.625em;"><strong>Video Models are Zero-S=
hot Learners and Reasoners</strong></h2><p style=3D"margin: 0 0 20px 0;colo=
r: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>Watching:</str=
ong><span> Veo 3 Video Model (</span><a href=3D"https://substack.com/redire=
ct/ac725fce-a57a-43ce-a447-e7b27802d7a5?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPm=
LQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,55);te=
xt-decoration: underline;">paper</a><span>/</span><a href=3D"https://substa=
ck.com/redirect/3ec29f01-5201-4e86-82b0-c2b0655e504e?j=3DeyJ1IjoiNWtiOTN6In=
0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb=
(54,55,55);text-decoration: underline;">project</a><span>)</span></p><div c=
lass=3D"captioned-image-container-static" style=3D"font-size: 16px;line-hei=
ght: 26px;margin: 32px auto;"><figure style=3D"width: 100%;margin: 0 auto;"=
><table class=3D"image-wrapper" width=3D"100%" border=3D"0" cellspacing=3D"=
0" cellpadding=3D"0" data-component-name=3D"Image2ToDOMStatic" style=3D"mso=
-padding-alt: 1em 0 1.6em;"><tbody><tr><td style=3D"text-align: center;"></=
td><td class=3D"content" align=3D"left" width=3D"1292" style=3D"text-align:=
 center;"><a class=3D"image-link" target=3D"_blank" href=3D"https://substac=
k.com/redirect/7b5907c9-784a-4338-ad19-c160c214ebfe?j=3DeyJ1IjoiNWtiOTN6In0=
.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"position: r=
elative;flex-direction: column;align-items: center;padding: 0;width: auto;h=
eight: auto;border: none;text-decoration: none;display: block;margin: 0;"><=
img class=3D"wide-image" data-attrs=3D"{&quot;src&quot;:&quot;https://subst=
ack-post-media.s3.amazonaws.com/public/images/e3484e87-248f-482b-aaed-02c73=
a47ef2c_1292x605.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen=
&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:605,&quot;width&q=
uot;:1292,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:322345,&quot;alt&q=
uot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&qu=
ot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false=
,&quot;internalRedirect&quot;:&quot;https://www.llmwatch.com/i/174513497?im=
g=3Dhttps%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F=
e3484e87-248f-482b-aaed-02c73a47ef2c_1292x605.png&quot;,&quot;isProcessing&=
quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt=3D"" widt=
h=3D"550" height=3D"257.546439628483" src=3D"https://substackcdn.com/image/=
fetch/$s_!hK--!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/http=
s%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3484e87=
-248f-482b-aaed-02c73a47ef2c_1292x605.png" style=3D"border: none !important=
;vertical-align: middle;display: block;-ms-interpolation-mode: bicubic;heig=
ht: auto;margin-bottom: 0;width: auto !important;max-width: 100% !important=
;margin: 0 auto;"></a></td><td style=3D"text-align: center;"></td></tr></tb=
ody></table></figure></div><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,=
55);line-height: 26px;font-size: 16px;"><strong>What problem does it solve?=
</strong><span> Large language models surprised the world by displaying </s=
pan><strong>emergent zero-shot abilities</strong><span> =E2=80=93 solving t=
asks they were never explicitly trained on =E2=80=93 thanks to scale and di=
verse training data. This paper asks: can </span><em>video generative model=
s</em><span> analogously become general problem-solvers in the visual domai=
n? Prior video models were usually evaluated on narrow tasks or short bench=
marks, leaving it unclear if they possess broad, human-like visual reasonin=
g capabilities.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,5=
5);line-height: 26px;font-size: 16px;"><strong>How does it solve the proble=
m?</strong><span> The authors take </span><strong>Veo 3</strong><span>, a s=
tate-of-the-art generative video model, and systematically test it on a </s=
pan><em>broad battery of 62 tasks</em><span> spanning classic vision, physi=
cal reasoning, and even tool use. Importantly, they use a </span><em>minima=
list prompt-based approach</em><span>: they feed the model an initial video=
 frame or image plus a textual instruction (like =E2=80=9Cshow the edges=E2=
=80=9D or =E2=80=9Csolve this maze=E2=80=9D), then let it generate an 8-sec=
ond video as the =E2=80=9Canswer=E2=80=9D. No fine-tuning or task-specific =
training =E2=80=93 this is true zero-shot evaluation. To isolate the video =
model=E2=80=99s own reasoning, they ensure that a standalone LLM (Google Ge=
mini 2.5) given just the image can=E2=80=99t solve the task, so any success=
 comes from the video generation process itself. Essentially, they=E2=80=99=
re prompting the video model to </span><em>think visually</em><span> step-b=
y-step (through the frames it generates) and then checking if the result sh=
ows the correct solution.</span></p><p style=3D"margin: 0 0 20px 0;color: r=
gb(54,55,55);line-height: 26px;font-size: 16px;"><strong>What are the key f=
indings?</strong><span> Remarkably, </span><strong>Veo 3 exhibits a wide ra=
nge of emergent skills</strong><span> without any task-specific optimizatio=
n. It can </span><strong>segment objects</strong><span>, </span><strong>det=
ect edges</strong><span>, do </span><strong>image editing</strong><span> (e=
.g. remove an object), infer </span><strong>physical properties</strong><sp=
an> (like an object=E2=80=99s mass by how it moves), recognize </span><stro=
ng>object affordances</strong><span> (how an object can be used), and even =
simulate </span><strong>tool use</strong><span> =E2=80=93 all zero-shot. Th=
ese perceptual and manipulation abilities enable higher-level </span><em>vi=
sual reasoning</em><span>: for example, Veo 3 solves mazes and symmetry puz=
zles by internally =E2=80=9Cimagining=E2=80=9D the solution path in the vid=
eo it generates. Quantitatively, across </span><strong>62 diverse tasks</st=
rong><span> tested, the model achieved high success rates on both low-level=
 vision tasks (e.g. 92% on edge detection, 100% on image de-noising) and mo=
re cognitive tasks like physical reasoning. Veo 3 also showed clear improve=
ment over its predecessor (Veo 2) on these tasks, indicating these capabili=
ties </span><em>scaled up</em><span> with model/version improvements. All t=
his suggests that video models, given sufficient scale and training, are fo=
llowing a similar trajectory to LLMs =E2=80=93 becoming </span><strong>gene=
ral-purpose =E2=80=9Cvision foundation models=E2=80=9D</strong><span> that =
can be prompted to tackle myriad tasks beyond their training.</span></p><p =
style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-siz=
e: 16px;"><strong>What=E2=80=99s next?</strong><span> This work implies tha=
t future AI may rely on </span><em>unified multimodal foundation models</em=
><span>: just as one LLM can handle many language tasks, one video model co=
uld handle many vision tasks. A key next step is to refine </span><em>promp=
ting techniques and benchmarks</em><span> for these video models =E2=80=93 =
analogous to how prompt engineering and standardized evals drove progress i=
n LLMs. Researchers will also explore whether introducing explicit reasonin=
g steps (e.g. text-based planning combined with video generation) can furth=
er enhance performance on complex tasks. On the application side, a powerfu=
l zero-shot video reasoner could be transformative: imagine robot assistant=
s that </span><strong>reason visually</strong><span> through consequences b=
efore acting, or scientific models that simulate physics experiments on the=
 fly. Ultimately, the convergence of capabilities in language and video mod=
els hints at a broader trend of </span><strong>generalist AI systems</stron=
g><span>, and understanding how and why these abilities emerge (e.g. what i=
n the training data or architecture leads to tool-use simulation?) is an ex=
citing question for fundamental research.</span></p><div style=3D"font-size=
: 16px;line-height: 26px;"><hr style=3D"margin: 32px 0;padding: 0;height: 1=
px;background: #313131;border: none;"></div><h2 class=3D"header-anchor-post=
" style=3D"position: relative;font-family: 'SF Pro Display',-apple-system-h=
eadline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvet=
ica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'=
;font-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smooth=
ing: antialiased;-webkit-appearance: optimizelegibility;-moz-appearance: op=
timizelegibility;appearance: optimizelegibility;margin: 1em 0 0.625em 0;col=
or: rgb(54,55,55);line-height: 1.16em;font-size: 1.625em;"><strong>Reinforc=
ement Learning on Pre-Training Data (RLPT)</strong></h2><p style=3D"margin:=
 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><stron=
g>Watching:</strong><span> RLPT (</span><a href=3D"https://substack.com/red=
irect/74b9a360-e267-4b53-b292-065df05c8708?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YF=
xPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,55)=
;text-decoration: underline;">paper</a><span>)</span></p><div class=3D"capt=
ioned-image-container-static" style=3D"font-size: 16px;line-height: 26px;ma=
rgin: 32px auto;"><figure style=3D"width: 100%;margin: 0 auto;"><table clas=
s=3D"image-wrapper" width=3D"100%" border=3D"0" cellspacing=3D"0" cellpaddi=
ng=3D"0" data-component-name=3D"Image2ToDOMStatic" style=3D"mso-padding-alt=
: 1em 0 1.6em;"><tbody><tr><td style=3D"text-align: center;"></td><td class=
=3D"content" align=3D"left" width=3D"1240" style=3D"text-align: center;"><a=
 class=3D"image-link" target=3D"_blank" href=3D"https://substack.com/redire=
ct/46994bdf-f788-4b72-aafe-be0899b2eb76?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPm=
LQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"position: relative;flex=
-direction: column;align-items: center;padding: 0;width: auto;height: auto;=
border: none;text-decoration: none;display: block;margin: 0;"><img class=3D=
"wide-image" data-attrs=3D"{&quot;src&quot;:&quot;https://substack-post-med=
ia.s3.amazonaws.com/public/images/7ed76579-1433-4492-8be0-518a3cf1a2b9_1240=
x382.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,=
&quot;imageSize&quot;:null,&quot;height&quot;:382,&quot;width&quot;:1240,&q=
uot;resizeWidth&quot;:null,&quot;bytes&quot;:192066,&quot;alt&quot;:null,&q=
uot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot=
;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;inter=
nalRedirect&quot;:&quot;https://www.llmwatch.com/i/174513497?img=3Dhttps%3A=
%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ed76579-143=
3-4492-8be0-518a3cf1a2b9_1240x382.png&quot;,&quot;isProcessing&quot;:false,=
&quot;align&quot;:null,&quot;offset&quot;:false}" alt=3D"" width=3D"550" he=
ight=3D"169.43548387096774" src=3D"https://substackcdn.com/image/fetch/$s_!=
yoEG!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2F=
substack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ed76579-1433-4492=
-8be0-518a3cf1a2b9_1240x382.png" style=3D"border: none !important;vertical-=
align: middle;display: block;-ms-interpolation-mode: bicubic;height: auto;m=
argin-bottom: 0;width: auto !important;max-width: 100% !important;margin: 0=
 auto;"></a></td><td style=3D"text-align: center;"></td></tr></tbody></tabl=
e></figure></div><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-h=
eight: 26px;font-size: 16px;"><strong>What problem does it solve?</strong><=
span> Scaling up LLMs by feeding them more text has hit a bottleneck: we ca=
n increase compute easily, but high-quality text data is finite. Moreover, =
simply predicting the next token (standard training) might not teach models=
 to reason through complex dependencies, because the model isn=E2=80=99t en=
couraged to </span><em>explore</em><span> beyond the distribution of its da=
ta. Prior methods like RLHF add some signal but require costly human feedba=
ck. In short, we need a way for LLMs to </span><strong>learn more from the =
data they already have</strong><span>, especially to acquire reasoning skil=
ls, </span><em>without</em><span> an army of human annotators.</span></p><p=
 style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-si=
ze: 16px;"><strong>How does it solve the problem?</strong><span> </span><st=
rong>RLPT</strong><span> turns an LLM=E2=80=99s original pre-training corpu=
s into an interactive training playground for </span><em>reinforcement lear=
ning</em><span>. It does this by defining a </span><strong>next-segment pre=
diction task</strong><span> as a sequential decision problem. Concretely, t=
he model reads some text context and then generates the next chunk of text;=
 a separate </span><strong>Generative Reward Model</strong><span> (or an im=
plicit ground-truth signal) gives a reward based on how well that generatio=
n matches the actual continuation in the corpus. The LLM thus treats the au=
thentic text as demonstrations of optimal behavior, exploring different con=
tinuations and getting feedback without any human labels. Importantly, thes=
e rewards come </span><em>directly</em><span> from the pre-training data (e=
.g. matching observed text), eliminating reliance on hand-crafted rewards o=
r human preference models. In effect, RLPT allows the model to </span><em>a=
utonomously practice reasoning</em><span> on unlabeled data: it might devia=
te from the immediate next token if it finds a longer-term path that yields=
 higher reward (better overall coherence with the text). This training-time=
 exploration is carefully scaled up to billions of tokens so the policy can=
 discover richer reasoning strategies across a broad domain of text.</span>=
</p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;f=
ont-size: 16px;"><strong>What are the key findings?</strong><span> When app=
lied to a 4-billion parameter base LLM (Qwen3-4B), RLPT dramatically improv=
ed its performance on multiple challenging benchmarks. For instance, it boo=
sted the model=E2=80=99s score on </span><strong>MMLU</strong><span> (a kno=
wledge exam) by </span><strong>+3.0</strong><span> points and on </span><st=
rong>MMLU-Pro</strong><span> (an advanced version) by </span><strong>+5.1</=
strong><span>. Gains were even larger on math and logic-heavy tasks: +8.1 o=
n a QA benchmark (GPQA-Diamond) and +6.6 on </span><strong>AIME24</strong><=
span> (math competition problems). These are </span><em>absolute</em><span>=
 improvements over an already strong base model, achieved </span><strong>wi=
thout any human-labeled data or task-specific finetuning</strong><span>. Mo=
reover, scaling studies indicate that as you give RLPT more compute (more t=
raining steps), the model keeps improving =E2=80=93 hinting that even bigge=
r gains are possible with larger budgets. The authors also note that RLPT-t=
rained models exhibit stronger </span><em>generalizable reasoning</em><span=
>: they extend the model=E2=80=99s ability to handle complex prompts and im=
prove performance of existing verification-based RL (RLVR) when used togeth=
er. In summary, RLPT offers a promising path to </span><strong>break the da=
ta scarcity barrier</strong><span> by extracting much more signal from the =
text we already have, effectively turning passive pre-training data into an=
 active learning experience.</span></p><p style=3D"margin: 0 0 20px 0;color=
: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>What=E2=80=99s =
next?</strong><span> RLPT=E2=80=99s success with self-supervised rewards pa=
ves the way for more </span><em>hybrid training regimes</em><span>. Future =
LLMs might alternate between passive reading and active exploration of text=
s, games, or simulations, all without human intervention. One immediate fol=
low-up is to apply RLPT to even larger models (e.g. 34B, 70B) and more doma=
ins =E2=80=93 does it similarly boost reasoning for code, or multimodal dat=
a? There=E2=80=99s also room to refine the reward modeling: the current nex=
t-segment reward might be enhanced by incorporating logical consistency or =
factual accuracy metrics derived automatically from the text. If those can =
be folded into RLPT, models could </span><em>self-police</em><span> their c=
oherence and truthfulness. In the big picture, RLPT is part of a broader tr=
end of LMs learning to </span><strong>think ahead</strong><span> (plan toke=
ns) rather than just mimic, so we can expect research that combines this wi=
th techniques like tree-of-thought or tool-use during training. All of this=
 moves toward LLMs that not only absorb internet text, but </span><em>activ=
ely practice and generalize</em><span> from it =E2=80=93 much like a studen=
t solving problems to better understand the material.</span></p><div style=
=3D"font-size: 16px;line-height: 26px;"><hr style=3D"margin: 32px 0;padding=
: 0;height: 1px;background: #313131;border: none;"></div><h2 class=3D"heade=
r-anchor-post" style=3D"position: relative;font-family: 'SF Pro Display',-a=
pple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',=
Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Seg=
oe UI Symbol';font-weight: bold;-webkit-font-smoothing: antialiased;-moz-os=
x-font-smoothing: antialiased;-webkit-appearance: optimizelegibility;-moz-a=
ppearance: optimizelegibility;appearance: optimizelegibility;margin: 1em 0 =
0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-size: 1.625em;"><st=
rong>Soft Tokens, Hard Truths</strong></h2><p style=3D"margin: 0 0 20px 0;c=
olor: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>Watching:</=
strong><span> Continuous CoT (</span><a href=3D"https://substack.com/redire=
ct/16242107-8395-4df8-b129-0bf1a9ec144f?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPm=
LQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,55);te=
xt-decoration: underline;">paper</a><span>)</span></p><div class=3D"caption=
ed-image-container-static" style=3D"font-size: 16px;line-height: 26px;margi=
n: 32px auto;"><figure style=3D"width: 100%;margin: 0 auto;"><table class=
=3D"image-wrapper" width=3D"100%" border=3D"0" cellspacing=3D"0" cellpaddin=
g=3D"0" data-component-name=3D"Image2ToDOMStatic" style=3D"mso-padding-alt:=
 1em 0 1.6em;"><tbody><tr><td style=3D"text-align: center;"></td><td class=
=3D"content" align=3D"left" width=3D"568" style=3D"text-align: center;"><a =
class=3D"image-link" target=3D"_blank" href=3D"https://substack.com/redirec=
t/604624f0-e1b6-4968-8961-ad5d8dc4c2fe?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmL=
QJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"position: relative;flex-=
direction: column;align-items: center;padding: 0;width: auto;height: auto;b=
order: none;text-decoration: none;display: block;margin: 0;"><img class=3D"=
wide-image" data-attrs=3D"{&quot;src&quot;:&quot;https://substack-post-medi=
a.s3.amazonaws.com/public/images/2dcac8cf-ce99-488b-a1ef-cde9a715aa0d_568x6=
31.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&q=
uot;imageSize&quot;:null,&quot;height&quot;:631,&quot;width&quot;:568,&quot=
;resizeWidth&quot;:null,&quot;bytes&quot;:65492,&quot;alt&quot;:null,&quot;=
title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:nu=
ll,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalR=
edirect&quot;:&quot;https://www.llmwatch.com/i/174513497?img=3Dhttps%3A%2F%=
2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2dcac8cf-ce99-48=
8b-a1ef-cde9a715aa0d_568x631.png&quot;,&quot;isProcessing&quot;:false,&quot=
;align&quot;:null,&quot;offset&quot;:false}" alt=3D"" width=3D"550" height=
=3D"611.0035211267606" src=3D"https://substackcdn.com/image/fetch/$s_!aDYB!=
,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubst=
ack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2dcac8cf-ce99-488b-a1ef=
-cde9a715aa0d_568x631.png" style=3D"border: none !important;vertical-align:=
 middle;display: block;-ms-interpolation-mode: bicubic;height: auto;margin-=
bottom: 0;width: auto !important;max-width: 100% !important;margin: 0 auto;=
"></a></td><td style=3D"text-align: center;"></td></tr></tbody></table></fi=
gure></div><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height:=
 26px;font-size: 16px;"><strong>What problem does it solve?</strong><span> =
Chain-of-thought prompting (having an LLM generate step-by-step reasoning) =
improves performance, but it uses discrete natural language tokens, which m=
ight not be the most efficient internal representation for reasoning. </spa=
n><strong>Continuous tokens</strong><span> =E2=80=93 essentially vectors th=
at aren=E2=80=99t constrained to the discrete vocabulary =E2=80=93 have the=
oretically </span><em>much greater expressiveness</em><span> and can encode=
 multiple ideas at once. In theory, a model that =E2=80=9Cthinks=E2=80=9D i=
n a continuous space could explore many reasoning paths in parallel (a supe=
rposition) instead of one-by-one. The problem is that actually </span><em>t=
raining</em><span> an LLM to use continuous, non-language tokens in its rea=
soning process is very hard: past attempts either only injected continuous =
tokens at inference (without training on them), or required distilling from=
 known human-written reasoning chains, which is cumbersome and limited to s=
hort chains. No one had shown a scalable way for a model to learn a useful =
</span><em>continuous chain-of-thought (CoT)</em><span> from scratch.</span=
></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;=
font-size: 16px;"><strong>How does it solve the problem?</strong><span> Thi=
s work presents the first successful method to train </span><strong>continu=
ous CoTs via reinforcement learning</strong><span>, without relying on any =
ground-truth human rationales. The idea is to let the model generate =E2=80=
=9Csoft=E2=80=9D tokens (continuous embeddings) between the prompt and the =
final answer, and use a reward signal to optimize their use. Specifically, =
they add a small amount of </span><strong>noise</strong><span> to the input=
 embeddings as a form of exploration, and then use policy-gradient RL to re=
ward the model if its final answer is correct. Essentially, the model is tr=
ying to </span><em>invent its own internal language</em><span> (the continu=
ous tokens and what they represent) that leads to better problem-solving ou=
tcomes. By avoiding any supervised training on discrete chains, there=E2=80=
=99s no human bias limiting what these soft tokens can do. Notably, the app=
roach adds minimal computational overhead, so they can afford to let the mo=
del use </span><strong>hundreds of continuous tokens</strong><span> in the =
reasoning phase during training =E2=80=93 orders of magnitude more =E2=80=
=9Cthought capacity=E2=80=9D than prior distillation methods allowed.</span=
></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;=
font-size: 16px;"><strong>What are the key findings?</strong><span> On math=
 reasoning benchmarks, LLMs trained with this continuous CoT technique achi=
eved performance on par with or better than those using traditional discret=
e chain-of-thoughts. For example, on GSM8K math problems, a Llama-7B model =
with continuous CoT matched the accuracy of the same model with standard (d=
iscrete) CoT when considering the single best answer (pass@1). However, whe=
n allowed to sample multiple answers (pass@32), the continuous-CoT model </=
span><strong>outperformed the discrete CoT model</strong><span>, indicating=
 it found a more diverse set of reasoning paths leading to correct answers.=
 This demonstrates one big advantage of continuous tokens =E2=80=93 they ca=
n capture a richer variety of solutions, which pays off when you can try mu=
ltiple outputs. Interestingly, the authors found the </span><em>best</em><s=
pan> strategy was a hybrid: </span><strong>train with continuous tokens, bu=
t use discrete tokens at inference</strong><span>. In other words, let the =
model think in vectors during training to gain the benefits, but at deploym=
ent it can just output normal text rationale if needed =E2=80=93 the traini=
ng still improved its latent reasoning ability. Moreover, continuous CoT tr=
aining caused </span><em>less interference</em><span> with the model=E2=80=
=99s other capabilities: the model retained its accuracy on unrelated tasks=
 better than a model trained on discrete CoT, meaning this approach is a =
=E2=80=9Csofter=E2=80=9D touch that avoids overfitting to the reasoning dat=
a. All told, this is a proof-of-concept that LLMs </span><em>can</em><span>=
 develop their own non-human-readable thought vectors that yield real probl=
em-solving gains.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55=
,55);line-height: 26px;font-size: 16px;"><strong>What=E2=80=99s next?</stro=
ng><span> Training LLMs to </span><strong>think in vectors</strong><span> o=
pens up many research directions. One immediate question is how to interpre=
t or visualize these learned continuous tokens =E2=80=93 do they correspond=
 to human-like concepts, or something entirely alien yet effective? There=
=E2=80=99s also potential to extend continuous CoT to multimodal reasoning =
(imagine an LLM that internally represents an image with =E2=80=9Csoft visu=
al tokens=E2=80=9D while reasoning). The success with reinforcement learnin=
g here may inspire using other reward signals to shape continuous thoughts,=
 such as logical consistency checks or factual verification as rewards to p=
roduce even more reliable reasoning. In practice, we might see hybrid syste=
ms where models do heavy-duty reasoning in continuous space and then distil=
l the outcome into a concise explanation for humans. The fact that the =E2=
=80=9Csoft=E2=80=9D model=E2=80=99s final answers can be </span><em>execute=
d in standard form</em><span> means adoption is easy =E2=80=93 e.g. a math =
tutor LLM could silently use continuous CoT to figure out a tough proof, th=
en present the answer in neat natural language. Overall, this work lays </s=
pan><strong>groundwork for more efficient, diverse reasoning in LLMs</stron=
g><span>, potentially overcoming some limits of discrete token thinking tha=
t chain-of-thought still had.</span></p><div style=3D"font-size: 16px;line-=
height: 26px;"><hr style=3D"margin: 32px 0;padding: 0;height: 1px;backgroun=
d: #313131;border: none;"></div><h2 class=3D"header-anchor-post" style=3D"p=
osition: relative;font-family: 'SF Pro Display',-apple-system-headline,syst=
em-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sa=
ns-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight=
: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing: antiali=
ased;-webkit-appearance: optimizelegibility;-moz-appearance: optimizelegibi=
lity;appearance: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,5=
5,55);line-height: 1.16em;font-size: 1.625em;"><strong>Thinking Augmented P=
re-Training (TPT)</strong></h2><p style=3D"margin: 0 0 20px 0;color: rgb(54=
,55,55);line-height: 26px;font-size: 16px;"><strong>Watching:</strong><span=
> TPT (</span><a href=3D"https://substack.com/redirect/08889166-c48a-416d-a=
a65-437ce07d02dc?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4t=
AjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,55);text-decoration: underlin=
e;">paper</a><span>)</span></p><div class=3D"captioned-image-container-stat=
ic" style=3D"font-size: 16px;line-height: 26px;margin: 32px auto;"><figure =
style=3D"width: 100%;margin: 0 auto;"><table class=3D"image-wrapper" width=
=3D"100%" border=3D"0" cellspacing=3D"0" cellpadding=3D"0" data-component-n=
ame=3D"Image2ToDOMStatic" style=3D"mso-padding-alt: 1em 0 1.6em;"><tbody><t=
r><td style=3D"text-align: center;"></td><td class=3D"content" align=3D"lef=
t" width=3D"971" style=3D"text-align: center;"><a class=3D"image-link" targ=
et=3D"_blank" href=3D"https://substack.com/redirect/86e2c1bb-7e76-47f3-8058=
-b219fa10fbe7?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjM=
RMPOw0" rel=3D"" style=3D"position: relative;flex-direction: column;align-i=
tems: center;padding: 0;width: auto;height: auto;border: none;text-decorati=
on: none;display: block;margin: 0;"><img class=3D"wide-image" data-attrs=3D=
"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public=
/images/63b2aee1-58e6-48b6-9623-d193bbf943a1_971x532.png&quot;,&quot;srcNoW=
atermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,=
&quot;height&quot;:532,&quot;width&quot;:971,&quot;resizeWidth&quot;:null,&=
quot;bytes&quot;:146491,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;t=
ype&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&qu=
ot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;http=
s://www.llmwatch.com/i/174513497?img=3Dhttps%3A%2F%2Fsubstack-post-media.s3=
.amazonaws.com%2Fpublic%2Fimages%2F63b2aee1-58e6-48b6-9623-d193bbf943a1_971=
x532.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;=
offset&quot;:false}" alt=3D"" width=3D"550" height=3D"301.33882595262617" s=
rc=3D"https://substackcdn.com/image/fetch/$s_!k0MW!,w_1100,c_limit,f_auto,q=
_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazon=
aws.com%2Fpublic%2Fimages%2F63b2aee1-58e6-48b6-9623-d193bbf943a1_971x532.pn=
g" style=3D"border: none !important;vertical-align: middle;display: block;-=
ms-interpolation-mode: bicubic;height: auto;margin-bottom: 0;width: auto !i=
mportant;max-width: 100% !important;margin: 0 auto;"></a></td><td style=3D"=
text-align: center;"></td></tr></tbody></table></figure></div><p style=3D"m=
argin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">=
<strong>What problem does it solve?</strong><span> High-quality training da=
ta for LLMs is limited, and some complex patterns in language are hard for =
a model to learn just by next-word prediction. Often, the </span><em>reason=
</em><span> behind a statement or the chain of logic connecting sentences i=
s not explicitly in the text =E2=80=93 it=E2=80=99s implicit or assumed. Th=
is makes certain =E2=80=9Chigh-quality tokens=E2=80=9D (like a step in a ma=
th proof, or a hidden logical connection in code) effectively very difficul=
t to learn. The result is data inefficiency: even with billions of words, t=
he model might still struggle with multi-step reasoning or harder comprehen=
sion, because it never sees the intermediate thinking. The challenge addres=
sed here is how to make better use of the data by </span><em>making the hid=
den reasoning explicit</em><span>.</span></p><p style=3D"margin: 0 0 20px 0=
;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>How does =
it solve the problem?</strong><span> </span><strong>TPT</strong><span> tack=
les this by </span><strong>augmenting the pre-training corpus with =E2=80=
=9Cthinking trajectories=E2=80=9D</strong><span> =E2=80=93 essentially gene=
rating step-by-step reasoning or explanatory content and inserting it along=
side the original text. For example, if the original text says =E2=80=9CThe=
 student solved the problem and got the answer 42,=E2=80=9D a thinking traj=
ectory might include the steps the student took to solve it. These trajecto=
ries are created automatically (likely using prompting on a strong LLM or h=
euristics) for a wide range of tasks and domains, and then interwoven with =
the original data for training. By doing so, TPT </span><em>increases the e=
ffective data volume</em><span> (since we add new tokens) and, crucially, <=
/span><strong>makes complex tokens easier to learn</strong><span> by breaki=
ng down their underlying rationale. The method is =E2=80=9Cuniversal=E2=80=
=9D =E2=80=93 they apply it in various settings: pre-training from scratch =
on limited data, augmenting an already large corpus, or even mid-training a=
n open-source model to further improve it. In each case, the presence of ex=
plicit reasoning chains helps the model generalize better from the same amo=
unt of original text.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(5=
4,55,55);line-height: 26px;font-size: 16px;"><strong>What are the key findi=
ngs?</strong><span> Across model sizes and training setups, TPT delivered s=
ubstantial performance boosts, indicating a huge win in data efficiency. No=
tably, the authors report that TPT improves the </span><em>data efficiency =
of pre-training by a factor of 3</em><span>. In practical terms, this means=
 an LLM trained on 100B tokens with TPT augmentation could achieve comparab=
le or better results than a model trained on 300B tokens of standard data. =
For a 3B-parameter model, they saw over </span><strong>+10% improvement on =
multiple challenging reasoning benchmarks</strong><span> just by incorporat=
ing thinking trajectories during training. Larger models and different fami=
lies (they tested both decoder-only and others) all benefited, suggesting T=
PT is robust. Importantly, these gains aren=E2=80=99t limited to niche task=
s =E2=80=93 the paper notes improvements =E2=80=9Cacross various model size=
s and families=E2=80=9D on general NLP benchmarks. This implies the method =
injects a broad understanding or skill, rather than overfitting to specific=
 problems. By explicitly including reasoning, the model is better at tasks =
requiring step-by-step logic, math word problems, complex QA, etc., while a=
lso not hurting performance on standard language tasks. Essentially, TPT sh=
ows that </span><em>more thinking per token</em><span> is as good as (or be=
tter than) just more tokens =E2=80=93 a significant result for efficient tr=
aining.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-=
height: 26px;font-size: 16px;"><strong>What=E2=80=99s next?</strong><span> =
TPT=E2=80=99s approach aligns with a growing trend of making LLM training m=
ore </span><em>intentional</em><span> or structured. Future research might =
explore </span><strong>automating the generation of thinking trajectories</=
strong><span> even further =E2=80=93 perhaps using one LLM to generate and =
another to verify or refine the reasoning before using it for training. The=
re=E2=80=99s also the possibility of extending this to other modalities: fo=
r instance, augmenting image captions with chains of visual reasoning, or c=
ode with chains of program logic, to similarly boost learning. In terms of =
immediate practical impact, companies training models could adopt TPT to re=
ach high performance with less data (or get better results with the same da=
ta), which is economically appealing. One could also combine TPT with RLPT =
(from paper #4 above): first augment data with reasoning (TPT), then let th=
e model </span><em>explore</em><span> that data via RL =E2=80=93 potentiall=
y a very powerful combo for self-improving AI. Finally, TPT prompts us to c=
onsider the </span><em>quality</em><span> of data over quantity; by focusin=
g on the =E2=80=9Chidden=E2=80=9D information in text and making it explici=
t, we might uncover new levels of LLM capability without needing an order-o=
f-magnitude more data.</span></p><div style=3D"font-size: 16px;line-height:=
 26px;"><hr style=3D"margin: 32px 0;padding: 0;height: 1px;background: #313=
131;border: none;"></div><h2 class=3D"header-anchor-post" style=3D"position=
: relative;font-family: 'SF Pro Display',-apple-system-headline,system-ui,-=
apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-seri=
f,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;=
-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing: antialiased;-w=
ebkit-appearance: optimizelegibility;-moz-appearance: optimizelegibility;ap=
pearance: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);l=
ine-height: 1.16em;font-size: 1.625em;"><strong>SimpleFold: Folding Protein=
s is Simpler than You Think</strong></h2><p style=3D"margin: 0 0 20px 0;col=
or: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>Watching:</st=
rong><span> SimpleFold (</span><a href=3D"https://substack.com/redirect/898=
f3f21-5f17-450c-af9b-6194c15866d7?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9h=
shgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,55);text-dec=
oration: underline;">paper</a><span>/</span><a href=3D"https://substack.com=
/redirect/cfd03e05-605b-4850-8d96-328b40a4a01d?j=3DeyJ1IjoiNWtiOTN6In0.zdzy=
88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55=
,55);text-decoration: underline;">code</a><span>)</span></p><div class=3D"c=
aptioned-image-container-static" style=3D"font-size: 16px;line-height: 26px=
;margin: 32px auto;"><figure style=3D"width: 100%;margin: 0 auto;"><table c=
lass=3D"image-wrapper" width=3D"100%" border=3D"0" cellspacing=3D"0" cellpa=
dding=3D"0" data-component-name=3D"Image2ToDOMStatic" style=3D"mso-padding-=
alt: 1em 0 1.6em;"><tbody><tr><td style=3D"text-align: center;"></td><td cl=
ass=3D"content" align=3D"left" width=3D"993" style=3D"text-align: center;">=
<a class=3D"image-link" target=3D"_blank" href=3D"https://substack.com/redi=
rect/506651cb-5489-49b4-bc93-b7968d746e81?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFx=
PmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"position: relative;fl=
ex-direction: column;align-items: center;padding: 0;width: auto;height: aut=
o;border: none;text-decoration: none;display: block;margin: 0;"><img class=
=3D"wide-image" data-attrs=3D"{&quot;src&quot;:&quot;https://substack-post-=
media.s3.amazonaws.com/public/images/dd956db6-284a-4dee-b4aa-89f64dce6f6a_9=
93x763.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:nul=
l,&quot;imageSize&quot;:null,&quot;height&quot;:763,&quot;width&quot;:993,&=
quot;resizeWidth&quot;:null,&quot;bytes&quot;:475692,&quot;alt&quot;:null,&=
quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quo=
t;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;inte=
rnalRedirect&quot;:&quot;https://www.llmwatch.com/i/174513497?img=3Dhttps%3=
A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd956db6-28=
4a-4dee-b4aa-89f64dce6f6a_993x763.png&quot;,&quot;isProcessing&quot;:false,=
&quot;align&quot;:null,&quot;offset&quot;:false}" alt=3D"" width=3D"550" he=
ight=3D"422.60825780463244" src=3D"https://substackcdn.com/image/fetch/$s_!=
AQVl!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2F=
substack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd956db6-284a-4dee=
-b4aa-89f64dce6f6a_993x763.png" style=3D"border: none !important;vertical-a=
lign: middle;display: block;-ms-interpolation-mode: bicubic;height: auto;ma=
rgin-bottom: 0;width: auto !important;max-width: 100% !important;margin: 0 =
auto;"></a></td><td style=3D"text-align: center;"></td></tr></tbody></table=
></figure></div><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-he=
ight: 26px;font-size: 16px;"><strong>What problem does it solve?</strong><s=
pan> Recent breakthroughs in protein folding (like AlphaFold) rely on very =
complex model architectures tailored to capturing protein-specific geometry=
 =E2=80=93 e.g. </span><strong>triangle attention modules, pairwise distanc=
e matrices, multiple bespoke loss terms</strong><span>, etc. While extremel=
y successful, these specialized designs are computationally heavy and depar=
t significantly from the =E2=80=9Cstandard=E2=80=9D architectures used in N=
LP or vision. This raises an intriguing question: do we </span><em>really</=
em><span> need all that domain-specific complexity, or could a much simpler=
, more generic model fold proteins with similar accuracy? In other words, i=
s protein folding fundamentally simpler than current models make it seem?</=
span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 2=
6px;font-size: 16px;"><strong>How does it solve the problem?</strong><span>=
 </span><strong>SimpleFold</strong><span> is a bold attempt to strip protei=
n folding models down to basics. It uses a </span><strong>general-purpose T=
ransformer</strong><span> architecture </span><em>with no special protein-s=
pecific blocks</em><span>. Instead of the customary tricks (triangular upda=
tes, separate 2D pair representation of amino acids, etc.), it relies on st=
andard self-attention layers (augmented by some adaptive gating layers) and=
 trains them end-to-end on protein structure data. The key insight is to ca=
st protein folding as a generative modeling problem: SimpleFold is trained =
with a </span><strong>flow-matching objective</strong><span>, which is rela=
ted to diffusion models or normalizing flows, guiding it to incrementally r=
efine a random structure into the correct folded structure. They include a =
minor additional loss term to encourage correct structural predictions (so =
it=E2=80=99s not </span><em>purely</em><span> generic, but almost). They th=
en scale this model up to 3 billion parameters and train on ~9 million prot=
ein structures (including a large set of distilled/predicted ones plus expe=
rimentally solved ones). Essentially, SimpleFold treats protein coordinates=
 like a data sequence and learns to =E2=80=9Cflow=E2=80=9D them into the co=
rrect shape using a Transformer, </span><em>without</em><span> explicitly e=
ncoding protein biophysics knowledge.</span></p><p style=3D"margin: 0 0 20p=
x 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>What a=
re the key findings?</strong><span> </span><strong>SimpleFold-3B=E2=80=99s =
performance rivals state-of-the-art specialized models</strong><span> on st=
andard protein folding benchmarks. It achieves competitive accuracy in pred=
icting 3D structures, showing that a vanilla Transformer can indeed learn t=
he complex dependencies needed for folding. Moreover, SimpleFold exhibits s=
trengths where deterministic models often struggle: because it=E2=80=99s ge=
nerative, it can naturally produce </span><em>an ensemble of different prob=
able structures</em><span>. The paper notes </span><strong>strong performan=
ce in ensemble prediction</strong><span> =E2=80=93 it can sample multiple f=
oldings and capture alternative conformations, which is typically hard for =
models like AlphaFold that give one answer. Another practical benefit is ef=
ficiency: with its simpler architecture, </span><strong>SimpleFold is easie=
r to deploy and runs faster on standard hardware</strong><span> (no special=
ized ops needed). The success of SimpleFold effectively </span><strong>chal=
lenges the notion that we need highly domain-specific designs</strong><span=
> for protein folding. It opens the door to using more off-the-shelf AI com=
ponents in scientific domains. In short, the paper demonstrates that </span=
><em>much of protein folding can be learned by a generic sequence model</em=
><span>, which is a surprising and encouraging finding.</span></p><p style=
=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16=
px;"><strong>What=E2=80=99s next?</strong><span> SimpleFold=E2=80=99s appro=
ach could spark a re-evaluation of how we design models for scientific prob=
lems. If a plain Transformer works for protein structures, perhaps other ta=
sks (molecular property prediction, DNA folding, etc.) can also shift to si=
mpler architectures with the right training approach. Future work might int=
egrate SimpleFold with downstream tasks =E2=80=93 for example, coupling it =
with drug binding prediction, where its generative ensemble ability could e=
xplore multiple protein conformations. The use of a flow-matching objective=
 also hints at connections to diffusion models; one could imagine a diffusi=
on-based folding model that further improves accuracy or captures dynamics =
by simulating folding as a time series. Additionally, because SimpleFold is=
 closer to standard AI models, it could potentially benefit from </span><em=
>transfer learning</em><span>: e.g., initialize with a language model=E2=80=
=99s weights or vice versa, to inject some cross-domain knowledge (there=E2=
=80=99s early speculation that some language features help in protein seque=
nces). The big takeaway is that </span><strong>simplicity can sometimes ach=
ieve the same ends as complexity</strong><span> =E2=80=93 a valuable lesson=
 that might lead researchers to try more =E2=80=9Cminimalist=E2=80=9D basel=
ines in domains dominated by hand-engineered networks. As this trend contin=
ues, we may see a convergence where the same core model type underpins prog=
ress in both science (protein folding, chemistry) and general AI, differing=
 mostly in training data rather than architecture.</span></p><div style=3D"=
font-size: 16px;line-height: 26px;"><hr style=3D"margin: 32px 0;padding: 0;=
height: 1px;background: #313131;border: none;"></div><h2 class=3D"header-an=
chor-post" style=3D"position: relative;font-family: 'SF Pro Display',-apple=
-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Robo=
to,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe U=
I Symbol';font-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-fo=
nt-smoothing: antialiased;-webkit-appearance: optimizelegibility;-moz-appea=
rance: optimizelegibility;appearance: optimizelegibility;margin: 1em 0 0.62=
5em 0;color: rgb(54,55,55);line-height: 1.16em;font-size: 1.625em;"><strong=
>LLMs4All: A Review on Large Language Models for Research and Applications =
in Academic Disciplines</strong></h2><p style=3D"margin: 0 0 20px 0;color: =
rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>Watching:</strong=
><span> LLMs4All Survey (</span><a href=3D"https://substack.com/redirect/10=
3298ce-2766-4247-8904-28d4aa80ae10?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9=
hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,55);text-de=
coration: underline;">paper</a><span>)</span></p><div class=3D"captioned-im=
age-container-static" style=3D"font-size: 16px;line-height: 26px;margin: 32=
px auto;"><figure style=3D"width: 100%;margin: 0 auto;"><table class=3D"ima=
ge-wrapper" width=3D"100%" border=3D"0" cellspacing=3D"0" cellpadding=3D"0"=
 data-component-name=3D"Image2ToDOMStatic" style=3D"mso-padding-alt: 1em 0 =
1.6em;"><tbody><tr><td style=3D"text-align: center;"></td><td class=3D"cont=
ent" align=3D"left" width=3D"1103" style=3D"text-align: center;"><a class=
=3D"image-link" target=3D"_blank" href=3D"https://substack.com/redirect/21f=
17adf-f5c2-4444-9aa8-ab8e364fe995?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9h=
shgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"position: relative;flex-direc=
tion: column;align-items: center;padding: 0;width: auto;height: auto;border=
: none;text-decoration: none;display: block;margin: 0;"><img class=3D"wide-=
image" data-attrs=3D"{&quot;src&quot;:&quot;https://substack-post-media.s3.=
amazonaws.com/public/images/f9ea97bb-b4ba-489b-982f-d245dab9c436_1103x515.p=
ng&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;=
imageSize&quot;:null,&quot;height&quot;:515,&quot;width&quot;:1103,&quot;re=
sizeWidth&quot;:null,&quot;bytes&quot;:150344,&quot;alt&quot;:null,&quot;ti=
tle&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null=
,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRed=
irect&quot;:&quot;https://www.llmwatch.com/i/174513497?img=3Dhttps%3A%2F%2F=
substack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9ea97bb-b4ba-489b=
-982f-d245dab9c436_1103x515.png&quot;,&quot;isProcessing&quot;:false,&quot;=
align&quot;:null,&quot;offset&quot;:false}" alt=3D"" width=3D"550" height=
=3D"256.79963735267455" src=3D"https://substackcdn.com/image/fetch/$s_!ELiF=
!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubs=
tack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9ea97bb-b4ba-489b-982=
f-d245dab9c436_1103x515.png" style=3D"border: none !important;vertical-alig=
n: middle;display: block;-ms-interpolation-mode: bicubic;height: auto;margi=
n-bottom: 0;width: auto !important;max-width: 100% !important;margin: 0 aut=
o;"></a></td><td style=3D"text-align: center;"></td></tr></tbody></table></=
figure></div><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-heigh=
t: 26px;font-size: 16px;"><strong>What problem does it solve?</strong><span=
> The impact of LLMs is </span><em>not</em><span> confined to computer scie=
nce =E2=80=93 they are rapidly permeating every academic field from history=
 to biology. However, knowledge of </span><strong>how to effectively use LL=
Ms in these diverse disciplines</strong><span> is scattered. Researchers in=
, say, law or chemistry might not be up-to-date on the latest LLM technique=
s relevant to their field. This paper addresses the need for a comprehensiv=
e </span><em>survey</em><span> that brings together the state-of-the-art LL=
M applications, opportunities, and challenges across the full spectrum of a=
cademic research areas.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb=
(54,55,55);line-height: 26px;font-size: 16px;"><strong>How does it solve th=
e problem?</strong><span> </span><strong>LLMs4All</strong><span> serves as =
an extensive review and guide, spanning </span><strong>three broad domains =
of academia</strong><span> and detailing how LLMs are being applied in each=
. The authors categorize fields into: (1) </span><strong>Arts, Humanities, =
and Law</strong><span> (like history, philosophy, political science, archit=
ecture, legal studies), (2) </span><strong>Economics and Business</strong><=
span> (finance, marketing, management, etc.), and (3) </span><strong>Scienc=
e and Engineering</strong><span> (mathematics, physics, biology, chemistry,=
 earth sciences, computer science, etc.). For each area, the survey outline=
s current use cases of LLMs in research and practice =E2=80=93 for instance=
, assisting historical text analysis, aiding legal document summarization, =
generating hypotheses in scientific research =E2=80=93 with examples of sta=
te-of-the-art models or systems in that domain. It also discusses how LLM c=
apabilities (like text generation, reasoning, coding, multilingual understa=
nding) are tailored or fine-tuned to meet discipline-specific needs. Beyond=
 applications, the review addresses </span><strong>key limitations and chal=
lenges</strong><span> in each field: data privacy in medicine, factual accu=
racy in history, ethical concerns in law (like bias or fairness), etc., as =
well as </span><em>open research questions</em><span> and future directions=
 for integrating LLMs. In effect, LLMs4All acts as a bridge between the AI =
frontier and domain experts, summarizing =E2=80=9Cwhat LLM can do for X fie=
ld=E2=80=9D in one place.</span></p><p style=3D"margin: 0 0 20px 0;color: r=
gb(54,55,55);line-height: 26px;font-size: 16px;"><strong>What are the key f=
indings?</strong><span> The survey=E2=80=99s primary contribution is qualit=
ative synthesis, but it offers important </span><strong>insights and observ=
ations</strong><span>. One overarching finding is that </span><em>virtually=
 no discipline is untouched</em><span> =E2=80=93 from using GPT-4 to draft =
legal contracts to employing generative models for experimental design in c=
hemistry, academia is experiencing a wave of LLM-driven innovation. However=
, the review notes that the maturity varies: some fields (like computer sci=
ence and law) already have numerous LLM applications, while others (say, ph=
ilosophy or the arts) are still exploring initial use cases. Common challen=
ges emerge across disciplines, such as concerns over LLMs generating plausi=
ble-sounding but incorrect information (hallucinations) which could mislead=
 non-expert users in fields like medicine or finance. The paper highlights =
that </span><strong>interdisciplinary collaboration</strong><span> is key =
=E2=80=93 e.g. combining an LLM with domain-specific knowledge bases or mod=
els yields better results (as seen in tools for scientific discovery that u=
se LLMs plus chemistry rules). A positive finding is that LLMs are acting a=
s a </span><em>democratizing force</em><span> in research: they enable indi=
viduals without advanced technical training to leverage AI for their domain=
 problems (for example, historians using GPT to translate and summarize anc=
ient texts). The survey also compiles best practices and ethical guidelines=
 that are emerging as communities grapple with responsible LLM use (such as=
 disclosure policies in academic writing if AI was used). Altogether, </spa=
n><strong>LLMs4All provides a roadmap</strong><span> for researchers in eac=
h field to understand the current landscape and to identify how they might =
use LLMs in their own work.</span></p><p style=3D"margin: 0 0 20px 0;color:=
 rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>Why does it matt=
er?</strong><span> As generative AI becomes as fundamental as data analysis=
, having a clear view of its role in each discipline is crucial. This surve=
y will help </span><em>educators and policymakers</em><span> as well =E2=80=
=93 for instance, university departments can refer to it when updating curr=
icula to include AI literacy relevant to their field. By documenting limita=
tions and future directions, the paper also points AI researchers towards i=
mportant unsolved problems (like improving factuality for scientific Q&amp;=
A, or aligning LLMs with legal reasoning). One likely outcome is that the s=
urvey will spur </span><strong>cross-disciplinary collaborations</strong><s=
pan>: a biologist reading about LLM use in chemistry might team up with AI =
experts to apply similar techniques to biology. It also emphasizes that des=
pite the hype, current LLMs have serious shortcomings in specialized domain=
s =E2=80=93 thus tempering expectations and encouraging more research on re=
liability, which is a recurring theme. In summary, </span><em>LLMs4All</em>=
<span> matters because it catalogues the transformation happening at the in=
tersection of AI and every other field, ensuring that knowledge is shared b=
roadly rather than siloed, and helping to steer the next phase of research =
where </span><strong>AI truly becomes an every-discipline tool</strong><spa=
n>.</span></p><div style=3D"font-size: 16px;line-height: 26px;"><hr style=
=3D"margin: 32px 0;padding: 0;height: 1px;background: #313131;border: none;=
"></div><h2 class=3D"header-anchor-post" style=3D"position: relative;font-f=
amily: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,Blin=
kMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Em=
oji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoo=
thing: antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearance:=
 optimizelegibility;-moz-appearance: optimizelegibility;appearance: optimiz=
elegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16e=
m;font-size: 1.625em;"><strong>Language Models that Think, Chat Better</str=
ong></h2><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 2=
6px;font-size: 16px;"><strong>Watching:</strong><span> RLMT (</span><a href=
=3D"https://substack.com/redirect/3b0bd9ae-8207-46a9-aca2-52bd861e7121?j=3D=
eyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" s=
tyle=3D"color: rgb(54,55,55);text-decoration: underline;">paper</a><span>/<=
/span><a href=3D"https://substack.com/redirect/edcdd08a-b6d0-44b5-8ef0-a861=
45f37ea8?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw=
0" rel=3D"" style=3D"color: rgb(54,55,55);text-decoration: underline;">code=
</a><span>)</span></p><div class=3D"captioned-image-container-static" style=
=3D"font-size: 16px;line-height: 26px;margin: 32px auto;"><figure style=3D"=
width: 100%;margin: 0 auto;"><table class=3D"image-wrapper" width=3D"100%" =
border=3D"0" cellspacing=3D"0" cellpadding=3D"0" data-component-name=3D"Ima=
ge2ToDOMStatic" style=3D"mso-padding-alt: 1em 0 1.6em;"><tbody><tr><td styl=
e=3D"text-align: center;"></td><td class=3D"content" align=3D"left" width=
=3D"569" style=3D"text-align: center;"><a class=3D"image-link" target=3D"_b=
lank" href=3D"https://substack.com/redirect/e49e381c-8081-4eea-8ef5-bc4dd00=
6f665?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" =
rel=3D"" style=3D"position: relative;flex-direction: column;align-items: ce=
nter;padding: 0;width: auto;height: auto;border: none;text-decoration: none=
;display: block;margin: 0;"><img class=3D"wide-image" data-attrs=3D"{&quot;=
src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/=
d8638651-490b-47e9-8288-c7e8758f3f87_569x793.png&quot;,&quot;srcNoWatermark=
&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;he=
ight&quot;:793,&quot;width&quot;:569,&quot;resizeWidth&quot;:null,&quot;byt=
es&quot;:140704,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot=
;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true=
,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.=
llmwatch.com/i/174513497?img=3Dhttps%3A%2F%2Fsubstack-post-media.s3.amazona=
ws.com%2Fpublic%2Fimages%2Fd8638651-490b-47e9-8288-c7e8758f3f87_569x793.png=
&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&q=
uot;:false}" alt=3D"" width=3D"550" height=3D"766.5202108963093" src=3D"htt=
ps://substackcdn.com/image/fetch/$s_!xjEI!,w_1100,c_limit,f_auto,q_auto:goo=
d,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2=
Fpublic%2Fimages%2Fd8638651-490b-47e9-8288-c7e8758f3f87_569x793.png" style=
=3D"border: none !important;vertical-align: middle;display: block;-ms-inter=
polation-mode: bicubic;height: auto;margin-bottom: 0;width: auto !important=
;max-width: 100% !important;margin: 0 auto;"></a></td><td style=3D"text-ali=
gn: center;"></td></tr></tbody></table></figure></div><p style=3D"margin: 0=
 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>=
What problem does it solve?</strong><span> Reinforcement learning from huma=
n feedback (RLHF) has become a standard way to finetune chat LLMs to be mor=
e helpful and safe. However, RLHF optimizes only the </span><em>final answe=
r</em><span> the model gives, not the reasoning process behind it =E2=80=93=
 the model might produce some hidden chain-of-thought internally, but the r=
eward model only judges the end response. This can lead to answers that sou=
nd good but aren=E2=80=99t deeply reasoned. There=E2=80=99s another approac=
h, </span><strong>RL with verifiable rewards (RLVR)</strong><span>, which f=
orces the model to output checkable working (like a math proof or code test=
) and rewards correctness. RLVR yields better reasoning but only works in d=
omains with objective checks. The open problem addressed here is: </span><e=
m>Can we get the generality of RLHF while also encouraging the model to act=
ually think through problems?</em></p><p style=3D"margin: 0 0 20px 0;color:=
 rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>How does it solv=
e the problem?</strong><span> The solution is a new training paradigm calle=
d </span><strong>RLMT (Reinforcement Learning with Model-rewarded Thinking)=
</strong><span>. In RLMT, during training the model is required to generate=
 a </span><strong>long chain-of-thought (CoT)</strong><span> </span><em>bef=
ore</em><span> its final answer. This CoT might be a detailed outline, step=
-by-step reasoning, or intermediate =E2=80=9Cthoughts.=E2=80=9D A reward mo=
del (the same kind used in RLHF, pretrained on human preference data) then =
scores not just the final answer but the combination of the CoT and the ans=
wer. Essentially, the model is rewarded for producing helpful reasoning tha=
t leads to a good answer. They implement this by using diverse real-world p=
rompts (open-ended tasks like writing an essay, planning a meal, answering =
a complex question) and apply policy gradient methods (PPO, DPO, etc.) to o=
ptimize the LLM=E2=80=99s policy to output better thought+answer pairs. The=
y do this across </span><em>40 separate training runs</em><span> on two bas=
e models (Llama-3.1 8B and Qwen-7B) under different settings to ensure the =
approach is robust. Importantly, they also explore </span><strong>training =
from scratch with RLMT (R1-Zero)</strong><span> =E2=80=93 meaning they take=
 a base model with no supervised fine-tuning and directly apply RLMT, to se=
e if they can skip the usual instruction-tuning phase.</span></p><p style=
=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16=
px;"><strong>What are the key findings?</strong><span> RLMT-trained models =
</span><strong>consistently outperform standard RLHF-trained models</strong=
><span> on a wide array of evaluations. For instance, on three different op=
en-ended chat benchmarks (AlpacaEval2, WildBench, Arena Hard), the RLMT mod=
els scored </span><strong>3=E2=80=937 points higher</strong><span> than equ=
ivalent RLHF models =E2=80=93 a sizable jump in quality. They also saw gene=
ral ability improvements, like +1=E2=80=933 point gains on creative writing=
 tasks and knowledge quizzes. Perhaps most impressively, their best RLMT-tu=
ned 8B model actually </span><strong>surpassed the performance of GPT-4 (op=
en variant)</strong><span> on those chat benchmarks and creative tasks, and=
 even approached </span><strong>Claude 2=E2=80=99s</strong><span> level on =
one benchmark. This is a remarkable result given the model is an order of m=
agnitude smaller than GPT-4. Another striking finding: an 8B Llama trained =
with only </span><em>7,000 RLMT prompts</em><span> (no supervised fine-tune=
 at all) </span><em>outperformed</em><span> the official Llama-3.1-8B that =
had been instruction-tuned on 25 million examples. In other words, a few th=
ousand carefully chosen scenarios with think-before-answer optimization bea=
t a massive conventional training =E2=80=93 that speaks to how powerfully e=
fficient RLMT is. Qualitatively, the authors observed the RLMT models produ=
ce more structured and thoughtful responses (e.g. making lists, reasoning o=
ut loud, considering alternatives) and fewer failure modes like going off-t=
opic. The results strongly suggest that </span><strong>rewarding the thinki=
ng process leads to measurably better chat performance</strong><span> than =
rewarding final answers alone.</span></p><p style=3D"margin: 0 0 20px 0;col=
or: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>What=E2=80=99=
s next?</strong><span> This work may prompt a paradigm shift in how we trai=
n conversational agents. Rather than treating chain-of-thought as just an o=
ptional byproduct, it might become </span><em>standard to explicitly train =
LLMs to articulate their reasoning</em><span>. Future directions include co=
mbining RLMT with </span><em>human-in-the-loop</em><span>: e.g., having hum=
an feedback not only on answers but on intermediate thoughts to further ref=
ine the reward model for reasoning quality (beyond what the existing prefer=
ence model can do). Also, applying RLMT to larger models (the paper did 8B;=
 doing this at 34B or 70B could yield even more powerful models, potentiall=
y surpassing even larger closed models in some areas). Another consideratio=
n is real-world deployment: RLMT models, by explaining more, might be </spa=
n><strong>more interpretable</strong><span>, which is a boon for safety =E2=
=80=93 but it also means they might divulge their =E2=80=9Cthoughts=E2=80=
=9D even when not prompted to, which could be tuned as needed. Finally, thi=
s research calls for understanding </span><em>why</em><span> RLMT is so eff=
ective: does the reward model indirectly favor certain structures in CoT th=
at align better with human preferences, or does the act of generating a lon=
ger context help the model avoid mistakes? Answering these questions could =
further improve training. All in all, </span><strong>Language Models that <=
/strong><em><strong>think</strong></em><strong> truly do chat better</stron=
g><span>, and we can expect the next generation of AI assistants to be much=
 more explicit in their reasoning as a result of techniques like this.</spa=
n></p><div style=3D"font-size: 16px;line-height: 26px;"><hr style=3D"margin=
: 32px 0;padding: 0;height: 1px;background: #313131;border: none;"></div><h=
2 class=3D"header-anchor-post" style=3D"position: relative;font-family: 'SF=
 Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystem=
Font,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Sego=
e UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoothing: ant=
ialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearance: optimizel=
egibility;-moz-appearance: optimizelegibility;appearance: optimizelegibilit=
y;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-siz=
e: 1.625em;"><strong>SciReasoner: Laying the Scientific Reasoning Ground Ac=
ross Disciplines</strong></h2><p style=3D"margin: 0 0 20px 0;color: rgb(54,=
55,55);line-height: 26px;font-size: 16px;"><strong>Watching:</strong><span>=
 SciReasoner (</span><a href=3D"https://substack.com/redirect/bdb6da27-4887=
-4443-8af6-b3677a6f354c?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnP=
G6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,55);text-decoration: u=
nderline;">paper</a><span>/</span><a href=3D"https://substack.com/redirect/=
92489bd1-1226-40aa-8199-2a2a18a3cc8b?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJ=
f9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,55);text-=
decoration: underline;">code</a><span>)</span></p><div class=3D"captioned-i=
mage-container-static" style=3D"font-size: 16px;line-height: 26px;margin: 3=
2px auto;"><figure style=3D"width: 100%;margin: 0 auto;"><table class=3D"im=
age-wrapper" width=3D"100%" border=3D"0" cellspacing=3D"0" cellpadding=3D"0=
" data-component-name=3D"Image2ToDOMStatic" style=3D"mso-padding-alt: 1em 0=
 1.6em;"><tbody><tr><td style=3D"text-align: center;"></td><td class=3D"con=
tent" align=3D"left" width=3D"971" style=3D"text-align: center;"><a class=
=3D"image-link" target=3D"_blank" href=3D"https://substack.com/redirect/ed5=
2a4cd-8594-456e-b389-37a5a88c0144?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9h=
shgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"position: relative;flex-direc=
tion: column;align-items: center;padding: 0;width: auto;height: auto;border=
: none;text-decoration: none;display: block;margin: 0;"><img class=3D"wide-=
image" data-attrs=3D"{&quot;src&quot;:&quot;https://substack-post-media.s3.=
amazonaws.com/public/images/56492b2d-99e9-4e46-8eea-4c2de95f0741_971x1003.p=
ng&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;=
imageSize&quot;:null,&quot;height&quot;:1003,&quot;width&quot;:971,&quot;re=
sizeWidth&quot;:null,&quot;bytes&quot;:486989,&quot;alt&quot;:null,&quot;ti=
tle&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null=
,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRed=
irect&quot;:&quot;https://www.llmwatch.com/i/174513497?img=3Dhttps%3A%2F%2F=
substack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56492b2d-99e9-4e46=
-8eea-4c2de95f0741_971x1003.png&quot;,&quot;isProcessing&quot;:false,&quot;=
align&quot;:null,&quot;offset&quot;:false}" alt=3D"" width=3D"550" height=
=3D"568.1256436663234" src=3D"https://substackcdn.com/image/fetch/$s_!f2Ru!=
,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubst=
ack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56492b2d-99e9-4e46-8eea=
-4c2de95f0741_971x1003.png" style=3D"border: none !important;vertical-align=
: middle;display: block;-ms-interpolation-mode: bicubic;height: auto;margin=
-bottom: 0;width: auto !important;max-width: 100% !important;margin: 0 auto=
;"></a></td><td style=3D"text-align: center;"></td></tr></tbody></table></f=
igure></div><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height=
: 26px;font-size: 16px;"><strong>What problem does it solve?</strong><span>=
 Science-focused LLMs to date have mostly been </span><em>specialists</em><=
span> =E2=80=93 models fine-tuned for a particular domain (like chemistry, =
or a math theorem solver). Real scientific research, however, often spans m=
ultiple disciplines and data formats (imagine linking a biology finding to =
a chemistry theory, with equations and text). There is a need for a </span>=
<strong>foundation model for scientific reasoning</strong><span> that can u=
nderstand </span><em>natural language</em><span> questions, but also handle=
 formulas, sequences (DNA, protein sequences), tables of properties, and mo=
re in a unified way. In short, the goal is to create an AI scientist that i=
sn=E2=80=99t siloed to one field, but has a broad, cross-disciplinary reaso=
ning ability with the various representations science uses.</span></p><p st=
yle=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size:=
 16px;"><strong>How does it solve the problem?</strong><span> </span><stron=
g>SciReasoner</strong><span> is built through an extensive multi-stage trai=
ning process to align language with diverse scientific representations. Fir=
st, it is </span><strong>pre-trained on a 206 billion token corpus</strong>=
<span> that includes not just scientific text from many fields, but also pu=
rely symbolic sequences and mixed sequence-text data. This means it sees th=
ings like amino acid sequences, chemical SMILES strings, math equations, al=
ongside explanatory text. After this large pre-training, they perform </spa=
n><strong>supervised fine-tuning on 40 million science-related instructions=
</strong><span>, covering an enormous range of tasks (the model supports </=
span><em>103 different scientific tasks</em><span>). These tasks fall into =
families: (i) translating between text and scientific formats (e.g., =E2=80=
=9Cdescribe this molecule structure in words=E2=80=9D and vice versa), (ii)=
 extracting knowledge from text or figures, (iii) predicting properties (gi=
ven a compound, predict melting point, etc.), (iv) classifying properties (=
e.g. classify a star as red dwarf or not from data), and (v) generating or =
designing sequences (like proposing a DNA sequence with certain properties)=
. After supervised tuning, they apply an </span><em>=E2=80=9Cannealed cold-=
start=E2=80=9D bootstrapping</em><span> to specifically teach the model </s=
pan><strong>long-form chain-of-thought reasoning</strong><span> for scienti=
fic problems. This likely involves prompting the model to generate step-by-=
step solutions for complex questions and using those as additional training=
 data (gradually increasing the complexity, hence =E2=80=9Cannealed=E2=80=
=9D). Finally, they use </span><strong>reinforcement learning with custom r=
eward shaping</strong><span> for scientific reasoning. This last step proba=
bly gives the model feedback on intermediate steps (like units consistency,=
 equation correctness, logical coherence) to firmly instill deliberate, rig=
orous reasoning habits. All training artifacts (model weights, instruction =
data, evaluation code) are released openly, making SciReasoner a community =
resource.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);lin=
e-height: 26px;font-size: 16px;"><strong>What are the key findings?</strong=
><span> SciReasoner emerges as a </span><em>single</em><span> model capable=
 of handling tasks that previously would require an ensemble of separate to=
ols. Compared to specialist models or baselines, SciReasoner shows </span><=
strong>broader instruction coverage</strong><span>, better cross-domain gen=
eralization, and higher fidelity in its outputs. For example, it can take a=
 chemistry problem described in text and output a step-by-step solution wit=
h equations, or translate a genomic sequence into a likely function descrip=
tion =E2=80=93 tasks bridging language and formal data =E2=80=93 with notab=
le accuracy. The paper indicates that training on multiple disciplines toge=
ther actually led to </span><em>improved transfer learning</em><span>: solv=
ing tasks in one domain improved its performance in others, because it lear=
ned general scientific reasoning strategies. This cross-pollination strengt=
hened the model=E2=80=99s reliability; e.g., the rigor it learned from phys=
ics equations helped it avoid errors in, say, accounting calculations. In e=
valuations, SciReasoner performed on par with or better than domain-specifi=
c models on many benchmarks, despite not being an expert of any single fiel=
d. And on challenges that require mixing knowledge (like a question involvi=
ng both biology and chemistry), it had a clear advantage. Essentially, </sp=
an><strong>SciReasoner lays a groundwork</strong><span>: it demonstrates th=
at one model can be a competent physicist, chemist, biologist, and more at =
the same time, and that this union actually makes each facet stronger. This=
 is a step toward AI that can reason across the entirety of science. The op=
en-sourcing of the model and its data is also a major outcome =E2=80=93 it =
provides the community with a powerful base to finetune further or to bench=
mark for scientific QA, hypothesis generation, etc., accelerating research =
in scientific AI.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55=
,55);line-height: 26px;font-size: 16px;"><strong>What=E2=80=99s next?</stro=
ng><span> SciReasoner opens up a host of new possibilities. In the near ter=
m, researchers might build on it to create specialized agents =E2=80=93 for=
 instance, a robot scientist that uses SciReasoner to generate experiments,=
 then executes them in a lab simulation. The training techniques (like the =
massive multi-format pre-training and the careful staged alignment) could b=
e applied to other domains requiring multi-representation reasoning, such a=
s economics (mixing text with spreadsheets and formulas) or social sciences=
 (mixing text and statistical data). Another likely direction is scaling: S=
ciReasoner-8B is impressive, but imagine a 70B model trained similarly =E2=
=80=93 it could potentially approach expert human level in many fields. The=
re will also be work on </span><em>evaluation</em><span>: the model covers =
103 tasks, but how do we thoroughly verify its reasoning quality and factua=
l accuracy in each? New interdisciplinary benchmarks may arise from this. F=
inally, SciReasoner=E2=80=99s release fosters a </span><strong>culture of o=
pen scientific AI</strong><span> =E2=80=93 as more researchers use and impr=
ove it, we could see a virtuous cycle leading to an =E2=80=9CAI Scientific =
Assistant=E2=80=9D that any researcher can use to boost their work, much li=
ke a powerful but wide-ranging colleague who=E2=80=99s read every textbook.=
 The long-term vision is an AI that can cross-pollinate insights between di=
sciplines (say, use a physics principle to solve a biology problem), and Sc=
iReasoner is a foundational step in that direction, illustrating the value =
of broad training for broad thinking in AI.</span></p><div style=3D"font-si=
ze: 16px;line-height: 26px;"><hr style=3D"margin: 32px 0;padding: 0;height:=
 1px;background: #313131;border: none;"></div><h3 class=3D"header-anchor-po=
st" style=3D"position: relative;font-family: 'SF Pro Display',-apple-system=
-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helv=
etica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbo=
l';font-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoo=
thing: antialiased;-webkit-appearance: optimizelegibility;-moz-appearance: =
optimizelegibility;appearance: optimizelegibility;margin: 1em 0 0.625em 0;c=
olor: rgb(54,55,55);line-height: 1.16em;font-size: 1.375em;">=E2=9D=A4=EF=
=B8=8F If you enjoyed this article, give it a like and share it with your p=
eers.</h3><div style=3D"font-size: 16px;line-height: 26px;"><hr style=3D"ma=
rgin: 32px 0;padding: 0;height: 1px;background: #313131;border: none;"></di=
v><p class=3D"button-wrapper" data-attrs=3D"{&quot;url&quot;:&quot;https://=
substack.com/app-link/post?publication_id=3D1428667&amp;post_id=3D174513497=
&amp;utm_source=3Dsubstack&amp;utm_medium=3Demail&amp;isFreemail=3Dtrue&amp=
;comments=3Dtrue&amp;token=3DeyJ1c2VyX2lkIjozMzY0NDgyMjMsInBvc3RfaWQiOjE3ND=
UxMzQ5NywiaWF0IjoxNzU4OTAwNzU5LCJleHAiOjE3NjE0OTI3NTksImlzcyI6InB1Yi0xNDI4N=
jY3Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.UIE1K4BxgVd7fxVfJHmvWAfy8qmWiUO20E4a0rR=
OGQs&amp;r=3D5kb93z&amp;utm_campaign=3Demail-half-magic-comments&amp;action=
=3Dpost-comment&quot;,&quot;text&quot;:&quot;Leave a comment&quot;,&quot;ac=
tion&quot;:null,&quot;class&quot;:null}" data-component-name=3D"ButtonCreat=
eButton" style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px=
;font-size: 16px;text-align: center;cursor: pointer;border-radius: 4px;"><a=
 class=3D"button primary" href=3D"https://substack.com/app-link/post?public=
ation_id=3D1428667&amp;post_id=3D174513497&amp;utm_source=3Dsubstack&amp;ut=
m_medium=3Demail&amp;isFreemail=3Dtrue&amp;comments=3Dtrue&amp;token=3DeyJ1=
c2VyX2lkIjozMzY0NDgyMjMsInBvc3RfaWQiOjE3NDUxMzQ5NywiaWF0IjoxNzU4OTAwNzU5LCJ=
leHAiOjE3NjE0OTI3NTksImlzcyI6InB1Yi0xNDI4NjY3Iiwic3ViIjoicG9zdC1yZWFjdGlvbi=
J9.UIE1K4BxgVd7fxVfJHmvWAfy8qmWiUO20E4a0rROGQs&amp;r=3D5kb93z&amp;utm_campa=
ign=3Demail-half-magic-comments&amp;action=3Dpost-comment" rel=3D"" style=
=3D"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Robo=
to,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe U=
I Symbol';display: inline-block;box-sizing: border-box;cursor: pointer;bord=
er: none;border-radius: 8px;font-size: 14px;line-height: 20px;font-weight: =
600;text-align: center;margin: 0;opacity: 1;outline: none;white-space: nowr=
ap;color: #363737 !important;text-decoration: none !important;background-co=
lor: #ffca4b;padding: 12px 20px;height: auto;"><span style=3D"color: #36373=
7;text-decoration: none;">Leave a comment</span></a></p><div class=3D"subsc=
ription-widget-wrap" style=3D"font-size: 16px;line-height: 26px;margin-bott=
om: 0;"><div class=3D"subscription-widget show-subscribe" style=3D"font-siz=
e: 16px;direction: ltr !important;font-weight: 400;text-decoration: none;fo=
nt-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Hel=
vetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symb=
ol';color: #363737;line-height: 1.5;max-width: 560px;margin: 24px auto;alig=
n-items: flex-start;display: block;text-align: center;padding: 0px 32px;"><=
div class=3D"preamble" style=3D"margin-top: 16px;font-family: system-ui,-ap=
ple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,=
'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-size: 18px;max-=
width: 384px;width: fit-content;line-height: 22px;display: flex;align-items=
: center;text-align: center;font-weight: 400;margin-left: auto;margin-right=
: auto;"><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 2=
6px;font-size: 16px;">Thanks for reading LLM Watch! Subscribe for free to r=
eceive new posts and support my work</p></div><div class=3D"subscribe-widge=
t is-signed-up" data-component-name=3D"SubscribeWidget" style=3D"margin: 0 =
0 1em;direction: ltr;font-size: 16px;line-height: 26px;"><div class=3D"penc=
raft pc-reset button-wrapper" style=3D"text-decoration: unset;list-style: n=
one;font-size: 16px;line-height: 26px;text-align: center;cursor: pointer;bo=
rder-radius: 4px;"><a class=3D"button subscribe-btn primary" href=3D"https:=
//substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly93d3cubGxtd2F0Y2guY29tL3N1YnNjc=
mliZT91dG1fc291cmNlPXBvc3QmdXRtX2NhbXBhaWduPWVtYWlsLWNoZWNrb3V0Jm5leHQ9aHR0=
cHMlM0ElMkYlMkZ3d3cubGxtd2F0Y2guY29tJTJGcCUyRjktcGFwZXJzLXlvdS1zaG91bGQta25=
vdy1hYm91dC0xZjUmcj01a2I5M3omdG9rZW49ZXlKMWMyVnlYMmxrSWpvek16WTBORGd5TWpNc0=
ltbGhkQ0k2TVRjMU9Ea3dNRGMxT1N3aVpYaHdJam94TnpZeE5Ea3lOelU1TENKcGMzTWlPaUp3Z=
FdJdE1UUXlPRFkyTnlJc0luTjFZaUk2SW1Ob1pXTnJiM1YwSW4wLk9JWWpaN1Y1SFRmZGdGMlJW=
VjViaExUcFQ1WHZ6Y0QtNkl1NFFQNlJjQTAiLCJwIjoxNzQ1MTM0OTcsInMiOjE0Mjg2NjcsImY=
iOnRydWUsInUiOjMzNjQ0ODIyMywiaWF0IjoxNzU4OTAwNzU5LCJleHAiOjIwNzQ0NzY3NTksIm=
lzcyI6InB1Yi0wIiwic3ViIjoibGluay1yZWRpcmVjdCJ9.T6ioyksdQtR2c0oMxyJ71AOQcex_=
mqAWjzomptlXKyw?&amp;utm_medium=3Demail&amp;utm_source=3Dsubscribe-widget-p=
reamble&amp;utm_content=3D174513497" style=3D"font-family: system-ui,-apple=
-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Ap=
ple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';display: inline-block;b=
ox-sizing: border-box;cursor: pointer;border: none;border-radius: 8px;font-=
size: 14px;line-height: 20px;font-weight: 600;text-align: center;opacity: 1=
;outline: none;white-space: nowrap;text-decoration: none !important;margin:=
 0 auto;background-color: #ffca4b;color: #363737 !important;padding: 12px 2=
0px;height: auto;"><span style=3D"color: #363737;text-decoration: none;">Up=
grade to paid</span></a></div></div></div></div></div></div><div class=3D"c=
ontainer-border" style=3D"margin: 32px 0 0;width: 100%;box-sizing: border-b=
ox;border-top: 1px solid #313131;font-size: 16px;line-height: 26px;"></div>=
<div class=3D"post-cta typography markup" style=3D"--image-offset-margin: -=
120px;text-align: initial;word-break: break-word;margin-bottom: 32px;margin=
: 32px 0;font-size: 16px;line-height: 26px;"><p style=3D"color: rgb(54,55,5=
5);margin: 0 auto 20px;text-align: center;width: 90%;line-height: 26px;font=
-size: 16px;margin-top: 0;"><span class=3D"pencraft pc-reset reset-IxiVJZ" =
translated=3D"" style=3D"list-style: none;color: unset;text-decoration: uns=
et;margin: 0;">You're currently a free subscriber to <a href=3D"https://sub=
stack.com/redirect/934dcdbc-1d57-4d7a-bd9d-092a275469ad?j=3DeyJ1IjoiNWtiOTN=
6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" style=3D"color: rgb(54,55=
,55);text-decoration: underline;">LLM Watch</a>. For the full experience, <=
a href=3D"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly93d3cubGxtd2F0Y=
2guY29tL3N1YnNjcmliZT91dG1fc291cmNlPXBvc3QmdXRtX2NhbXBhaWduPWVtYWlsLWNoZWNr=
b3V0Jm5leHQ9aHR0cHMlM0ElMkYlMkZ3d3cubGxtd2F0Y2guY29tJTJGcCUyRjktcGFwZXJzLXl=
vdS1zaG91bGQta25vdy1hYm91dC0xZjUmcj01a2I5M3omdG9rZW49ZXlKMWMyVnlYMmxrSWpvek=
16WTBORGd5TWpNc0ltbGhkQ0k2TVRjMU9Ea3dNRGMxT1N3aVpYaHdJam94TnpZeE5Ea3lOelU1T=
ENKcGMzTWlPaUp3ZFdJdE1UUXlPRFkyTnlJc0luTjFZaUk2SW1Ob1pXTnJiM1YwSW4wLk9JWWpa=
N1Y1SFRmZGdGMlJWVjViaExUcFQ1WHZ6Y0QtNkl1NFFQNlJjQTAiLCJwIjoxNzQ1MTM0OTcsInM=
iOjE0Mjg2NjcsImYiOnRydWUsInUiOjMzNjQ0ODIyMywiaWF0IjoxNzU4OTAwNzU5LCJleHAiOj=
IwNzQ0NzY3NTksImlzcyI6InB1Yi0wIiwic3ViIjoibGluay1yZWRpcmVjdCJ9.T6ioyksdQtR2=
c0oMxyJ71AOQcex_mqAWjzomptlXKyw?&amp;utm_source=3Dsubstack&amp;utm_medium=
=3Demail&amp;utm_content=3Dpostcta" style=3D"color: rgb(54,55,55);text-deco=
ration: underline;">upgrade your subscription.</a></span></p><p class=3D"ct=
a-box" style=3D"color: rgb(54,55,55);margin: 0 auto 20px;width: 90%;line-he=
ight: 26px;font-size: 16px;margin-bottom: 0;text-align: center;margin-left:=
 auto;margin-right: auto;"><a class=3D"button primary" role=3D"button" href=
=3D"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly93d3cubGxtd2F0Y2guY29=
tL3N1YnNjcmliZT91dG1fc291cmNlPXBvc3QmdXRtX2NhbXBhaWduPWVtYWlsLWNoZWNrb3V0Jm=
5leHQ9aHR0cHMlM0ElMkYlMkZ3d3cubGxtd2F0Y2guY29tJTJGcCUyRjktcGFwZXJzLXlvdS1za=
G91bGQta25vdy1hYm91dC0xZjUmcj01a2I5M3omdG9rZW49ZXlKMWMyVnlYMmxrSWpvek16WTBO=
RGd5TWpNc0ltbGhkQ0k2TVRjMU9Ea3dNRGMxT1N3aVpYaHdJam94TnpZeE5Ea3lOelU1TENKcGM=
zTWlPaUp3ZFdJdE1UUXlPRFkyTnlJc0luTjFZaUk2SW1Ob1pXTnJiM1YwSW4wLk9JWWpaN1Y1SF=
RmZGdGMlJWVjViaExUcFQ1WHZ6Y0QtNkl1NFFQNlJjQTAiLCJwIjoxNzQ1MTM0OTcsInMiOjE0M=
jg2NjcsImYiOnRydWUsInUiOjMzNjQ0ODIyMywiaWF0IjoxNzU4OTAwNzU5LCJleHAiOjIwNzQ0=
NzY3NTksImlzcyI6InB1Yi0wIiwic3ViIjoibGluay1yZWRpcmVjdCJ9.T6ioyksdQtR2c0oMxy=
J71AOQcex_mqAWjzomptlXKyw?&amp;utm_source=3Dsubstack&amp;utm_medium=3Demail=
&amp;utm_content=3Dpostcta" style=3D"font-family: system-ui,-apple-system,B=
linkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color=
 Emoji','Segoe UI Emoji','Segoe UI Symbol';display: inline-block;box-sizing=
: border-box;cursor: pointer;border: none;height: 40px;border-radius: 8px;f=
ont-size: 14px;line-height: 20px;font-weight: 600;text-align: center;paddin=
g: 10px 20px;margin: 0;opacity: 1;outline: none;white-space: nowrap;color: =
#363737 !important;text-decoration: none !important;background-color: #ffca=
4b;">Upgrade to paid</a></p></div><table class=3D"email-ufi-2-bottom" role=
=3D"presentation" width=3D"100%" border=3D"0" cellspacing=3D"0" cellpadding=
=3D"0" style=3D"border-top: 1px solid rgb(0,0,0,.1);border-bottom: 1px soli=
d rgb(0,0,0,.1);min-width: 100%;"><tbody><tr height=3D"16"><td height=3D"16=
" style=3D"font-size:0px;line-height:0;">&nbsp;</td></tr><tr><td><table rol=
e=3D"presentation" width=3D"100%" border=3D"0" cellspacing=3D"0" cellpaddin=
g=3D"0"><tbody><tr><td><table role=3D"presentation" width=3D"auto" border=
=3D"0" cellspacing=3D"0" cellpadding=3D"0" style=3D"margin:0 auto;"><tbody>=
<tr><td style=3D"vertical-align:middle;"><table role=3D"presentation" width=
=3D"auto" border=3D"0" cellspacing=3D"0" cellpadding=3D"0"><tbody><tr><td a=
lign=3D"center"><a class=3D"email-button-outline" href=3D"https://substack.=
com/app-link/post?publication_id=3D1428667&amp;post_id=3D174513497&amp;utm_=
source=3Dsubstack&amp;isFreemail=3Dtrue&amp;submitLike=3Dtrue&amp;token=3De=
yJ1c2VyX2lkIjozMzY0NDgyMjMsInBvc3RfaWQiOjE3NDUxMzQ5NywicmVhY3Rpb24iOiLinaQi=
LCJpYXQiOjE3NTg5MDA3NTksImV4cCI6MTc2MTQ5Mjc1OSwiaXNzIjoicHViLTE0Mjg2NjciLCJ=
zdWIiOiJyZWFjdGlvbiJ9.M6fTXptMkeSixoEYv2ZL7os6gegvjkVfcMMVOPLYiSk&amp;utm_m=
edium=3Demail&amp;utm_campaign=3Demail-reaction&amp;r=3D5kb93z" style=3D"fo=
nt-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Hel=
vetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symb=
ol';display: inline-block;font-weight: 500;border: 1px solid rgb(0,0,0,.1);=
border-radius: 9999px;text-transform: uppercase;font-size: 12px;line-height=
: 12px;padding: 9px 14px;text-decoration: none;color: rgb(119,119,119);"><i=
mg class=3D"icon" src=3D"https://substackcdn.com/image/fetch/$s_!PeVs!,w_36=
,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%=
2Ficon%2FLucideHeart%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%252380=
8080%26strokeWidth%3D2" width=3D"18" height=3D"18" style=3D"margin-right: 8=
px;min-width: 18px;min-height: 18px;border: none;vertical-align: middle;max=
-width: 18px" alt=3D""><span class=3D"email-button-text" style=3D"vertical-=
align: middle;">Like</span></a></td></tr></tbody></table></td><td width=3D"=
8" style=3D"min-width: 8px"></td><td style=3D"vertical-align:middle;"><tabl=
e role=3D"presentation" width=3D"auto" border=3D"0" cellspacing=3D"0" cellp=
adding=3D"0"><tbody><tr><td align=3D"center"><a class=3D"email-button-outli=
ne" href=3D"https://substack.com/app-link/post?publication_id=3D1428667&amp=
;post_id=3D174513497&amp;utm_source=3Dsubstack&amp;utm_medium=3Demail&amp;i=
sFreemail=3Dtrue&amp;comments=3Dtrue&amp;token=3DeyJ1c2VyX2lkIjozMzY0NDgyMj=
MsInBvc3RfaWQiOjE3NDUxMzQ5NywiaWF0IjoxNzU4OTAwNzU5LCJleHAiOjE3NjE0OTI3NTksI=
mlzcyI6InB1Yi0xNDI4NjY3Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.UIE1K4BxgVd7fxVfJHm=
vWAfy8qmWiUO20E4a0rROGQs&amp;r=3D5kb93z&amp;utm_campaign=3Demail-half-magic=
-comments&amp;action=3Dpost-comment&amp;utm_source=3Dsubstack&amp;utm_mediu=
m=3Demail" style=3D"font-family: system-ui,-apple-system,BlinkMacSystemFont=
,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI=
 Emoji','Segoe UI Symbol';display: inline-block;font-weight: 500;border: 1p=
x solid rgb(0,0,0,.1);border-radius: 9999px;text-transform: uppercase;font-=
size: 12px;line-height: 12px;padding: 9px 14px;text-decoration: none;color:=
 rgb(119,119,119);"><img class=3D"icon" src=3D"https://substackcdn.com/imag=
e/fetch/$s_!x1tS!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https=
%3A%2F%2Fsubstack.com%2Ficon%2FLucideComments%3Fv%3D4%26height%3D36%26fill%=
3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2" width=3D"18" height=3D"18"=
 style=3D"margin-right: 8px;min-width: 18px;min-height: 18px;border: none;v=
ertical-align: middle;max-width: 18px" alt=3D""><span class=3D"email-button=
-text" style=3D"vertical-align: middle;">Comment</span></a></td></tr></tbod=
y></table></td><td width=3D"8" style=3D"min-width: 8px"></td><td style=3D"v=
ertical-align:middle;"><table role=3D"presentation" width=3D"auto" border=
=3D"0" cellspacing=3D"0" cellpadding=3D"0"><tbody><tr><td align=3D"center">=
<a class=3D"email-button-outline" href=3D"https://substack.com/redirect/2/e=
yJlIjoiaHR0cHM6Ly9vcGVuLnN1YnN0YWNrLmNvbS9wdWIveGFpZ3V5L3AvOS1wYXBlcnMteW91=
LXNob3VsZC1rbm93LWFib3V0LTFmNT91dG1fc291cmNlPXN1YnN0YWNrJnV0bV9tZWRpdW09ZW1=
haWwmdXRtX2NhbXBhaWduPWVtYWlsLXJlc3RhY2stY29tbWVudCZhY3Rpb249cmVzdGFjay1jb2=
1tZW50JnI9NWtiOTN6JnRva2VuPWV5SjFjMlZ5WDJsa0lqb3pNelkwTkRneU1qTXNJbkJ2YzNSZ=
mFXUWlPakUzTkRVeE16UTVOeXdpYVdGMElqb3hOelU0T1RBd056VTVMQ0psZUhBaU9qRTNOakUw=
T1RJM05Ua3NJbWx6Y3lJNkluQjFZaTB4TkRJNE5qWTNJaXdpYzNWaUlqb2ljRzl6ZEMxeVpXRmp=
kR2x2YmlKOS5VSUUxSzRCeGdWZDdmeFZmSkhtdldBZnk4cW1XaVVPMjBFNGEwclJPR1FzIiwicC=
I6MTc0NTEzNDk3LCJzIjoxNDI4NjY3LCJmIjp0cnVlLCJ1IjozMzY0NDgyMjMsImlhdCI6MTc1O=
DkwMDc1OSwiZXhwIjoyMDc0NDc2NzU5LCJpc3MiOiJwdWItMCIsInN1YiI6ImxpbmstcmVkaXJl=
Y3QifQ.qJd-zqJN3nvZNTmSYwqClH7Ztt8JBNirzf9q_GF3rkI?&amp;utm_source=3Dsubsta=
ck&amp;utm_medium=3Demail" style=3D"font-family: system-ui,-apple-system,Bl=
inkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color =
Emoji','Segoe UI Emoji','Segoe UI Symbol';display: inline-block;font-weight=
: 500;border: 1px solid rgb(0,0,0,.1);border-radius: 9999px;text-transform:=
 uppercase;font-size: 12px;line-height: 12px;padding: 9px 14px;text-decorat=
ion: none;color: rgb(119,119,119);"><img class=3D"icon" src=3D"https://subs=
tackcdn.com/image/fetch/$s_!5EGt!,w_36,c_scale,f_png,q_auto:good,fl_progres=
sive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FNoteForwardIcon%3Fv%3D4%26he=
ight%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2" width=3D"=
18" height=3D"18" style=3D"margin-right: 8px;min-width: 18px;min-height: 18=
px;border: none;vertical-align: middle;max-width: 18px" alt=3D""><span clas=
s=3D"email-button-text" style=3D"vertical-align: middle;">Restack</span></a=
></td></tr></tbody></table></td></tr></tbody></table></td><td align=3D"righ=
t"><table role=3D"presentation" width=3D"auto" border=3D"0" cellspacing=3D"=
0" cellpadding=3D"0"><tbody><tr></tr></tbody></table></td></tr></tbody></ta=
ble></td></tr><tr height=3D"16"><td height=3D"16" style=3D"font-size:0px;li=
ne-height:0;">&nbsp;</td></tr></tbody></table><div class=3D"footer footer-Z=
M59BM" style=3D"color: rgb(119,119,119);text-align: center;font-size: 16px;=
line-height: 26px;padding: 24px0;"><div style=3D"font-size: 16px;line-heigh=
t: 26px;padding-bottom: 24px"><p class=3D"pencraft pc-reset color-secondary=
-ls1g8s size-12-mmZ61m reset-IxiVJZ small meta-B2bqa5" style=3D"list-style:=
 none;font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Ro=
boto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe=
 UI Symbol';padding-bottom: 0;font-size: 12px;line-height: 16px;margin: 0;c=
olor: rgb(119,119,119);text-decoration: unset;">=C2=A9 2025 <span>Pascal Bi=
ese</span> <br><a href=3D"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6L=
y93d3cubGxtd2F0Y2guY29tL2FjdGlvbi9kaXNhYmxlX2VtYWlsP3Rva2VuPWV5SjFjMlZ5WDJs=
a0lqb3pNelkwTkRneU1qTXNJbkJ2YzNSZmFXUWlPakUzTkRVeE16UTVOeXdpYVdGMElqb3hOelU=
0T1RBd056VTVMQ0psZUhBaU9qRTNPVEEwTXpZM05Ua3NJbWx6Y3lJNkluQjFZaTB4TkRJNE5qWT=
NJaXdpYzNWaUlqb2laR2x6WVdKc1pWOWxiV0ZwYkNKOS5zN1ZHOV9nOFdILVJWSnZEQzZEWTMzY=
2xaZGxXSVp0a21OWXF1RHBkcnUwIiwicCI6MTc0NTEzNDk3LCJzIjoxNDI4NjY3LCJmIjp0cnVl=
LCJ1IjozMzY0NDgyMjMsImlhdCI6MTc1ODkwMDc1OSwiZXhwIjoyMDc0NDc2NzU5LCJpc3MiOiJ=
wdWItMCIsInN1YiI6ImxpbmstcmVkaXJlY3QifQ.1vZLZ751gF3eZarHuC9koEM_I3XGLSDMlMu=
gcDUsnHw?" style=3D"text-decoration: underline;color: rgb(119,119,119);"><s=
pan style=3D"color: rgb(119,119,119);text-decoration: underline;">Unsubscri=
be</span></a></p></div><p class=3D"footerSection-EHR0jG small powered-by-su=
bstack" style=3D"padding: 0 24px;font-size: 12px;line-height: 20px;margin: =
0;color: rgb(119,119,119);font-family: system-ui,-apple-system,BlinkMacSyst=
emFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Se=
goe UI Emoji','Segoe UI Symbol';padding-bottom: 0;margin-top: 0;"><a href=
=3D"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9zdWJzdGFjay5jb20vc2l=
nbnVwP3V0bV9zb3VyY2U9c3Vic3RhY2smdXRtX21lZGl1bT1lbWFpbCZ1dG1fY29udGVudD1mb2=
90ZXImdXRtX2NhbXBhaWduPWF1dG9maWxsZWQtZm9vdGVyJmZyZWVTaWdudXBFbWFpbD1laXRhb=
kBlaXNsYXcuY28uaWwmcj01a2I5M3oiLCJwIjoxNzQ1MTM0OTcsInMiOjE0Mjg2NjcsImYiOnRy=
dWUsInUiOjMzNjQ0ODIyMywiaWF0IjoxNzU4OTAwNzU5LCJleHAiOjIwNzQ0NzY3NTksImlzcyI=
6InB1Yi0wIiwic3ViIjoibGluay1yZWRpcmVjdCJ9.il4H1pb-SGa2kbZjzZ7qTtfgNnxCUEM-V=
pWsxLrcfaU?" style=3D"color: rgb(119,119,119);text-decoration: none;display=
: inline-block;margin: 0 4px;"><img src=3D"https://substackcdn.com/image/fe=
tch/$s_!LkrL!,w_270,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3=
A%2F%2Fsubstack.com%2Fimg%2Femail%2Fpublish-button%402x.png" srcset=3D"http=
s://substackcdn.com/image/fetch/$s_!wgfj!,w_135,c_limit,f_auto,q_auto:good,=
fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Femail%2Fpublish-but=
ton.png, https://substackcdn.com/image/fetch/$s_!LkrL!,w_270,c_limit,f_auto=
,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Femail%=
2Fpublish-button%402x.png 2x, https://substackcdn.com/image/fetch/$s_!KjtY!=
,w_405,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubsta=
ck.com%2Fimg%2Femail%2Fpublish-button%403x.png 3x" width=3D"135" alt=3D"Sta=
rt writing" height=3D"40" style=3D"max-width: 550px;border: none !important=
;vertical-align: middle;"></a></p></div></div></td><td></td></tr></tbody></=
table><img src=3D"https://eotrx.substackcdn.com/open?token=3DeyJtIjoiPDIwMj=
UwOTI2MTUzMDU4LjMuZjkwZGNhYTk5NTAzNTliNkBtZy1kMS5zdWJzdGFjay5jb20-IiwidSI6M=
zM2NDQ4MjIzLCJyIjoiZWl0YW5AZWlzbGF3LmNvLmlsIiwiZCI6Im1nLWQxLnN1YnN0YWNrLmNv=
bSIsInAiOjE3NDUxMzQ5NywidCI6Im5ld3NsZXR0ZXIiLCJhIjoiZXZlcnlvbmUiLCJzIjoxNDI=
4NjY3LCJjIjoicG9zdCIsImYiOnRydWUsInBvc2l0aW9uIjoiYm90dG9tIiwiaWF0IjoxNzU4OT=
AwNzU5LCJleHAiOjE3NjE0OTI3NTksImlzcyI6InB1Yi0wIiwic3ViIjoiZW8ifQ.XkyIBBKY5N=
qU05tSfQ8ii8_7-fZYs6nNoKwipxQ4coE" alt=3D"" width=3D"1" height=3D"1" border=
=3D"0" style=3D"height:1px !important;width:1px !important;border-width:0 !=
important;margin-top:0 !important;margin-bottom:0 !important;margin-right:0=
 !important;margin-left:0 !important;padding-top:0 !important;padding-botto=
m:0 !important;padding-right:0 !important;padding-left:0 !important;"><img =
width=3D"1" height=3D"1" alt=3D"" src=3D"https://email.mg-d1.substack.com/o=
/eJw8kFtuwyAQRVcT_mphXoYP1mLNwMRFtSHikdS7r_JQf89ojo5ugE5bqae_ldZZ9NIht8Ew8v=
OireN8MZzRAWlfN8pUoVNcof9ftZvtwr49tyijlqAiogxihgVRCOesMkqhRJa84EJzJ8ysJdd2k=
tPV8RgAnNNcaofmovixfcV5agNbh_AzhXKw1NZrpVeC73UQe5auMGKiHMjTnepZ8gen6OdF6Vkq=
t7xJP2_kMz3aTr1TZbeBayjHMXLq50oZcKf4EQ_cU4CeSn6JlLDGLKx6Sh3yRXFKbYfHFMqUdtY=
GxnJAyv4X0jZO1t8bjkb1-S6lUcoKIdndi78AAAD__5Vkdd8"></body></html>

--478d3da90589b4d5b597a56878732fe63b2890310289e5f131c8487c2bf9--
