Received: from PA1PPFA9F486045.eurprd01.prod.exchangelabs.com
 (2603:10a6:108:1::2a5) by AS8PR01MB10319.eurprd01.prod.exchangelabs.com with
 HTTPS; Fri, 1 Aug 2025 18:09:09 +0000
Received: from AM8P191CA0030.EURP191.PROD.OUTLOOK.COM (2603:10a6:20b:21a::35)
 by PA1PPFA9F486045.eurprd01.prod.exchangelabs.com (2603:10a6:108:1::2a5) with
 Microsoft SMTP Server (version=TLS1_2,
 cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id 15.20.8989.11; Fri, 1 Aug
 2025 18:09:07 +0000
Received: from AM4PEPF00027A62.eurprd04.prod.outlook.com
 (2603:10a6:20b:21a:cafe::4d) by AM8P191CA0030.outlook.office365.com
 (2603:10a6:20b:21a::35) with Microsoft SMTP Server (version=TLS1_3,
 cipher=TLS_AES_256_GCM_SHA384) id 15.20.8989.16 via Frontend Transport; Fri,
 1 Aug 2025 18:09:07 +0000
Authentication-Results: spf=pass (sender IP is 159.112.244.14)
 smtp.mailfrom=mg2.substack.com; dkim=pass (signature was verified)
 header.d=mg2.substack.com;dmarc=pass action=none
 header.from=substack.com;compauth=pass reason=100
Received-SPF: Pass (protection.outlook.com: domain of mg2.substack.com
 designates 159.112.244.14 as permitted sender)
 receiver=protection.outlook.com; client-ip=159.112.244.14;
 helo=m244-14.mailgun.net; pr=C
Received: from m244-14.mailgun.net (159.112.244.14) by
 AM4PEPF00027A62.mail.protection.outlook.com (10.167.16.71) with Microsoft
 SMTP Server (version=TLS1_2, cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id
 15.20.9009.8 via Frontend Transport; Fri, 1 Aug 2025 18:09:06 +0000
DKIM-Signature: a=rsa-sha256; v=1; c=relaxed/relaxed; d=mg2.substack.com; q=dns/txt; s=mailo; t=1754071745; x=1754078945;
 h=List-Unsubscribe-Post: List-Unsubscribe: List-Post: List-Id: List-Archive: List-Owner: Reply-To: In-Reply-To: References: sender: sender: Date: Message-Id: To: To: From: From: Subject: Subject: Content-Type: Mime-Version;
 bh=4dr7p0SsNwXS14wCouDl99vw+6S/M9Lfz0ohcpDt1Iw=;
 b=ZYXn/kix+rCL5WqmMRf61CMadyoCZ4WRKb6BUnZ2uRVE5UrRoQ/Y37vhk71i/+1UzM3kul/LxhXl838J3hRK0Xx6XC4Mvy2osux+SGtOxKMDZslIOfU1U1FuhdPUMnHtgNDSpv/AWqSyfa8vxiLOVMuhuh2JPI3FFrncjewpbwk=
X-Mailgun-Sid: WyI5MGQ3NCIsImVpdGFuQGVpc2xhdy5jby5pbCIsIjRkMTg1OCJd
Received: by c06b95fbc66e with HTTP id 688d02c1a7655bd4ba55d0bf; Fri, 01 Aug 2025
 18:09:04 GMT
X-Mailgun-Sending-Ip: 159.112.244.14
X-Mailgun-Batch-Id: 688d02c0ab59c7dd2cb39783
Content-Type: multipart/alternative;
 boundary="2f01162e9de12d1b406ea5774817f4a9378d6128010d961bb946b905b0eb"
Subject: 11 Papers You Should Know About
From: LLM Watch <xaiguy@substack.com>
To: eitan@eislaw.co.il
X-Mailgun-Tag: post
X-Mailgun-Track-Clicks: false
Message-Id: <20250801180834.3.5a54d098a4ed7c97@mg2.substack.com>
Date: Fri, 1 Aug 2025 18:08:34 +0000
Feedback-ID: post-169670030:cat-post:pub-1428667:substack
sender: LLM Watch <xaiguy@substack.com>
References: <post-169670030@substack.com>
In-Reply-To: <post-169670030@substack.com>
Reply-To: LLM Watch
 <reply+2t0m8e&5kb93z&&258f9d797c6388368b3547e17517e3217b8fab2c1197f5976ed18d664396085a@mg1.substack.com>
List-Owner: <mailto:xaiguy@substack.com>
List-URL: <https://www.llmwatch.com/>
List-Archive: <https://www.llmwatch.com/archive>
List-Id: <xaiguy.substack.com>
List-Post: <https://www.llmwatch.com/p/11-papers-you-should-know-about>
List-Unsubscribe: <https://www.llmwatch.com/action/disable_email/disable?token=eyJ1c2VyX2lkIjozMzY0NDgyMjMsInBvc3RfaWQiOjE2OTY3MDAzMCwiaWF0IjoxNzU0MDcxNzQyLCJleHAiOjE3ODU2MDc3NDIsImlzcyI6InB1Yi0xNDI4NjY3Iiwic3ViIjoiZGlzYWJsZV9lbWFpbCJ9.n_1qwcr1U0PRVS0O_iZV5LxH5KL6VJmv_xEtWiFLe6A&all_sections=true>
List-Unsubscribe-Post: List-Unsubscribe=One-Click
X-Mailgun-Variables: {"category": "post", "email_generated_at": "1754071743663", "is_freemail":
 "true", "post_audience": "everyone", "post_id": "169670030", "post_type":
 "newsletter", "pub_community_enabled": "true", "publication_id": "1428667",
 "subdomain": "xaiguy", "user_id": "336448223"}
Return-Path: bounce+b49665.4d1858-eitan=eislaw.co.il@mg2.substack.com
X-MS-Exchange-Organization-ExpirationStartTime: 01 Aug 2025 18:09:07.0941
 (UTC)
X-MS-Exchange-Organization-ExpirationStartTimeReason: OriginalSubmit
X-MS-Exchange-Organization-ExpirationInterval: 1:00:00:00.0000000
X-MS-Exchange-Organization-ExpirationIntervalReason: OriginalSubmit
X-MS-Exchange-Organization-Network-Message-Id: 4e15a790-e7e9-479f-803d-08ddd12681ab
X-EOPAttributedMessage: 0
X-EOPTenantAttributedMessage: 384c4129-e818-4ea7-8f8b-189d997170d1:0
X-MS-Exchange-Organization-MessageDirectionality: Incoming
X-MS-PublicTrafficType: Email
X-MS-TrafficTypeDiagnostic: AM4PEPF00027A62:EE_|PA1PPFA9F486045:EE_|AS8PR01MB10319:EE_
X-MS-Exchange-Organization-AuthSource: AM4PEPF00027A62.eurprd04.prod.outlook.com
X-MS-Exchange-Organization-AuthAs: Anonymous
X-MS-Office365-Filtering-Correlation-Id: 4e15a790-e7e9-479f-803d-08ddd12681ab
X-MS-Exchange-Organization-SCL: 1
X-Microsoft-Antispam: BCL:5;ARA:13230040|2092899012|69100299015|4022899009|1032899013|12012899012|29132699027|8096899003|2066899003|13003099007|20203002999003;
X-Forefront-Antispam-Report: CIP:159.112.244.14;CTRY:US;LANG:en;SCL:1;SRV:;IPV:NLI;SFV:NSPM;H:m244-14.mailgun.net;PTR:m244-14.mailgun.net;CAT:NONE;SFS:(13230040)(2092899012)(69100299015)(4022899009)(1032899013)(12012899012)(29132699027)(8096899003)(2066899003)(13003099007)(20203002999003);DIR:INB;
X-MS-Exchange-CrossTenant-OriginalArrivalTime: 01 Aug 2025 18:09:06.6447
 (UTC)
X-MS-Exchange-CrossTenant-Network-Message-Id: 4e15a790-e7e9-479f-803d-08ddd12681ab
X-MS-Exchange-CrossTenant-Id: 384c4129-e818-4ea7-8f8b-189d997170d1
X-MS-Exchange-CrossTenant-AuthSource: AM4PEPF00027A62.eurprd04.prod.outlook.com
X-MS-Exchange-CrossTenant-AuthAs: Anonymous
X-MS-Exchange-CrossTenant-FromEntityHeader: Internet
X-MS-Exchange-Transport-CrossTenantHeadersStamped: PA1PPFA9F486045
X-MS-Exchange-Transport-EndToEndLatency: 00:00:02.8712356
X-MS-Exchange-Processed-By-BccFoldering: 15.20.9009.002
X-Microsoft-Antispam-Mailbox-Delivery:
	ucf:0;jmr:0;auth:0;dest:I;ENG:(910005)(944506478)(944626604)(920097)(930097)(140003);
X-Microsoft-Antispam-Message-Info:
	=?utf-8?B?dStzcEtZazU3UVVySDJTMHVhVGNGMzQyQ2NOekltSWRLSHBQM0U0Q2JEZEpu?=
 =?utf-8?B?cy9Sd2Z1T0ZDbFJCVWo4U1U5RWgxQjQ4b0ZwQVAvdStEbXA2RFpKU21VSXhs?=
 =?utf-8?B?WFFBVEJMKzN4WnVveVAwWDQzN3dmK3pZclVXdUpUVU9JbVhWcnI3NEk3bmRG?=
 =?utf-8?B?V0hyK28vRUhNaVlWemJqQ09uTEUxTzJvaFRNSzRuczAzbElsOCsreHBZUm5W?=
 =?utf-8?B?NXF3S3QyWmo5cFZVcHFaclNpT2d4b3VTT3ZnRzZsVWk2bTdUQm9RQ2p4ZFIx?=
 =?utf-8?B?d2xaR3IyaEpDcCs0bndKQzk1MW9uVG9DTjAwenVFbS9XOVRBV0JIVkJaRjN4?=
 =?utf-8?B?NHdQdFdyM1VDZ09mM2srTHF3bnovVjFxalNMRjFQVWNMc0k0ZDMva1dzS2Q5?=
 =?utf-8?B?eEJpVjJERVBqeTlDUGQxMjZadWptTU40S0VmZzViM0pMOGs0SjlsdzNTQUY1?=
 =?utf-8?B?bnRXZlFHYU80YmoyTGVqbnJLMUtyeVErWTRsWGF3TjVpNGhGSG5aK1NqYkhC?=
 =?utf-8?B?MGdPbHdrcU1aaG5tUVIrSDR3VThSWWpPSTZudGdOSlVvamI5VWZsR3ZJR3pz?=
 =?utf-8?B?OENaaVhTdG13ajJUY2FCNkJXRkxLNVBVQ2kxWlFjQ3J1b2lHZENnUlI0NTZZ?=
 =?utf-8?B?M1JNQ2VuWitJaDQxTGxSenZXZU9oNG1nV2JFTG0wbXhYMXk1cFlwQWdPVktD?=
 =?utf-8?B?TmdXT2RCVXR2WE1ya3p1NkM1TEtnZ3RTMk5HSXRDeDYzS3ZFNHhDVkFkTkkz?=
 =?utf-8?B?dGdVd2EzeHR2bi9LRktkbFBsM0VHTE1MbUVDald3d0MrWFBVWDNvT2lvVGEw?=
 =?utf-8?B?WEJoOW5YSFFNcHpoaHd3VzlwZFlCMU51SERYUGQ2NWpwMFl6V2ZPc2Q3NDdR?=
 =?utf-8?B?V3pRSWQ5d1F0YllLMWFpa1IrZEtXSDhLUGlHSysrKzE3VWtsM3EwQkdRUVRm?=
 =?utf-8?B?ZXM2Y1I5cVVtWlNkM1VKUVRqR2gwVDZXREpvVlhRVmZxamFKVExyRXhTRXNJ?=
 =?utf-8?B?WmZOc1ZmSVVWMmVNZWQ0WHRPMEtlVVY0SnBLRDc4b1kxbTFJZTBIZVRmbStk?=
 =?utf-8?B?QUZpUjhHVUYvL2pBRmpjRm9RTmdldEZERFdDZVJmNER5K0ZrQSt6UnJBTjVZ?=
 =?utf-8?B?UVZZVXNUbXVMenMzckZkdDV1cFNHcUZLUU1jdUhEN3NtVjNMbW9SbVlnc0Zz?=
 =?utf-8?B?dG9pYjhtTG5COG80bHR0QXkzZWxaakxUVGxRSlh4b3lYL2k5ckVVdE44dTlT?=
 =?utf-8?B?TlZJZWlTUnR3aXRaTzF6Q2lpSXNQclBuZCtVZFZFZmtWQkxhRDE3ZEQzNE1E?=
 =?utf-8?B?UGFZTWRBdjJTVGxwVWhFYnk3VTRGUWNaaHJXSERjdkVxSkJSMUJaOExVNzQ0?=
 =?utf-8?B?aU1OUERWalg5NXN3NmJPRU80TDhRblpUenFQNkkwU252OEJTVi9tQUtwYlE0?=
 =?utf-8?B?VDJkemVVQjNuS1hzM1V6OHZtOGFOaTJjY0JWS0NRMTBBa1A2WXI1OHRLdVU0?=
 =?utf-8?B?a2lNT0JWd2hZeFM5cnZ2WEpEc3NPRmxVc2xhTWFqSVFxWXJLS0RmQ3pGZlJY?=
 =?utf-8?B?dnl4aFZ0bHlVaGVSRHBkaS95NzVnSE1Iai85OVhPZXRqakVEMjg2eFloTXB0?=
 =?utf-8?B?OWdZV1h2WE52MDJLUEpGVXZyTWJHcEVGNDYyKzUrYzV5SjJlWFdDYUJmbGlE?=
 =?utf-8?B?UWRTNzhkZXNrTXJ3Q0VERmMyYUw2dzFVdUN3UitoS0JDWjE1Nk9VMldFT1dG?=
 =?utf-8?B?Nmp3Q29ydXAxNGlRUzlPdTJaUDdPNHdNSUIyTjBFV1A3RnMwdTlRUm5OM2dF?=
 =?utf-8?B?TjBGUUlLNVo0bElNdmcyUjB5QUdSR1pHbnFwWTc0cXZROVp2d0dkWmNYbzhR?=
 =?utf-8?B?NVY2a0RrcXFhVnZyaE5PTjh2aVc4djM1Mkp6MXRBMEdKNFlyVlNkTndqRUJY?=
 =?utf-8?B?VGFCVHVaYmZkYmFvdnNDQTBacGJLazN0bXNCanpJVm00YzFVVHNJS1JxTlYv?=
 =?utf-8?B?U05ESW1aZEVDb3k2Z29PeHAvZUdSdm1UR1ZQeEhwWjU3WHJFckE4U05VWjR4?=
 =?utf-8?B?NlJvSHBnQU1lUnJSaXZ2NVFUdnVkVGphQWpmWUdIN0E0ZXRFMmFwQ2JnSCtJ?=
 =?utf-8?B?dHVGY1hGaTBpUmhYUEtwUHBhbWxYenNKMWFQQ09GajRvYmY2QUhWajZwNnFS?=
 =?utf-8?B?T2dTdE94L29IaGdFaXlSN1NETkZJQXVTbk5Bd3U2dlVFNDByRVN5N1dJS0lU?=
 =?utf-8?B?YWk2ZGF6RExLVkhxcHpUb1R4V0ZQekZhakVNNUZMcGtaVkk3UWdDRG9JdzAw?=
 =?utf-8?B?RDA5bHRCbTJUdEpEanlPV2xEN3BLbE9ZLzdKK1RTa1M4cXZRZks3M3RzNkFl?=
 =?utf-8?B?QTdPMkZkaVJkRXlwTHdwNW0zUGNBRUxGckpvVmJ4eGVDV0ZPR3doL2RLczlN?=
 =?utf-8?B?aEgwbVJsaklCVGxzV3l6d2psTU9NeFVqMkZTSWVGVzhUYnhUenJBWDRDQmtY?=
 =?utf-8?B?NVRjRmxWQy80OERPQ3hWREdrQlJZellmSTFoQVI3a0lHbVlSSWN3eUtDU3RY?=
 =?utf-8?B?a0d2M0VlTHRYQ3U5T00veEQzb3loMHpRbXNPeFdlSTJYMzBOTm1KaC91SWd2?=
 =?utf-8?B?T3lubHdzZVBQLzAwWlZVbkhJMzlMOUliaVNQQ3dYeTg2T084aDI2MnhCaUpO?=
 =?utf-8?B?c3EwS1NXNnl5UjFkaFJLM29EWC9CTmRkNDJCN2U1ZW5sbC9ubGtQK29BZE1s?=
 =?utf-8?B?RDI0M3ZFL0JuYjVIR0lOTUM2QmxUSXppVFphY0p1b1ljSXJtdnRLZHBTY0x4?=
 =?utf-8?B?eXloSUpYRzI4N0FUektXcHJXajJ0TGVEVmo2Y1VNMHpNSEV6T3NoWUFtUnly?=
 =?utf-8?B?K3ZEbVBjdE1XU0FyaWxQL1kvN1R0b2V1OWVGSnRTT2owWWNmRTBSMWF0S04x?=
 =?utf-8?B?ajVyTERMRWpubnY1QWRnWHRlSCtkT3QxRS8yUmtsejM5RndIa1VSL1FZcUFt?=
 =?utf-8?Q?zZMl3k7e8vZiGs=3D?=
MIME-Version: 1.0

--2f01162e9de12d1b406ea5774817f4a9378d6128010d961bb946b905b0eb
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable

View this post on the web at https://www.llmwatch.com/p/11-papers-you-shoul=
d-know-about

Welcome, Watcher! This week in LLM Watch:
Why self-evolving agents might be the real path to ASI (not just bigger mod=
els)=20
How reflective language beats reinforcement learning by 35x efficiency=20
When hybrid architectures demolish compute-performance trade-offs=20
Don't forget to subscribe to never miss an update again.
Quick Glossary (for the uninitiated)
ASI (Artificial Super Intelligence): AI that surpasses human intelligence i=
n all domains - the hypothetical next stage after AGI where machines outthi=
nk us at everything
MoE (Mixture of Experts): An architecture where different specialized sub-m=
odels handle different inputs - like having a team of specialists instead o=
f one generalist
SSM (State Space Model): An efficient alternative to Transformers for proce=
ssing sequences - think of it as a streamlined way to remember context
GraphRAG: Retrieval-Augmented Generation using knowledge graphs - like givi=
ng AI a structured map of information instead of just a pile of documents
Knowledge hypergraphs: Advanced data structures where connections can link =
multiple nodes at once - like mind maps on steroids
MCP (Model Context Protocol): A standardized way for AI agents to interact =
with tools and external systems - like USB for AI connections
1. A Survey of Self-Evolving Agents: On Path to Artificial Super Intelligen=
ce
Watching: Self-Evolving Agents (paper [ https://substack.com/redirect/0290d=
1c9-b08d-4fd9-9316-13770204f8cc?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hsh=
gUC7BDnPG6RxQ4tAjMRMPOw0 ]/repo [ https://substack.com/redirect/5c432d79-79=
48-4700-8d9d-c7d94d8811d0?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BD=
nPG6RxQ4tAjMRMPOw0 ])
What problem does it solve? Large language models today are powerful but fu=
ndamentally static, unable to update their knowledge or skills once trained=
=2E When deployed in open-ended environ=
ments, this rigidity becomes a serious=
 bottleneck =E2=80=93 the AI cannot adapt to new tasks, evolving informatio=
n, or dynamic interactions. This survey identifies the shift from static LL=
Ms to self-evolving agents as crucial for breaking that bottleneck. In esse=
nce, the problem is how to build AI systems that can learn and improve cont=
inuously in real time, rather than remaining fixed after pre-training.
How does it solve the problem? The authors provide the first comprehensive =
roadmap for building adaptive, self-improving AI agents. They organize the =
vast literature (1,400+ papers) around three foundational dimensions of evo=
lution: what to evolve (which components, e.g. the model=E2=80=99s paramete=
rs, memory, or tool use), when to evolve (during or between tasks), and how=
 to evolve (the algorithms or feedback that drive learning). They examine m=
echanisms that allow an agent to update itself =E2=80=93 from modifying its=
 neural weights, to expanding toolsets or adjusting its architecture =E2=80=
=93 and classify these by whether adaptation happens on the fly during a ta=
sk or in between episodes. The survey also reviews how to give agents riche=
r feedback beyond sparse rewards (like using textual self-reflection or mul=
ti-agent critiques) and covers evaluation benchmarks and domains (coding, e=
ducation, healthcare) where self-evolving agents are being tested.
What are the key findings? A major takeaway is that designing self-evolving=
 AI requires integrating techniques from across subfields into a unified fr=
amework. Successful agents will need to coordinate multiple forms of learni=
ng =E2=80=93 e.g. updating memory modules, adjusting plans, and fine-tuning=
 skills =E2=80=93 all under principled control of when and how to apply eac=
h. The survey highlights that recent systems already demonstrate pieces of =
this puzzle (like agents that update via textual feedback or share knowledg=
e in a team), but we lack a standard toolbox for making them reliably adapt=
ive. It also identifies open challenges: ensuring safety (so an agent doesn=
=E2=80=99t evolve undesired behaviors), achieving scalability (so learning =
on the fly doesn=E2=80=99t require prohibitive compute), and handling co-ev=
olution in multi-agent settings. By synthesizing lessons across studies, th=
e authors deliver a structured understanding of what=E2=80=99s been achieve=
d and what gaps remain on the path toward truly self-improving AI.
Why does it matter? This work essentially charts the path toward AI that ca=
n continuously learn, a key stepping stone toward more general and autonomo=
us intelligence. Instead of repeatedly retraining models offline for new ab=
ilities, future AI agents might evolve on the job, much like humans do by l=
earning from experience. Such agents could remain state-of-the-art without =
constant human intervention, adjusting to new information and tasks as they=
 emerge. In the long run, the survey=E2=80=99s vision of self-evolving agen=
ts paints a route to systems that inch closer to Artificial Super Intellige=
nce, by operating at or beyond human-level capability across many tasks thr=
ough ongoing self-improvement. For practitioners and researchers, this road=
map provides concrete guidance on building AI that doesn=E2=80=99t plateau =
=E2=80=93 it adapts, optimizes itself, and potentially, gets better with ea=
ch interaction.
2. Kimi K2: Open Agentic Intelligence
Watching: Kimi K2 (paper [ https://substack.com/redirect/073b336a-6764-4eb8=
-a8db-a28657abcd04?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ=
4tAjMRMPOw0 ]/model [ https://substack.com/redirect/ccd41580-bcfd-4b13-a7bb=
-1b0ba94dafde?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjM=
RMPOw0 ])
What problem does it solve? The performance gap between proprietary AI gian=
ts and open-source models remains a significant hurdle =E2=80=93 especially=
 for =E2=80=9Cagentic=E2=80=9D tasks where an AI must plan, act, and reason=
 with tools or environments. Kimi K2 tackles this by pushing open-source LL=
Ms to unprecedented scale and capability. Training ultra-large models is no=
toriously unstable (loss spikes, divergence) and costly. Moreover, imbuing =
an LLM with agent-like abilities (the kind needed to interact with environm=
ents) typically requires complex fine-tuning that few open projects have ac=
hieved. In short, the problem addressed here is how to build an open model =
that is both massive in scale and skilled in agentic reasoning, without fal=
ling prey to training instabilities or closed data.
How does it solve the problem? Kimi K2 introduces a Mixture-of-Experts (MoE=
) architecture with a staggering 1 trillion parameters (of which 32 billion=
 are =E2=80=9Cactive=E2=80=9D per token). This MoE design allows scaling th=
e model=E2=80=99s capacity without a proportional increase in computation f=
or every token. To overcome training instability, the team developed a new =
optimizer called MuonClip (improving on the Muon optimizer) that uses a nov=
el QK-clip technique to prevent the divergence issues that often occur in M=
oE training. With this, they successfully pre-trained Kimi K2 on an immense=
 15.5 trillion tokens without any loss spikes. After pre-training, they did=
n=E2=80=99t stop at a generic model =E2=80=93 they put K2 through a multi-s=
tage post-training regimen. This included a large-scale agentic data synthe=
sis (generating diverse scenarios requiring tool use and reasoning) and a j=
oint reinforcement learning stage where K2 interacted with both real and si=
mulated environments to improve its decision-making. In essence, Kimi K2 wa=
s taught not just to predict text, but to behave as an agent, refining its =
skills via trial-and-error feedback.
What are the key findings? Kimi K2 now stands as one of the most capable op=
en-source LLMs to date. Thanks to its scale and training, it achieves state=
-of-the-art results among open models across a variety of benchmarks. Notab=
ly, it excels in what the authors call "non-thinking" (direct response) set=
tings: for example, it scored 66.1 on the Tau2 reasoning benchmark and 76.5=
 on ACE (English) =E2=80=93 outperforming most existing open and even close=
d models when they=E2=80=99re not allowed to use chain-of-thought prompting=
=2E It also shows strong prowess in dom=
ains like coding and math, with a 53.7=
% on LiveCode (code generation) and 49.5% on the AIME 2025 math test. These=
 figures are significant improvements, often surpassing models many times i=
ts size. Equally important, K2=E2=80=99s training pipeline proved that inte=
ractive fine-tuning (through environment interactions) measurably boosts an=
 LLM=E2=80=99s problem-solving abilities. The model=E2=80=99s releases incl=
ude both the base 1T-parameter checkpoint and a further post-trained checkp=
oint, giving the community a powerful new foundation to experiment with.
Why does it matter? Kimi K2=E2=80=99s success is a milestone for the open A=
I ecosystem. It demonstrates that open-source researchers can not only reac=
h the trillion-parameter scale, but also instill advanced reasoning and age=
ntic behaviors that were previously seen only in flagship proprietary model=
s. In practical terms, an open model with these capabilities lowers the bar=
rier for a wide range of applications =E2=80=93 from complex coding assista=
nts to autonomous research agents =E2=80=93 without needing API access to a=
 closed AI. The innovations in training (like MuonClip) may also benefit ot=
hers working on large models, making it easier to train huge systems reliab=
ly. More broadly, Kimi K2 validates a paradigm: by combining massive scale =
with targeted agentic training, we can produce AI that is both broadly know=
ledgeable and able to apply that knowledge in multi-step, interactive conte=
xts. It=E2=80=99s a step toward AI that not only contains facts and skills,=
 but can deploy them autonomously in pursuit of goals =E2=80=93 and it=E2=
=80=99s all happening in the open.
3. Flow Matching Policy Gradients
Watching: Flow Matching (paper [ https://substack.com/redirect/d6dcee97-688=
8-442d-9b91-d4ee30c6744b?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDn=
PG6RxQ4tAjMRMPOw0 ]/code [ https://substack.com/redirect/d5241ffb-5f00-4823=
-bea2-347735652e36?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ=
4tAjMRMPOw0 ])
What problem does it solve? In reinforcement learning for continuous contro=
l (think robotics or game physics), policies are usually represented with s=
imple probability distributions (typically Gaussians) that struggle with mu=
lti-modal decisions. For example, if there are two very different yet equal=
ly good ways to achieve a goal, a Gaussian policy tends to average them int=
o a mediocre single mode. Recent advances in generative modeling =E2=80=93 =
like diffusion models =E2=80=93 can represent complex, multi-modal distribu=
tions, but integrating them into RL has been awkward. Prior attempts to use=
 diffusion models in RL required fixing a specific sampling procedure or ca=
lculating exact probabilities at every step, which is cumbersome and can li=
mit performance. So the core problem is how to train an RL agent that can l=
everage the expressive power of diffusion/flow-based models for its action =
decisions without complicating the training process or being tied to a sing=
le sampling method.
How does it solve the problem? The authors propose Flow Policy Optimization=
 (FPO), an on-policy RL algorithm that marries diffusion-like flow modeling=
 with standard policy gradients. In essence, they recast the policy learnin=
g objective as a flow matching problem: instead of directly maximizing expe=
cted reward, FPO trains the policy by matching the =E2=80=9Cprobability flo=
w=E2=80=9D of an optimal policy. Concretely, they derive an update rule tha=
t maximizes the advantage-weighted geometric mean of probabilities (rather =
than arithmetic mean), aligning with a conditional flow matching loss. This=
 approach slots neatly into the popular PPO (Proximal Policy Optimization) =
framework =E2=80=93 they even use a PPO-style clipping mechanism to keep up=
dates stable. The key innovation is that FPO doesn=E2=80=99t require comput=
ing exact likelihoods for diffusion model outputs at every step (bypassing =
a major hurdle). It also treats the choice of diffusion sampler as a plug-a=
nd-play detail, meaning the training isn=E2=80=99t handcuffed to any partic=
ular way of generating samples. The authors implement FPO and train diffusi=
on-based policies from scratch on classic continuous control tasks (like lo=
comotion and manipulation), effectively turning those tasks into a playgrou=
nd for generative models to act as agents.
What are the key findings? Flow Matching Policy Gradients prove remarkably =
effective. The diffusion-style policies learned via FPO can capture rich, m=
ulti-modal action distributions that a traditional Gaussian policy simply c=
ouldn=E2=80=99t express. In practical terms, on several benchmark tasks the=
 FPO-trained agents achieved higher rewards than baseline methods. They par=
ticularly shine in scenarios with ambiguity (so-called under-conditioned se=
ttings, where the optimal action isn=E2=80=99t uniquely determined by the s=
tate) =E2=80=93 here, the ability to represent multiple likely futures give=
s the agent an edge. Another notable finding is that this performance gain =
comes without sacrificing stability; the training curves are smooth, thanks=
 to the built-in clipping and the robustness of using a geometric mean obje=
ctive. By comparing to prior diffusion-in-RL approaches, the authors show t=
hat FPO=E2=80=99s agnosticism to sampler and avoidance of exact likelihood =
calculations make it more flexible and broadly applicable. All told, the re=
sults indicate that advanced generative models can be plugged into RL frame=
works to significantly improve policy quality.
Why does it matter? FPO is a cross-disciplinary breakthrough that bridges t=
he gap between two evolving fronts of AI: generative modeling and decision-=
making. For the RL community, it opens the door to a new class of agents th=
at don=E2=80=99t have to choose one action when they can prepare for many =
=E2=80=93 which could be crucial for complex tasks like self-driving, where=
 multiple responses might be viable. The method is also noteworthy for impr=
oving efficiency: by better matching the true landscape of optimal actions,=
 an FPO agent may require fewer trial-and-error iterations to learn good st=
rategies. More philosophically, this work hints at a future where technique=
s from diffusion models (designed for images and text) help create smarter =
policies for robots and autonomous systems. It=E2=80=99s a reminder that as=
 AI systems become more general, ideas from different subfields will combin=
e to overcome each other=E2=80=99s limitations. Here, the precision of rein=
forcement learning meets the creativity of generative models =E2=80=93 and =
the outcome is an agent that can genuinely do more by considering a wider s=
pace of possibilities when deciding how to act.
4. GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning
Watching: GEPA (paper [ https://substack.com/redirect/04c86654-559a-4c11-b4=
fc-87e96cc892a9?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tA=
jMRMPOw0 ])
What problem does it solve? Fine-tuning LLMs for new tasks often relies on =
reinforcement learning (RL) with very sparse feedback =E2=80=93 a model gen=
erates an answer and gets a single reward (right or wrong). Methods like GR=
PO (Group Relative Policy Optimization) can align LLMs to tasks but are pai=
nfully sample-inefficient, sometimes needing thousands of trial-and-error e=
pisodes to see improvement. Moreover, using only a scalar reward misses all=
 the nuanced reasons why an answer was wrong, which the model might infer i=
f given the chance. The problem, then, is how to adapt an LLM to a task fas=
ter and more richly than traditional RL, possibly by leveraging the model=
=E2=80=99s own understanding (since outputs are language, the model could a=
nalyze them) instead of treating it like a black-box policy that only sees =
rewards.
How does it solve the problem? GEPA introduces a new paradigm of reflective=
 prompt evolution. Instead of adjusting model weights via gradients, GEPA k=
eeps the model fixed and optimizes the prompts and instructions given to it=
=2E It works like a scientist running e=
xperiments: for a given task, GEPA has=
 the LLM attempt the task (producing reasoning traces, tool calls, etc.), t=
hen it has the LLM reflect in natural language on those attempts =E2=80=93 =
identifying errors or inefficiencies in its reasoning. Based on this self-c=
ritique, GEPA generates proposed modifications to the prompts (or chain-of-=
thought guidelines). It doesn=E2=80=99t rely on a single brainstorm: it pro=
duces a diverse set of candidate prompts and uses a Genetic-Pareto strategy=
 to combine the best parts of different candidates. In other words, it=E2=
=80=99s performing natural language evolution: each =E2=80=9Cgeneration=E2=
=80=9D consists of the model analyzing its failures and mutating the prompt=
 to address them. This process can turn even a handful of task rollouts int=
o substantial quality gains, because each rollout yields a wealth of inform=
ation (the model=E2=80=99s own commentary) rather than just a win/lose sign=
al.
What are the key findings? GEPA dramatically outperforms traditional RL fin=
e-tuning in both effectiveness and efficiency. Across four different tasks,=
 it achieved on average a 10% higher success rate than a strong RL baseline=
 (GRPO), and in some cases up to 20% higher. Crucially, it did so using far=
 fewer trials =E2=80=93 up to 35=C3=97 fewer rollouts =E2=80=93 meaning it =
squeezed much more learning out of each attempt. It also beat the previous =
state-of-the-art prompt optimization method (MIPRO v2) by over 10% on share=
d benchmarks. Qualitatively, the prompts evolved by GEPA were found to enco=
de high-level =E2=80=9Crules=E2=80=9D and insights (often expressed in plai=
n English) that helped the model avoid specific pitfalls. For example, on a=
 coding task, GEPA might add a reminder like =E2=80=9Cfirst check for edge =
cases such as null inputs,=E2=80=9D which an RL reward alone would never ex=
plicitly give. Another interesting finding is that GEPA=E2=80=99s approach =
can serve as an online strategy: the authors demonstrated it improving a mo=
del=E2=80=99s outputs on the fly during inference, by iteratively refining =
the prompt for each new problem. This blurs the line between training and u=
sage =E2=80=93 the model is essentially self-improving in real time by rewr=
iting its own instructions.
Why does it matter? GEPA=E2=80=99s success hints at an alternative (or comp=
lement) to reinforcement learning for aligning and enhancing LLMs. Instead =
of treating the model as a reinforcement learner that must be coaxed with n=
umeric rewards, GEPA treats the model as a reasoner that can read and write=
 its own improvement instructions. This approach can be far more data-effic=
ient (as seen by the 35=C3=97 reduction in needed trials) =E2=80=93 an impo=
rtant practical advantage when each trial might involve costly API calls or=
 human evaluations. It=E2=80=99s also more interpretable: we end up with an=
 improved prompt that humans can read and understand, rather than a mysteri=
ous set of weight changes inside the model. For the field of AI, this work =
underscores the power of letting models =E2=80=9Cthink about their thinking=
=2E=E2=80=9D By using the medium of langua=
ge, GEPA shows that LLMs can levera=
ge their internal knowledge to correct themselves, which is a step toward a=
utonomous self-correction in AI systems. In a broader sense, it exemplifies=
 a trend of language-native optimization: using the model=E2=80=99s own nat=
ural outputs (explanations, reflections) as feedback, which could be applie=
d to many domains beyond these tasks. If LLMs can continue to refine their =
behavior through such reflective loops, we might achieve robust performance=
 gains without always resorting to brute-force reinforcement signals.
5. Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency=
 and Performance
Watching: Falcon-H1 (paper [ https://substack.com/redirect/9db5dbb6-b4af-45=
a1-976a-f9a4f931e5ac?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6R=
xQ4tAjMRMPOw0 ]/code [ https://substack.com/redirect/86f9560e-81a4-4ee0-996=
9-d9075403db6a?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAj=
MRMPOw0 ])
What problem does it solve? Bigger isn=E2=80=99t always better =E2=80=93 es=
pecially if you can barely run =E2=80=9Cbigger.=E2=80=9D The AI community h=
as wrestled with the trade-off between model size and practicality. Enormou=
s transformer models deliver great performance but at enormous computationa=
l cost, while smaller models run efficiently but often lag far behind in ab=
ility. Another challenge is context length: standard transformers struggle =
to handle very long inputs due to their quadratic scaling. Falcon-H1 direct=
ly addresses how to get more bang for your parameter buck =E2=80=93 achievi=
ng top-tier performance at a fraction of the model size, and doing so with =
much longer context windows than usual. In short, the problem is designing =
an architecture that can match or exceed the performance of models double o=
r quadruple its size, while also being memory- and runtime-efficient (and e=
xtending to long contexts), something that could hugely benefit deployments=
 on limited hardware.
How does it solve the problem? The team behind Falcon-H1 took a bold hybrid=
 approach. Instead of using a pure Transformer like most LLMs, Falcon-H1 co=
mbines Transformer-based self-attention with State Space Model (SSM) compon=
ents in a parallel =E2=80=9Cdual head=E2=80=9D architecture. SSMs (inspired=
 by models like S4) are known for handling long sequences with linear compu=
tational complexity and providing superior long-term memory. By integrating=
 SSM layers alongside traditional attention, Falcon-H1 leverages the streng=
ths of both: attention for complex short-term dependencies and SSM for effi=
cient long-range processing. They didn=E2=80=99t stop at architecture =E2=
=80=93 the designers revisited everything from model depth to training data=
 strategy to optimize for efficiency. Falcon-H1 comes in a spectrum of size=
s (base and instruction-tuned variants at 0.5B, 1.5B, 3B, 7B, and 34B param=
eters, plus a special =E2=80=9C1.5B-deep=E2=80=9D version). Notably, all mo=
dels support a context window up to 256k tokens =E2=80=93 orders of magnitu=
de beyond typical limits =E2=80=93 enabled by the SSM component managing lo=
ng-term memory. They also released many models in quantized form (int8) for=
 easy deployment, totaling over 30 checkpoints on HuggingFace. The entire s=
uite is open-source under a permissive license, meaning anyone can use or f=
ine-tune them without heavy restrictions.
What are the key findings? Falcon-H1 models deliver state-of-the-art result=
s with drastically fewer parameters than competitors. The flagship 34B mode=
l matches or outperforms models in the 70B parameter range on a wide array =
of benchmarks. For instance, it=E2=80=99s reported to either match or beat =
recent open models like Qwen2.5-72B and even a hypothetical Llama3-70B on t=
asks spanning reasoning, math, multilingual understanding, and following in=
structions. Impressively, the efficiency gains hold at smaller scales too: =
the Falcon-H1-1.5B-Deep model (an enhanced 1.5B parameter model) is on par =
with many 7B=E2=80=9310B models from just last year, and the tiniest Falcon=
-H1 0.5B model performs comparably to older 7B models from 2024. These are =
huge leaps in the price/performance ratio of language models. Beyond raw ac=
curacy, the long-context capability stands out: the models can handle input=
s like entire books or multi-day dialogues (up to 256,000 tokens) without e=
xternal memory tricks, something practically unheard of in this space. This=
 suggests they maintain coherence and understanding over extremely lengthy =
texts, which the evaluation confirms with strong performance on long-form a=
nd multi-turn tasks. An important aspect is that all these gains come with =
no proprietary data or code =E2=80=93 Falcon-H1=E2=80=99s advancements are =
a testament to clever architecture and training, not just secret sauce. The=
 release underlines that through innovation, open models can rival or even =
surpass the giants.
Why does it matter? Falcon-H1 marks a significant step towards making power=
ful AI more accessible. By achieving top performance at smaller model sizes=
, it means organizations and researchers with limited compute can deploy or=
 fine-tune models that previously were out of reach. This democratization i=
s bolstered by the open license =E2=80=93 anyone can build on Falcon-H1 for=
 their applications. The hybrid Transformer-SSM approach also pioneers a pa=
th forward for model design: rather than blindly scaling up, we can scale s=
marter. Long contexts of 256k tokens open up new application domains =E2=80=
=93 imagine AI models that can read and analyze hundreds of pages of text o=
r code in one go, enabling deep analysis in finance, law, or literature wit=
hout chopping the input. In essence, Falcon-H1 suggests that the era of sol=
ely chasing parameter count is waning; optimization of architecture and tra=
ining can yield =E2=80=9Csize-efficient=E2=80=9D models that are just as ca=
pable. For the field, it=E2=80=99s a proof-of-concept that blending differe=
nt sequence modeling paradigms (like attention and SSM) can overcome limita=
tions neither could solve alone. Expect to see more hybrids in the future, =
as others build on Falcon-H1=E2=80=99s recipe to push AI toward being faste=
r, leaner, and yet even more powerful.
6. EDGE-GRPO: Entropy-Driven GRPO with Guided Error Correction for Advantag=
e Diversity
Watching: EDGE-GRPO (paper [ https://substack.com/redirect/b75dea53-d8d6-4f=
0b-879e-14f9be27d690?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6R=
xQ4tAjMRMPOw0 ])
What problem does it solve? Recent methods like GRPO have improved LLM reas=
oning by using reinforcement learning on reasoning traces, but they hit a n=
asty snag: advantage collapse. In GRPO, model outputs are grouped and only =
the best group gets a reward =E2=80=93 if multiple different answers all ea=
rn the same (e.g. zero) reward, the algorithm can=E2=80=99t tell which dire=
ction is better to update towards. Essentially, the learning signal within =
each group collapses to nothing, making it hard for the model to improve es=
pecially on challenging tasks where it initially gets everything wrong. Thi=
s leads to stagnation: the policy doesn=E2=80=99t learn nuanced differences=
 between a terrible answer and a almost-correct answer, since both get labe=
led =E2=80=9C0=E2=80=9D. Prior approaches tried to fix this by forcing more=
 diverse answers (so that maybe one of them gets a reward) or by adding ext=
ra internal feedback signals, but these had limitations. The fundamental pr=
oblem remains: how to preserve informative gradient signals even when expli=
cit rewards are sparse or identical for many samples, so that the model con=
tinues to learn rather than plateauing.
How does it solve the problem? EDGE-GRPO introduces two key innovations to =
keep training signals rich: Entropy-Driven Advantage and Guided Error Corre=
ction (hence the acronym EDGE). The entropy-driven advantage means the algo=
rithm uses the model=E2=80=99s own confidence (entropy of its output distri=
bution) as part of the advantage computation. Intuitively, if the model is =
very unsure (high entropy) versus very confident (low entropy) about an ans=
wer, EDGE-GRPO will treat those cases differently even if both got the same=
 final reward. This helps avoid the case where all bad answers look equally=
 bad =E2=80=93 a hesitant wrong answer might be penalized differently than =
a confidently wrong answer, encouraging the model to learn to be confidentl=
y correct. The guided error correction component involves providing targete=
d feedback or updates for wrong answers specifically. While the paper=E2=80=
=99s details are technical, the concept is that the training process active=
ly corrects errors by nudging the model in the direction of known-good reas=
oning steps (possibly through an auxiliary reward for making certain improv=
ements or via human-provided hints integrated into the reward). Together, t=
hese mechanisms ensure that even within a group of responses that all fail,=
 there is gradient diversity =E2=80=93 some responses get a bit more advant=
age than others based on their entropy or partial progress, and the model r=
eceives guidance on how to fix its mistakes beyond just =E2=80=9Ctry again.=
=E2=80=9D
What are the key findings? By attacking advantage collapse at its root, EDG=
E-GRPO achieves more stable and improved training outcomes on several chall=
enging reasoning benchmarks. The paper reports that models trained with EDG=
E-GRPO steadily increase their reasoning scores where baseline GRPO would o=
ften flatline. One striking observation: after EDGE-GRPO training, the mode=
l=E2=80=99s correct answers tend to have lower entropy (i.e. the model is m=
ore confident and decisive) while its incorrect attempts remain high-entrop=
y (the model expresses uncertainty when it=E2=80=99s likely wrong). This is=
 a desirable trait =E2=80=93 it means the AI knows when it knows something,=
 and hesitates when it doesn=E2=80=99t, which is important for trustworthin=
ess. In terms of raw performance, EDGE-GRPO-trained models outperformed the=
ir vanilla GRPO counterparts across the board, especially on problems that =
require multiple reasoning steps. They also found that some previous fixes =
(like forcing the model to reflect or self-criticize) did increase diversit=
y but didn=E2=80=99t fully solve the issue, whereas EDGE-GRPO=E2=80=99s ent=
ropy-based strategy clearly reduced instances of advantage collapse. The ap=
proach showed its strength even as tasks scaled in difficulty, indicating b=
etter generalization and resilience of the learning process.
Why does it matter? As LLMs are pushed to perform more complex reasoning (t=
hink multi-step math, logical puzzles, code generation with debugging), rei=
nforcement learning is one of the only ways to really supervise the process=
=2E Making RL work well for these model=
s is thus critical. EDGE-GRPO provides=
 a new toolkit for robustly training reasoning agents, ensuring they contin=
ue to improve even when facing mostly failure at the start (common in very =
hard tasks). In practical terms, this could translate to smarter AI assista=
nts that learn to solve problems that initially stumped them, without givin=
g up due to a lack of feedback signal. The notion of using the model=E2=80=
=99s own entropy as a teaching signal is also intriguing =E2=80=93 it lever=
ages an internal metric (confidence) to guide learning, which might be appl=
icable to other ML scenarios beyond language. Lastly, by better aligning re=
wards with actual reasoning quality (not just final correctness), approache=
s like EDGE-GRPO could produce models with more calibrated confidence and f=
ewer spurious =E2=80=9Chigh-confidence wrong=E2=80=9D answers. For anyone b=
uilding complex AI reasoning systems, these improvements in training stabil=
ity and performance are stepping stones toward reliable and continuously le=
arning reasoning AI.
7. Magentic-UI: Towards Human-in-the-loop Agentic Systems
Watching: Magentic-UI (paper [ https://substack.com/redirect/e819ef8d-aa23-=
481e-af86-ce5959816427?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG=
6RxQ4tAjMRMPOw0 ])
What problem does it solve? Autonomous LLM-based agents (think AutoGPT-styl=
e systems that browse, code, or execute tasks on their own) are exciting bu=
t unreliable and potentially unsafe. They often mis-execute instructions, g=
et stuck, or even pursue harmful actions if not reined in, because they lac=
k human judgment. At the same time, having a human monitor every step defea=
ts the purpose of automation. The problem Magentic-UI tackles is: How can w=
e integrate human oversight into AI agents in a seamless, efficient way, so=
 that humans can guide the agent when needed (ensuring safety and correctne=
ss) without micromanaging everything? This involves both a technical challe=
nge (building a system where humans and AI can interact fluidly) and an HCI=
 challenge (figuring out what control mechanisms make sense to end-users). =
Essentially, it=E2=80=99s about combining human-in-the-loop control with ag=
ent autonomy to get the best of both =E2=80=93 high success rates and safet=
y, with minimal human effort.
How does it solve the problem? The authors developed Magentic-UI, an open-s=
ource web-based interface and framework explicitly designed for human-agent=
 collaboration. Under the hood, it runs a flexible multi-agent system (mean=
ing you can have an LLM agent, tool-specific sub-agents, etc.) that can use=
 external tools like web browsers, code interpreters, file systems, and mor=
e via a standardized protocol (the Model Context Protocol, MCP). The UI par=
t is what the human sees and interacts with. Magentic-UI offers six interac=
tion mechanisms that let a human intervene or cooperate with the agent at d=
ifferent levels. For example, co-planning lets a human and the AI draft a t=
ask plan together, co-tasking might allow the human to handle one subtask w=
hile the AI handles another, and multi-tasking could enable overseeing mult=
iple agents in parallel. There are action guards, where certain potentially=
 risky actions (like sending an email or deleting a file) are paused for hu=
man approval. A long-term memory mechanism allows both the agent and human =
to reference information persistently across a session (ensuring the agent =
doesn=E2=80=99t forget earlier context or corrections). These are just some=
 of the six =E2=80=93 the system essentially creates a control panel for th=
e AI. The design philosophy is to keep human involvement =E2=80=9Clow frict=
ion=E2=80=9D: the human can inject themselves into the loop with minimal ef=
fort when needed, and step back out when the agent is doing fine. To valida=
te the setup, Magentic-UI was tested in multiple modes =E2=80=93 from a ful=
ly Autonomous mode (agent has full tool control) to a Workflow mode (agent =
has no autonomy, strictly following a human-defined script) and a middle-gr=
ound Hybrid mode.
What are the key findings? Across extensive evaluations =E2=80=93 including=
 autonomous task completion benchmarks, simulated user studies, real user f=
eedback sessions, and targeted safety tests =E2=80=93 Magentic-UI showed th=
at the human-in-the-loop approach can significantly boost task success and =
safety. In autonomous benchmark runs, agents using Magentic-UI=E2=80=99s fr=
amework completed more multi-step tasks correctly than fully-autonomous age=
nts, largely because the human could step in at crucial junctures to preven=
t failure. The interaction mechanisms proved effective: for instance, in us=
er trials, even non-expert users were able to use the co-planning and actio=
n guard features to steer the agent away from errors (like choosing the cor=
rect intermediate tool or aborting a dubious action). Quantitatively, the p=
aper notes that Hybrid mode (some autonomy, some human control) achieved th=
e best balance =E2=80=93 it nearly matched the success rate of fully autono=
mous runs (which benefit from the AI=E2=80=99s speed) while maintaining the=
 safety and correctness of the workflow mode (where nothing dangerous or in=
correct slips through without a human check). Meanwhile, Workflow mode gave=
 users deterministic control and was preferred for sensitive tasks, though =
it slowed things down (as expected). The safety assessment found that Magen=
tic-UI=E2=80=99s guardrails (like action approval dialogues and the ability=
 to inspect an agent=E2=80=99s reasoning) helped catch misaligned actions a=
nd adversarial manipulations that a user might otherwise miss until it=E2=
=80=99s too late. Users in qualitative studies reported feeling more confid=
ence and trust in the AI when using Magentic-UI, since they had transparenc=
y into what the agent was doing and the ability to intervene. All these fin=
dings underscore that thoughtfully adding a human loop doesn=E2=80=99t just=
 avoid disasters =E2=80=93 it can genuinely improve the agent=E2=80=99s per=
formance on complex tasks.
Why does it matter? As we integrate AI agents into real-world applications =
(from autonomous coding assistants to AI customer support reps), pure auton=
omy is often a liability. Magentic-UI presents a practical path forward: AI=
-human collaboration interfaces that amplify the strengths of both. Humans =
provide oversight, strategic guidance, and moral judgment; AI provides spee=
d, consistency, and ability to handle the grunt work. This synergy can tack=
le problems neither could alone =E2=80=93 AI might get 80% of a complex tas=
k done quickly, and a human can ensure the last 20% (and the overall direct=
ion) are correct. The fact that Magentic-UI is open-source is also signific=
ant: it gives researchers and developers a ready-made platform to study and=
 deploy human-in-loop agents, accelerating progress in this crucial area. W=
e often talk about =E2=80=9CAI alignment=E2=80=9D in abstract terms, but he=
re is a concrete alignment tool =E2=80=93 put a human in the loop in a stru=
ctured way. In a broader sense, Magentic-UI is a step toward making AI agen=
ts trustworthy and user-friendly. Instead of fearing what an autonomous age=
nt might do, a user can collaborate with it, guiding it like a teammate. Th=
is could ease the adoption of agentic AI in high-stakes domains (like medic=
ine or finance) where a human will always need to have a say. Overall, it s=
hifts the narrative from humans versus AI to humans and AI solving problems=
 together, which is arguably how many real systems will be designed in the =
foreseeable future.
8. Agentic Reinforced Policy Optimization (ARPO)
Watching: ARPO ()
What problem does it solve? Large language model agents that operate over m=
ultiple steps =E2=80=93 especially those that can call tools or APIs =E2=80=
=93 pose a challenge for traditional reinforcement learning. Standard RL ei=
ther treats each action in isolation or looks only at final success, and ne=
ither is ideal for a multi-step reasoning scenario. For example, imagine an=
 LLM agent that has to solve a puzzle by doing web searches (tools) and the=
n giving an answer. If it gets the answer wrong, traditional RL might just =
give a zero reward at the end, without clarity on which tool use or step in=
 the reasoning was at fault. Moreover, the agent=E2=80=99s confidence can f=
luctuate wildly during the process =E2=80=93 the authors observed that righ=
t after the agent uses a tool and gets new information, its next action=E2=
=80=99s prediction entropy spikes (essentially, the agent says =E2=80=9Cnow=
 what?=E2=80=9D with high uncertainty). Existing RL algorithms don=E2=80=99=
t account for these =E2=80=9Coh no, I=E2=80=99m confused=E2=80=9D moments; =
they sample actions uniformly and might waste a lot of time exploring even =
when the model is confident, or conversely, not exploring enough when the m=
odel is confused. So the problem ARPO tackles is how to do fine-grained RL =
for multi-turn LLM agents, making sure the learning algorithm knows which p=
arts of the interaction need more exploration or specialized credit assignm=
ent (like after using a tool), instead of treating an entire trajectory wit=
h one blunt feedback.
How does it solve the problem? ARPO is a custom-tailored RL algorithm for L=
LM-based agents in interactive environments. It introduces an entropy-based=
 adaptive rollout mechanism: in simpler terms, the training dynamically all=
ocates more exploration to those steps where the agent is highly uncertain =
(high entropy) =E2=80=93 often immediately after tool usage or a big step i=
n reasoning. For instance, if after consulting a database the model seems u=
nsure how to use that info, ARPO will encourage trying different next moves=
 there more than it would elsewhere. This prevents the agent from repeatedl=
y skipping over critical decision points in a hurry; it focuses learning ef=
fort where the agent struggles. Additionally, ARPO implements a step-level =
advantage attribution. Instead of just giving one reward at the end or a pe=
r-turn reward, it computes how much each action (each question asked, each =
tool invoked) contributed to the final success or failure. This means the m=
odel gets a nuanced learning signal =E2=80=93 maybe the final answer was wr=
ong, but some earlier steps were actually beneficial and should be reinforc=
ed (or vice versa). Under the hood, ARPO balances global trajectory samplin=
g (exploring different overall sequences of actions) with fine-grained step=
 sampling (re-sampling specific decisions in the sequence), adjusting on th=
e fly. It=E2=80=99s like a coach that sometimes lets the agent play a whole=
 game, and other times says =E2=80=9Clet=E2=80=99s replay just that tricky =
part one more time,=E2=80=9D thereby efficiently honing the agent=E2=80=99s=
 skills at difficult junctures.
What are the key findings? ARPO-trained agents delivered substantially bett=
er performance on a suite of 13 challenging benchmarks, which included comp=
utational reasoning tasks, knowledge reasoning, and deep search problems. T=
hey outperformed agents trained with conventional reinforcement learning ap=
proaches, particularly in scenarios where multiple tool calls and reasoning=
 steps were needed. One headline result: ARPO achieved the same or better p=
erformance using only half the tool interactions compared to baseline metho=
ds. In practical terms, if a naive agent might call an external calculator =
or wiki browser 10 times to get an answer right, an ARPO agent might solve =
it with only 5 calls because it=E2=80=99s learning to use tools more effici=
ently. This suggests ARPO agents are learning more strategic and purposeful=
 tool use, rather than flailing around. Another observation is that ARPO=E2=
=80=99s focus on uncertain moments paid off =E2=80=93 the entropy spikes th=
e authors noticed became opportunities for learning, and over time the agen=
ts became more confident and accurate in those once-problematic steps. On t=
asks requiring long reasoning chains, ARPO agents maintained coherence and =
didn=E2=80=99t get as derailed by earlier irrelevant actions (a sign that t=
he advantage attribution helped credit the right actions). The researchers =
also note that ARPO bridges a gap between two extremes: pure end-to-end RL =
versus scripted tool use. By combining high-level reward with step-level gu=
idance, it kind of gets the advantages of both (global optimization with lo=
cal feedback). The end result is an agent that is more competent in realist=
ic multi-turn settings =E2=80=93 it makes fewer needless tool calls, and wh=
en it does act, its actions are more often actually helpful toward the goal=
=2E
Why does it matter? Training AI agents to reliably perform multi-step tasks=
 (like researching a topic online and writing a summary, or debugging a pie=
ce of code using documentation) is one of the frontiers of AI right now. AR=
PO is a significant advancement in that it acknowledges and addresses the u=
nique challenges in this setting =E2=80=93 long horizons, external tools, a=
nd varying uncertainty. For the AI community, it=E2=80=99s a proof that we =
can adapt reinforcement learning to the quirks of LLM-based reasoning. This=
 means future agents can be trained more effectively, reaching higher compe=
tence with less training data or costly interaction. The fact that ARPO cut=
s tool usage in half is also economically relevant =E2=80=93 many tool call=
s (e.g., APIs, database queries) have costs or latency, so making agents mo=
re frugal without losing performance is a big win. Moreover, ARPO=E2=80=99s=
 ideas might inspire algorithms in other domains where an AI=E2=80=99s conf=
idence fluctuates (imagine a self-driving car that becomes uncertain in new=
 traffic situations =E2=80=93 a similar adaptive exploration could be usefu=
l). Big picture: ARPO brings us closer to AI agents that are both efficient=
 and adaptive in dynamic tasks. It=E2=80=99s a step away from brittle one-s=
hot prompt answering and toward agents that can learn from and react to an =
environment intelligently, which is a core component of any hoped-for gener=
al AI.
9. RLVMR: Reinforcement Learning with Verifiable Meta-Reasoning Rewards for=
 Robust Long-Horizon Agents
Watching: RLVMR (paper [ https://substack.com/redirect/d32dd155-5661-4619-b=
c4d-0e63b634449f?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4t=
AjMRMPOw0 ])
What problem does it solve? Reinforcement learning agents, especially langu=
age-model-based ones, can sometimes learn to get the right answers for the =
wrong reasons. If an RL agent receives a reward only for completing a task,=
 it might end up exploiting quirks or repeating trial-and-error sequences t=
hat achieve the goal without truly understanding the task =E2=80=93 leading=
 to brittle behavior that fails if conditions change even slightly. This is=
 a known issue: optimizing solely for final success often reinforces flawed=
 or inefficient reasoning paths. The agent doesn=E2=80=99t learn =E2=80=9Ch=
ow to reason,=E2=80=9D it just learns =E2=80=9Chow to win,=E2=80=9D which m=
ight involve hacks that don=E2=80=99t generalize. For long-horizon tasks (e=
=2Eg., interactive game environments or=
 complex problem-solving), this means=20=
you get agents that reach the end by stumbling through, but they=E2=80=99re=
 brittle and not interpretable. The problem RLVMR addresses is how to make =
RL agents actually learn to think coherently and not just luck into the cor=
rect outcomes. In other words, can we reward the process of reasoning, not =
just the outcome, so that an agent=E2=80=99s internal decision-making becom=
es sound and reliable?
How does it solve the problem? RLVMR introduces a novel training framework =
that injects dense, process-level feedback into the reinforcement learning =
loop. It works by having the agent explicitly tag or demarcate steps of its=
 own reasoning (for example, an agent might label parts of its action seque=
nce as =E2=80=9Cplanning=E2=80=9D, =E2=80=9Cexploration=E2=80=9D, =E2=80=9C=
reflection=E2=80=9D, etc.), and then the trainer provides verifiable reward=
s for good behavior in those meta-reasoning steps. These rewards are progra=
mmatic and rule-based: for instance, the agent could get a small reward for=
 laying out a clear plan of attack (because a plan can be checked for coher=
ence), another reward for exploring a new path after failing rather than re=
peating an old failed action (encouraging it to not get stuck in loops), an=
d a reward for self-reflecting and correcting an error. These are =E2=80=9C=
verifiable=E2=80=9D in that the environment or a monitoring process can che=
ck if the agent did what it claimed (e.g., it said it was exploring and ind=
eed it tried a new solution). RLVMR combines these process rewards with the=
 usual final outcome reward into a single learning objective, which they op=
timize with a policy gradient method (notably, they do it critic-free, simp=
lifying training). Essentially, RLVMR is teaching the agent how to reason w=
ell by rewarding good reasoning steps along the way. The agent=E2=80=99s po=
licy isn=E2=80=99t just outputting actions; it=E2=80=99s outputting actions=
 plus annotations of its cognitive steps, which the training algorithm cont=
inuously critiques and reinforces. This guided approach pushes the agent to=
 adopt strategies a human might consider common-sense: make a plan, follow =
through, adapt if needed, rather than randomly thrashing until something wo=
rks.
What are the key findings? Agents trained with RLVMR become significantly m=
ore robust and effective in long-horizon environments. On challenging inter=
active benchmarks like ALFWorld and ScienceWorld, RLVMR set new state-of-th=
e-art performance =E2=80=93 for example, a 7B-parameter LLM agent achieved =
an 83.6% success rate on the hardest unseen task split in ALFWorld, whereas=
 previous approaches were much lower. These tasks involve multiple steps an=
d require carrying out procedures in a simulated world, so that high succes=
s rate indicates the agent can reliably handle long tasks it hasn=E2=80=99t=
 explicitly seen before. More illuminating, the agents=E2=80=99 behavior ch=
anged qualitatively: reasoning quality improved dramatically. They exhibite=
d far fewer redundant or pointless actions =E2=80=93 meaning the agent wasn=
=E2=80=99t just flailing around to accidentally solve the task, but was fol=
lowing more efficient, logical sequences. When mistakes happened, RLVMR age=
nts showed enhanced error recovery: instead of getting stuck or repeating t=
he same failed action, they would try a different strategy or backtrack app=
ropriately (just as a human problem-solver might do upon realizing an appro=
ach isn=E2=80=99t working). Another benefit was interpretability: because t=
he agent tags its cognitive steps and is rewarded for doing so clearly, one=
 can literally see the agent =E2=80=9Cthinking=E2=80=9D in a structured way=
=2E The researchers could verify that t=
he performance gains indeed stemmed fr=
om better reasoning =E2=80=93 for instance, the agent=E2=80=99s reflections=
 often identified exactly why a previous attempt failed and how to fix it, =
demonstrating a depth of understanding that a pure outcome-based RL agent l=
acked. All of this suggests that the dense rewards successfully shaped not =
just what the agent does, but how it does it, leading to solutions that are=
 both correct and grounded in sound reasoning. The combination of process a=
nd outcome rewards meant the agent didn=E2=80=99t sacrifice success for the=
 sake of following rules =E2=80=93 it actually achieved higher success beca=
use following those reasoning =E2=80=9Crules=E2=80=9D led to more robust st=
rategies.
Why does it matter? RLVMR is a compelling proof that we can train not just =
for task success, but for the quality of reasoning, and get better agents a=
s a result. This has implications for the development of trustworthy AI. Ag=
ents that genuinely reason through problems (as opposed to those that accid=
entally solve them) are more likely to handle new situations and to fail gr=
acefully when they do fail. By rewarding meta-reasoning, RLVMR also aligns =
the agent=E2=80=99s incentives with behaviors we consider desirable (like d=
eliberation, exploration, self-correction) =E2=80=93 which could reduce the=
 incidence of weird, unexplainable shortcuts or cheats that pure reward max=
imization might encourage. In practical terms, this could lead to AI assist=
ants that, say, show their work and double-check critical steps when they c=
ode or solve math problems, because those behaviors were baked into their t=
raining rewards. The success on long-horizon tasks is also a green light fo=
r tackling more complex, multi-step real-world problems with AI. It suggest=
s that if we can formalize what good reasoning looks like in a domain (even=
 via simple rules), we can substantially boost an agent=E2=80=99s performan=
ce and reliability in that domain. Lastly, RLVMR adds to the toolbox of ali=
gnment techniques =E2=80=93 it=E2=80=99s a way of telling the AI =E2=80=9Cn=
ot only do the job, but do it in a way that we consider logically sound.=E2=
=80=9D As AI systems become more autonomous, methods like this will be key =
to ensure they don=E2=80=99t just get results, but get them in a safe and i=
nterpretable manner.
10. MemTool: Optimizing Short-Term Memory Management for Dynamic Tool Calli=
ng in LLM Agent Conversations
Watching: MemTool (paper [ https://substack.com/redirect/d73d8ff6-7273-40b0=
-834b-5d7f80f86a42?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ=
4tAjMRMPOw0 ])
What problem does it solve? LLM-based agents that carry on extended multi-t=
urn conversations and invoke tools face a practical memory issue: the conte=
xt window isn=E2=80=99t infinite. As an agent interacts (say it=E2=80=99s a=
 chatbot that can use a calculator, search engine, etc.), each tool=E2=80=
=99s output and the agent=E2=80=99s intermediate reasoning take up space in=
 the prompt. Over many turns, this =E2=80=9Cshort-term memory=E2=80=9D can =
overflow, forcing the agent to drop older information =E2=80=93 which might=
 be important later. Without strategy, agents might forget key facts or, co=
nversely, hang on to too much irrelevant data and drown in context. The pro=
blem is how to intelligently manage the working memory of an agent: decidin=
g what tool outputs or intermediate results to keep and what to discard as =
a conversation or task progresses. It=E2=80=99s analogous to our own short-=
term memory management (we remember what=E2=80=99s relevant and let trivial=
 details fade). For AI, getting this wrong either leads to failures (forget=
ting needed info) or inefficiency (context blow-up).
How does it solve the problem? MemTool provides a dedicated framework and s=
et of strategies for dynamic context management in tool-using agents. In pr=
actice, it offers three modes of operation:
Autonomous Agent Mode: The agent itself has full autonomy to decide when to=
 drop or retain tool-related information from its context. For example, aft=
er using a tool, the agent can choose to =E2=80=9Cforget=E2=80=9D the tool=
=E2=80=99s output in subsequent turns if it deems it no longer needed.
Workflow Mode: A deterministic, human-defined policy controls memory =E2=80=
=93 essentially no autonomy. This could be a simple rule like =E2=80=9Calwa=
ys remove the result of a tool call after 3 turns=E2=80=9D or =E2=80=9Conly=
 keep the last result from each tool.=E2=80=9D The agent doesn=E2=80=99t de=
cide; it just follows a fixed procedure for memory.
Hybrid Mode: A mix of both =E2=80=93 perhaps the agent can suggest what to =
remove, but there are overrides or confirmations via set rules. Or certain =
critical data is always kept by rule, while the agent can manage the rest.
MemTool is implemented as an extension on top of a multi-agent conversation=
 architecture (using the Model Context Protocol to interface with tools). I=
t monitors the context window and applies the above policies to prune or pe=
rsist tool outputs across turns. The design allows plugging in these modes =
without changing the underlying agent: you can swap between letting the age=
nt think about memory or enforcing a policy, based on model capability and =
use case. To evaluate it, the authors stress-tested MemTool on a benchmark =
called ScaleMCP with conversations spanning 100+ user interactions, which w=
ould normally overflow any context. They measured how effectively each mode=
 prunes irrelevant content (short-term memory efficiency) and how that impa=
cts task completion accuracy =E2=80=93 does the agent still get the answers=
 right?
What are the key findings? Memory management can indeed be handled, and the=
 best approach depends on the model=E2=80=99s sophistication. In Autonomous=
 mode, very capable LLMs (think GPT-4 class or similarly strong models) wer=
e able to achieve extremely high memory efficiency =E2=80=93 they removed a=
bout 90=E2=80=9394% of unnecessary tool content from the context (averaged =
over a 3-turn window). This indicates that advanced models can learn or be =
prompted to recognize what information is safe to forget. However, medium-s=
ized or less advanced models struggled when left on their own, with only 0=
=E2=80=9360% efficiency (sometimes basically forgetting nothing or forgetti=
ng the wrong things). They lack the judgment to prune memory well. The Work=
flow mode, by contrast, consistently maintained a tidy context regardless o=
f model size =E2=80=93 as expected, since a fixed policy was ensuring obsol=
ete info was dropped. There=E2=80=99s no ambiguity or model error in decidi=
ng; the rules did the job. But the trade-off came in task performance: Auto=
nomous and Hybrid modes tended to excel at task completion, especially on c=
omplex tasks, compared to a strict workflow. This is likely because the age=
nt in those modes could choose to keep information it knew would be relevan=
t later, whereas a rigid workflow policy might blindly throw out something =
that, unbeknownst to it, the agent could have used in a future step. The Hy=
brid mode often hit a sweet spot =E2=80=93 tool removal was effective and n=
ear-optimal, yet the final accuracy remained high. It seems giving the agen=
t some say (especially a smart agent) in what to remember, while enforcing =
basic rules, leads to both a clean context and correct answers. One concret=
e example from the evaluations: a large LLM in Hybrid mode might remove =E2=
=80=9Ctemporary=E2=80=9D tool outputs (like intermediate calculations) but =
hold on to a key fact it found via a tool, even if the workflow policy woul=
d normally purge it =E2=80=93 and that fact later leads to a correct answer=
, demonstrating the value of agent insight in memory management. Across the=
 100-turn conversations, MemTool prevented context overflow and thereby all=
owed the dialogues to continue smoothly with relevant information always at=
 hand.
Why does it matter? This work is a stepping stone toward scalable, long-run=
ning AI agents. In real-world usage, we want AI that can engage in prolonge=
d tasks =E2=80=93 think of an AI assistant that can research, plan, and exe=
cute over hours or days of interaction. Without good memory management, suc=
h agents would either forget early instructions or become impossibly expens=
ive to run (stuffing huge transcripts into their context window). MemTool s=
hows that with a combination of model smarts and engineered policy, we can =
extend an agent=E2=80=99s effective memory without infinite context windows=
=2E It=E2=80=99s like giving the agent a w=
orking memory akin to an organized=20=
notebook: it writes down what it needs, erases what=E2=80=99s done, and alw=
ays has room for the next idea. For practitioners, these findings also sugg=
est practical guidelines: if you=E2=80=99re using a strong model, you can t=
rust it more to handle its context (maybe just give it a framework like Mem=
Tool=E2=80=99s autonomous mode with some prompts to encourage forgetting). =
If you=E2=80=99re on a smaller model, you might implement a strict memory c=
learance policy or use a hybrid approach to avoid the model drowning in old=
 info. In broader AI research, MemTool touches on the concept of meta-cogni=
tion =E2=80=93 the agent thinking about its own memory usage. That=E2=80=99=
s a vital aspect of human cognition (we constantly decide what to keep in m=
ind or not), and implementing it in AI can lead to more efficient and human=
-like problem solving. Finally, this contributes to making AI more efficien=
t and cheaper to deploy: if an agent knows to drop irrelevant context, it u=
ses fewer tokens in prompts over time, which could dramatically cut costs f=
or API-based models. In summary, MemTool points the way to AI that can hand=
le longer and more complex interactions by cleverly managing its finite mem=
ory, an essential capability for any advanced autonomous system.
11. Graph-R1: Towards Agentic GraphRAG via End-to-end Reinforcement Learnin=
g
Watching: Graph-R1 (paper [ https://substack.com/redirect/3ce42c96-33ed-413=
1-be1b-8e57483a6369?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6Rx=
Q4tAjMRMPOw0 ]/code [ https://substack.com/redirect/ae62c852-f25b-484e-8bcd=
-fd70482bf479?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjM=
RMPOw0 ])
What problem does it solve? Retrieval-Augmented Generation (RAG) mitigates =
hallucination in LLMs by incorporating external knowledge, but relies on ch=
unk-based retrieval that lacks structural semantics. GraphRAG methods impro=
ve RAG by modeling knowledge as entity-relation graphs, but still face chal=
lenges in high construction cost, fixed one-time retrieval, and reliance on=
 long-context reasoning and prompt design. The problem Graph-R1 addresses i=
s how to make knowledge retrieval interactive and agentic: the AI should be=
 able to walk through a knowledge graph step by step, deciding which nodes =
(topics) to explore next based on the current query, and do so in a trained=
 optimal way. In short, it=E2=80=99s tackling the gap between static knowle=
dge retrieval and the kind of dynamic, exploratory search a human might do =
when answering a complex question, and doing this without an expensive upfr=
ont graph construction or complicated prompt engineering.
How does it solve the problem? Graph-R1 transforms the retrieval process in=
to an interactive environment and trains a reinforcement learning agent to =
operate within it. Instead of retrieving text chunks once, the system const=
ructs a lightweight knowledge hypergraph of the information =E2=80=93 a str=
ucture of entities and their relations that is cheaper to build and rich in=
 semantics. The Graph-R1 agent then models retrieval as a multi-turn agent-=
environment interaction: it treats the knowledge base like a world to navig=
ate, where each action might be following a link or querying a related conc=
ept. The agent is optimized end-to-end via a reward mechanism that aligns w=
ith finding the correct answer. This means the agent learns policies for re=
trieval =E2=80=93 when to stick with the current topic, when to pivot to a =
related entity, when to stop gathering information =E2=80=93 all through tr=
ial and error guided by whether it leads to better answers. Crucially, by u=
sing RL, Graph-R1 doesn=E2=80=99t require manual prompt strategies for mult=
i-hop queries; the agent organically figures out a strategy to fetch what i=
t needs. The authors also introduce tricks for lightweight hypergraph const=
ruction, so that the agent isn=E2=80=99t burdened by an enormous graph: it =
likely builds or expands the graph on the fly as needed, keeping retrieval =
targeted. Overall, Graph-R1 is an agentic GraphRAG: a graph-guided retrieva=
l system where an RL-trained agent actively and intelligently controls the =
retrieval process, bridging structured knowledge with the language model=E2=
=80=99s reasoning in an integrated loop.
What are the key findings? Graph-R1 sets a new bar for accuracy and efficie=
ncy in knowledge-intensive tasks. Experiments on standard RAG datasets show=
 that Graph-R1 outperforms both traditional GraphRAG and RL-enhanced RAG me=
thods in reasoning accuracy, retrieval efficiency, and generation quality. =
In terms of reasoning accuracy, Graph-R1=E2=80=99s answers were more often =
correct on multi-hop and knowledge-dense questions, indicating the agent su=
ccessfully gathered the needed evidence that static one-shot retrieval migh=
t miss. For retrieval efficiency, the agent found relevant information with=
 fewer steps and less irrelevant data =E2=80=93 it doesn=E2=80=99t shotgun =
a dozen documents if only two well-chosen pieces suffice. The generation qu=
ality also benefited: answers were coherent and well-supported by the retri=
eved facts, since the agent could ensure all necessary context was obtained=
 and extraneous bits were left out. Notably, Graph-R1=E2=80=99s performance=
 didn=E2=80=99t come at the cost of massively increasing computation; by ke=
eping the knowledge search targeted via the hypergraph and RL policy, it re=
mained cost-effective. The study also likely found that end-to-end RL helpe=
d the model navigate ambiguous queries: the agent could try one line of inq=
uiry, and if it turned out to be a dead end (leading to low reward), learn =
to try an alternate path in future episodes =E2=80=93 something a fixed ret=
rieval strategy can=E2=80=99t adaptively do. By bridging structured graphs =
with the flexibility of an agent, Graph-R1 demonstrated that learning to re=
trieve can beat even strong static retrieval heuristics.
Why does it matter? This result is an exciting step toward more intelligent=
 information-seeking AI. Instead of treating the knowledge source as a stat=
ic library, Graph-R1 treats it as a navigable world =E2=80=93 bringing rein=
forcement learning, which is typically used in games or robotics, into the =
realm of knowledge retrieval. The payoff is an AI that=E2=80=99s better at =
knowing when it has enough information and where to find it rather than gue=
ssing or hallucinating. This could dramatically improve systems like search=
 engines, virtual assistants, or any AI that needs to base answers on exter=
nal knowledge: we=E2=80=99ll get answers that are not just confident, but g=
rounded in a verifiable path of retrieved evidence (making the AI=E2=80=99s=
 reasoning more transparent too). Moreover, Graph-R1=E2=80=99s success sugg=
ests that building lighter weight knowledge representations (like hypergrap=
hs) and training agents on them can outperform brute-force approaches that =
dump massive documents into an LLM. It=E2=80=99s a win for efficiency and s=
calability =E2=80=93 as knowledge bases grow, an agent that can selectively=
 explore will be far more tractable than trying to cram everything into con=
text. In a broader sense, Graph-R1 is a convergence of symbolic and neural =
approaches: it uses a symbolic structure (graph) and a neural policy (RL ag=
ent) together. This hints at future AI systems that combine the reliability=
 of symbolic reasoning with the flexibility of learning. All said, Graph-R1=
 advances the state of the art in making AI that is both knowledgeable and =
able to actively gather knowledge, a crucial ability for any AI aspiring to=
 truly assist with complex, real-world questions.
Putting It All Together: Toward Smarter and Safer LLM Agents
From these papers, we can see multiple convergent themes:
First, scaling and new architectures (like Kimi K2=E2=80=99s trillion-param=
eter MoE and Falcon-H1=E2=80=99s hybrid design) are giving open models unpr=
ecedented power, narrowing the gap with closed models.=20
Second, a host of novel training methods =E2=80=93 from smarter reinforceme=
nt learning algorithms to language-based self-evolution=E2=80=93 are greatl=
y boosting reasoning capabilities and sample efficiency.=20
Third, researchers are acknowledging that human guidance and knowledge stru=
cture remain crucial: frameworks like Magentic-UI put humans in the loop fo=
r safety, and Graph-R1=E2=80=99s agent navigates structured knowledge to av=
oid hallucination.=20
Finally, solutions for long-horizon tasks (MemTool=E2=80=99s memory managem=
ent, self-evolving agents) are enabling AI to operate coherently over exten=
ded interactions.=20
In essence, the community is engineering LLM systems that are not just bigg=
er, but designed smarter =E2=80=93 able to learn continually, reason throug=
h complex problems, leverage tools and knowledge bases effectively, and wor=
k alongside humans.=20
=E2=9D=A4=EF=B8=8F If you enjoyed this article, give it a like and share it=
 with your peers.
Thanks for reading LLM Watch! Subscribe for free to receive new posts and s=
upport my work

Unsubscribe https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly93d3cubGxtd2F=
0Y2guY29tL2FjdGlvbi9kaXNhYmxlX2VtYWlsP3Rva2VuPWV5SjFjMlZ5WDJsa0lqb3pNelkwTk=
RneU1qTXNJbkJ2YzNSZmFXUWlPakUyT1RZM01EQXpNQ3dpYVdGMElqb3hOelUwTURjeE56UXlMQ=
0psZUhBaU9qRTNPRFUyTURjM05ESXNJbWx6Y3lJNkluQjFZaTB4TkRJNE5qWTNJaXdpYzNWaUlq=
b2laR2x6WVdKc1pWOWxiV0ZwYkNKOS5uXzFxd2NyMVUwUFJWUzBPX2laVjVMeEg1S0w2Vkptdl9=
4RXRXaUZMZTZBIiwicCI6MTY5NjcwMDMwLCJzIjoxNDI4NjY3LCJmIjp0cnVlLCJ1IjozMzY0ND=
gyMjMsImlhdCI6MTc1NDA3MTc0MiwiZXhwIjoyMDY5NjQ3NzQyLCJpc3MiOiJwdWItMCIsInN1Y=
iI6ImxpbmstcmVkaXJlY3QifQ.DTG1-0duZ7WoxgQQBrhsfgbSqoLXoqbtIRZcyY3Hpiw?

--2f01162e9de12d1b406ea5774817f4a9378d6128010d961bb946b905b0eb
Content-Type: text/html; charset="utf-8"
Content-Transfer-Encoding: quoted-printable

<html style=3D"scrollbar-width: thin;scrollbar-color: rgb(219,219,219)rgb(2=
55,255,255);"><head>
<meta http-equiv=3D"Content-Type" content=3D"text/html; charset=3Dutf-8"><t=
itle>11 Papers You Should Know About</title><style>
@media (max-width: 1024px) {
  .typography .pullquote-align-left,
  .typography.editor .pullquote-align-left,
  .typography .pullquote-align-right,
  .typography.editor .pullquote-align-right,
  .typography .pullquote-align-wide,
  .typography.editor .pullquote-align-wide,
  .typography .pullquote-align-center,
  .typography.editor .pullquote-align-center {
    float: none;
    margin: 0 auto;
    width: 100%;
    max-width: 100%;
  }
}
@media all and (-ms-high-contrast: none), (-ms-high-contrast: active) {
  .typography .markup table.image-wrapper img,
  .typography.editor .markup table.image-wrapper img,
  .typography .markup table.kindle-wrapper img,
  .typography.editor .markup table.kindle-wrapper img {
    max-width: 550px;
  }
}
@media (min-width: 1024px) {
  .typography:not(:has(#toc)) .captioned-image-container figure:has(> a.ima=
ge2-offset-left),
  .typography.editor:not(:has(#toc)) .captioned-image-container figure:has(=
> a.image2-offset-left) {
    margin-left: var(--image-offset-margin);
  }
  .typography:not(:has(#toc)) .captioned-image-container figure:has(> a.ima=
ge2-offset-right),
  .typography.editor:not(:has(#toc)) .captioned-image-container figure:has(=
> a.image2-offset-right) {
    margin-right: var(--image-offset-margin);
  }
}
@media (min-width: 1300px) {
  .typography .captioned-image-container figure:has(> a.image2-offset-left)=
,
  .typography.editor .captioned-image-container figure:has(> a.image2-offse=
t-left) {
    margin-left: var(--image-offset-margin);
  }
  .typography .captioned-image-container figure:has(> a.image2-offset-right=
),
  .typography.editor .captioned-image-container figure:has(> a.image2-offse=
t-right) {
    margin-right: var(--image-offset-margin);
  }
}
@media (max-width: 1024px) {
  .typography,
  .typography.editor {
    /* Disable offset on mobile/tablet */
  }
  .typography .captioned-image-container figure:has(> a.image2-align-left),
  .typography.editor .captioned-image-container figure:has(> a.image2-align=
-left),
  .typography .captioned-image-container figure:has(> a.image2-align-right)=
,
  .typography.editor .captioned-image-container figure:has(> a.image2-align=
-right) {
    float: none;
    margin: 1em auto;
    max-width: 100%;
    width: auto;
    padding: 0;
  }
  .typography .captioned-image-container figure:has(> a.image2-align-left.t=
hefp),
  .typography.editor .captioned-image-container figure:has(> a.image2-align=
-left.thefp),
  .typography .captioned-image-container figure:has(> a.image2-align-right.=
thefp),
  .typography.editor .captioned-image-container figure:has(> a.image2-align=
-right.thefp) {
    margin: 1em auto;
  }
  .typography .captioned-image-container figure:has(> a.image2-offset-left)=
,
  .typography.editor .captioned-image-container figure:has(> a.image2-offse=
t-left),
  .typography .captioned-image-container figure:has(> a.image2-offset-right=
),
  .typography.editor .captioned-image-container figure:has(> a.image2-offse=
t-right) {
    margin: 1em auto;
  }
  .typography .captioned-image-container figure:has(> a.image2-align-left) =
.image2-inset,
  .typography.editor .captioned-image-container figure:has(> a.image2-align=
-left) .image2-inset,
  .typography .captioned-image-container figure:has(> a.image2-align-right)=
 .image2-inset,
  .typography.editor .captioned-image-container figure:has(> a.image2-align=
-right) .image2-inset {
    display: block;
    justify-content: initial;
  }
}
@media (max-width: 768px) {
  .typography .markup div.sponsorship-campaign-embed,
  .typography.editor .markup div.sponsorship-campaign-embed {
    margin-top: 24px;
    margin-bottom: 24px;
  }
  .typography .markup div.sponsorship-campaign-embed:first-child,
  .typography.editor .markup div.sponsorship-campaign-embed:first-child {
    margin-top: 0px;
  }
}
@media screen and (max-width: 650px) {
  .typography .markup div.youtube-overlay,
  .typography.editor .markup div.youtube-overlay,
  .typography .markup div.vimeo-overlay,
  .typography.editor .markup div.vimeo-overlay {
    display: none !important;
  }
}
@media screen and (max-width: 370px) {
  .typography .markup div.tiktok-wrap,
  .typography.editor .markup div.tiktok-wrap {
    width: calc(95vw - 32px);
    height: calc((95vw - 32px - 2px) / 0.485714);
  }
}
@media screen and (max-width: 650px) {
  .typography .markup div.embedded-publication-wrap .embedded-publication.s=
how-subscribe,
  .typography.editor .markup div.embedded-publication-wrap .embedded-public=
ation.show-subscribe {
    padding: 24px;
  }
}
@media screen and (max-width: 650px) {
  .typography .markup div.subscription-widget-wrap .subscription-widget.sho=
w-subscribe,
  .typography.editor .markup div.subscription-widget-wrap .subscription-wid=
get.show-subscribe,
  .typography .markup div.subscription-widget-wrap-editor .subscription-wid=
get.show-subscribe,
  .typography.editor .markup div.subscription-widget-wrap-editor .subscript=
ion-widget.show-subscribe,
  .typography .markup div.captioned-button-wrap .subscription-widget.show-s=
ubscribe,
  .typography.editor .markup div.captioned-button-wrap .subscription-widget=
.show-subscribe {
    padding: 0px 24px;
  }
}
@media screen and (max-width: 650px) {
  .typography .markup div.subscription-widget-wrap .subscription-widget.sho=
w-subscribe .subscription-widget-subscribe .button,
  .typography.editor .markup div.subscription-widget-wrap .subscription-wid=
get.show-subscribe .subscription-widget-subscribe .button,
  .typography .markup div.subscription-widget-wrap-editor .subscription-wid=
get.show-subscribe .subscription-widget-subscribe .button,
  .typography.editor .markup div.subscription-widget-wrap-editor .subscript=
ion-widget.show-subscribe .subscription-widget-subscribe .button,
  .typography .markup div.captioned-button-wrap .subscription-widget.show-s=
ubscribe .subscription-widget-subscribe .button,
  .typography.editor .markup div.captioned-button-wrap .subscription-widget=
.show-subscribe .subscription-widget-subscribe .button {
    padding: 10px 12px;
    min-width: 110px;
  }
}
@media (max-width: 650px) {
  .typography .markup .tweet,
  .typography.editor .markup .tweet {
    padding: 12px;
  }
}
@media (max-width: 650px) {
  .typography .markup .tweet .tweet-text,
  .typography.editor .markup .tweet .tweet-text {
    font-size: 14px;
    line-height: 20px;
  }
}
@media (max-width: 650px) {
  .typography .markup .tweet .tweet-photos-container.two,
  .typography.editor .markup .tweet .tweet-photos-container.two,
  .typography .markup .tweet .tweet-photos-container.three,
  .typography.editor .markup .tweet .tweet-photos-container.three,
  .typography .markup .tweet .tweet-photos-container.four,
  .typography.editor .markup .tweet .tweet-photos-container.four {
    height: 200px;
  }
}
@media (max-width: 650px) {
  .typography .markup .tweet a.expanded-link .expanded-link-img,
  .typography.editor .markup .tweet a.expanded-link .expanded-link-img {
    max-height: 180px;
  }
}
@media (max-width: 650px) {
  .typography .markup .tweet a.expanded-link .expanded-link-description,
  .typography.editor .markup .tweet a.expanded-link .expanded-link-descript=
ion {
    display: none;
  }
}
@media screen and (max-width: 650px) {
  .typography .markup .apple-podcast-container,
  .typography.editor .markup .apple-podcast-container {
    width: unset;
  }
}
@media (max-width: 420px) {
  .typography .markup .install-substack-app-embed img.install-substack-app-=
embed-img,
  .typography.editor .markup .install-substack-app-embed img.install-substa=
ck-app-embed-img {
    margin: 0 auto 16px auto;
  }
}
@media (max-width: 420px) {
  .typography .markup .install-substack-app-embed .install-substack-app-emb=
ed-text,
  .typography.editor .markup .install-substack-app-embed .install-substack-=
app-embed-text {
    margin: 0 0 12px 0;
    max-width: 100%;
    width: auto;
    text-align: center;
  }
}
@media (max-width: 420px) {
  .typography .markup .install-substack-app-embed .install-substack-app-emb=
ed-link,
  .typography.editor .markup .install-substack-app-embed .install-substack-=
app-embed-link {
    display: flex;
    justify-content: center;
  }
}
@media screen and (min-width: 481px) {
  .share-button-container {
    height: 38px;
  }
}
@media screen and (min-width: 481px) {
  .share-button-container a.comment {
    height: 38px;
    line-height: 38px;
    padding-right: 10px;
  }
}
@media screen and (max-width: 480px) {
  .share-button-container .separator {
    display: block;
    margin: 0;
    height: 8px;
    border-left: none;
  }
}
@media screen and (max-width: 480px) {
  .share-button-container a.share.first img {
    padding-left: 0;
  }
}
@media screen and (min-width: 481px) {
  .share-button-container a.mobile {
    display: none !important;
  }
}
@media screen and (min-width: 541px) {
  .settings-add-pub-modal-wrapper .container .add-recommending-pub-modal-co=
ntainer {
    padding: 36px;
    height: 680px;
  }
}
@media screen and (min-width: 541px) {
  .settings-add-pub-modal-wrapper .container .add-recommending-pub-modal-co=
ntainer .footer {
    position: absolute;
    bottom: 36px;
    margin: 0px;
  }
}
@media screen and (max-width: 650px) {
  .header-anchor-parent {
    display: none;
  }
}
@media screen and (max-width: 768px) {
  .post {
    padding: 16px 0 0 0;
  }
}
@media screen and (max-width: 650px) {
  .post .post-header .post-label {
    margin-top: 8px;
  }
}
@media screen and (max-width: 650px) {
  .post .post-header .meta-author-wrap.alternative-meta .meta-right-column =
.post-meta {
    margin-top: 6px;
  }
}
@media screen and (max-width: 650px) {
  .post .footer-facepile-container {
    height: 64px;
    padding: 0 16px;
    display: flex;
    align-items: center;
    justify-content: flex-start;
    width: 100%;
  }
}
@media screen and (max-width: 650px) {
  .post .post-footer.use-separators {
    justify-content: center;
  }
}
@media screen and (max-width: 650px) {
  .post .post-footer.next-prev {
    height: 64px;
    justify-content: space-between;
    box-sizing: border-box;
  }
}
@media screen and (max-width: 650px) {
  .post-contributor-footer .post-contributor-bio-table {
    display: block;
  }
  .post-contributor-footer .post-contributor-bio-table-row {
    display: flex;
    flex-direction: row;
  }
  .post-contributor-footer .post-contributor-bio-userhead-cell,
  .post-contributor-footer .post-contributor-bio-body-cell {
    display: block;
  }
  .post-contributor-footer .post-contributor-bio-body-cell {
    flex-grow: 1;
  }
  .post-contributor-footer .post-contributor-bio-body-table {
    display: block;
  }
  .post-contributor-footer .post-contributor-bio-body-table-row {
    display: block;
  }
  .post-contributor-footer .post-contributor-bio-copy-cell,
  .post-contributor-footer .post-contributor-bio-controls-cell {
    display: block;
  }
  .post-contributor-footer .post-contributor-bio-copy-cell {
    margin: 0 0 16px 0;
  }
  .post-contributor-footer .post-contributor-bio-controls-cell {
    width: auto;
  }
  .post-contributor-footer .post-contributor-bio-controls {
    margin: auto;
  }
  .post-contributor-footer .post-contributor-bio-controls .button.primary {
    width: 100%;
  }
  .post-contributor-footer .post-contributor-bio-text {
    font-size: 14px;
  }
}
@media screen and (min-width: 768px) {
  .post-silhouette {
    padding: 32px 0;
  }
}
@media screen and (max-width: 650px) {
  .post-silhouette .post-silhouette-title {
    margin-top: 10.44225025px;
    height: 120px;
  }
}
@media screen and (max-width: 650px) {
  .post-silhouette .post-silhouette-meta {
    width: 75%;
  }
}
@media screen and (max-width: 650px) {
  .post-silhouette .post-silhouette-meta.with-byline-image {
    margin: 20px 0;
  }
}
@media screen and (max-width: 650px) {
  .use-theme-bg .post-meta.alternative-meta .post-meta-item,
  .post-meta.alternative-meta .post-meta-item {
    padding-right: 16px;
  }
}
@media screen and (max-width: 370px) {
  .use-theme-bg .post-meta.alternative-meta .post-meta-item,
  .post-meta.alternative-meta .post-meta-item {
    font-size: 14px;
  }
}
@media screen and (max-width: 650px) {
  .use-theme-bg .post-meta.alternative-meta .post-meta-item.guest-author-pu=
blication,
  .post-meta.alternative-meta .post-meta-item.guest-author-publication {
    display: none;
  }
}
@media screen and (max-width: 370px) {
  .post-meta .post-meta-item .post-meta-button {
    height: 36px !important;
    /* important to override in-line height style on emails */
  }
  .post-meta .post-meta-item .post-meta-button .meta-button-label {
    display: none;
  }
  .post-meta .post-meta-item .post-meta-button > svg {
    margin-right: 0;
  }
}
@media screen and (max-width: 370px) {
  .post-meta .post-meta-item {
    font-size: 12px;
  }
}
@media screen and (max-width: 650px) {
  .post .floating-subscribe-button {
    bottom: 20px;
    right: 20px;
  }
}
@media (max-width: 1024px) {
  body .pullquote-align-left,
  body .pullquote-align-right,
  body .pullquote-align-wide,
  body .pullquote-align-center {
    float: none;
    margin: 0 auto;
    width: 100%;
    max-width: 100%;
  }
}
@media all and (-ms-high-contrast: none), (-ms-high-contrast: active) {
  body .markup table.image-wrapper img,
  body .markup table.kindle-wrapper img {
    max-width: 550px;
  }
}
@media (min-width: 1024px) {
  body:not(:has(#toc)) .captioned-image-container figure:has(> a.image2-off=
set-left) {
    margin-left: var(--image-offset-margin);
  }
  body:not(:has(#toc)) .captioned-image-container figure:has(> a.image2-off=
set-right) {
    margin-right: var(--image-offset-margin);
  }
}
@media (min-width: 1300px) {
  body .captioned-image-container figure:has(> a.image2-offset-left) {
    margin-left: var(--image-offset-margin);
  }
  body .captioned-image-container figure:has(> a.image2-offset-right) {
    margin-right: var(--image-offset-margin);
  }
}
@media (max-width: 1024px) {
  body {
    /* Disable offset on mobile/tablet */
  }
  body .captioned-image-container figure:has(> a.image2-align-left),
  body .captioned-image-container figure:has(> a.image2-align-right) {
    float: none;
    margin: 1em auto;
    max-width: 100%;
    width: auto;
    padding: 0;
  }
  body .captioned-image-container figure:has(> a.image2-align-left.thefp),
  body .captioned-image-container figure:has(> a.image2-align-right.thefp) =
{
    margin: 1em auto;
  }
  body .captioned-image-container figure:has(> a.image2-offset-left),
  body .captioned-image-container figure:has(> a.image2-offset-right) {
    margin: 1em auto;
  }
  body .captioned-image-container figure:has(> a.image2-align-left) .image2=
-inset,
  body .captioned-image-container figure:has(> a.image2-align-right) .image=
2-inset {
    display: block;
    justify-content: initial;
  }
}
@media (max-width: 768px) {
  body .markup div.sponsorship-campaign-embed {
    margin-top: 24px;
    margin-bottom: 24px;
  }
  body .markup div.sponsorship-campaign-embed:first-child {
    margin-top: 0px;
  }
}
@media screen and (max-width: 650px) {
  body .markup div.youtube-overlay,
  body .markup div.vimeo-overlay {
    display: none !important;
  }
}
@media screen and (max-width: 370px) {
  body .markup div.tiktok-wrap {
    width: calc(95vw - 32px);
    height: calc((95vw - 32px - 2px) / 0.485714);
  }
}
@media screen and (max-width: 650px) {
  body .markup div.embedded-publication-wrap .embedded-publication.show-sub=
scribe {
    padding: 24px;
  }
}
@media screen and (max-width: 650px) {
  body .markup div.subscription-widget-wrap .subscription-widget.show-subsc=
ribe,
  body .markup div.subscription-widget-wrap-editor .subscription-widget.sho=
w-subscribe,
  body .markup div.captioned-button-wrap .subscription-widget.show-subscrib=
e {
    padding: 0px 24px;
  }
}
@media screen and (max-width: 650px) {
  body .markup div.subscription-widget-wrap .subscription-widget.show-subsc=
ribe .subscription-widget-subscribe .button,
  body .markup div.subscription-widget-wrap-editor .subscription-widget.sho=
w-subscribe .subscription-widget-subscribe .button,
  body .markup div.captioned-button-wrap .subscription-widget.show-subscrib=
e .subscription-widget-subscribe .button {
    padding: 10px 12px;
    min-width: 110px;
  }
}
@media (max-width: 650px) {
  body .markup .tweet {
    padding: 12px;
  }
}
@media (max-width: 650px) {
  body .markup .tweet .tweet-text {
    font-size: 14px;
    line-height: 20px;
  }
}
@media (max-width: 650px) {
  body .markup .tweet .tweet-photos-container.two,
  body .markup .tweet .tweet-photos-container.three,
  body .markup .tweet .tweet-photos-container.four {
    height: 200px;
  }
}
@media (max-width: 650px) {
  body .markup .tweet a.expanded-link .expanded-link-img {
    max-height: 180px;
  }
}
@media (max-width: 650px) {
  body .markup .tweet a.expanded-link .expanded-link-description {
    display: none;
  }
}
@media screen and (max-width: 650px) {
  body .markup .apple-podcast-container {
    width: unset;
  }
}
@media (max-width: 420px) {
  body .markup .install-substack-app-embed img.install-substack-app-embed-i=
mg {
    margin: 0 auto 16px auto;
  }
}
@media (max-width: 420px) {
  body .markup .install-substack-app-embed .install-substack-app-embed-text=
 {
    margin: 0 0 12px 0;
    max-width: 100%;
    width: auto;
    text-align: center;
  }
}
@media (max-width: 420px) {
  body .markup .install-substack-app-embed .install-substack-app-embed-link=
 {
    display: flex;
    justify-content: center;
  }
}
@media screen and (min-width: 500px) {
  body .header a.logo {
    width: 42px;
    height: 42px;
    border-radius: 12px;
  }
}
@media screen and (max-width: 420px) {
  body .subscription-receipt table:first-of-type .subscription-amount .subs=
cription-discount {
    width: 72px !important;
  }
}
@media screen and (min-width: 481px) {
  body .share-button-container {
    height: auto;
  }
}
@media screen and (max-width: 480px) {
  body .share-button-container .separator {
    display: block !important;
    margin: 0 !important;
    height: 8px !important;
    border-left: none !important;
  }
}
@media screen and (max-width: 650px) {
  .digest .item .post-meta-item.audience {
    display: none;
  }
}
@media screen and (min-width: 500px) {
  .digest-publication .logo img {
    width: 42px;
    height: 42px;
    border-radius: 8px;
  }
}
@media screen and (max-width: 650px) {
  .comments-page .container .comment-list .collapsed-reply {
    margin-left: calc(10 + 32px - 24px);
  }
}
@media screen and (max-width: 650px) {
  .comment > .comment-list {
    padding-left: 24px;
  }
}
@media screen and (max-width: 650px) {
  .finish-magic-login-modal .modal-content .container {
    padding: 24px 0;
  }
}
@media (max-width: 650px) {
  .reader2-text-b3 {
    line-height: 24px;
  }
}
@media screen and (max-width: 650px) {
  .reader2-text-h4 {
    line-height: 24px;
  }
}
@media screen and (min-width: 541px) {
  .user-profile-modal {
    padding-left: 12px;
    padding-right: 12px;
  }
}
@media screen and (max-width: 650px) {
  .subscribe-widget form.form .sideBySideWrap button.rightButton {
    padding: 10px 12px;
  }
}
@media screen and (min-width: 541px) {
  .pub-icon:hover .logo-hover,
  .feed-item-icon:hover .logo-hover {
    display: block;
  }
}
@media screen and (max-width: 650px) {
  .post-ufi.single-full-width-button .post-ufi-button-wrapper {
    width: 100%;
    padding: 16px;
  }
  .post-ufi.single-full-width-button .post-ufi-button-wrapper:empty {
    display: none;
  }
  .post-ufi.single-full-width-button .post-ufi-button {
    width: 100%;
    justify-content: center;
  }
}
@media screen and (max-width: 768px) {
  .file-embed-wrapper {
    padding: 0;
  }
}
@media screen and (max-width: 768px) {
  .file-embed-wrapper-editor {
    padding: 0;
  }
}
@media screen and (max-width: 768px) {
  .file-embed-wrapper-editor:active {
    padding: 0;
  }
}
@media only screen and (max-width: 650px) {
  .file-embed-button.wide,
  .file-embed-error-button.wide {
    display: none;
  }
}
@media only screen and (min-width: 630px) {
  .file-embed-button.narrow,
  .file-embed-error-button.narrow {
    display: none;
  }
}
@media screen and (min-width: 541px) {
  .audio-player-wrapper .audio-player {
    min-width: 500px;
  }
}
@media screen and (max-width: 650px) {
  .audio-player-wrapper .audio-player .audio-player-progress {
    border-left-width: 16px;
    border-right-width: 16px;
  }
}
@media screen and (max-width: 650px) {
  .audio-player-wrapper .audio-player .audio-player-progress .audio-player-=
progress-bar .audio-player-progress-bar-popup {
    top: -54px;
  }
}
@media screen and (max-width: 650px) {
  .audio-player-wrapper-fancy .audio-player .audio-player-progress {
    border-left-width: 16px;
    border-right-width: 16px;
  }
}
@media screen and (max-width: 650px) {
  .audio-player-wrapper-fancy .audio-player .audio-player-progress .audio-p=
layer-progress-bar .audio-player-progress-bar-popup {
    top: -54px;
  }
}
@media (min-width: 250px) {
  .audio-player-wrapper-fancy .audio-player {
    padding: 32px;
  }
  .audio-player-wrapper-fancy .audio-player .btn-group {
    display: flex;
  }
  .audio-player-wrapper-fancy .audio-player .btn-group .button:last-of-type=
 {
    display: block;
  }
}
@media (min-width: 300px) {
  .audio-player-wrapper-fancy .audio-player .btn-group {
    display: block;
  }
  .audio-player-wrapper-fancy .audio-player .btn-group .button:first-of-typ=
e {
    display: block;
  }
}
@media (min-width: 350px) {
  .audio-player-wrapper-fancy .audio-player .audio-player-substack-logo {
    display: block;
  }
  .audio-player-wrapper-fancy .audio-player .audio-player-title {
    margin-top: 16px;
  }
  .audio-player-wrapper-fancy .audio-player .audio-player-hero-image-contai=
ner {
    padding-top: 15%;
    width: 15%;
    display: block;
  }
  .audio-player-wrapper-fancy .audio-player .btn-group .button:first-of-typ=
e {
    display: block;
  }
  .audio-player-wrapper-fancy .audio-player .audio-player-substack-logo {
    display: block;
  }
}
@media (min-width: 350px) {
  .audio-player-wrapper-fancy .audio-player .audio-player-hero-image-contai=
ner {
    padding-top: 25%;
    width: 25%;
    display: block;
  }
  .audio-player-wrapper-fancy .audio-player .btn-group {
    display: flex;
  }
  .audio-player-wrapper-fancy .audio-player .btn-group .button:first-of-typ=
e {
    display: block;
  }
}
@media (min-width: 400px) {
  .audio-player-wrapper-fancy .audio-player .audio-player-hero-image-contai=
ner {
    padding-top: 40%;
    width: 40%;
  }
}
@media (max-width: 400px) {
  .audio-player-wrapper-fancy .audio-player .btn-group {
    margin-top: 12px;
  }
  .audio-player-wrapper-fancy .audio-player .btn-group .button {
    font-size: 13px;
    padding: 6px 12px;
    height: auto;
    margin-top: 10px;
  }
}
@media (min-width: 600px) {
  .audio-player-wrapper-fancy .audio-player .audio-player-hero-image-contai=
ner {
    padding-top: 55%;
    width: 55%;
  }
}
@media (max-width: 650px) {
  .poll-editor-modal {
    min-width: calc(100% - 20px);
  }
}
@media (max-width: 750px) {
  .poll-embed .poll-anchor-target .poll-anchor-copy-button {
    left: 8px;
    top: 45px;
  }
}
@media screen and (min-width: 541px) {
  .poll-embed .poll-wrapper.poll-web .poll-dialog .modal-table .modal-row .=
modal-content > .container {
    width: 552px;
    padding: 26px 24px;
  }
}
@media screen and (max-width: 650px) {
  .poll-embed .poll-wrapper.poll-web .poll-dialog .modal-table .modal-row .=
modal-content > .container {
    padding: 40px 0;
  }
}
@media screen and (max-width: 650px) {
  .poll-embed .poll-wrapper.poll-web .poll-dialog .modal-row .modal-cell .m=
odal-exit-btn {
    margin-right: -20px;
  }
}</style></head><body class=3D"email-body" style=3D"font-kerning: auto;--im=
age-offset-margin: -120px;"><img src=3D"https://eotrx.substackcdn.com/open?=
token=3DeyJtIjoiPDIwMjUwODAxMTgwODM0LjMuNWE1NGQwOThhNGVkN2M5N0BtZzIuc3Vic3R=
hY2suY29tPiIsInUiOjMzNjQ0ODIyMywiciI6ImVpdGFuQGVpc2xhdy5jby5pbCIsImQiOiJtZz=
Iuc3Vic3RhY2suY29tIiwicCI6MTY5NjcwMDMwLCJ0IjoibmV3c2xldHRlciIsImEiOiJldmVye=
W9uZSIsInMiOjE0Mjg2NjcsImMiOiJwb3N0IiwiZiI6dHJ1ZSwicG9zaXRpb24iOiJ0b3AiLCJp=
YXQiOjE3NTQwNzE3NDMsImV4cCI6MTc1NjY2Mzc0MywiaXNzIjoicHViLTAiLCJzdWIiOiJlbyJ=
9.f1bYRJZ1O8ozQANYWFtcJ_GKCHKfMHeJpgpK6Z3QudE" alt=3D"" width=3D"1" height=
=3D"1" border=3D"0" style=3D"height:1px !important;width:1px !important;bor=
der-width:0 !important;margin-top:0 !important;margin-bottom:0 !important;m=
argin-right:0 !important;margin-left:0 !important;padding-top:0 !important;=
padding-bottom:0 !important;padding-right:0 !important;padding-left:0 !impo=
rtant;"><div class=3D"preview" style=3D"display:none;font-size:1px;color:#3=
33333;line-height:1px;max-height:0px;max-width:0px;opacity:0;overflow:hidde=
n;">Ahead of the curve with LLM Watch</div><div class=3D"preview" style=3D"=
display:none;font-size:1px;color:#333333;line-height:1px;max-height:0px;max=
-width:0px;opacity:0;overflow:hidden;">=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F=
 &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=
=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=
=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=
=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=
=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =
=E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD</div><table class=3D"email=
-body-container" role=3D"presentation" width=3D"100%" border=3D"0" cellspac=
ing=3D"0" cellpadding=3D"0"><tbody><tr><td></td><td class=3D"content" width=
=3D"550"></td><td></td></tr><tr><td></td><td class=3D"content" width=3D"550=
" align=3D"left"><div style=3D"font-size: 16px;line-height: 26px;max-width:=
 550px;width: 100%;margin: 0 auto;overflow-wrap: break-word;"><table role=
=3D"presentation" width=3D"100%" border=3D"0" cellspacing=3D"0" cellpadding=
=3D"0"><tbody><tr><td align=3D"right" style=3D"height:20px;"><table role=3D=
"presentation" width=3D"auto" border=3D"0" cellspacing=3D"0" cellpadding=3D=
"0"><tbody><tr><td style=3D"vertical-align:middle;"><span class=3D"pencraft=
 pc-reset reset-IxiVJZ tw-font-body tw-text-ssm tw-text-substack-secondary"=
 style=3D"font-family: SF Pro Text, -apple-system, system-ui, BlinkMacSyste=
mFont, Inter, Segoe UI, Roboto, Helvetica, Arial, sans-serif, Apple Color E=
moji, Segoe UI Emoji, Segoe UI Symbol;font-size: 13px;color: unset;list-sty=
le: none;text-decoration: unset;margin: 0;"><div class=3D"pencraft pc-reset=
 align-right-VJbKw5 size-12-mmZ61m reset-IxiVJZ" style=3D"list-style: none;=
color: unset;text-align: right;font-size: 12px;line-height: 16px;text-decor=
ation: unset;margin: 0;"><span class=3D"pencraft pc-reset reset-IxiVJZ" tra=
nslated=3D"" style=3D"list-style: none;color: unset;text-decoration: unset;=
margin: 0;">Forwarded this email? <a class=3D"pencraft pc-reset decoration-=
underline-ClTkYc reset-IxiVJZ" href=3D"https://substack.com/redirect/2/eyJl=
IjoiaHR0cHM6Ly93d3cubGxtd2F0Y2guY29tL3N1YnNjcmliZT91dG1fc291cmNlPWVtYWlsJnV=
0bV9jYW1wYWlnbj1lbWFpbC1zdWJzY3JpYmUmcj01a2I5M3ombmV4dD1odHRwcyUzQSUyRiUyRn=
d3dy5sbG13YXRjaC5jb20lMkZwJTJGMTEtcGFwZXJzLXlvdS1zaG91bGQta25vdy1hYm91dCIsI=
nAiOjE2OTY3MDAzMCwicyI6MTQyODY2NywiZiI6dHJ1ZSwidSI6MzM2NDQ4MjIzLCJpYXQiOjE3=
NTQwNzE3NDIsImV4cCI6MjA2OTY0Nzc0MiwiaXNzIjoicHViLTAiLCJzdWIiOiJsaW5rLXJlZGl=
yZWN0In0.9NnnWyhTiD5hSyP0lkUc7CxTVbtppswI9OxhpjlXgSc?" style=3D"list-style:=
 none;color: unset;text-decoration: unset;margin: 0;-webkit-text-decoration=
-line: underline;text-decoration-line: underline;">Subscribe here</a> for m=
ore</span></div></span></td></tr></tbody></table></td></tr></tbody></table>=
<table class=3D"header graphic-header" role=3D"presentation" style=3D"borde=
r-spacing: 0;padding: 16px 0 32px;"><tbody><tr><td align=3D"center" style=
=3D"text-align: center;padding: 0;"><a href=3D"https://substack.com/redirec=
t/2/eyJlIjoiaHR0cHM6Ly93d3cubGxtd2F0Y2guY29tL3AvMTEtcGFwZXJzLXlvdS1zaG91bGQ=
ta25vdy1hYm91dD91dG1fY2FtcGFpZ249ZW1haWwtaGFsZi1wb3N0JnI9NWtiOTN6JnRva2VuPW=
V5SjFjMlZ5WDJsa0lqb3pNelkwTkRneU1qTXNJbkJ2YzNSZmFXUWlPakUyT1RZM01EQXpNQ3dpY=
VdGMElqb3hOelUwTURjeE56UXlMQ0psZUhBaU9qRTNOVFkyTmpNM05ESXNJbWx6Y3lJNkluQjFZ=
aTB4TkRJNE5qWTNJaXdpYzNWaUlqb2ljRzl6ZEMxeVpXRmpkR2x2YmlKOS5EcWxjU0VXREVTTHR=
PSjBBMVIzLTVjNnpGdWJPUlA5Vmd3TVl5MkJhSzdFIiwicCI6MTY5NjcwMDMwLCJzIjoxNDI4Nj=
Y3LCJmIjp0cnVlLCJ1IjozMzY0NDgyMjMsImlhdCI6MTc1NDA3MTc0MiwiZXhwIjoyMDY5NjQ3N=
zQyLCJpc3MiOiJwdWItMCIsInN1YiI6ImxpbmstcmVkaXJlY3QifQ.26w8_FRu8FJybr4lrDeqm=
NEFJEiNsLWS6t7a8Eqct28?"><img class=3D"header-image" role=3D"presentation" =
width=3D"550" height=3D"165" src=3D"https://substackcdn.com/image/fetch/$s_=
!Zt7H!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2=
Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F869583e6-3f79-490=
c-888a-07a2b002fe94_2000x600.png" style=3D"border: none !important;vertical=
-align: middle;max-width: 550px;display: block;margin: 0 auto;height: auto;=
width: 100%;"></a></td></tr></tbody></table><div class=3D"post typography" =
dir=3D"auto" style=3D"--image-offset-margin: -120px;padding: 32px 0 0 0;fon=
t-size: 16px;line-height: 26px;"><div class=3D"post-header" role=3D"region"=
 aria-label=3D"Post header" style=3D"font-size: 16px;line-height: 26px;"><h=
1 class=3D"post-title published" style=3D"color: rgb(54,55,55);font-family:=
 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSy=
stemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','=
Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoothing:=
 antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearance: optim=
izelegibility;-moz-appearance: optimizelegibility;appearance: optimizelegib=
ility;margin: 0;line-height: 36px;font-size: 32px;"><a href=3D"https://subs=
tack.com/app-link/post?publication_id=3D1428667&amp;post_id=3D169670030&amp=
;utm_source=3Dpost-email-title&amp;utm_campaign=3Demail-post-title&amp;isFr=
eemail=3Dtrue&amp;r=3D5kb93z&amp;token=3DeyJ1c2VyX2lkIjozMzY0NDgyMjMsInBvc3=
RfaWQiOjE2OTY3MDAzMCwiaWF0IjoxNzU0MDcxNzQyLCJleHAiOjE3NTY2NjM3NDIsImlzcyI6I=
nB1Yi0xNDI4NjY3Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.DqlcSEWDESLtOJ0A1R3-5c6zFub=
ORP9VgwMYy2BaK7E" style=3D"color: rgb(54,55,55);text-decoration: none;">11 =
Papers You Should Know About</a></h1><h3 class=3D"subtitle" style=3D"font-f=
amily: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,Blin=
kMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Em=
oji','Segoe UI Emoji','Segoe UI Symbol';font-weight: normal;-webkit-font-sm=
oothing: antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearanc=
e: optimizelegibility;-moz-appearance: optimizelegibility;appearance: optim=
izelegibility;margin: 4px 0 0;color: #777777;line-height: 24px;font-size: 1=
8px;margin-top: 12px;">Ahead of the curve with LLM Watch</h3><table class=
=3D"post-meta custom" cellpadding=3D"0" cellspacing=3D"0" style=3D"margin: =
1em 0;height: 20px;align-items: center;"><tbody><tr><td class=3D"post-meta-=
item post-date" title=3D"2025-08-01T18:08:43.771Z" style=3D"position: relat=
ive;padding: 0px 12px 0px 0;height: 14px;font-size: 14px;font-family: syste=
m-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,san=
s-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';color: #7777=
77;text-decoration: none;margin-right: 0;padding-right: 0;white-space: nowr=
ap;font-weight: 400;padding-top: 0;padding-bottom: 0;"><div class=3D"pencra=
ft pc-reset color-secondary-ls1g8s line-height-20-t4M0El font-meta-MWBumP s=
ize-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq reset-IxiVJZ =
meta-EgzBVA" style=3D"list-style: none;font-size: 11px;line-height: 20px;te=
xt-decoration: unset;color: rgb(119,119,119);margin: 0;font-family: 'SF Com=
pact',-apple-system,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',R=
oboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Sego=
e UI Symbol';font-weight: 500;text-transform: uppercase;letter-spacing: .2p=
x;"><time datetime=3D"2025-08-01T18:08:43.771Z">Aug 1</time></div></td></tr=
></tbody></table><table class=3D"email-ufi-2-top" role=3D"presentation" wid=
th=3D"100%" border=3D"0" cellspacing=3D"0" cellpadding=3D"0" style=3D"borde=
r-top: 1px solid rgb(0,0,0,.1);border-bottom: 1px solid rgb(0,0,0,.1);min-w=
idth: 100%;"><tbody><tr height=3D"16"><td height=3D"16" style=3D"font-size:=
0px;line-height:0;">&nbsp;</td></tr><tr><td><table role=3D"presentation" wi=
dth=3D"100%" border=3D"0" cellspacing=3D"0" cellpadding=3D"0"><tbody><tr><t=
d><table role=3D"presentation" width=3D"auto" border=3D"0" cellspacing=3D"0=
" cellpadding=3D"0"><tbody><tr><td style=3D"vertical-align:middle;"><table =
role=3D"presentation" width=3D"38" border=3D"0" cellspacing=3D"0" cellpaddi=
ng=3D"0"><tbody><tr><td align=3D"center"><a class=3D"email-icon-button" hre=
f=3D"https://substack.com/app-link/post?publication_id=3D1428667&amp;post_i=
d=3D169670030&amp;utm_source=3Dsubstack&amp;isFreemail=3Dtrue&amp;submitLik=
e=3Dtrue&amp;token=3DeyJ1c2VyX2lkIjozMzY0NDgyMjMsInBvc3RfaWQiOjE2OTY3MDAzMC=
wicmVhY3Rpb24iOiLinaQiLCJpYXQiOjE3NTQwNzE3NDIsImV4cCI6MTc1NjY2Mzc0MiwiaXNzI=
joicHViLTE0Mjg2NjciLCJzdWIiOiJyZWFjdGlvbiJ9.JpiuIFzFYFagH14A6ifSLYUmG5rhKqx=
Eme7-YsoLuug&amp;utm_medium=3Demail&amp;utm_campaign=3Demail-reaction&amp;r=
=3D5kb93z" style=3D"font-family: system-ui,-apple-system,BlinkMacSystemFont=
,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI=
 Emoji','Segoe UI Symbol';display: inline-block;font-weight: 500;border: 1p=
x solid rgb(0,0,0,.1);border-radius: 9999px;text-transform: uppercase;font-=
size: 12px;line-height: 1;padding: 9px 0;text-decoration: none;color: rgb(1=
19,119,119);min-width: 38px;box-sizing: border-box;width: 38px"><img class=
=3D"icon" src=3D"https://substackcdn.com/image/fetch/$s_!PeVs!,w_36,c_scale=
,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2=
FLucideHeart%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26s=
trokeWidth%3D2" width=3D"18" height=3D"18" style=3D"border: none;vertical-a=
lign: middle;max-width: 18px" alt=3D""></a></td></tr></tbody></table></td><=
td width=3D"8" style=3D"min-width: 8px"></td><td style=3D"vertical-align:mi=
ddle;"><table role=3D"presentation" width=3D"38" border=3D"0" cellspacing=
=3D"0" cellpadding=3D"0"><tbody><tr><td align=3D"center"><a class=3D"email-=
icon-button" href=3D"https://substack.com/app-link/post?publication_id=3D14=
28667&amp;post_id=3D169670030&amp;utm_source=3Dsubstack&amp;utm_medium=3Dem=
ail&amp;isFreemail=3Dtrue&amp;comments=3Dtrue&amp;token=3DeyJ1c2VyX2lkIjozM=
zY0NDgyMjMsInBvc3RfaWQiOjE2OTY3MDAzMCwiaWF0IjoxNzU0MDcxNzQyLCJleHAiOjE3NTY2=
NjM3NDIsImlzcyI6InB1Yi0xNDI4NjY3Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.DqlcSEWDES=
LtOJ0A1R3-5c6zFubORP9VgwMYy2BaK7E&amp;r=3D5kb93z&amp;utm_campaign=3Demail-h=
alf-magic-comments&amp;action=3Dpost-comment&amp;utm_source=3Dsubstack&amp;=
utm_medium=3Demail" style=3D"font-family: system-ui,-apple-system,BlinkMacS=
ystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji',=
'Segoe UI Emoji','Segoe UI Symbol';display: inline-block;font-weight: 500;b=
order: 1px solid rgb(0,0,0,.1);border-radius: 9999px;text-transform: upperc=
ase;font-size: 12px;line-height: 1;padding: 9px 0;text-decoration: none;col=
or: rgb(119,119,119);min-width: 38px;box-sizing: border-box;width: 38px"><i=
mg class=3D"icon" src=3D"https://substackcdn.com/image/fetch/$s_!x1tS!,w_36=
,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%=
2Ficon%2FLucideComments%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%252=
3808080%26strokeWidth%3D2" width=3D"18" height=3D"18" style=3D"border: none=
;vertical-align: middle;max-width: 18px" alt=3D""></a></td></tr></tbody></t=
able></td><td width=3D"8" style=3D"min-width: 8px"></td><td style=3D"vertic=
al-align:middle;"><table role=3D"presentation" width=3D"38" border=3D"0" ce=
llspacing=3D"0" cellpadding=3D"0"><tbody><tr><td align=3D"center"><a class=
=3D"email-icon-button" href=3D"https://substack.com/app-link/post?publicati=
on_id=3D1428667&amp;post_id=3D169670030&amp;utm_source=3Dsubstack&amp;utm_m=
edium=3Demail&amp;utm_content=3Dshare&amp;utm_campaign=3Demail-share&amp;ac=
tion=3Dshare&amp;triggerShare=3Dtrue&amp;isFreemail=3Dtrue&amp;r=3D5kb93z&a=
mp;token=3DeyJ1c2VyX2lkIjozMzY0NDgyMjMsInBvc3RfaWQiOjE2OTY3MDAzMCwiaWF0Ijox=
NzU0MDcxNzQyLCJleHAiOjE3NTY2NjM3NDIsImlzcyI6InB1Yi0xNDI4NjY3Iiwic3ViIjoicG9=
zdC1yZWFjdGlvbiJ9.DqlcSEWDESLtOJ0A1R3-5c6zFubORP9VgwMYy2BaK7E" style=3D"fon=
t-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helv=
etica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbo=
l';display: inline-block;font-weight: 500;border: 1px solid rgb(0,0,0,.1);b=
order-radius: 9999px;text-transform: uppercase;font-size: 12px;line-height:=
 1;padding: 9px 0;text-decoration: none;color: rgb(119,119,119);min-width: =
38px;box-sizing: border-box;width: 38px"><img class=3D"icon" src=3D"https:/=
/substackcdn.com/image/fetch/$s_!_L14!,w_36,c_scale,f_png,q_auto:good,fl_pr=
ogressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideShare2%3Fv%3D4%26=
height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2" width=
=3D"18" height=3D"18" style=3D"border: none;vertical-align: middle;max-widt=
h: 18px" alt=3D""></a></td></tr></tbody></table></td><td width=3D"8" style=
=3D"min-width: 8px"></td><td style=3D"vertical-align:middle;"><table role=
=3D"presentation" width=3D"38" border=3D"0" cellspacing=3D"0" cellpadding=
=3D"0"><tbody><tr><td align=3D"center"><a class=3D"email-icon-button" href=
=3D"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9vcGVuLnN1YnN0YWNrLmN=
vbS9wdWIveGFpZ3V5L3AvMTEtcGFwZXJzLXlvdS1zaG91bGQta25vdy1hYm91dD91dG1fc291cm=
NlPXN1YnN0YWNrJnV0bV9tZWRpdW09ZW1haWwmdXRtX2NhbXBhaWduPWVtYWlsLXJlc3RhY2stY=
29tbWVudCZhY3Rpb249cmVzdGFjay1jb21tZW50JnI9NWtiOTN6JnRva2VuPWV5SjFjMlZ5WDJs=
a0lqb3pNelkwTkRneU1qTXNJbkJ2YzNSZmFXUWlPakUyT1RZM01EQXpNQ3dpYVdGMElqb3hOelU=
wTURjeE56UXlMQ0psZUhBaU9qRTNOVFkyTmpNM05ESXNJbWx6Y3lJNkluQjFZaTB4TkRJNE5qWT=
NJaXdpYzNWaUlqb2ljRzl6ZEMxeVpXRmpkR2x2YmlKOS5EcWxjU0VXREVTTHRPSjBBMVIzLTVjN=
npGdWJPUlA5Vmd3TVl5MkJhSzdFIiwicCI6MTY5NjcwMDMwLCJzIjoxNDI4NjY3LCJmIjp0cnVl=
LCJ1IjozMzY0NDgyMjMsImlhdCI6MTc1NDA3MTc0MiwiZXhwIjoyMDY5NjQ3NzQyLCJpc3MiOiJ=
wdWItMCIsInN1YiI6ImxpbmstcmVkaXJlY3QifQ.mYVS9_TThTFlchQrRioyUhzi60hHBu3-L1Y=
Ty5lT3aU?&amp;utm_source=3Dsubstack&amp;utm_medium=3Demail" style=3D"font-f=
amily: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helveti=
ca,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';=
display: inline-block;font-weight: 500;border: 1px solid rgb(0,0,0,.1);bord=
er-radius: 9999px;text-transform: uppercase;font-size: 12px;line-height: 1;=
padding: 9px 0;text-decoration: none;color: rgb(119,119,119);min-width: 38p=
x;box-sizing: border-box;width: 38px"><img class=3D"icon" src=3D"https://su=
bstackcdn.com/image/fetch/$s_!5EGt!,w_36,c_scale,f_png,q_auto:good,fl_progr=
essive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FNoteForwardIcon%3Fv%3D4%26=
height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2" width=
=3D"18" height=3D"18" style=3D"border: none;vertical-align: middle;max-widt=
h: 18px" alt=3D""></a></td></tr></tbody></table></td></tr></tbody></table><=
/td><td align=3D"right"><table role=3D"presentation" width=3D"auto" border=
=3D"0" cellspacing=3D"0" cellpadding=3D"0"><tbody><tr><td style=3D"vertical=
-align:middle;"><table role=3D"presentation" width=3D"auto" border=3D"0" ce=
llspacing=3D"0" cellpadding=3D"0"><tbody><tr><td align=3D"center"><a class=
=3D"email-button-outline" href=3D"https://open.substack.com/pub/xaiguy/p/11=
-papers-you-should-know-about?utm_source=3Demail&amp;redirect=3Dapp-store&a=
mp;utm_campaign=3Demail-read-in-app" style=3D"font-family: system-ui,-apple=
-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Ap=
ple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';display: inline-block;f=
ont-weight: 500;border: 1px solid rgb(0,0,0,.1);border-radius: 9999px;text-=
transform: uppercase;font-size: 12px;line-height: 12px;padding: 9px 14px;te=
xt-decoration: none;color: rgb(119,119,119);"><div class=3D"email-button-sp=
acer" style=3D"font-size: 16px;line-height: 26px;display: inline-block;vert=
ical-align: middle;max-width: 0;min-height: 18px;"></div><span class=3D"ema=
il-button-text" style=3D"vertical-align: middle;margin-right: 4px">READ IN =
APP</span><img class=3D"icon text-icon" src=3D"https://substackcdn.com/imag=
e/fetch/$s_!ET-_!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https=
%3A%2F%2Fsubstack.com%2Ficon%2FLucideArrowUpRight%3Fv%3D4%26height%3D36%26f=
ill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2" width=3D"18" height=3D=
"18" style=3D"min-width: 18px;min-height: 18px;border: none;vertical-align:=
 middle;margin-right: 0;margin-left: 0;max-width: 18px" alt=3D""></a></td><=
/tr></tbody></table></td></tr></tbody></table></td></tr></tbody></table></t=
d></tr><tr height=3D"16"><td height=3D"16" style=3D"font-size:0px;line-heig=
ht:0;">&nbsp;</td></tr></tbody></table></div></div><div class=3D"post typog=
raphy" dir=3D"auto" style=3D"--image-offset-margin: -120px;padding: 32px 0 =
0 0;font-size: 16px;line-height: 26px;"><div class=3D"body markup" dir=3D"a=
uto" style=3D"text-align: initial;font-size: 16px;line-height: 26px;width: =
100%;word-break: break-word;margin-bottom: 16px;"><p style=3D"margin: 0 0 2=
0px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;margin-top: 0;=
">Welcome, Watcher! This week in LLM Watch:</p><ol style=3D"margin-top: 0;p=
adding: 0;"><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,5=
5,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-lef=
t: 4px;font-size: 16px;margin: 0;">Why self-evolving agents might be the re=
al path to ASI (not just bigger models) </p></li><li style=3D"margin: 8px 0=
 0 32px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom:=
 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">How=
 reflective language beats reinforcement learning by 35x efficiency </p></l=
i><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line=
-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;fon=
t-size: 16px;margin: 0;">When hybrid architectures demolish compute-perform=
ance trade-offs </p></li></ol><p style=3D"margin: 0 0 20px 0;color: rgb(54,=
55,55);line-height: 26px;font-size: 16px;">Don't forget to subscribe to nev=
er miss an update again.</p><div style=3D"font-size: 16px;line-height: 26px=
;"><hr style=3D"margin: 32px 0;padding: 0;height: 1px;background: #313131;b=
order: none;"></div><h2 class=3D"header-anchor-post" style=3D"position: rel=
ative;font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple=
-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Ap=
ple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webk=
it-font-smoothing: antialiased;-moz-osx-font-smoothing: antialiased;-webkit=
-appearance: optimizelegibility;-moz-appearance: optimizelegibility;appeara=
nce: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-h=
eight: 1.16em;font-size: 1.625em;">Quick Glossary (for the uninitiated)</h2=
><blockquote style=3D"border-left: 4px solid #ffca4b;margin: 20px 0;padding=
: 0;"><ul style=3D"margin-left: 20px;margin-top: 0;padding: 0;"><li style=
=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rg=
b(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;paddi=
ng-left: 4px;font-size: 16px;margin: 0;"><strong>ASI (Artificial Super Inte=
lligence)</strong><span>: AI that surpasses human intelligence in all domai=
ns - the hypothetical next stage after AGI where machines outthink us at ev=
erything</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-special-forma=
t: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-botto=
m: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><=
strong>MoE (Mixture of Experts)</strong><span>: An architecture where diffe=
rent specialized sub-models handle different inputs - like having a team of=
 specialists instead of one generalist</span></p></li><li style=3D"margin: =
8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);=
line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px=
;font-size: 16px;margin: 0;"><strong>SSM (State Space Model)</strong><span>=
: An efficient alternative to Transformers for processing sequences - think=
 of it as a streamlined way to remember context</span></p></li><li style=3D=
"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(5=
4,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-=
left: 4px;font-size: 16px;margin: 0;"><strong>GraphRAG</strong><span>: Retr=
ieval-Augmented Generation using knowledge graphs - like giving AI a struct=
ured map of information instead of just a pile of documents</span></p></li>=
<li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"=
color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-=
box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Knowledge hypergr=
aphs</strong><span>: Advanced data structures where connections can link mu=
ltiple nodes at once - like mind maps on steroids</span></p></li><li style=
=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rg=
b(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;paddi=
ng-left: 4px;font-size: 16px;margin: 0;"><strong>MCP (Model Context Protoco=
l)</strong><span>: A standardized way for AI agents to interact with tools =
and external systems - like USB for AI connections</span></p></li></ul></bl=
ockquote><div style=3D"font-size: 16px;line-height: 26px;"><hr style=3D"mar=
gin: 32px 0;padding: 0;height: 1px;background: #313131;border: none;"></div=
><h2 class=3D"header-anchor-post" style=3D"position: relative;font-family: =
'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSys=
temFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','S=
egoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoothing: =
antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearance: optimi=
zelegibility;-moz-appearance: optimizelegibility;appearance: optimizelegibi=
lity;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-=
size: 1.625em;">1. A Survey of Self-Evolving Agents: On Path to Artificial =
Super Intelligence</h2><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);=
line-height: 26px;font-size: 16px;"><span>Watching: Self-Evolving Agents (<=
/span><a href=3D"https://substack.com/redirect/0290d1c9-b08d-4fd9-9316-1377=
0204f8cc?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw=
0" rel=3D"" style=3D"color: rgb(54,55,55);text-decoration: underline;">pape=
r</a><span>/</span><a href=3D"https://substack.com/redirect/5c432d79-7948-4=
700-8d9d-c7d94d8811d0?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6=
RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,55);text-decoration: und=
erline;">repo</a><span>)</span></p><div class=3D"captioned-image-container-=
static" style=3D"font-size: 16px;line-height: 26px;margin: 32px auto;"><fig=
ure style=3D"width: 100%;margin: 0 auto;"><table class=3D"image-wrapper" wi=
dth=3D"100%" border=3D"0" cellspacing=3D"0" cellpadding=3D"0" data-componen=
t-name=3D"Image2ToDOMStatic" style=3D"mso-padding-alt: 1em 0 1.6em;"><tbody=
><tr><td style=3D"text-align: center;"></td><td class=3D"content" align=3D"=
left" width=3D"996" style=3D"text-align: center;"><a class=3D"image-link" t=
arget=3D"_blank" href=3D"https://substack.com/redirect/82cb596b-b61b-4946-9=
c1e-b6746171310c?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4t=
AjMRMPOw0" rel=3D"" style=3D"position: relative;flex-direction: column;alig=
n-items: center;padding: 0;width: auto;height: auto;border: none;text-decor=
ation: none;display: block;margin: 0;"><img class=3D"wide-image" data-attrs=
=3D"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/pub=
lic/images/01dfb5bd-37b7-48b9-a1ba-098055551e93_996x201.png&quot;,&quot;src=
NoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:nu=
ll,&quot;height&quot;:201,&quot;width&quot;:996,&quot;resizeWidth&quot;:nul=
l,&quot;bytes&quot;:148671,&quot;alt&quot;:null,&quot;title&quot;:null,&quo=
t;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold=
&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;h=
ttps://www.llmwatch.com/i/169670030?img=3Dhttps%3A%2F%2Fsubstack-post-media=
.s3.amazonaws.com%2Fpublic%2Fimages%2F01dfb5bd-37b7-48b9-a1ba-098055551e93_=
996x201.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&qu=
ot;offset&quot;:false}" alt=3D"" width=3D"550" height=3D"110.99397590361446=
" src=3D"https://substackcdn.com/image/fetch/$s_!4LWk!,w_1100,c_limit,f_aut=
o,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.ama=
zonaws.com%2Fpublic%2Fimages%2F01dfb5bd-37b7-48b9-a1ba-098055551e93_996x201=
.png" style=3D"border: none !important;vertical-align: middle;display: bloc=
k;-ms-interpolation-mode: bicubic;height: auto;margin-bottom: 0;width: auto=
 !important;max-width: 100% !important;margin: 0 auto;"></a></td><td style=
=3D"text-align: center;"></td></tr></tbody></table></figure></div><p style=
=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16=
px;"><strong>What problem does it solve?</strong><span> Large language mode=
ls today are powerful but fundamentally </span><strong>static</strong><span=
>, unable to update their knowledge or skills once trained. When deployed i=
n open-ended environments, this rigidity becomes a serious bottleneck =E2=
=80=93 the AI cannot adapt to new tasks, evolving information, or dynamic i=
nteractions. This survey identifies the shift from static LLMs to </span><e=
m>self-evolving agents</em><span> as crucial for breaking that bottleneck. =
In essence, the problem is how to build AI systems that can </span><strong>=
learn and improve continuously</strong><span> in real time, rather than rem=
aining fixed after pre-training.</span></p><p style=3D"margin: 0 0 20px 0;c=
olor: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>How does it=
 solve the problem?</strong><span> The authors provide the first comprehens=
ive roadmap for building adaptive, self-improving AI agents. They organize =
the vast literature (1,400+ papers) around </span><strong>three foundationa=
l dimensions</strong><span> of evolution: </span><em>what</em><span> to evo=
lve (which components, e.g. the model=E2=80=99s parameters, memory, or tool=
 use), </span><em>when</em><span> to evolve (during or between tasks), and =
</span><em>how</em><span> to evolve (the algorithms or feedback that drive =
learning). They examine mechanisms that allow an agent to update itself =E2=
=80=93 from modifying its neural weights, to expanding toolsets or adjustin=
g its architecture =E2=80=93 and classify these by whether adaptation happe=
ns on the fly during a task or in between episodes. The survey also reviews=
 how to give agents richer feedback beyond sparse rewards (like using textu=
al self-reflection or multi-agent critiques) and covers </span><strong>eval=
uation benchmarks</strong><span> and domains (coding, education, healthcare=
) where self-evolving agents are being tested.</span></p><p style=3D"margin=
: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><stro=
ng>What are the key findings?</strong><span> A major takeaway is that desig=
ning self-evolving AI requires integrating techniques from across subfields=
 into a </span><strong>unified framework</strong><span>. Successful agents =
will need to coordinate multiple forms of learning =E2=80=93 e.g. updating =
memory modules, adjusting plans, and fine-tuning skills =E2=80=93 all under=
 principled control of </span><em>when</em><span> and </span><em>how</em><s=
pan> to apply each. The survey highlights that recent systems already demon=
strate pieces of this puzzle (like agents that update via textual feedback =
or share knowledge in a team), but we lack a standard toolbox for making th=
em reliably </span><strong>adaptive</strong><span>. It also identifies open=
 challenges: ensuring </span><strong>safety</strong><span> (so an agent doe=
sn=E2=80=99t evolve undesired behaviors), achieving </span><strong>scalabil=
ity</strong><span> (so learning on the fly doesn=E2=80=99t require prohibit=
ive compute), and handling </span><strong>co-evolution</strong><span> in mu=
lti-agent settings. By synthesizing lessons across studies, the authors del=
iver a structured understanding of what=E2=80=99s been achieved and what ga=
ps remain on the path toward truly self-improving AI.</span></p><p style=3D=
"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;=
"><strong>Why does it matter?</strong><span> This work essentially </span><=
strong>charts the path toward AI that can continuously learn</strong><span>=
, a key stepping stone toward more general and autonomous intelligence. Ins=
tead of repeatedly retraining models offline for new abilities, future AI a=
gents might </span><strong>evolve on the job</strong><span>, much like huma=
ns do by learning from experience. Such agents could remain state-of-the-ar=
t without constant human intervention, adjusting to new information and tas=
ks as they emerge. In the long run, the survey=E2=80=99s vision of self-evo=
lving agents paints a route to systems that inch closer to </span><em>Artif=
icial Super Intelligence</em><span>, by operating at or beyond human-level =
capability across many tasks through ongoing self-improvement. For practiti=
oners and researchers, this roadmap provides concrete guidance on building =
AI that doesn=E2=80=99t plateau =E2=80=93 it adapts, optimizes itself, and =
potentially, gets better with each interaction.</span></p><h2 class=3D"head=
er-anchor-post" style=3D"position: relative;font-family: 'SF Pro Display',-=
apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI'=
,Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Se=
goe UI Symbol';font-weight: bold;-webkit-font-smoothing: antialiased;-moz-o=
sx-font-smoothing: antialiased;-webkit-appearance: optimizelegibility;-moz-=
appearance: optimizelegibility;appearance: optimizelegibility;margin: 1em 0=
 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-size: 1.625em;">2.=
 Kimi K2: Open Agentic Intelligence</h2><p style=3D"margin: 0 0 20px 0;colo=
r: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>Watching: Kimi K=
2 (</span><a href=3D"https://substack.com/redirect/073b336a-6764-4eb8-a8db-=
a28657abcd04?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMR=
MPOw0" rel=3D"" style=3D"color: rgb(54,55,55);text-decoration: underline;">=
paper</a><span>/</span><a href=3D"https://substack.com/redirect/ccd41580-bc=
fd-4b13-a7bb-1b0ba94dafde?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BD=
nPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,55);text-decoration:=
 underline;">model</a><span>)</span></p><div class=3D"captioned-image-conta=
iner-static" style=3D"font-size: 16px;line-height: 26px;margin: 32px auto;"=
><figure style=3D"width: 100%;margin: 0 auto;"><table class=3D"image-wrappe=
r" width=3D"100%" border=3D"0" cellspacing=3D"0" cellpadding=3D"0" data-com=
ponent-name=3D"Image2ToDOMStatic" style=3D"mso-padding-alt: 1em 0 1.6em;"><=
tbody><tr><td style=3D"text-align: center;"></td><td class=3D"content" alig=
n=3D"left" width=3D"1112" style=3D"text-align: center;"><a class=3D"image-l=
ink" target=3D"_blank" href=3D"https://substack.com/redirect/57730918-1f96-=
4d3c-8053-e5b9ef5037e8?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG=
6RxQ4tAjMRMPOw0" rel=3D"" style=3D"position: relative;flex-direction: colum=
n;align-items: center;padding: 0;width: auto;height: auto;border: none;text=
-decoration: none;display: block;margin: 0;"><img class=3D"wide-image" data=
-attrs=3D"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.c=
om/public/images/55ff5b30-db05-40b9-b5c0-d99283e076c0_1112x704.png&quot;,&q=
uot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&q=
uot;:null,&quot;height&quot;:704,&quot;width&quot;:1112,&quot;resizeWidth&q=
uot;:null,&quot;bytes&quot;:179300,&quot;alt&quot;:null,&quot;title&quot;:n=
ull,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belo=
wTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;=
:&quot;https://www.llmwatch.com/i/169670030?img=3Dhttps%3A%2F%2Fsubstack-po=
st-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55ff5b30-db05-40b9-b5c0-d9928=
3e076c0_1112x704.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;=
:null,&quot;offset&quot;:false}" alt=3D"" width=3D"550" height=3D"348.20143=
88489209" src=3D"https://substackcdn.com/image/fetch/$s_!Hw_k!,w_1100,c_lim=
it,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-medi=
a.s3.amazonaws.com%2Fpublic%2Fimages%2F55ff5b30-db05-40b9-b5c0-d99283e076c0=
_1112x704.png" style=3D"border: none !important;vertical-align: middle;disp=
lay: block;-ms-interpolation-mode: bicubic;height: auto;margin-bottom: 0;wi=
dth: auto !important;max-width: 100% !important;margin: 0 auto;"></a></td><=
td style=3D"text-align: center;"></td></tr></tbody></table></figure></div><=
p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-s=
ize: 16px;"><strong>What problem does it solve?</strong><span> The performa=
nce gap between proprietary AI giants and open-source models remains a sign=
ificant hurdle =E2=80=93 especially for =E2=80=9Cagentic=E2=80=9D tasks whe=
re an AI must plan, act, and reason with tools or environments. Kimi K2 tac=
kles this by pushing </span><strong>open-source LLMs to unprecedented scale=
 and capability</strong><span>. Training ultra-large models is notoriously =
unstable (loss spikes, divergence) and costly. Moreover, imbuing an LLM wit=
h agent-like abilities (the kind needed to interact with environments) typi=
cally requires complex fine-tuning that few open projects have achieved. In=
 short, the problem addressed here is how to build an open model that is bo=
th </span><strong>massive in scale</strong><span> and </span><strong>skille=
d in agentic reasoning</strong><span>, without falling prey to training ins=
tabilities or closed data.</span></p><p style=3D"margin: 0 0 20px 0;color: =
rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>How does it solve=
 the problem?</strong><span> Kimi K2 introduces a </span><strong>Mixture-of=
-Experts (MoE)</strong><span> architecture with a staggering 1 trillion par=
ameters (of which 32 billion are =E2=80=9Cactive=E2=80=9D per token). This =
MoE design allows scaling the model=E2=80=99s capacity without a proportion=
al increase in computation for every token. To overcome training instabilit=
y, the team developed a new optimizer called </span><em>MuonClip</em><span>=
 (improving on the Muon optimizer) that uses a novel QK-clip technique to p=
revent the divergence issues that often occur in MoE training. With this, t=
hey successfully pre-trained Kimi K2 on an immense 15.5 trillion tokens </s=
pan><strong>without any loss spikes</strong><span>. After pre-training, the=
y didn=E2=80=99t stop at a generic model =E2=80=93 they put K2 through a mu=
lti-stage </span><em>post-training</em><span> regimen. This included a larg=
e-scale </span><strong>agentic data synthesis</strong><span> (generating di=
verse scenarios requiring tool use and reasoning) and a joint </span><stron=
g>reinforcement learning</strong><span> stage where K2 interacted with both=
 real and simulated environments to improve its decision-making. In essence=
, Kimi K2 was taught not just to predict text, but to behave as an </span><=
em>agent</em><span>, refining its skills via trial-and-error feedback.</spa=
n></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px=
;font-size: 16px;"><strong>What are the key findings?</strong><span> Kimi K=
2 now stands as one of the </span><strong>most capable open-source LLMs</st=
rong><span> to date. Thanks to its scale and training, it achieves state-of=
-the-art results among open models across a variety of benchmarks. Notably,=
 it excels in what the authors call &quot;non-thinking&quot; (direct respon=
se) settings: for example, it scored 66.1 on the Tau2 reasoning benchmark a=
nd 76.5 on ACE (English) =E2=80=93 outperforming most existing open and eve=
n closed models when they=E2=80=99re not allowed to use chain-of-thought pr=
ompting. It also shows strong prowess in domains like coding and math, with=
 a 53.7% on LiveCode (code generation) and 49.5% on the AIME 2025 math test=
. These figures are significant improvements, often surpassing models many =
times its size. Equally important, K2=E2=80=99s training pipeline proved th=
at </span><em>interactive fine-tuning</em><span> (through environment inter=
actions) measurably boosts an LLM=E2=80=99s problem-solving abilities. The =
model=E2=80=99s releases include both the base 1T-parameter checkpoint and =
a further post-trained checkpoint, giving the community a powerful new foun=
dation to experiment with.</span></p><p style=3D"margin: 0 0 20px 0;color: =
rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>Why does it matte=
r?</strong><span> Kimi K2=E2=80=99s success is a </span><strong>milestone f=
or the open AI ecosystem</strong><span>. It demonstrates that open-source r=
esearchers can not only reach the trillion-parameter scale, but also instil=
l advanced reasoning and agentic behaviors that were previously seen only i=
n flagship proprietary models. In practical terms, an open model with these=
 capabilities lowers the barrier for a wide range of applications =E2=80=93=
 from complex coding assistants to autonomous research agents =E2=80=93 wit=
hout needing API access to a closed AI. The innovations in training (like M=
uonClip) may also benefit others working on large models, making it easier =
to train huge systems reliably. More broadly, Kimi K2 validates a paradigm:=
 by combining massive scale with targeted </span><em>agentic training</em><=
span>, we can produce AI that is both </span><strong>broadly knowledgeable =
and able to apply that knowledge in multi-step, interactive contexts</stron=
g><span>. It=E2=80=99s a step toward AI that not only contains facts and sk=
ills, but can deploy them autonomously in pursuit of goals =E2=80=93 and it=
=E2=80=99s all happening in the open.</span></p><h2 class=3D"header-anchor-=
post" style=3D"position: relative;font-family: 'SF Pro Display',-apple-syst=
em-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,He=
lvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Sym=
bol';font-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-sm=
oothing: antialiased;-webkit-appearance: optimizelegibility;-moz-appearance=
: optimizelegibility;appearance: optimizelegibility;margin: 1em 0 0.625em 0=
;color: rgb(54,55,55);line-height: 1.16em;font-size: 1.625em;">3. Flow Matc=
hing Policy Gradients</h2><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,5=
5);line-height: 26px;font-size: 16px;"><span>Watching: Flow Matching (</spa=
n><a href=3D"https://substack.com/redirect/d6dcee97-6888-442d-9b91-d4ee30c6=
744b?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" r=
el=3D"" style=3D"color: rgb(54,55,55);text-decoration: underline;">paper</a=
><span>/</span><a href=3D"https://substack.com/redirect/d5241ffb-5f00-4823-=
bea2-347735652e36?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4=
tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,55);text-decoration: underli=
ne;">code</a><span>)</span></p><div class=3D"captioned-image-container-stat=
ic" style=3D"font-size: 16px;line-height: 26px;margin: 32px auto;"><figure =
style=3D"width: 100%;margin: 0 auto;"><table class=3D"image-wrapper" width=
=3D"100%" border=3D"0" cellspacing=3D"0" cellpadding=3D"0" data-component-n=
ame=3D"Image2ToDOMStatic" style=3D"mso-padding-alt: 1em 0 1.6em;"><tbody><t=
r><td style=3D"text-align: center;"></td><td class=3D"content" align=3D"lef=
t" width=3D"1003" style=3D"text-align: center;"><a class=3D"image-link" tar=
get=3D"_blank" href=3D"https://substack.com/redirect/44a263de-fc92-4f2a-aeb=
c-613261944685?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAj=
MRMPOw0" rel=3D"" style=3D"position: relative;flex-direction: column;align-=
items: center;padding: 0;width: auto;height: auto;border: none;text-decorat=
ion: none;display: block;margin: 0;"><img class=3D"wide-image" data-attrs=
=3D"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/pub=
lic/images/f9996358-ecdd-4af9-a3a2-96ad3a076941_1003x525.png&quot;,&quot;sr=
cNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:n=
ull,&quot;height&quot;:525,&quot;width&quot;:1003,&quot;resizeWidth&quot;:n=
ull,&quot;bytes&quot;:122158,&quot;alt&quot;:null,&quot;title&quot;:null,&q=
uot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFo=
ld&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot=
;https://www.llmwatch.com/i/169670030?img=3Dhttps%3A%2F%2Fsubstack-post-med=
ia.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9996358-ecdd-4af9-a3a2-96ad3a07694=
1_1003x525.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,=
&quot;offset&quot;:false}" alt=3D"" width=3D"550" height=3D"287.88634097706=
88" src=3D"https://substackcdn.com/image/fetch/$s_!siIc!,w_1100,c_limit,f_a=
uto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.a=
mazonaws.com%2Fpublic%2Fimages%2Ff9996358-ecdd-4af9-a3a2-96ad3a076941_1003x=
525.png" style=3D"border: none !important;vertical-align: middle;display: b=
lock;-ms-interpolation-mode: bicubic;height: auto;margin-bottom: 0;width: a=
uto !important;max-width: 100% !important;margin: 0 auto;"></a></td><td sty=
le=3D"text-align: center;"></td></tr></tbody></table></figure></div><p styl=
e=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 1=
6px;"><strong>What problem does it solve?</strong><span> In reinforcement l=
earning for continuous control (think robotics or game physics), policies a=
re usually represented with simple probability distributions (typically Gau=
ssians) that </span><strong>struggle with multi-modal decisions</strong><sp=
an>. For example, if there are two very different yet equally good ways to =
achieve a goal, a Gaussian policy tends to average them into a mediocre sin=
gle mode. Recent advances in generative modeling =E2=80=93 like diffusion m=
odels =E2=80=93 can represent complex, multi-modal distributions, but integ=
rating them into RL has been awkward. Prior attempts to use diffusion model=
s in RL required fixing a specific sampling procedure or calculating exact =
probabilities at every step, which is cumbersome and can limit performance.=
 So the core problem is how to train an RL agent that can leverage the expr=
essive power of diffusion/flow-based models for its action decisions </span=
><strong>without</strong><span> complicating the training process or being =
tied to a single sampling method.</span></p><p style=3D"margin: 0 0 20px 0;=
color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>How does i=
t solve the problem?</strong><span> The authors propose </span><strong>Flow=
 Policy Optimization (FPO)</strong><span>, an on-policy RL algorithm that m=
arries diffusion-like flow modeling with standard policy gradients. In esse=
nce, they recast the policy learning objective as a </span><strong>flow mat=
ching</strong><span> problem: instead of directly maximizing expected rewar=
d, FPO trains the policy by matching the =E2=80=9Cprobability flow=E2=80=9D=
 of an optimal policy. Concretely, they derive an update rule that maximize=
s the </span><em>advantage-weighted geometric mean</em><span> of probabilit=
ies (rather than arithmetic mean), aligning with a conditional flow matchin=
g loss. This approach slots neatly into the popular PPO (Proximal Policy Op=
timization) framework =E2=80=93 they even use a PPO-style clipping mechanis=
m to keep updates stable. The key innovation is that FPO doesn=E2=80=99t re=
quire computing exact likelihoods for diffusion model outputs at every step=
 (bypassing a major hurdle). It also treats the choice of diffusion sampler=
 as a plug-and-play detail, meaning the training isn=E2=80=99t handcuffed t=
o any particular way of generating samples. The authors implement FPO and t=
rain </span><strong>diffusion-based policies from scratch</strong><span> on=
 classic continuous control tasks (like locomotion and manipulation), effec=
tively turning those tasks into a playground for generative models to act a=
s agents.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);lin=
e-height: 26px;font-size: 16px;"><strong>What are the key findings?</strong=
><span> Flow Matching Policy Gradients prove remarkably effective. The diff=
usion-style policies learned via FPO can capture </span><strong>rich, multi=
-modal action distributions</strong><span> that a traditional Gaussian poli=
cy simply couldn=E2=80=99t express. In practical terms, on several benchmar=
k tasks the FPO-trained agents achieved higher rewards than baseline method=
s. They particularly shine in scenarios with ambiguity (so-called </span><e=
m>under-conditioned</em><span> settings, where the optimal action isn=E2=80=
=99t uniquely determined by the state) =E2=80=93 here, the ability to repre=
sent multiple likely futures gives the agent an edge. Another notable findi=
ng is that this performance gain comes </span><em>without</em><span> sacrif=
icing stability; the training curves are smooth, thanks to the built-in cli=
pping and the robustness of using a geometric mean objective. By comparing =
to prior diffusion-in-RL approaches, the authors show that FPO=E2=80=99s ag=
nosticism to sampler and avoidance of exact likelihood calculations make it=
 more flexible and broadly applicable. All told, the results indicate that =
advanced generative models can be plugged into RL frameworks to significant=
ly improve policy quality.</span></p><p style=3D"margin: 0 0 20px 0;color: =
rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>Why does it matte=
r?</strong><span> FPO is a cross-disciplinary breakthrough that </span><str=
ong>bridges the gap between two evolving fronts of AI</strong><span>: gener=
ative modeling and decision-making. For the RL community, it opens the door=
 to a new class of agents that don=E2=80=99t have to choose one action when=
 they can prepare for many =E2=80=93 which could be crucial for complex tas=
ks like self-driving, where multiple responses might be viable. The method =
is also noteworthy for improving efficiency: by better matching the true la=
ndscape of optimal actions, an FPO agent may require fewer trial-and-error =
iterations to learn good strategies. More philosophically, this work hints =
at a future where techniques from </span><em>diffusion models (designed for=
 images and text)</em><span> help create smarter </span><em>policies for ro=
bots and autonomous systems</em><span>. It=E2=80=99s a reminder that as AI =
systems become more general, ideas from different subfields will combine to=
 overcome each other=E2=80=99s limitations. Here, the precision of reinforc=
ement learning meets the creativity of generative models =E2=80=93 and the =
outcome is an agent that can genuinely do more by considering a </span><str=
ong>wider space of possibilities</strong><span> when deciding how to act.</=
span></p><h2 class=3D"header-anchor-post" style=3D"position: relative;font-=
family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,Bli=
nkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color E=
moji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smo=
othing: antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearance=
: optimizelegibility;-moz-appearance: optimizelegibility;appearance: optimi=
zelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16=
em;font-size: 1.625em;">4. GEPA: Reflective Prompt Evolution Can Outperform=
 Reinforcement Learning</h2><p style=3D"margin: 0 0 20px 0;color: rgb(54,55=
,55);line-height: 26px;font-size: 16px;"><span>Watching: GEPA (</span><a hr=
ef=3D"https://substack.com/redirect/04c86654-559a-4c11-b4fc-87e96cc892a9?j=
=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"=
" style=3D"color: rgb(54,55,55);text-decoration: underline;">paper</a><span=
>)</span></p><div class=3D"captioned-image-container-static" style=3D"font-=
size: 16px;line-height: 26px;margin: 32px auto;"><figure style=3D"width: 10=
0%;margin: 0 auto;"><table class=3D"image-wrapper" width=3D"100%" border=3D=
"0" cellspacing=3D"0" cellpadding=3D"0" data-component-name=3D"Image2ToDOMS=
tatic" style=3D"mso-padding-alt: 1em 0 1.6em;"><tbody><tr><td style=3D"text=
-align: center;"></td><td class=3D"content" align=3D"left" width=3D"610" st=
yle=3D"text-align: center;"><a class=3D"image-link" target=3D"_blank" href=
=3D"https://substack.com/redirect/013170a9-c2b8-47f7-b177-6b14ae4ade39?j=3D=
eyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" s=
tyle=3D"position: relative;flex-direction: column;align-items: center;paddi=
ng: 0;width: auto;height: auto;border: none;text-decoration: none;display: =
block;margin: 0;"><img class=3D"wide-image" data-attrs=3D"{&quot;src&quot;:=
&quot;https://substack-post-media.s3.amazonaws.com/public/images/1d398ed8-c=
544-4f85-b29a-43f02d87e4d4_610x447.png&quot;,&quot;srcNoWatermark&quot;:nul=
l,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;=
:447,&quot;width&quot;:610,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:5=
7950,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;ima=
ge/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topI=
mage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.llmwatch.co=
m/i/169670030?img=3Dhttps%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpu=
blic%2Fimages%2F1d398ed8-c544-4f85-b29a-43f02d87e4d4_610x447.png&quot;,&quo=
t;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}=
" alt=3D"" width=3D"550" height=3D"403.0327868852459" src=3D"https://substa=
ckcdn.com/image/fetch/$s_!DnaL!,w_1100,c_limit,f_auto,q_auto:good,fl_progre=
ssive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fi=
mages%2F1d398ed8-c544-4f85-b29a-43f02d87e4d4_610x447.png" style=3D"border: =
none !important;vertical-align: middle;display: block;-ms-interpolation-mod=
e: bicubic;height: auto;margin-bottom: 0;width: auto !important;max-width: =
100% !important;margin: 0 auto;"></a></td><td style=3D"text-align: center;"=
></td></tr></tbody></table></figure></div><p style=3D"margin: 0 0 20px 0;co=
lor: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>What problem=
 does it solve?</strong><span> Fine-tuning LLMs for new tasks often relies =
on reinforcement learning (RL) with very sparse feedback =E2=80=93 a model =
generates an answer and gets a single reward (right or wrong). Methods like=
 GRPO (Group Relative Policy Optimization) can align LLMs to tasks but are =
</span><em>painfully sample-inefficient</em><span>, sometimes needing thous=
ands of trial-and-error episodes to see improvement. Moreover, using only a=
 scalar reward misses all the nuanced reasons </span><em>why</em><span> an =
answer was wrong, which the model might infer if given the chance. The prob=
lem, then, is how to adapt an LLM to a task </span><strong>faster and more =
richly</strong><span> than traditional RL, possibly by leveraging the model=
=E2=80=99s own understanding (since outputs are language, the model could a=
nalyze them) instead of treating it like a black-box policy that only sees =
rewards.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line=
-height: 26px;font-size: 16px;"><strong>How does it solve the problem?</str=
ong><span> GEPA introduces a new paradigm of </span><em>reflective prompt e=
volution</em><span>. Instead of adjusting model weights via gradients, GEPA=
 keeps the model fixed and </span><strong>optimizes the prompts and instruc=
tions</strong><span> given to it. It works like a scientist running experim=
ents: for a given task, GEPA has the LLM attempt the task (producing reason=
ing traces, tool calls, etc.), then it has the LLM </span><em>reflect in na=
tural language</em><span> on those attempts =E2=80=93 identifying errors or=
 inefficiencies in its reasoning. Based on this self-critique, GEPA generat=
es proposed modifications to the prompts (or chain-of-thought guidelines). =
It doesn=E2=80=99t rely on a single brainstorm: it produces a diverse set o=
f candidate prompts and uses a </span><em>Genetic-Pareto</em><span> strateg=
y to combine the best parts of different candidates. In other words, it=E2=
=80=99s performing </span><strong>natural language evolution</strong><span>=
: each =E2=80=9Cgeneration=E2=80=9D consists of the model analyzing its fai=
lures and mutating the prompt to address them. This process can turn even a=
 handful of task rollouts into substantial quality gains, because each roll=
out yields a wealth of information (the model=E2=80=99s own commentary) rat=
her than just a win/lose signal.</span></p><p style=3D"margin: 0 0 20px 0;c=
olor: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>What are th=
e key findings?</strong><span> GEPA dramatically outperforms traditional RL=
 fine-tuning in both </span><strong>effectiveness and efficiency</strong><s=
pan>. Across four different tasks, it achieved on average a 10% higher succ=
ess rate than a strong RL baseline (GRPO), and in some cases up to 20% high=
er. Crucially, it did so using far fewer trials =E2=80=93 up to 35=C3=97 fe=
wer rollouts =E2=80=93 meaning it squeezed much more learning out of each a=
ttempt. It also beat the previous state-of-the-art prompt optimization meth=
od (MIPRO v2) by over 10% on shared benchmarks. Qualitatively, the prompts =
evolved by GEPA were found to encode high-level =E2=80=9Crules=E2=80=9D and=
 insights (often expressed in plain English) that helped the model avoid sp=
ecific pitfalls. For example, on a coding task, GEPA might add a reminder l=
ike =E2=80=9Cfirst check for edge cases such as null inputs,=E2=80=9D which=
 an RL reward alone would never explicitly give. Another interesting findin=
g is that GEPA=E2=80=99s approach can serve as an </span><em>online strateg=
y</em><span>: the authors demonstrated it improving a model=E2=80=99s outpu=
ts on the fly during inference, by iteratively refining the prompt for each=
 new problem. This blurs the line between training and usage =E2=80=93 the =
model is essentially </span><strong>self-improving in real time</strong><sp=
an> by rewriting its own instructions.</span></p><p style=3D"margin: 0 0 20=
px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>Why d=
oes it matter?</strong><span> GEPA=E2=80=99s success hints at an alternativ=
e (or complement) to reinforcement learning for aligning and enhancing LLMs=
. Instead of treating the model as a reinforcement learner that must be coa=
xed with numeric rewards, GEPA treats the model as a </span><em>reasoner</e=
m><span> that can read and write its own improvement instructions. This app=
roach can be far more </span><strong>data-efficient</strong><span> (as seen=
 by the 35=C3=97 reduction in needed trials) =E2=80=93 an important practic=
al advantage when each trial might involve costly API calls or human evalua=
tions. It=E2=80=99s also more interpretable: we end up with an improved pro=
mpt that humans can read and understand, rather than a mysterious set of we=
ight changes inside the model. For the field of AI, this work underscores t=
he power of letting models =E2=80=9Cthink about their thinking.=E2=80=9D By=
 using the medium of language, GEPA shows that LLMs can leverage their inte=
rnal knowledge to correct themselves, which is a step toward </span><strong=
>autonomous self-correction</strong><span> in AI systems. In a broader sens=
e, it exemplifies a trend of </span><em>language-native optimization</em><s=
pan>: using the model=E2=80=99s own natural outputs (explanations, reflecti=
ons) as feedback, which could be applied to many domains beyond these tasks=
. If LLMs can continue to refine their behavior through such reflective loo=
ps, we might achieve robust performance gains without always resorting to b=
rute-force reinforcement signals.</span></p><h2 class=3D"header-anchor-post=
" style=3D"position: relative;font-family: 'SF Pro Display',-apple-system-h=
eadline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvet=
ica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'=
;font-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smooth=
ing: antialiased;-webkit-appearance: optimizelegibility;-moz-appearance: op=
timizelegibility;appearance: optimizelegibility;margin: 1em 0 0.625em 0;col=
or: rgb(54,55,55);line-height: 1.16em;font-size: 1.625em;">5. Falcon-H1: A =
Family of Hybrid-Head Language Models Redefining Efficiency and Performance=
</h2><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;=
font-size: 16px;"><span>Watching: Falcon-H1 (</span><a href=3D"https://subs=
tack.com/redirect/9db5dbb6-b4af-45a1-976a-f9a4f931e5ac?j=3DeyJ1IjoiNWtiOTN6=
In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: r=
gb(54,55,55);text-decoration: underline;">paper</a><span>/</span><a href=3D=
"https://substack.com/redirect/86f9560e-81a4-4ee0-9969-d9075403db6a?j=3DeyJ=
1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" styl=
e=3D"color: rgb(54,55,55);text-decoration: underline;">code</a><span>)</spa=
n></p><div class=3D"captioned-image-container-static" style=3D"font-size: 1=
6px;line-height: 26px;margin: 32px auto;"><figure style=3D"width: 100%;marg=
in: 0 auto;"><table class=3D"image-wrapper" width=3D"100%" border=3D"0" cel=
lspacing=3D"0" cellpadding=3D"0" data-component-name=3D"Image2ToDOMStatic" =
style=3D"mso-padding-alt: 1em 0 1.6em;"><tbody><tr><td style=3D"text-align:=
 center;"></td><td class=3D"content" align=3D"left" width=3D"1026" style=3D=
"text-align: center;"><a class=3D"image-link" target=3D"_blank" href=3D"htt=
ps://substack.com/redirect/f8643f0f-1533-46d5-8827-bbbbbede15d6?j=3DeyJ1Ijo=
iNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D=
"position: relative;flex-direction: column;align-items: center;padding: 0;w=
idth: auto;height: auto;border: none;text-decoration: none;display: block;m=
argin: 0;"><img class=3D"wide-image" data-attrs=3D"{&quot;src&quot;:&quot;h=
ttps://substack-post-media.s3.amazonaws.com/public/images/5c783ffe-c48e-49a=
9-bc48-8d4f06b73ebf_1026x772.png&quot;,&quot;srcNoWatermark&quot;:null,&quo=
t;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:772,&=
quot;width&quot;:1026,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:99798,=
&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/pn=
g&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&=
quot;:false,&quot;internalRedirect&quot;:&quot;https://www.llmwatch.com/i/1=
69670030?img=3Dhttps%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%=
2Fimages%2F5c783ffe-c48e-49a9-bc48-8d4f06b73ebf_1026x772.png&quot;,&quot;is=
Processing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" al=
t=3D"" width=3D"550" height=3D"413.8401559454191" src=3D"https://substackcd=
n.com/image/fetch/$s_!v06r!,w_1100,c_limit,f_auto,q_auto:good,fl_progressiv=
e:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimage=
s%2F5c783ffe-c48e-49a9-bc48-8d4f06b73ebf_1026x772.png" style=3D"border: non=
e !important;vertical-align: middle;display: block;-ms-interpolation-mode: =
bicubic;height: auto;margin-bottom: 0;width: auto !important;max-width: 100=
% !important;margin: 0 auto;"></a></td><td style=3D"text-align: center;"></=
td></tr></tbody></table></figure></div><p style=3D"margin: 0 0 20px 0;color=
: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>What problem do=
es it solve?</strong><span> Bigger isn=E2=80=99t always better =E2=80=93 es=
pecially if you can barely run =E2=80=9Cbigger.=E2=80=9D The AI community h=
as wrestled with the trade-off between </span><strong>model size and practi=
cality</strong><span>. Enormous transformer models deliver great performanc=
e but at enormous computational cost, while smaller models run efficiently =
but often lag far behind in ability. Another challenge is context length: s=
tandard transformers struggle to handle very long inputs due to their quadr=
atic scaling. Falcon-H1 directly addresses how to get </span><strong>more b=
ang for your parameter buck</strong><span> =E2=80=93 achieving top-tier per=
formance at a fraction of the model size, and doing so with much longer con=
text windows than usual. In short, the problem is designing an architecture=
 that can </span><em>match or exceed</em><span> the performance of models d=
ouble or quadruple its size, while also being memory- and runtime-efficient=
 (and extending to long contexts), something that could hugely benefit depl=
oyments on limited hardware.</span></p><p style=3D"margin: 0 0 20px 0;color=
: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>How does it sol=
ve the problem?</strong><span> The team behind Falcon-H1 took a bold hybrid=
 approach. Instead of using a pure Transformer like most LLMs, Falcon-H1 co=
mbines Transformer-based self-attention with </span><strong>State Space Mod=
el (SSM)</strong><span> components in a parallel =E2=80=9Cdual head=E2=80=
=9D architecture. SSMs (inspired by models like S4) are known for handling =
long sequences with linear computational complexity and providing superior =
long-term memory. By integrating SSM layers alongside traditional attention=
, Falcon-H1 leverages the </span><em>strengths of both</em><span>: attentio=
n for complex short-term dependencies and SSM for efficient long-range proc=
essing. They didn=E2=80=99t stop at architecture =E2=80=93 the designers re=
visited everything from model depth to training data strategy to optimize f=
or efficiency. Falcon-H1 comes in a spectrum of sizes (base and instruction=
-tuned variants at 0.5B, 1.5B, 3B, 7B, and 34B parameters, plus a special =
=E2=80=9C1.5B-deep=E2=80=9D version). Notably, all models support a context=
 window up to </span><strong>256k tokens</strong><span> =E2=80=93 orders of=
 magnitude beyond typical limits =E2=80=93 enabled by the SSM component man=
aging long-term memory. They also released many models in quantized form (i=
nt8) for easy deployment, totaling over 30 checkpoints on HuggingFace. The =
entire suite is open-source under a permissive license, meaning anyone can =
use or fine-tune them without heavy restrictions.</span></p><p style=3D"mar=
gin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><s=
trong>What are the key findings?</strong><span> Falcon-H1 models deliver </=
span><strong>state-of-the-art results with drastically fewer parameters</st=
rong><span> than competitors. The flagship 34B model </span><em>matches or =
outperforms</em><span> models in the 70B parameter range on a wide array of=
 benchmarks. For instance, it=E2=80=99s reported to either match or beat re=
cent open models like Qwen2.5-72B and even a hypothetical Llama3-70B on tas=
ks spanning reasoning, math, multilingual understanding, and following inst=
ructions. Impressively, the efficiency gains hold at smaller scales too: th=
e Falcon-H1-1.5B-Deep model (an enhanced 1.5B parameter model) is on par wi=
th many 7B=E2=80=9310B models from just last year, and the tiniest Falcon-H=
1 0.5B model performs comparably to older 7B models from 2024. These are hu=
ge leaps in the price/performance ratio of language models. Beyond raw accu=
racy, the long-context capability stands out: the models can handle inputs =
like entire books or multi-day dialogues (up to 256,000 tokens) without ext=
ernal memory tricks, something practically unheard of in this space. This s=
uggests they maintain coherence and understanding over extremely lengthy te=
xts, which the evaluation confirms with strong performance on long-form and=
 multi-turn tasks. An important aspect is that all these gains come with </=
span><strong>no proprietary data or code</strong><span> =E2=80=93 Falcon-H1=
=E2=80=99s advancements are a testament to clever architecture and training=
, not just secret sauce. The release underlines that through innovation, op=
en models can rival or even surpass the giants.</span></p><p style=3D"margi=
n: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><str=
ong>Why does it matter?</strong><span> Falcon-H1 marks a significant step t=
owards </span><strong>making powerful AI more accessible</strong><span>. By=
 achieving top performance at smaller model sizes, it means organizations a=
nd researchers with limited compute can deploy or fine-tune models that pre=
viously were out of reach. This democratization is bolstered by the open li=
cense =E2=80=93 anyone can build on Falcon-H1 for their applications. The h=
ybrid Transformer-SSM approach also pioneers a path forward for model desig=
n: rather than blindly scaling up, we can </span><strong>scale smarter</str=
ong><span>. Long contexts of 256k tokens open up new application domains =
=E2=80=93 imagine AI models that can read and analyze hundreds of pages of =
text or code in one go, enabling deep analysis in finance, law, or literatu=
re without chopping the input. In essence, Falcon-H1 suggests that the era =
of solely chasing parameter count is waning; optimization of architecture a=
nd training can yield </span><em>=E2=80=9Csize-efficient=E2=80=9D</em><span=
> models that are just as capable. For the field, it=E2=80=99s a proof-of-c=
oncept that blending different sequence modeling paradigms (like attention =
and SSM) can overcome limitations neither could solve alone. Expect to see =
more hybrids in the future, as others build on Falcon-H1=E2=80=99s recipe t=
o push AI toward being faster, leaner, and yet even more powerful.</span></=
p><h2 class=3D"header-anchor-post" style=3D"position: relative;font-family:=
 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSy=
stemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','=
Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoothing:=
 antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearance: optim=
izelegibility;-moz-appearance: optimizelegibility;appearance: optimizelegib=
ility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font=
-size: 1.625em;">6. EDGE-GRPO: Entropy-Driven GRPO with Guided Error Correc=
tion for Advantage Diversity</h2><p style=3D"margin: 0 0 20px 0;color: rgb(=
54,55,55);line-height: 26px;font-size: 16px;"><span>Watching: EDGE-GRPO (</=
span><a href=3D"https://substack.com/redirect/b75dea53-d8d6-4f0b-879e-14f9b=
e27d690?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0=
" rel=3D"" style=3D"color: rgb(54,55,55);text-decoration: underline;">paper=
</a><span>)</span></p><div class=3D"captioned-image-container-static" style=
=3D"font-size: 16px;line-height: 26px;margin: 32px auto;"><figure style=3D"=
width: 100%;margin: 0 auto;"><table class=3D"image-wrapper" width=3D"100%" =
border=3D"0" cellspacing=3D"0" cellpadding=3D"0" data-component-name=3D"Ima=
ge2ToDOMStatic" style=3D"mso-padding-alt: 1em 0 1.6em;"><tbody><tr><td styl=
e=3D"text-align: center;"></td><td class=3D"content" align=3D"left" width=
=3D"619" style=3D"text-align: center;"><a class=3D"image-link" target=3D"_b=
lank" href=3D"https://substack.com/redirect/3766da02-362a-4358-b850-a9cd2cd=
56504?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" =
rel=3D"" style=3D"position: relative;flex-direction: column;align-items: ce=
nter;padding: 0;width: auto;height: auto;border: none;text-decoration: none=
;display: block;margin: 0;"><img class=3D"wide-image" data-attrs=3D"{&quot;=
src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/=
e36ee8a8-ee38-4211-86b9-036c29a679f6_619x669.png&quot;,&quot;srcNoWatermark=
&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;he=
ight&quot;:669,&quot;width&quot;:619,&quot;resizeWidth&quot;:null,&quot;byt=
es&quot;:90358,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;=
:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,=
&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.l=
lmwatch.com/i/169670030?img=3Dhttps%3A%2F%2Fsubstack-post-media.s3.amazonaw=
s.com%2Fpublic%2Fimages%2Fe36ee8a8-ee38-4211-86b9-036c29a679f6_619x669.png&=
quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&qu=
ot;:false}" alt=3D"" width=3D"550" height=3D"594.4264943457189" src=3D"http=
s://substackcdn.com/image/fetch/$s_!3K-p!,w_1100,c_limit,f_auto,q_auto:good=
,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2F=
public%2Fimages%2Fe36ee8a8-ee38-4211-86b9-036c29a679f6_619x669.png" style=
=3D"border: none !important;vertical-align: middle;display: block;-ms-inter=
polation-mode: bicubic;height: auto;margin-bottom: 0;width: auto !important=
;max-width: 100% !important;margin: 0 auto;"></a></td><td style=3D"text-ali=
gn: center;"></td></tr></tbody></table></figure></div><p style=3D"margin: 0=
 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>=
What problem does it solve?</strong><span> Recent methods like GRPO have im=
proved LLM reasoning by using reinforcement learning on reasoning traces, b=
ut they hit a nasty snag: </span><em>advantage collapse</em><span>. In GRPO=
, model outputs are grouped and only the best group gets a reward =E2=80=93=
 if multiple different answers all earn the same (e.g. zero) reward, the al=
gorithm can=E2=80=99t tell which direction is better to update towards. Ess=
entially, the learning signal within each group collapses to nothing, makin=
g it hard for the model to improve especially on challenging tasks where it=
 initially gets everything wrong. This leads to stagnation: the policy does=
n=E2=80=99t learn nuanced differences between a terrible answer and a almos=
t-correct answer, since both get labeled =E2=80=9C0=E2=80=9D. Prior approac=
hes tried to fix this by forcing more diverse answers (so that maybe one of=
 them gets a reward) or by adding extra internal feedback signals, but thes=
e had limitations. The fundamental problem remains: </span><strong>how to p=
reserve informative gradient signals</strong><span> even when explicit rewa=
rds are sparse or identical for many samples, so that the model continues t=
o learn rather than plateauing.</span></p><p style=3D"margin: 0 0 20px 0;co=
lor: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>How does it =
solve the problem?</strong><span> EDGE-GRPO introduces two key innovations =
to keep training signals rich: </span><strong>Entropy-Driven Advantage</str=
ong><span> and </span><strong>Guided Error Correction</strong><span> (hence=
 the acronym EDGE). The entropy-driven advantage means the algorithm uses t=
he model=E2=80=99s own confidence (entropy of its output distribution) as p=
art of the advantage computation. Intuitively, if the model is very unsure =
(high entropy) versus very confident (low entropy) about an answer, EDGE-GR=
PO will treat those cases differently even if both got the same final rewar=
d. This helps avoid the case where all bad answers look equally bad =E2=80=
=93 a hesitant wrong answer might be penalized differently than a confident=
ly wrong answer, encouraging the model to </span><strong>learn to be confid=
ently correct</strong><span>. The guided error correction component involve=
s providing targeted feedback or updates for </span><em>wrong answers speci=
fically</em><span>. While the paper=E2=80=99s details are technical, the co=
ncept is that the training process actively corrects errors by nudging the =
model in the direction of known-good reasoning steps (possibly through an a=
uxiliary reward for making certain improvements or via human-provided hints=
 integrated into the reward). Together, these mechanisms ensure that even w=
ithin a group of responses that all fail, there is </span><strong>gradient =
diversity</strong><span> =E2=80=93 some responses get a bit more advantage =
than others based on their entropy or partial progress, and the model recei=
ves guidance on how to fix its mistakes beyond just =E2=80=9Ctry again.=E2=
=80=9D</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-h=
eight: 26px;font-size: 16px;"><strong>What are the key findings?</strong><s=
pan> By attacking advantage collapse at its root, EDGE-GRPO achieves more s=
table and improved training outcomes on several challenging reasoning bench=
marks. The paper reports that models trained with EDGE-GRPO steadily increa=
se their reasoning scores where baseline GRPO would often flatline. One str=
iking observation: after EDGE-GRPO training, the model=E2=80=99s correct an=
swers tend to have </span><strong>lower entropy</strong><span> (i.e. the mo=
del is more confident and decisive) while its incorrect attempts remain hig=
h-entropy (the model expresses uncertainty when it=E2=80=99s likely wrong).=
 This is a desirable trait =E2=80=93 it means the AI knows when it knows so=
mething, and hesitates when it doesn=E2=80=99t, which is important for trus=
tworthiness. In terms of raw performance, EDGE-GRPO-trained models outperfo=
rmed their vanilla GRPO counterparts across the board, especially on proble=
ms that require multiple reasoning steps. They also found that some previou=
s fixes (like forcing the model to reflect or self-criticize) did increase =
diversity but didn=E2=80=99t fully solve the issue, whereas EDGE-GRPO=E2=80=
=99s entropy-based strategy clearly reduced instances of advantage collapse=
. The approach showed its strength even as tasks scaled in difficulty, indi=
cating better generalization and resilience of the learning process.</span>=
</p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;f=
ont-size: 16px;"><strong>Why does it matter?</strong><span> As LLMs are pus=
hed to perform more complex reasoning (think multi-step math, logical puzzl=
es, code generation with debugging), reinforcement learning is one of the o=
nly ways to really supervise the process. Making RL work well for these mod=
els is thus critical. EDGE-GRPO provides a </span><strong>new toolkit for r=
obustly training reasoning agents</strong><span>, ensuring they continue to=
 improve even when facing mostly failure at the start (common in very hard =
tasks). In practical terms, this could translate to smarter AI assistants t=
hat learn to solve problems that initially stumped them, without giving up =
due to a lack of feedback signal. The notion of using the model=E2=80=99s <=
/span><strong>own entropy as a teaching signal</strong><span> is also intri=
guing =E2=80=93 it leverages an internal metric (confidence) to guide learn=
ing, which might be applicable to other ML scenarios beyond language. Lastl=
y, by better aligning rewards with actual reasoning quality (not just final=
 correctness), approaches like EDGE-GRPO could produce models with more cal=
ibrated confidence and fewer spurious =E2=80=9Chigh-confidence wrong=E2=80=
=9D answers. For anyone building complex AI reasoning systems, these improv=
ements in training stability and performance are stepping stones toward </s=
pan><strong>reliable and continuously learning reasoning AI</strong><span>.=
</span></p><h2 class=3D"header-anchor-post" style=3D"position: relative;fon=
t-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,B=
linkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color=
 Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-s=
moothing: antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearan=
ce: optimizelegibility;-moz-appearance: optimizelegibility;appearance: opti=
mizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.=
16em;font-size: 1.625em;">7. Magentic-UI: Towards Human-in-the-loop Agentic=
 Systems</h2><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-heigh=
t: 26px;font-size: 16px;"><span>Watching: Magentic-UI (</span><a href=3D"ht=
tps://substack.com/redirect/e819ef8d-aa23-481e-af86-ce5959816427?j=3DeyJ1Ij=
oiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=
=3D"color: rgb(54,55,55);text-decoration: underline;">paper</a><span>)</spa=
n></p><div class=3D"captioned-image-container-static" style=3D"font-size: 1=
6px;line-height: 26px;margin: 32px auto;"><figure style=3D"width: 100%;marg=
in: 0 auto;"><table class=3D"image-wrapper" width=3D"100%" border=3D"0" cel=
lspacing=3D"0" cellpadding=3D"0" data-component-name=3D"Image2ToDOMStatic" =
style=3D"mso-padding-alt: 1em 0 1.6em;"><tbody><tr><td style=3D"text-align:=
 center;"></td><td class=3D"content" align=3D"left" width=3D"1033" style=3D=
"text-align: center;"><a class=3D"image-link" target=3D"_blank" href=3D"htt=
ps://substack.com/redirect/4a784676-41d7-4089-a6ba-b10b5d1bb549?j=3DeyJ1Ijo=
iNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D=
"position: relative;flex-direction: column;align-items: center;padding: 0;w=
idth: auto;height: auto;border: none;text-decoration: none;display: block;m=
argin: 0;"><img class=3D"wide-image" data-attrs=3D"{&quot;src&quot;:&quot;h=
ttps://substack-post-media.s3.amazonaws.com/public/images/a4cc079c-60a7-405=
d-8247-448b266af371_1033x688.png&quot;,&quot;srcNoWatermark&quot;:null,&quo=
t;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:688,&=
quot;width&quot;:1033,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:258212=
,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/p=
ng&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage=
&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.llmwatch.com/i/=
169670030?img=3Dhttps%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic=
%2Fimages%2Fa4cc079c-60a7-405d-8247-448b266af371_1033x688.png&quot;,&quot;i=
sProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" a=
lt=3D"" width=3D"550" height=3D"366.3117134559535" src=3D"https://substackc=
dn.com/image/fetch/$s_!vtzW!,w_1100,c_limit,f_auto,q_auto:good,fl_progressi=
ve:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimag=
es%2Fa4cc079c-60a7-405d-8247-448b266af371_1033x688.png" style=3D"border: no=
ne !important;vertical-align: middle;display: block;-ms-interpolation-mode:=
 bicubic;height: auto;margin-bottom: 0;width: auto !important;max-width: 10=
0% !important;margin: 0 auto;"></a></td><td style=3D"text-align: center;"><=
/td></tr></tbody></table></figure></div><p style=3D"margin: 0 0 20px 0;colo=
r: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>What problem d=
oes it solve?</strong><span> Autonomous LLM-based agents (think AutoGPT-sty=
le systems that browse, code, or execute tasks on their own) are exciting b=
ut </span><strong>unreliable and potentially unsafe</strong><span>. They of=
ten mis-execute instructions, get stuck, or even pursue harmful actions if =
not reined in, because they lack human judgment. At the same time, having a=
 human monitor every step defeats the purpose of automation. The problem Ma=
gentic-UI tackles is: </span><em>How can we integrate human oversight into =
AI agents in a seamless, efficient way</em><span>, so that humans can guide=
 the agent when needed (ensuring safety and correctness) without micromanag=
ing everything? This involves both a technical challenge (building a system=
 where humans and AI can interact fluidly) and an HCI challenge (figuring o=
ut what control mechanisms make sense to end-users). Essentially, it=E2=80=
=99s about </span><strong>combining human-in-the-loop control with agent au=
tonomy</strong><span> to get the best of both =E2=80=93 high success rates =
and safety, with minimal human effort.</span></p><p style=3D"margin: 0 0 20=
px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>How d=
oes it solve the problem?</strong><span> The authors developed </span><stro=
ng>Magentic-UI</strong><span>, an open-source web-based interface and frame=
work explicitly designed for human-agent collaboration. Under the hood, it =
runs a flexible multi-agent system (meaning you can have an LLM agent, tool=
-specific sub-agents, etc.) that can use external tools like web browsers, =
code interpreters, file systems, and more via a standardized protocol (the =
Model Context Protocol, MCP). The </span><em>UI</em><span> part is what the=
 human sees and interacts with. Magentic-UI offers six interaction mechanis=
ms that let a human intervene or cooperate with the agent at different leve=
ls. For example, </span><strong>co-planning</strong><span> lets a human and=
 the AI draft a task plan together, </span><strong>co-tasking</strong><span=
> might allow the human to handle one subtask while the AI handles another,=
 and </span><strong>multi-tasking</strong><span> could enable overseeing mu=
ltiple agents in parallel. There are </span><strong>action guards</strong><=
span>, where certain potentially risky actions (like sending an email or de=
leting a file) are paused for human approval. A </span><strong>long-term me=
mory</strong><span> mechanism allows both the agent and human to reference =
information persistently across a session (ensuring the agent doesn=E2=80=
=99t forget earlier context or corrections). These are just some of the six=
 =E2=80=93 the system essentially creates a control panel for the AI. The d=
esign philosophy is to keep human involvement =E2=80=9Clow friction=E2=80=
=9D: the human can inject themselves into the loop with minimal effort when=
 needed, and step back out when the agent is doing fine. To validate the se=
tup, Magentic-UI was tested in multiple modes =E2=80=93 from a fully </span=
><strong>Autonomous mode</strong><span> (agent has full tool control) to a =
</span><strong>Workflow mode</strong><span> (agent has no autonomy, strictl=
y following a human-defined script) and a middle-ground </span><strong>Hybr=
id mode</strong><span>.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb=
(54,55,55);line-height: 26px;font-size: 16px;"><strong>What are the key fin=
dings?</strong><span> Across extensive evaluations =E2=80=93 including auto=
nomous task completion benchmarks, simulated user studies, real user feedba=
ck sessions, and targeted safety tests =E2=80=93 Magentic-UI showed that th=
e human-in-the-loop approach can significantly boost </span><strong>task su=
ccess and safety</strong><span>. In autonomous benchmark runs, agents using=
 Magentic-UI=E2=80=99s framework completed more multi-step tasks correctly =
than fully-autonomous agents, largely because the human could step in at cr=
ucial junctures to prevent failure. The interaction mechanisms proved effec=
tive: for instance, in user trials, even non-expert users were able to use =
the co-planning and action guard features to steer the agent away from erro=
rs (like choosing the correct intermediate tool or aborting a dubious actio=
n). Quantitatively, the paper notes that </span><em>Hybrid mode</em><span> =
(some autonomy, some human control) achieved the best balance =E2=80=93 it =
nearly matched the success rate of fully autonomous runs (which benefit fro=
m the AI=E2=80=99s speed) while maintaining the safety and correctness of t=
he workflow mode (where nothing dangerous or incorrect slips through withou=
t a human check). Meanwhile, </span><strong>Workflow mode</strong><span> ga=
ve users deterministic control and was preferred for sensitive tasks, thoug=
h it slowed things down (as expected). The safety assessment found that Mag=
entic-UI=E2=80=99s guardrails (like action approval dialogues and the abili=
ty to inspect an agent=E2=80=99s reasoning) helped catch misaligned actions=
 and adversarial manipulations that a user might otherwise miss until it=E2=
=80=99s too late. Users in qualitative studies reported feeling more </span=
><strong>confidence and trust</strong><span> in the AI when using Magentic-=
UI, since they had transparency into what the agent was doing and the abili=
ty to intervene. All these findings underscore that thoughtfully adding a h=
uman loop doesn=E2=80=99t just avoid disasters =E2=80=93 it can genuinely i=
mprove the agent=E2=80=99s performance on complex tasks.</span></p><p style=
=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16=
px;"><strong>Why does it matter?</strong><span> As we integrate AI agents i=
nto real-world applications (from autonomous coding assistants to AI custom=
er support reps), pure autonomy is often a liability. Magentic-UI presents =
a practical path forward: </span><strong>AI-human collaboration interfaces<=
/strong><span> that amplify the strengths of both. Humans provide oversight=
, strategic guidance, and moral judgment; AI provides speed, consistency, a=
nd ability to handle the grunt work. This synergy can tackle problems neith=
er could alone =E2=80=93 AI might get 80% of a complex task done quickly, a=
nd a human can ensure the last 20% (and the overall direction) are correct.=
 The fact that Magentic-UI is open-source is also significant: it gives res=
earchers and developers a ready-made platform to study and deploy human-in-=
loop agents, accelerating progress in this crucial area. We often talk abou=
t =E2=80=9CAI alignment=E2=80=9D in abstract terms, but here is a concrete =
alignment tool =E2=80=93 put a human in the loop in a structured way. In a =
broader sense, Magentic-UI is a step toward </span><strong>making AI agents=
 trustworthy and user-friendly</strong><span>. Instead of fearing what an a=
utonomous agent might do, a user can collaborate with it, guiding it like a=
 teammate. This could ease the adoption of agentic AI in high-stakes domain=
s (like medicine or finance) where a human will always need to have a say. =
Overall, it shifts the narrative from humans </span><em>versus</em><span> A=
I to humans </span><em>and</em><span> AI solving problems together, which i=
s arguably how many real systems will be designed in the foreseeable future=
.</span></p><h2 class=3D"header-anchor-post" style=3D"position: relative;fo=
nt-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,=
BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Colo=
r Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-=
smoothing: antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appeara=
nce: optimizelegibility;-moz-appearance: optimizelegibility;appearance: opt=
imizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1=
.16em;font-size: 1.625em;">8. Agentic Reinforced Policy Optimization (ARPO)=
</h2><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;=
font-size: 16px;">Watching: ARPO ()</p><p style=3D"margin: 0 0 20px 0;color=
: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>What problem do=
es it solve?</strong><span> Large language model agents that operate over m=
ultiple steps =E2=80=93 especially those that can call tools or APIs =E2=80=
=93 pose a challenge for traditional reinforcement learning. Standard RL ei=
ther treats each action in isolation or looks only at final success, and ne=
ither is ideal for a multi-step reasoning scenario. For example, imagine an=
 LLM agent that has to solve a puzzle by doing web searches (tools) and the=
n giving an answer. If it gets the answer wrong, traditional RL might just =
give a zero reward at the end, without clarity on which tool use or step in=
 the reasoning was at fault. Moreover, the agent=E2=80=99s </span><strong>c=
onfidence can fluctuate wildly during the process</strong><span> =E2=80=93 =
the authors observed that right after the agent uses a tool and gets new in=
formation, its next action=E2=80=99s prediction entropy spikes (essentially=
, the agent says =E2=80=9Cnow what?=E2=80=9D with high uncertainty). Existi=
ng RL algorithms don=E2=80=99t account for these =E2=80=9Coh no, I=E2=80=99=
m confused=E2=80=9D moments; they sample actions uniformly and might waste =
a lot of time exploring even when the model is confident, or conversely, no=
t exploring enough when the model is confused. So the problem ARPO tackles =
is how to do </span><strong>fine-grained RL for multi-turn LLM agents</stro=
ng><span>, making sure the learning algorithm knows </span><em>which parts<=
/em><span> of the interaction need more exploration or specialized credit a=
ssignment (like after using a tool), instead of treating an entire trajecto=
ry with one blunt feedback.</span></p><p style=3D"margin: 0 0 20px 0;color:=
 rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>How does it solv=
e the problem?</strong><span> ARPO is a custom-tailored RL algorithm for LL=
M-based agents in interactive environments. It introduces an </span><strong=
>entropy-based adaptive rollout mechanism</strong><span>: in simpler terms,=
 the training dynamically allocates more exploration to those steps where t=
he agent is highly uncertain (high entropy) =E2=80=93 often immediately aft=
er tool usage or a big step in reasoning. For instance, if after consulting=
 a database the model seems unsure how to use that info, ARPO will encourag=
e trying different next moves there more than it would elsewhere. This prev=
ents the agent from repeatedly skipping over critical decision points in a =
hurry; it focuses learning effort where the agent struggles. Additionally, =
ARPO implements a </span><strong>step-level advantage attribution</strong><=
span>. Instead of just giving one reward at the end or a per-turn reward, i=
t computes how much each action (each question asked, each tool invoked) co=
ntributed to the final success or failure. This means the model gets a nuan=
ced learning signal =E2=80=93 maybe the final answer was wrong, but some ea=
rlier steps were actually beneficial and should be reinforced (or vice vers=
a). Under the hood, ARPO balances </span><em>global trajectory sampling</em=
><span> (exploring different overall sequences of actions) with </span><em>=
fine-grained step sampling</em><span> (re-sampling specific decisions in th=
e sequence), adjusting on the fly. It=E2=80=99s like a coach that sometimes=
 lets the agent play a whole game, and other times says =E2=80=9Clet=E2=80=
=99s replay just that tricky part one more time,=E2=80=9D thereby efficient=
ly honing the agent=E2=80=99s skills at difficult junctures.</span></p><p s=
tyle=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size=
: 16px;"><strong>What are the key findings?</strong><span> ARPO-trained age=
nts delivered </span><strong>substantially better performance</strong><span=
> on a suite of 13 challenging benchmarks, which included computational rea=
soning tasks, knowledge reasoning, and deep search problems. They outperfor=
med agents trained with conventional reinforcement learning approaches, par=
ticularly in scenarios where multiple tool calls and reasoning steps were n=
eeded. One headline result: ARPO achieved the same or better performance </=
span><em>using only half the tool interactions</em><span> compared to basel=
ine methods. In practical terms, if a naive agent might call an external ca=
lculator or wiki browser 10 times to get an answer right, an ARPO agent mig=
ht solve it with only 5 calls because it=E2=80=99s learning to use tools mo=
re efficiently. This suggests ARPO agents are learning more </span><strong>=
strategic and purposeful</strong><span> tool use, rather than flailing arou=
nd. Another observation is that ARPO=E2=80=99s focus on uncertain moments p=
aid off =E2=80=93 the entropy spikes the authors noticed became opportuniti=
es for learning, and over time the agents became more confident and accurat=
e in those once-problematic steps. On tasks requiring long reasoning chains=
, ARPO agents maintained coherence and didn=E2=80=99t get as derailed by ea=
rlier irrelevant actions (a sign that the advantage attribution helped cred=
it the right actions). The researchers also note that ARPO bridges a gap be=
tween two extremes: pure end-to-end RL versus scripted tool use. By combini=
ng high-level reward with step-level guidance, it kind of gets the advantag=
es of both (global optimization with local feedback). The end result is an =
agent that is more competent in </span><strong>realistic multi-turn setting=
s</strong><span> =E2=80=93 it makes fewer needless tool calls, and when it =
does act, its actions are more often actually helpful toward the goal.</spa=
n></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px=
;font-size: 16px;"><strong>Why does it matter?</strong><span> Training AI a=
gents to reliably perform multi-step tasks (like researching a topic online=
 and writing a summary, or debugging a piece of code using documentation) i=
s one of the frontiers of AI right now. ARPO is a significant advancement i=
n that it acknowledges and addresses the unique challenges in this setting =
=E2=80=93 long horizons, external tools, and varying uncertainty. For the A=
I community, it=E2=80=99s a proof that we can </span><strong>adapt reinforc=
ement learning to the quirks of LLM-based reasoning</strong><span>. This me=
ans future agents can be trained more effectively, reaching higher competen=
ce with less training data or costly interaction. The fact that ARPO cuts t=
ool usage in half is also economically relevant =E2=80=93 many tool calls (=
e.g., APIs, database queries) have costs or latency, so making agents more =
frugal without losing performance is a big win. Moreover, ARPO=E2=80=99s id=
eas might inspire algorithms in other domains where an AI=E2=80=99s confide=
nce fluctuates (imagine a self-driving car that becomes uncertain in new tr=
affic situations =E2=80=93 a similar adaptive exploration could be useful).=
 Big picture: ARPO brings us closer to </span><strong>AI agents that are bo=
th efficient and adaptive in dynamic tasks</strong><span>. It=E2=80=99s a s=
tep away from brittle one-shot prompt answering and toward agents that can =
</span><strong>learn from and react to an environment intelligently</strong=
><span>, which is a core component of any hoped-for general AI.</span></p><=
h2 class=3D"header-anchor-post" style=3D"position: relative;font-family: 'S=
F Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSyste=
mFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Seg=
oe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoothing: an=
tialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearance: optimize=
legibility;-moz-appearance: optimizelegibility;appearance: optimizelegibili=
ty;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-si=
ze: 1.625em;">9. RLVMR: Reinforcement Learning with Verifiable Meta-Reasoni=
ng Rewards for Robust Long-Horizon Agents</h2><p style=3D"margin: 0 0 20px =
0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>Watching: =
RLVMR (</span><a href=3D"https://substack.com/redirect/d32dd155-5661-4619-b=
c4d-0e63b634449f?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4t=
AjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,55);text-decoration: underlin=
e;">paper</a><span>)</span></p><div class=3D"captioned-image-container-stat=
ic" style=3D"font-size: 16px;line-height: 26px;margin: 32px auto;"><figure =
style=3D"width: 100%;margin: 0 auto;"><table class=3D"image-wrapper" width=
=3D"100%" border=3D"0" cellspacing=3D"0" cellpadding=3D"0" data-component-n=
ame=3D"Image2ToDOMStatic" style=3D"mso-padding-alt: 1em 0 1.6em;"><tbody><t=
r><td style=3D"text-align: center;"></td><td class=3D"content" align=3D"lef=
t" width=3D"1194" style=3D"text-align: center;"><a class=3D"image-link" tar=
get=3D"_blank" href=3D"https://substack.com/redirect/c11ccd5c-0613-4ca9-a9d=
a-3a31273c72de?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAj=
MRMPOw0" rel=3D"" style=3D"position: relative;flex-direction: column;align-=
items: center;padding: 0;width: auto;height: auto;border: none;text-decorat=
ion: none;display: block;margin: 0;"><img class=3D"wide-image" data-attrs=
=3D"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/pub=
lic/images/69640390-bb1b-4494-a50b-1ef901ed9015_1194x596.png&quot;,&quot;sr=
cNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:n=
ull,&quot;height&quot;:596,&quot;width&quot;:1194,&quot;resizeWidth&quot;:n=
ull,&quot;bytes&quot;:133184,&quot;alt&quot;:null,&quot;title&quot;:null,&q=
uot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFo=
ld&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot=
;https://www.llmwatch.com/i/169670030?img=3Dhttps%3A%2F%2Fsubstack-post-med=
ia.s3.amazonaws.com%2Fpublic%2Fimages%2F69640390-bb1b-4494-a50b-1ef901ed901=
5_1194x596.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,=
&quot;offset&quot;:false}" alt=3D"" width=3D"550" height=3D"274.53936348408=
71" src=3D"https://substackcdn.com/image/fetch/$s_!QyeK!,w_1100,c_limit,f_a=
uto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.a=
mazonaws.com%2Fpublic%2Fimages%2F69640390-bb1b-4494-a50b-1ef901ed9015_1194x=
596.png" style=3D"border: none !important;vertical-align: middle;display: b=
lock;-ms-interpolation-mode: bicubic;height: auto;margin-bottom: 0;width: a=
uto !important;max-width: 100% !important;margin: 0 auto;"></a></td><td sty=
le=3D"text-align: center;"></td></tr></tbody></table></figure></div><p styl=
e=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 1=
6px;"><strong>What problem does it solve?</strong><span> Reinforcement lear=
ning agents, especially language-model-based ones, can sometimes learn to g=
et the right answers </span><em>for the wrong reasons</em><span>. If an RL =
agent receives a reward only for completing a task, it might end up exploit=
ing quirks or repeating trial-and-error sequences that achieve the goal wit=
hout truly understanding the task =E2=80=93 leading to brittle behavior tha=
t fails if conditions change even slightly. This is a known issue: optimizi=
ng solely for final success often </span><strong>reinforces flawed or ineff=
icient reasoning paths</strong><span>. The agent doesn=E2=80=99t learn =E2=
=80=9Chow to reason,=E2=80=9D it just learns =E2=80=9Chow to win,=E2=80=9D =
which might involve hacks that don=E2=80=99t generalize. For long-horizon t=
asks (e.g., interactive game environments or complex problem-solving), this=
 means you get agents that reach the end by stumbling through, but they=E2=
=80=99re brittle and not interpretable. The problem RLVMR addresses is how =
to make RL agents actually </span><strong>learn to think coherently</strong=
><span> and not just luck into the correct outcomes. In other words, can we=
 reward the </span><em>process</em><span> of reasoning, not just the outcom=
e, so that an agent=E2=80=99s internal decision-making becomes sound and re=
liable?</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-=
height: 26px;font-size: 16px;"><strong>How does it solve the problem?</stro=
ng><span> RLVMR introduces a novel training framework that injects </span><=
strong>dense, process-level feedback</strong><span> into the reinforcement =
learning loop. It works by having the agent explicitly tag or demarcate ste=
ps of its own reasoning (for example, an agent might label parts of its act=
ion sequence as =E2=80=9Cplanning=E2=80=9D, =E2=80=9Cexploration=E2=80=9D, =
=E2=80=9Creflection=E2=80=9D, etc.), and then the trainer provides </span><=
strong>verifiable rewards</strong><span> for good behavior in those meta-re=
asoning steps. These rewards are programmatic and rule-based: for instance,=
 the agent could get a small reward for laying out a clear plan of attack (=
because a plan can be checked for coherence), another reward for exploring =
a new path after failing rather than repeating an old failed action (encour=
aging it to not get stuck in loops), and a reward for self-reflecting and c=
orrecting an error. These are =E2=80=9Cverifiable=E2=80=9D in that the envi=
ronment or a monitoring process can check if the agent did what it claimed =
(e.g., it said it was exploring and indeed it tried a new solution). RLVMR =
combines these process rewards with the usual final outcome reward into a s=
ingle learning objective, which they optimize with a policy gradient method=
 (notably, they do it </span><strong>critic-free</strong><span>, simplifyin=
g training). Essentially, RLVMR is </span><strong>teaching the agent how to=
 reason well</strong><span> by rewarding good reasoning steps along the way=
. The agent=E2=80=99s policy isn=E2=80=99t just outputting actions; it=E2=
=80=99s outputting actions plus annotations of its cognitive steps, which t=
he training algorithm continuously critiques and reinforces. This guided ap=
proach pushes the agent to adopt strategies a human might consider common-s=
ense: make a plan, follow through, adapt if needed, rather than randomly th=
rashing until something works.</span></p><p style=3D"margin: 0 0 20px 0;col=
or: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>What are the =
key findings?</strong><span> Agents trained with RLVMR become </span><stron=
g>significantly more robust and effective</strong><span> in long-horizon en=
vironments. On challenging interactive benchmarks like ALFWorld and Science=
World, RLVMR set new state-of-the-art performance =E2=80=93 for example, a =
7B-parameter LLM agent achieved an 83.6% success rate on the hardest unseen=
 task split in ALFWorld, whereas previous approaches were much lower. These=
 tasks involve multiple steps and require carrying out procedures in a simu=
lated world, so that high success rate indicates the agent can reliably han=
dle long tasks it hasn=E2=80=99t explicitly seen before. More illuminating,=
 the agents=E2=80=99 behavior changed qualitatively: </span><strong>reasoni=
ng quality improved dramatically</strong><span>. They exhibited far fewer r=
edundant or pointless actions =E2=80=93 meaning the agent wasn=E2=80=99t ju=
st flailing around to accidentally solve the task, but was following more e=
fficient, logical sequences. When mistakes happened, RLVMR agents showed en=
hanced error recovery: instead of getting stuck or repeating the same faile=
d action, they would try a different strategy or backtrack appropriately (j=
ust as a human problem-solver might do upon realizing an approach isn=E2=80=
=99t working). Another benefit was </span><strong>interpretability</strong>=
<span>: because the agent tags its cognitive steps and is rewarded for doin=
g so clearly, one can literally see the agent =E2=80=9Cthinking=E2=80=9D in=
 a structured way. The researchers could verify that the performance gains =
indeed stemmed from better reasoning =E2=80=93 for instance, the agent=E2=
=80=99s reflections often identified exactly why a previous attempt failed =
and how to fix it, demonstrating a depth of understanding that a pure outco=
me-based RL agent lacked. All of this suggests that the dense rewards succe=
ssfully shaped not just what the agent does, but </span><em>how</em><span> =
it does it, leading to solutions that are both correct and grounded in soun=
d reasoning. The combination of process and outcome rewards meant the agent=
 didn=E2=80=99t sacrifice success for the sake of following rules =E2=80=93=
 it actually achieved higher success because following those reasoning =E2=
=80=9Crules=E2=80=9D led to more robust strategies.</span></p><p style=3D"m=
argin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">=
<strong>Why does it matter?</strong><span> RLVMR is a compelling proof that=
 we can </span><strong>train not just for task success, but for the quality=
 of reasoning</strong><span>, and get better agents as a result. This has i=
mplications for the development of trustworthy AI. Agents that genuinely re=
ason through problems (as opposed to those that accidentally solve them) ar=
e more likely to handle new situations and to fail gracefully when they do =
fail. By rewarding meta-reasoning, RLVMR also aligns the agent=E2=80=99s in=
centives with behaviors we consider desirable (like deliberation, explorati=
on, self-correction) =E2=80=93 which could reduce the incidence of weird, u=
nexplainable shortcuts or cheats that pure reward maximization might encour=
age. In practical terms, this could lead to AI assistants that, say, show t=
heir work and double-check critical steps when they code or solve math prob=
lems, because those behaviors were baked into their training rewards. The s=
uccess on long-horizon tasks is also a green light for tackling more comple=
x, multi-step real-world problems with AI. It suggests that if we can forma=
lize what good reasoning looks like in a domain (even via simple rules), we=
 can substantially boost an agent=E2=80=99s performance and reliability in =
that domain. Lastly, RLVMR adds to the toolbox of </span><em>alignment tech=
niques</em><span> =E2=80=93 it=E2=80=99s a way of telling the AI =E2=80=9Cn=
ot only do the job, but do it in a way that we consider logically sound.=E2=
=80=9D As AI systems become more autonomous, methods like this will be key =
to ensure they don=E2=80=99t just get results, but get them in a safe and i=
nterpretable manner.</span></p><h2 class=3D"header-anchor-post" style=3D"po=
sition: relative;font-family: 'SF Pro Display',-apple-system-headline,syste=
m-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,san=
s-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight:=
 bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing: antialia=
sed;-webkit-appearance: optimizelegibility;-moz-appearance: optimizelegibil=
ity;appearance: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55=
,55);line-height: 1.16em;font-size: 1.625em;">10. MemTool: Optimizing Short=
-Term Memory Management for Dynamic Tool Calling in LLM Agent Conversations=
</h2><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;=
font-size: 16px;"><span>Watching: MemTool (</span><a href=3D"https://substa=
ck.com/redirect/d73d8ff6-7273-40b0-834b-5d7f80f86a42?j=3DeyJ1IjoiNWtiOTN6In=
0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb=
(54,55,55);text-decoration: underline;">paper</a><span>)</span></p><div cla=
ss=3D"captioned-image-container-static" style=3D"font-size: 16px;line-heigh=
t: 26px;margin: 32px auto;"><figure style=3D"width: 100%;margin: 0 auto;"><=
table class=3D"image-wrapper" width=3D"100%" border=3D"0" cellspacing=3D"0"=
 cellpadding=3D"0" data-component-name=3D"Image2ToDOMStatic" style=3D"mso-p=
adding-alt: 1em 0 1.6em;"><tbody><tr><td style=3D"text-align: center;"></td=
><td class=3D"content" align=3D"left" width=3D"1247" style=3D"text-align: c=
enter;"><a class=3D"image-link" target=3D"_blank" href=3D"https://substack.=
com/redirect/dcb210b7-558e-4001-ad9e-fdab2f23e8e2?j=3DeyJ1IjoiNWtiOTN6In0.z=
dzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"position: rel=
ative;flex-direction: column;align-items: center;padding: 0;width: auto;hei=
ght: auto;border: none;text-decoration: none;display: block;margin: 0;"><im=
g class=3D"wide-image" data-attrs=3D"{&quot;src&quot;:&quot;https://substac=
k-post-media.s3.amazonaws.com/public/images/d20497cd-1943-421b-9d10-7cdc4fc=
9fbb7_1247x826.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&q=
uot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:826,&quot;width&quo=
t;:1247,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:252091,&quot;alt&quo=
t;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot=
;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&=
quot;internalRedirect&quot;:&quot;https://www.llmwatch.com/i/169670030?img=
=3Dhttps%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd=
20497cd-1943-421b-9d10-7cdc4fc9fbb7_1247x826.png&quot;,&quot;isProcessing&q=
uot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt=3D"" width=
=3D"550" height=3D"364.31435445068166" src=3D"https://substackcdn.com/image=
/fetch/$s_!2zg2!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/htt=
ps%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd20497c=
d-1943-421b-9d10-7cdc4fc9fbb7_1247x826.png" style=3D"border: none !importan=
t;vertical-align: middle;display: block;-ms-interpolation-mode: bicubic;hei=
ght: auto;margin-bottom: 0;width: auto !important;max-width: 100% !importan=
t;margin: 0 auto;"></a></td><td style=3D"text-align: center;"></td></tr></t=
body></table></figure></div><p style=3D"margin: 0 0 20px 0;color: rgb(54,55=
,55);line-height: 26px;font-size: 16px;"><strong>What problem does it solve=
?</strong><span> LLM-based agents that carry on extended multi-turn convers=
ations and invoke tools face a practical memory issue: the context window i=
sn=E2=80=99t infinite. As an agent interacts (say it=E2=80=99s a chatbot th=
at can use a calculator, search engine, etc.), each tool=E2=80=99s output a=
nd the agent=E2=80=99s intermediate reasoning take up space in the prompt. =
Over many turns, this =E2=80=9Cshort-term memory=E2=80=9D can overflow, for=
cing the agent to drop older information =E2=80=93 which might be important=
 later. Without strategy, agents might forget key facts or, conversely, han=
g on to too much irrelevant data and drown in context. The problem is how t=
o intelligently </span><strong>manage the working memory</strong><span> of =
an agent: deciding what tool outputs or intermediate results to keep and wh=
at to discard as a conversation or task progresses. It=E2=80=99s analogous =
to our own short-term memory management (we remember what=E2=80=99s relevan=
t and let trivial details fade). For AI, getting this wrong either leads to=
 failures (forgetting needed info) or inefficiency (context blow-up).</span=
></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;=
font-size: 16px;"><strong>How does it solve the problem?</strong><span> Mem=
Tool provides a dedicated framework and set of strategies for </span><stron=
g>dynamic context management</strong><span> in tool-using agents. In practi=
ce, it offers three modes of operation:</span></p><ul style=3D"margin-top: =
0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet=
;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-=
sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Au=
tonomous Agent Mode:</strong><span> The agent itself has full autonomy to d=
ecide when to drop or retain tool-related information from its context. For=
 example, after using a tool, the agent can choose to =E2=80=9Cforget=E2=80=
=9D the tool=E2=80=99s output in subsequent turns if it deems it no longer =
needed.</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format=
: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom=
: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><s=
trong>Workflow Mode:</strong><span> A deterministic, human-defined policy c=
ontrols memory =E2=80=93 essentially no autonomy. This could be a simple ru=
le like =E2=80=9Calways remove the result of a tool call after 3 turns=E2=
=80=9D or =E2=80=9Conly keep the last result from each tool.=E2=80=9D The a=
gent doesn=E2=80=99t decide; it just follows a fixed procedure for memory.<=
/span></p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet=
;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-=
sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Hy=
brid Mode:</strong><span> A mix of both =E2=80=93 perhaps the agent can sug=
gest what to remove, but there are overrides or confirmations via set rules=
. Or certain critical data is always kept by rule, while the agent can mana=
ge the rest.</span><br><span>MemTool is implemented as an extension on top =
of a multi-agent conversation architecture (using the Model Context Protoco=
l to interface with tools). It monitors the context window and applies the =
above policies to </span><strong>prune or persist tool outputs</strong><spa=
n> across turns. The design allows plugging in these modes without changing=
 the underlying agent: you can swap between letting the agent think about m=
emory or enforcing a policy, based on model capability and use case. To eva=
luate it, the authors stress-tested MemTool on a benchmark called ScaleMCP =
with conversations spanning 100+ user interactions, which would normally ov=
erflow any context. They measured how effectively each mode prunes irreleva=
nt content (short-term memory efficiency) and how that impacts </span><stro=
ng>task completion accuracy</strong><span> =E2=80=93 does the agent still g=
et the answers right?</span></p></li></ul><p style=3D"margin: 0 0 20px 0;co=
lor: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>What are the=
 key findings?</strong><span> Memory management can indeed be handled, and =
the best approach depends on the model=E2=80=99s sophistication. In </span>=
<strong>Autonomous mode</strong><span>, very capable LLMs (think GPT-4 clas=
s or similarly strong models) were able to achieve extremely high memory ef=
ficiency =E2=80=93 they removed about 90=E2=80=9394% of unnecessary tool co=
ntent from the context (averaged over a 3-turn window). This indicates that=
 advanced models can learn or be prompted to recognize what information is =
safe to forget. However, medium-sized or less advanced models struggled whe=
n left on their own, with only 0=E2=80=9360% efficiency (sometimes basicall=
y forgetting nothing or forgetting the wrong things). They lack the judgmen=
t to prune memory well. The </span><strong>Workflow mode</strong><span>, by=
 contrast, consistently maintained a tidy context regardless of model size =
=E2=80=93 as expected, since a fixed policy was ensuring obsolete info was =
dropped. There=E2=80=99s no ambiguity or model error in deciding; the rules=
 did the job. But the trade-off came in task performance: Autonomous and Hy=
brid modes tended to </span><strong>excel at task completion</strong><span>=
, especially on complex tasks, compared to a strict workflow. This is likel=
y because the agent in those modes could choose to keep information it knew=
 would be relevant later, whereas a rigid workflow policy might blindly thr=
ow out something that, unbeknownst to it, the agent could have used in a fu=
ture step. The </span><strong>Hybrid mode</strong><span> often hit a sweet =
spot =E2=80=93 tool removal was effective and near-optimal, yet the final a=
ccuracy remained high. It seems giving the agent some say (especially a sma=
rt agent) in what to remember, while enforcing basic rules, leads to both a=
 clean context and correct answers. One concrete example from the evaluatio=
ns: a large LLM in Hybrid mode might remove =E2=80=9Ctemporary=E2=80=9D too=
l outputs (like intermediate calculations) but hold on to a key fact it fou=
nd via a tool, even if the workflow policy would normally purge it =E2=80=
=93 and that fact later leads to a correct answer, demonstrating the value =
of agent insight in memory management. Across the 100-turn conversations, M=
emTool prevented context overflow and thereby allowed the dialogues to cont=
inue smoothly with relevant information always at hand.</span></p><p style=
=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16=
px;"><strong>Why does it matter?</strong><span> This work is a stepping sto=
ne toward </span><strong>scalable, long-running AI agents</strong><span>. I=
n real-world usage, we want AI that can engage in prolonged tasks =E2=80=93=
 think of an AI assistant that can research, plan, and execute over hours o=
r days of interaction. Without good memory management, such agents would ei=
ther forget early instructions or become impossibly expensive to run (stuff=
ing huge transcripts into their context window). MemTool shows that with a =
combination of model smarts and engineered policy, we can extend an agent=
=E2=80=99s effective memory </span><strong>without infinite context windows=
</strong><span>. It=E2=80=99s like giving the agent a working memory akin t=
o an organized notebook: it writes down what it needs, erases what=E2=80=99=
s done, and always has room for the next idea. For practitioners, these fin=
dings also suggest practical guidelines: if you=E2=80=99re using a strong m=
odel, you can trust it more to handle its context (maybe just give it a fra=
mework like MemTool=E2=80=99s autonomous mode with some prompts to encourag=
e forgetting). If you=E2=80=99re on a smaller model, you might implement a =
strict memory clearance policy or use a hybrid approach to avoid the model =
drowning in old info. In broader AI research, MemTool touches on the concep=
t of </span><strong>meta-cognition</strong><span> =E2=80=93 the agent think=
ing about its own memory usage. That=E2=80=99s a vital aspect of human cogn=
ition (we constantly decide what to keep in mind or not), and implementing =
it in AI can lead to more efficient and human-like problem solving. Finally=
, this contributes to making AI </span><strong>more efficient and cheaper</=
strong><span> to deploy: if an agent knows to drop irrelevant context, it u=
ses fewer tokens in prompts over time, which could dramatically cut costs f=
or API-based models. In summary, MemTool points the way to AI that can hand=
le longer and more complex interactions by cleverly managing its finite mem=
ory, an essential capability for any advanced autonomous system.</span></p>=
<h2 class=3D"header-anchor-post" style=3D"position: relative;font-family: '=
SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSyst=
emFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Se=
goe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoothing: a=
ntialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearance: optimiz=
elegibility;-moz-appearance: optimizelegibility;appearance: optimizelegibil=
ity;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-s=
ize: 1.625em;">11. Graph-R1: Towards Agentic GraphRAG via End-to-end Reinfo=
rcement Learning</h2><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);li=
ne-height: 26px;font-size: 16px;"><span>Watching: Graph-R1 (</span><a href=
=3D"https://substack.com/redirect/3ce42c96-33ed-4131-be1b-8e57483a6369?j=3D=
eyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" s=
tyle=3D"color: rgb(54,55,55);text-decoration: underline;">paper</a><span>/<=
/span><a href=3D"https://substack.com/redirect/ae62c852-f25b-484e-8bcd-fd70=
482bf479?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw=
0" rel=3D"" style=3D"color: rgb(54,55,55);text-decoration: underline;">code=
</a><span>)</span></p><div class=3D"captioned-image-container-static" style=
=3D"font-size: 16px;line-height: 26px;margin: 32px auto;"><figure style=3D"=
width: 100%;margin: 0 auto;"><table class=3D"image-wrapper" width=3D"100%" =
border=3D"0" cellspacing=3D"0" cellpadding=3D"0" data-component-name=3D"Ima=
ge2ToDOMStatic" style=3D"mso-padding-alt: 1em 0 1.6em;"><tbody><tr><td styl=
e=3D"text-align: center;"></td><td class=3D"content" align=3D"left" width=
=3D"734" style=3D"text-align: center;"><a class=3D"image-link" target=3D"_b=
lank" href=3D"https://substack.com/redirect/3e67da8a-1a0c-4bb0-8fea-5466699=
feaba?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" =
rel=3D"" style=3D"position: relative;flex-direction: column;align-items: ce=
nter;padding: 0;width: auto;height: auto;border: none;text-decoration: none=
;display: block;margin: 0;"><img class=3D"wide-image" data-attrs=3D"{&quot;=
src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/=
f39e0c6b-3c45-44bf-b672-6729a57a2616_734x544.png&quot;,&quot;srcNoWatermark=
&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;he=
ight&quot;:544,&quot;width&quot;:734,&quot;resizeWidth&quot;:null,&quot;byt=
es&quot;:132749,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot=
;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true=
,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.=
llmwatch.com/i/169670030?img=3Dhttps%3A%2F%2Fsubstack-post-media.s3.amazona=
ws.com%2Fpublic%2Fimages%2Ff39e0c6b-3c45-44bf-b672-6729a57a2616_734x544.png=
&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&q=
uot;:false}" alt=3D"" width=3D"550" height=3D"407.6294277929155" src=3D"htt=
ps://substackcdn.com/image/fetch/$s_!Bwox!,w_1100,c_limit,f_auto,q_auto:goo=
d,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2=
Fpublic%2Fimages%2Ff39e0c6b-3c45-44bf-b672-6729a57a2616_734x544.png" style=
=3D"border: none !important;vertical-align: middle;display: block;-ms-inter=
polation-mode: bicubic;height: auto;margin-bottom: 0;width: auto !important=
;max-width: 100% !important;margin: 0 auto;"></a></td><td style=3D"text-ali=
gn: center;"></td></tr></tbody></table></figure></div><p style=3D"margin: 0=
 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>=
What problem does it solve?</strong><span> Retrieval-Augmented Generation (=
RAG) mitigates hallucination in LLMs by incorporating external knowledge, b=
ut relies on chunk-based retrieval that lacks structural semantics. GraphRA=
G methods improve RAG by modeling knowledge as entity-relation graphs, but =
still face challenges in high construction cost, fixed one-time retrieval, =
and reliance on long-context reasoning and prompt design. The problem Graph=
-R1 addresses is how to make knowledge retrieval </span><strong>interactive=
 and agentic</strong><span>: the AI should be able to walk through a knowle=
dge graph step by step, deciding which nodes (topics) to explore next based=
 on the current query, and do so in a trained optimal way. In short, it=E2=
=80=99s tackling the gap between static knowledge retrieval and the kind of=
 dynamic, exploratory search a human might do when answering a complex ques=
tion, and doing this without an expensive upfront graph construction or com=
plicated prompt engineering.</span></p><p style=3D"margin: 0 0 20px 0;color=
: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>How does it sol=
ve the problem?</strong><span> Graph-R1 transforms the retrieval process in=
to an </span><strong>interactive environment</strong><span> and trains a re=
inforcement learning agent to operate within it. Instead of retrieving text=
 chunks once, the system constructs a </span><strong>lightweight knowledge =
hypergraph</strong><span> of the information =E2=80=93 a structure of entit=
ies and their relations that is cheaper to build and rich in semantics. The=
 Graph-R1 agent then models retrieval as a </span><strong>multi-turn agent-=
environment interaction</strong><span>: it treats the knowledge base like a=
 world to navigate, where each action might be following a link or querying=
 a related concept. The agent is optimized end-to-end via a reward mechanis=
m that aligns with finding the correct answer. This means the agent learns =
</span><em>policies</em><span> for retrieval =E2=80=93 when to stick with t=
he current topic, when to pivot to a related entity, when to stop gathering=
 information =E2=80=93 all through trial and error guided by whether it lea=
ds to better answers. Crucially, by using RL, Graph-R1 doesn=E2=80=99t requ=
ire manual prompt strategies for multi-hop queries; the agent organically f=
igures out a strategy to fetch what it needs. The authors also introduce tr=
icks for </span><strong>lightweight hypergraph construction</strong><span>,=
 so that the agent isn=E2=80=99t burdened by an enormous graph: it likely b=
uilds or expands the graph on the fly as needed, keeping retrieval targeted=
. Overall, Graph-R1 is an </span><em>agentic GraphRAG</em><span>: a graph-g=
uided retrieval system where an RL-trained agent actively and intelligently=
 controls the retrieval process, bridging structured knowledge with the lan=
guage model=E2=80=99s reasoning in an integrated loop.</span></p><p style=
=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16=
px;"><strong>What are the key findings?</strong><span> Graph-R1 sets a new =
bar for </span><strong>accuracy and efficiency in knowledge-intensive tasks=
</strong><span>. Experiments on standard RAG datasets show that Graph-R1 ou=
tperforms both traditional GraphRAG and RL-enhanced RAG methods in reasonin=
g accuracy, retrieval efficiency, and generation quality. In terms of reaso=
ning accuracy, Graph-R1=E2=80=99s answers were more often correct on multi-=
hop and knowledge-dense questions, indicating the agent successfully gather=
ed the needed evidence that static one-shot retrieval might miss. For retri=
eval efficiency, the agent found relevant information with fewer steps and =
less irrelevant data =E2=80=93 it doesn=E2=80=99t shotgun a dozen documents=
 if only two well-chosen pieces suffice. The generation quality also benefi=
ted: answers were coherent and well-supported by the retrieved facts, since=
 the agent could ensure all necessary context was obtained and extraneous b=
its were left out. Notably, Graph-R1=E2=80=99s performance didn=E2=80=99t c=
ome at the cost of massively increasing computation; by keeping the knowled=
ge search targeted via the hypergraph and RL policy, it remained cost-effec=
tive. The study also likely found that end-to-end RL helped the model navig=
ate ambiguous queries: the agent could try one line of inquiry, and if it t=
urned out to be a dead end (leading to low reward), learn to try an alterna=
te path in future episodes =E2=80=93 something a fixed retrieval strategy c=
an=E2=80=99t adaptively do. By bridging structured graphs with the flexibil=
ity of an agent, Graph-R1 demonstrated that </span><em>learning to retrieve=
</em><span> can beat even strong static retrieval heuristics.</span></p><p =
style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-siz=
e: 16px;"><strong>Why does it matter?</strong><span> This result is an exci=
ting step toward </span><strong>more intelligent information-seeking AI</st=
rong><span>. Instead of treating the knowledge source as a static library, =
Graph-R1 treats it as a navigable world =E2=80=93 bringing reinforcement le=
arning, which is typically used in games or robotics, into the realm of kno=
wledge retrieval. The payoff is an AI that=E2=80=99s better at knowing </sp=
an><em>when it has enough information and where to find it</em><span> rathe=
r than guessing or hallucinating. This could dramatically improve systems l=
ike search engines, virtual assistants, or any AI that needs to base answer=
s on external knowledge: we=E2=80=99ll get answers that are not just confid=
ent, but grounded in a verifiable path of retrieved evidence (making the AI=
=E2=80=99s reasoning more transparent too). Moreover, Graph-R1=E2=80=99s su=
ccess suggests that building lighter weight knowledge representations (like=
 hypergraphs) and training agents on them can outperform brute-force approa=
ches that dump massive documents into an LLM. It=E2=80=99s a win for </span=
><strong>efficiency and scalability</strong><span> =E2=80=93 as knowledge b=
ases grow, an agent that can selectively explore will be far more tractable=
 than trying to cram everything into context. In a broader sense, Graph-R1 =
is a convergence of symbolic and neural approaches: it uses a symbolic stru=
cture (graph) and a neural policy (RL agent) together. This hints at future=
 AI systems that combine the reliability of symbolic reasoning with the fle=
xibility of learning. All said, Graph-R1 advances the state of the art in m=
aking AI that is both </span><strong>knowledgeable and able to actively gat=
her knowledge</strong><span>, a crucial ability for any AI aspiring to trul=
y assist with complex, real-world questions.</span></p><div style=3D"font-s=
ize: 16px;line-height: 26px;"><hr style=3D"margin: 32px 0;padding: 0;height=
: 1px;background: #313131;border: none;"></div><div class=3D"captioned-imag=
e-container-static" style=3D"font-size: 16px;line-height: 26px;margin: 32px=
 auto;"><figure style=3D"width: 100%;margin: 0 auto;"><table class=3D"image=
-wrapper" width=3D"100%" border=3D"0" cellspacing=3D"0" cellpadding=3D"0" d=
ata-component-name=3D"Image2ToDOMStatic" style=3D"mso-padding-alt: 1em 0 1.=
6em;"><tbody><tr><td style=3D"text-align: center;"></td><td class=3D"conten=
t" align=3D"left" width=3D"1200" style=3D"text-align: center;"><a class=3D"=
image-link" target=3D"_blank" href=3D"https://substack.com/redirect/1c09fad=
b-754a-40ad-a8b7-7c3bd15bb1ea?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgU=
C7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"position: relative;flex-direction=
: column;align-items: center;padding: 0;width: auto;height: auto;border: no=
ne;text-decoration: none;display: block;margin: 0;"><img class=3D"wide-imag=
e" data-attrs=3D"{&quot;src&quot;:&quot;https://substack-post-media.s3.amaz=
onaws.com/public/images/bb8ce834-f0ab-4f4a-b2ff-76953a3c52d5_1200x627.png&q=
uot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imag=
eSize&quot;:null,&quot;height&quot;:627,&quot;width&quot;:1200,&quot;resize=
Width&quot;:null,&quot;bytes&quot;:167540,&quot;alt&quot;:&quot;&quot;,&quo=
t;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:=
null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;interna=
lRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,=
&quot;offset&quot;:false}" alt=3D"" title=3D"" width=3D"550" height=3D"287.=
375" src=3D"https://substackcdn.com/image/fetch/$s_!e9ua!,w_1100,c_limit,f_=
auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.=
amazonaws.com%2Fpublic%2Fimages%2Fbb8ce834-f0ab-4f4a-b2ff-76953a3c52d5_1200=
x627.png" style=3D"border: none !important;vertical-align: middle;display: =
block;-ms-interpolation-mode: bicubic;height: auto;margin-bottom: 0;width: =
auto !important;max-width: 100% !important;margin: 0 auto;"></a></td><td st=
yle=3D"text-align: center;"></td></tr></tbody></table></figure></div><div c=
lass=3D"subscribe-widget is-signed-up" data-component-name=3D"SubscribeWidg=
et" style=3D"margin: 0 0 1em;direction: ltr;font-size: 16px;line-height: 26=
px;"><div class=3D"pencraft pc-reset button-wrapper" style=3D"text-decorati=
on: unset;list-style: none;font-size: 16px;line-height: 26px;text-align: ce=
nter;cursor: pointer;border-radius: 4px;"><a class=3D"button subscribe-btn =
primary" href=3D"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly93d3cubG=
xtd2F0Y2guY29tL3N1YnNjcmliZT91dG1fc291cmNlPXBvc3QmdXRtX2NhbXBhaWduPWVtYWlsL=
WNoZWNrb3V0Jm5leHQ9aHR0cHMlM0ElMkYlMkZ3d3cubGxtd2F0Y2guY29tJTJGcCUyRjExLXBh=
cGVycy15b3Utc2hvdWxkLWtub3ctYWJvdXQmcj01a2I5M3omdG9rZW49ZXlKMWMyVnlYMmxrSWp=
vek16WTBORGd5TWpNc0ltbGhkQ0k2TVRjMU5EQTNNVGMwTWl3aVpYaHdJam94TnpVMk5qWXpOel=
F5TENKcGMzTWlPaUp3ZFdJdE1UUXlPRFkyTnlJc0luTjFZaUk2SW1Ob1pXTnJiM1YwSW4wLnZxV=
2F5LWphQmNKUUtlWm56LWpPNmozV0sxNTdna05UTldSSG5FOTF0ZVkiLCJwIjoxNjk2NzAwMzAs=
InMiOjE0Mjg2NjcsImYiOnRydWUsInUiOjMzNjQ0ODIyMywiaWF0IjoxNzU0MDcxNzQyLCJleHA=
iOjIwNjk2NDc3NDIsImlzcyI6InB1Yi0wIiwic3ViIjoibGluay1yZWRpcmVjdCJ9.pnsURViCg=
gqq_8KTPB6EceRsudbPOswkZEFhANknXC0?&amp;utm_medium=3Demail&amp;utm_source=
=3Dsubscribe-widget&amp;utm_content=3D169670030" style=3D"font-family: syst=
em-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sa=
ns-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';display: in=
line-block;box-sizing: border-box;cursor: pointer;border: none;border-radiu=
s: 8px;font-size: 14px;line-height: 20px;font-weight: 600;text-align: cente=
r;opacity: 1;outline: none;white-space: nowrap;text-decoration: none !impor=
tant;margin: 0 auto;background-color: #ffca4b;color: #363737 !important;pad=
ding: 12px 20px;height: auto;"><span style=3D"color: #363737;text-decoratio=
n: none;">Upgrade to paid</span></a></div></div><div style=3D"font-size: 16=
px;line-height: 26px;"><hr style=3D"margin: 32px 0;padding: 0;height: 1px;b=
ackground: #313131;border: none;"></div><h2 class=3D"header-anchor-post" st=
yle=3D"position: relative;font-family: 'SF Pro Display',-apple-system-headl=
ine,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,=
Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';fon=
t-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing:=
 antialiased;-webkit-appearance: optimizelegibility;-moz-appearance: optimi=
zelegibility;appearance: optimizelegibility;margin: 1em 0 0.625em 0;color: =
rgb(54,55,55);line-height: 1.16em;font-size: 1.625em;">Putting It All Toget=
her: Toward Smarter and Safer LLM Agents</h2><p style=3D"margin: 0 0 20px 0=
;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">From these papers=
, we can see multiple convergent themes:</p><p style=3D"margin: 0 0 20px 0;=
color: rgb(54,55,55);line-height: 26px;font-size: 16px;">First, scaling and=
 new architectures (like Kimi K2=E2=80=99s trillion-parameter MoE and Falco=
n-H1=E2=80=99s hybrid design) are giving open models unprecedented power, n=
arrowing the gap with closed models. </p><p style=3D"margin: 0 0 20px 0;col=
or: rgb(54,55,55);line-height: 26px;font-size: 16px;">Second, a host of nov=
el training methods =E2=80=93 from smarter reinforcement learning algorithm=
s to language-based self-evolution=E2=80=93 are greatly boosting reasoning =
capabilities and sample efficiency. </p><p style=3D"margin: 0 0 20px 0;colo=
r: rgb(54,55,55);line-height: 26px;font-size: 16px;">Third, researchers are=
 acknowledging that human guidance and knowledge structure remain crucial: =
frameworks like Magentic-UI put humans in the loop for safety, and Graph-R1=
=E2=80=99s agent navigates structured knowledge to avoid hallucination. </p=
><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font=
-size: 16px;">Finally, solutions for long-horizon tasks (MemTool=E2=80=99s =
memory management, self-evolving agents) are enabling AI to operate coheren=
tly over extended interactions. </p><p style=3D"margin: 0 0 20px 0;color: r=
gb(54,55,55);line-height: 26px;font-size: 16px;"><span>In essence, the comm=
unity is engineering LLM systems that are not just bigger, but </span><em>d=
esigned </em><span>smarter =E2=80=93 able to learn continually, reason thro=
ugh complex problems, leverage tools and knowledge bases effectively, and w=
ork alongside humans. </span></p><div style=3D"font-size: 16px;line-height:=
 26px;"><hr style=3D"margin: 32px 0;padding: 0;height: 1px;background: #313=
131;border: none;"></div><h3 class=3D"header-anchor-post" style=3D"position=
: relative;font-family: 'SF Pro Display',-apple-system-headline,system-ui,-=
apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-seri=
f,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;=
-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing: antialiased;-w=
ebkit-appearance: optimizelegibility;-moz-appearance: optimizelegibility;ap=
pearance: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);l=
ine-height: 1.16em;font-size: 1.375em;">=E2=9D=A4=EF=B8=8F If you enjoyed t=
his article, give it a like and share it with your peers.</h3><div style=3D=
"font-size: 16px;line-height: 26px;"><hr style=3D"margin: 32px 0;padding: 0=
;height: 1px;background: #313131;border: none;"></div><p class=3D"button-wr=
apper" data-attrs=3D"{&quot;url&quot;:&quot;https://substack.com/app-link/p=
ost?publication_id=3D1428667&amp;post_id=3D169670030&amp;utm_source=3Dsubst=
ack&amp;utm_medium=3Demail&amp;isFreemail=3Dtrue&amp;comments=3Dtrue&amp;to=
ken=3DeyJ1c2VyX2lkIjozMzY0NDgyMjMsInBvc3RfaWQiOjE2OTY3MDAzMCwiaWF0IjoxNzU0M=
DcxNzQyLCJleHAiOjE3NTY2NjM3NDIsImlzcyI6InB1Yi0xNDI4NjY3Iiwic3ViIjoicG9zdC1y=
ZWFjdGlvbiJ9.DqlcSEWDESLtOJ0A1R3-5c6zFubORP9VgwMYy2BaK7E&amp;r=3D5kb93z&amp=
;utm_campaign=3Demail-half-magic-comments&amp;action=3Dpost-comment&quot;,&=
quot;text&quot;:&quot;Leave a comment&quot;,&quot;action&quot;:null,&quot;c=
lass&quot;:null}" data-component-name=3D"ButtonCreateButton" style=3D"margi=
n: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;text-a=
lign: center;cursor: pointer;border-radius: 4px;"><a class=3D"button primar=
y" href=3D"https://substack.com/app-link/post?publication_id=3D1428667&amp;=
post_id=3D169670030&amp;utm_source=3Dsubstack&amp;utm_medium=3Demail&amp;is=
Freemail=3Dtrue&amp;comments=3Dtrue&amp;token=3DeyJ1c2VyX2lkIjozMzY0NDgyMjM=
sInBvc3RfaWQiOjE2OTY3MDAzMCwiaWF0IjoxNzU0MDcxNzQyLCJleHAiOjE3NTY2NjM3NDIsIm=
lzcyI6InB1Yi0xNDI4NjY3Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.DqlcSEWDESLtOJ0A1R3-=
5c6zFubORP9VgwMYy2BaK7E&amp;r=3D5kb93z&amp;utm_campaign=3Demail-half-magic-=
comments&amp;action=3Dpost-comment" rel=3D"" style=3D"font-family: system-u=
i,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-s=
erif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';display: inline=
-block;box-sizing: border-box;cursor: pointer;border: none;border-radius: 8=
px;font-size: 14px;line-height: 20px;font-weight: 600;text-align: center;ma=
rgin: 0;opacity: 1;outline: none;white-space: nowrap;color: #363737 !import=
ant;text-decoration: none !important;background-color: #ffca4b;padding: 12p=
x 20px;height: auto;"><span style=3D"color: #363737;text-decoration: none;"=
>Leave a comment</span></a></p><div class=3D"subscription-widget-wrap" styl=
e=3D"font-size: 16px;line-height: 26px;margin-bottom: 0;"><div class=3D"sub=
scription-widget show-subscribe" style=3D"font-size: 16px;direction: ltr !i=
mportant;font-weight: 400;text-decoration: none;font-family: system-ui,-app=
le-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'=
Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';color: #363737;line-h=
eight: 1.5;max-width: 560px;margin: 24px auto;align-items: flex-start;displ=
ay: block;text-align: center;padding: 0px 32px;"><div class=3D"preamble" st=
yle=3D"margin-top: 16px;font-family: system-ui,-apple-system,BlinkMacSystem=
Font,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Sego=
e UI Emoji','Segoe UI Symbol';font-size: 18px;max-width: 384px;width: fit-c=
ontent;line-height: 22px;display: flex;align-items: center;text-align: cent=
er;font-weight: 400;margin-left: auto;margin-right: auto;"><p style=3D"marg=
in: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">Tha=
nks for reading LLM Watch! Subscribe for free to receive new posts and supp=
ort my work</p></div><div class=3D"subscribe-widget is-signed-up" data-comp=
onent-name=3D"SubscribeWidget" style=3D"margin: 0 0 1em;direction: ltr;font=
-size: 16px;line-height: 26px;"><div class=3D"pencraft pc-reset button-wrap=
per" style=3D"text-decoration: unset;list-style: none;font-size: 16px;line-=
height: 26px;text-align: center;cursor: pointer;border-radius: 4px;"><a cla=
ss=3D"button subscribe-btn primary" href=3D"https://substack.com/redirect/2=
/eyJlIjoiaHR0cHM6Ly93d3cubGxtd2F0Y2guY29tL3N1YnNjcmliZT91dG1fc291cmNlPXBvc3=
QmdXRtX2NhbXBhaWduPWVtYWlsLWNoZWNrb3V0Jm5leHQ9aHR0cHMlM0ElMkYlMkZ3d3cubGxtd=
2F0Y2guY29tJTJGcCUyRjExLXBhcGVycy15b3Utc2hvdWxkLWtub3ctYWJvdXQmcj01a2I5M3om=
dG9rZW49ZXlKMWMyVnlYMmxrSWpvek16WTBORGd5TWpNc0ltbGhkQ0k2TVRjMU5EQTNNVGMwTWl=
3aVpYaHdJam94TnpVMk5qWXpOelF5TENKcGMzTWlPaUp3ZFdJdE1UUXlPRFkyTnlJc0luTjFZaU=
k2SW1Ob1pXTnJiM1YwSW4wLnZxV2F5LWphQmNKUUtlWm56LWpPNmozV0sxNTdna05UTldSSG5FO=
TF0ZVkiLCJwIjoxNjk2NzAwMzAsInMiOjE0Mjg2NjcsImYiOnRydWUsInUiOjMzNjQ0ODIyMywi=
aWF0IjoxNzU0MDcxNzQyLCJleHAiOjIwNjk2NDc3NDIsImlzcyI6InB1Yi0wIiwic3ViIjoibGl=
uay1yZWRpcmVjdCJ9.pnsURViCggqq_8KTPB6EceRsudbPOswkZEFhANknXC0?&amp;utm_medi=
um=3Demail&amp;utm_source=3Dsubscribe-widget-preamble&amp;utm_content=3D169=
670030" style=3D"font-family: system-ui,-apple-system,BlinkMacSystemFont,'S=
egoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Em=
oji','Segoe UI Symbol';display: inline-block;box-sizing: border-box;cursor:=
 pointer;border: none;border-radius: 8px;font-size: 14px;line-height: 20px;=
font-weight: 600;text-align: center;opacity: 1;outline: none;white-space: n=
owrap;text-decoration: none !important;margin: 0 auto;background-color: #ff=
ca4b;color: #363737 !important;padding: 12px 20px;height: auto;"><span styl=
e=3D"color: #363737;text-decoration: none;">Upgrade to paid</span></a></div=
></div></div></div></div></div><div class=3D"container-border" style=3D"mar=
gin: 32px 0 0;width: 100%;box-sizing: border-box;border-top: 1px solid #313=
131;font-size: 16px;line-height: 26px;"></div><div class=3D"post-cta typogr=
aphy markup" style=3D"--image-offset-margin: -120px;text-align: initial;wor=
d-break: break-word;margin-bottom: 32px;margin: 32px 0;font-size: 16px;line=
-height: 26px;"><p style=3D"color: rgb(54,55,55);margin: 0 auto 20px;text-a=
lign: center;width: 90%;line-height: 26px;font-size: 16px;margin-top: 0;"><=
span class=3D"pencraft pc-reset reset-IxiVJZ" translated=3D"" style=3D"list=
-style: none;color: unset;text-decoration: unset;margin: 0;">You're current=
ly a free subscriber to <a href=3D"https://substack.com/redirect/e5bb33eb-4=
218-4b69-909f-791a1fb7eaf2?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7B=
DnPG6RxQ4tAjMRMPOw0" style=3D"color: rgb(54,55,55);text-decoration: underli=
ne;">LLM Watch</a>. For the full experience, <a href=3D"https://substack.co=
m/redirect/2/eyJlIjoiaHR0cHM6Ly93d3cubGxtd2F0Y2guY29tL3N1YnNjcmliZT91dG1fc2=
91cmNlPXBvc3QmdXRtX2NhbXBhaWduPWVtYWlsLWNoZWNrb3V0Jm5leHQ9aHR0cHMlM0ElMkYlM=
kZ3d3cubGxtd2F0Y2guY29tJTJGcCUyRjExLXBhcGVycy15b3Utc2hvdWxkLWtub3ctYWJvdXQm=
cj01a2I5M3omdG9rZW49ZXlKMWMyVnlYMmxrSWpvek16WTBORGd5TWpNc0ltbGhkQ0k2TVRjMU5=
EQTNNVGMwTWl3aVpYaHdJam94TnpVMk5qWXpOelF5TENKcGMzTWlPaUp3ZFdJdE1UUXlPRFkyTn=
lJc0luTjFZaUk2SW1Ob1pXTnJiM1YwSW4wLnZxV2F5LWphQmNKUUtlWm56LWpPNmozV0sxNTdna=
05UTldSSG5FOTF0ZVkiLCJwIjoxNjk2NzAwMzAsInMiOjE0Mjg2NjcsImYiOnRydWUsInUiOjMz=
NjQ0ODIyMywiaWF0IjoxNzU0MDcxNzQyLCJleHAiOjIwNjk2NDc3NDIsImlzcyI6InB1Yi0wIiw=
ic3ViIjoibGluay1yZWRpcmVjdCJ9.pnsURViCggqq_8KTPB6EceRsudbPOswkZEFhANknXC0?&=
amp;utm_source=3Dsubstack&amp;utm_medium=3Demail&amp;utm_content=3Dpostcta"=
 style=3D"color: rgb(54,55,55);text-decoration: underline;">upgrade your su=
bscription.</a></span></p><p class=3D"cta-box" style=3D"color: rgb(54,55,55=
);margin: 0 auto 20px;width: 90%;line-height: 26px;font-size: 16px;margin-b=
ottom: 0;text-align: center;margin-left: auto;margin-right: auto;"><a class=
=3D"button primary" role=3D"button" href=3D"https://substack.com/redirect/2=
/eyJlIjoiaHR0cHM6Ly93d3cubGxtd2F0Y2guY29tL3N1YnNjcmliZT91dG1fc291cmNlPXBvc3=
QmdXRtX2NhbXBhaWduPWVtYWlsLWNoZWNrb3V0Jm5leHQ9aHR0cHMlM0ElMkYlMkZ3d3cubGxtd=
2F0Y2guY29tJTJGcCUyRjExLXBhcGVycy15b3Utc2hvdWxkLWtub3ctYWJvdXQmcj01a2I5M3om=
dG9rZW49ZXlKMWMyVnlYMmxrSWpvek16WTBORGd5TWpNc0ltbGhkQ0k2TVRjMU5EQTNNVGMwTWl=
3aVpYaHdJam94TnpVMk5qWXpOelF5TENKcGMzTWlPaUp3ZFdJdE1UUXlPRFkyTnlJc0luTjFZaU=
k2SW1Ob1pXTnJiM1YwSW4wLnZxV2F5LWphQmNKUUtlWm56LWpPNmozV0sxNTdna05UTldSSG5FO=
TF0ZVkiLCJwIjoxNjk2NzAwMzAsInMiOjE0Mjg2NjcsImYiOnRydWUsInUiOjMzNjQ0ODIyMywi=
aWF0IjoxNzU0MDcxNzQyLCJleHAiOjIwNjk2NDc3NDIsImlzcyI6InB1Yi0wIiwic3ViIjoibGl=
uay1yZWRpcmVjdCJ9.pnsURViCggqq_8KTPB6EceRsudbPOswkZEFhANknXC0?&amp;utm_sour=
ce=3Dsubstack&amp;utm_medium=3Demail&amp;utm_content=3Dpostcta" style=3D"fo=
nt-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Hel=
vetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symb=
ol';display: inline-block;box-sizing: border-box;cursor: pointer;border: no=
ne;height: 40px;border-radius: 8px;font-size: 14px;line-height: 20px;font-w=
eight: 600;text-align: center;padding: 10px 20px;margin: 0;opacity: 1;outli=
ne: none;white-space: nowrap;color: #363737 !important;text-decoration: non=
e !important;background-color: #ffca4b;">Upgrade to paid</a></p></div><tabl=
e class=3D"email-ufi-2-bottom" role=3D"presentation" width=3D"100%" border=
=3D"0" cellspacing=3D"0" cellpadding=3D"0" style=3D"border-top: 1px solid r=
gb(0,0,0,.1);border-bottom: 1px solid rgb(0,0,0,.1);min-width: 100%;"><tbod=
y><tr height=3D"16"><td height=3D"16" style=3D"font-size:0px;line-height:0;=
">&nbsp;</td></tr><tr><td><table role=3D"presentation" width=3D"100%" borde=
r=3D"0" cellspacing=3D"0" cellpadding=3D"0"><tbody><tr><td><table role=3D"p=
resentation" width=3D"auto" border=3D"0" cellspacing=3D"0" cellpadding=3D"0=
" style=3D"margin:0 auto;"><tbody><tr><td style=3D"vertical-align:middle;">=
<table role=3D"presentation" width=3D"auto" border=3D"0" cellspacing=3D"0" =
cellpadding=3D"0"><tbody><tr><td align=3D"center"><a class=3D"email-button-=
outline" href=3D"https://substack.com/app-link/post?publication_id=3D142866=
7&amp;post_id=3D169670030&amp;utm_source=3Dsubstack&amp;isFreemail=3Dtrue&a=
mp;submitLike=3Dtrue&amp;token=3DeyJ1c2VyX2lkIjozMzY0NDgyMjMsInBvc3RfaWQiOj=
E2OTY3MDAzMCwicmVhY3Rpb24iOiLinaQiLCJpYXQiOjE3NTQwNzE3NDIsImV4cCI6MTc1NjY2M=
zc0MiwiaXNzIjoicHViLTE0Mjg2NjciLCJzdWIiOiJyZWFjdGlvbiJ9.JpiuIFzFYFagH14A6if=
SLYUmG5rhKqxEme7-YsoLuug&amp;utm_medium=3Demail&amp;utm_campaign=3Demail-re=
action&amp;r=3D5kb93z" style=3D"font-family: system-ui,-apple-system,BlinkM=
acSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoj=
i','Segoe UI Emoji','Segoe UI Symbol';display: inline-block;font-weight: 50=
0;border: 1px solid rgb(0,0,0,.1);border-radius: 9999px;text-transform: upp=
ercase;font-size: 12px;line-height: 12px;padding: 9px 14px;text-decoration:=
 none;color: rgb(119,119,119);"><img class=3D"icon" src=3D"https://substack=
cdn.com/image/fetch/$s_!PeVs!,w_36,c_scale,f_png,q_auto:good,fl_progressive=
:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideHeart%3Fv%3D4%26height%3D3=
6%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2" width=3D"18" heig=
ht=3D"18" style=3D"margin-right: 8px;min-width: 18px;min-height: 18px;borde=
r: none;vertical-align: middle;max-width: 18px" alt=3D""><span class=3D"ema=
il-button-text" style=3D"vertical-align: middle;">Like</span></a></td></tr>=
</tbody></table></td><td width=3D"8" style=3D"min-width: 8px"></td><td styl=
e=3D"vertical-align:middle;"><table role=3D"presentation" width=3D"auto" bo=
rder=3D"0" cellspacing=3D"0" cellpadding=3D"0"><tbody><tr><td align=3D"cent=
er"><a class=3D"email-button-outline" href=3D"https://substack.com/app-link=
/post?publication_id=3D1428667&amp;post_id=3D169670030&amp;utm_source=3Dsub=
stack&amp;utm_medium=3Demail&amp;isFreemail=3Dtrue&amp;comments=3Dtrue&amp;=
token=3DeyJ1c2VyX2lkIjozMzY0NDgyMjMsInBvc3RfaWQiOjE2OTY3MDAzMCwiaWF0IjoxNzU=
0MDcxNzQyLCJleHAiOjE3NTY2NjM3NDIsImlzcyI6InB1Yi0xNDI4NjY3Iiwic3ViIjoicG9zdC=
1yZWFjdGlvbiJ9.DqlcSEWDESLtOJ0A1R3-5c6zFubORP9VgwMYy2BaK7E&amp;r=3D5kb93z&a=
mp;utm_campaign=3Demail-half-magic-comments&amp;action=3Dpost-comment&amp;u=
tm_source=3Dsubstack&amp;utm_medium=3Demail" style=3D"font-family: system-u=
i,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-s=
erif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';display: inline=
-block;font-weight: 500;border: 1px solid rgb(0,0,0,.1);border-radius: 9999=
px;text-transform: uppercase;font-size: 12px;line-height: 12px;padding: 9px=
 14px;text-decoration: none;color: rgb(119,119,119);"><img class=3D"icon" s=
rc=3D"https://substackcdn.com/image/fetch/$s_!x1tS!,w_36,c_scale,f_png,q_au=
to:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideComm=
ents%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWid=
th%3D2" width=3D"18" height=3D"18" style=3D"margin-right: 8px;min-width: 18=
px;min-height: 18px;border: none;vertical-align: middle;max-width: 18px" al=
t=3D""><span class=3D"email-button-text" style=3D"vertical-align: middle;">=
Comment</span></a></td></tr></tbody></table></td><td width=3D"8" style=3D"m=
in-width: 8px"></td><td style=3D"vertical-align:middle;"><table role=3D"pre=
sentation" width=3D"auto" border=3D"0" cellspacing=3D"0" cellpadding=3D"0">=
<tbody><tr><td align=3D"center"><a class=3D"email-button-outline" href=3D"h=
ttps://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9vcGVuLnN1YnN0YWNrLmNvbS9w=
dWIveGFpZ3V5L3AvMTEtcGFwZXJzLXlvdS1zaG91bGQta25vdy1hYm91dD91dG1fc291cmNlPXN=
1YnN0YWNrJnV0bV9tZWRpdW09ZW1haWwmdXRtX2NhbXBhaWduPWVtYWlsLXJlc3RhY2stY29tbW=
VudCZhY3Rpb249cmVzdGFjay1jb21tZW50JnI9NWtiOTN6JnRva2VuPWV5SjFjMlZ5WDJsa0lqb=
3pNelkwTkRneU1qTXNJbkJ2YzNSZmFXUWlPakUyT1RZM01EQXpNQ3dpYVdGMElqb3hOelUwTURj=
eE56UXlMQ0psZUhBaU9qRTNOVFkyTmpNM05ESXNJbWx6Y3lJNkluQjFZaTB4TkRJNE5qWTNJaXd=
pYzNWaUlqb2ljRzl6ZEMxeVpXRmpkR2x2YmlKOS5EcWxjU0VXREVTTHRPSjBBMVIzLTVjNnpGdW=
JPUlA5Vmd3TVl5MkJhSzdFIiwicCI6MTY5NjcwMDMwLCJzIjoxNDI4NjY3LCJmIjp0cnVlLCJ1I=
jozMzY0NDgyMjMsImlhdCI6MTc1NDA3MTc0MiwiZXhwIjoyMDY5NjQ3NzQyLCJpc3MiOiJwdWIt=
MCIsInN1YiI6ImxpbmstcmVkaXJlY3QifQ.mYVS9_TThTFlchQrRioyUhzi60hHBu3-L1YTy5lT=
3aU?&amp;utm_source=3Dsubstack&amp;utm_medium=3Demail" style=3D"font-family=
: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Ar=
ial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';displ=
ay: inline-block;font-weight: 500;border: 1px solid rgb(0,0,0,.1);border-ra=
dius: 9999px;text-transform: uppercase;font-size: 12px;line-height: 12px;pa=
dding: 9px 14px;text-decoration: none;color: rgb(119,119,119);"><img class=
=3D"icon" src=3D"https://substackcdn.com/image/fetch/$s_!5EGt!,w_36,c_scale=
,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2=
FNoteForwardIcon%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080=
%26strokeWidth%3D2" width=3D"18" height=3D"18" style=3D"margin-right: 8px;m=
in-width: 18px;min-height: 18px;border: none;vertical-align: middle;max-wid=
th: 18px" alt=3D""><span class=3D"email-button-text" style=3D"vertical-alig=
n: middle;">Restack</span></a></td></tr></tbody></table></td></tr></tbody><=
/table></td><td align=3D"right"><table role=3D"presentation" width=3D"auto"=
 border=3D"0" cellspacing=3D"0" cellpadding=3D"0"><tbody><tr></tr></tbody><=
/table></td></tr></tbody></table></td></tr><tr height=3D"16"><td height=3D"=
16" style=3D"font-size:0px;line-height:0;">&nbsp;</td></tr></tbody></table>=
<div class=3D"footer footer-ZM59BM" style=3D"color: rgb(119,119,119);text-a=
lign: center;font-size: 16px;line-height: 26px;padding: 24px0;"><div style=
=3D"font-size: 16px;line-height: 26px;padding-bottom: 24px"><p class=3D"pen=
craft pc-reset color-secondary-ls1g8s size-12-mmZ61m reset-IxiVJZ small met=
a-B2bqa5" style=3D"list-style: none;font-family: system-ui,-apple-system,Bl=
inkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color =
Emoji','Segoe UI Emoji','Segoe UI Symbol';padding-bottom: 0;font-size: 12px=
;line-height: 16px;margin: 0;color: rgb(119,119,119);text-decoration: unset=
;">=C2=A9 2025 <span>Pascal Biese</span> <br><a href=3D"https://substack.co=
m/redirect/2/eyJlIjoiaHR0cHM6Ly93d3cubGxtd2F0Y2guY29tL2FjdGlvbi9kaXNhYmxlX2=
VtYWlsP3Rva2VuPWV5SjFjMlZ5WDJsa0lqb3pNelkwTkRneU1qTXNJbkJ2YzNSZmFXUWlPakUyT=
1RZM01EQXpNQ3dpYVdGMElqb3hOelUwTURjeE56UXlMQ0psZUhBaU9qRTNPRFUyTURjM05ESXNJ=
bWx6Y3lJNkluQjFZaTB4TkRJNE5qWTNJaXdpYzNWaUlqb2laR2x6WVdKc1pWOWxiV0ZwYkNKOS5=
uXzFxd2NyMVUwUFJWUzBPX2laVjVMeEg1S0w2Vkptdl94RXRXaUZMZTZBIiwicCI6MTY5NjcwMD=
MwLCJzIjoxNDI4NjY3LCJmIjp0cnVlLCJ1IjozMzY0NDgyMjMsImlhdCI6MTc1NDA3MTc0MiwiZ=
XhwIjoyMDY5NjQ3NzQyLCJpc3MiOiJwdWItMCIsInN1YiI6ImxpbmstcmVkaXJlY3QifQ.DTG1-=
0duZ7WoxgQQBrhsfgbSqoLXoqbtIRZcyY3Hpiw?" style=3D"text-decoration: underlin=
e;color: rgb(119,119,119);"><span style=3D"color: rgb(119,119,119);text-dec=
oration: underline;">Unsubscribe</span></a></p></div><p class=3D"footerSect=
ion-EHR0jG small powered-by-substack" style=3D"padding: 0 24px;font-size: 1=
2px;line-height: 20px;margin: 0;color: rgb(119,119,119);font-family: system=
-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans=
-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';padding-botto=
m: 0;margin-top: 0;"><a href=3D"https://substack.com/redirect/2/eyJlIjoiaHR=
0cHM6Ly9zdWJzdGFjay5jb20vc2lnbnVwP3V0bV9zb3VyY2U9c3Vic3RhY2smdXRtX21lZGl1bT=
1lbWFpbCZ1dG1fY29udGVudD1mb290ZXImdXRtX2NhbXBhaWduPWF1dG9maWxsZWQtZm9vdGVyJ=
mZyZWVTaWdudXBFbWFpbD1laXRhbkBlaXNsYXcuY28uaWwmcj01a2I5M3oiLCJwIjoxNjk2NzAw=
MzAsInMiOjE0Mjg2NjcsImYiOnRydWUsInUiOjMzNjQ0ODIyMywiaWF0IjoxNzU0MDcxNzQyLCJ=
leHAiOjIwNjk2NDc3NDIsImlzcyI6InB1Yi0wIiwic3ViIjoibGluay1yZWRpcmVjdCJ9.j7d7d=
QZXZfsVfgh0Rq-qtDKsAp8g9_cFmGZHleiZNPU?" style=3D"color: rgb(119,119,119);t=
ext-decoration: none;display: inline-block;margin: 0 4px;"><img src=3D"http=
s://substackcdn.com/image/fetch/$s_!LkrL!,w_270,c_limit,f_auto,q_auto:good,=
fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Femail%2Fpublish-but=
ton%402x.png" srcset=3D"https://substackcdn.com/image/fetch/$s_!wgfj!,w_135=
,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com=
%2Fimg%2Femail%2Fpublish-button.png, https://substackcdn.com/image/fetch/$s=
_!LkrL!,w_270,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2=
Fsubstack.com%2Fimg%2Femail%2Fpublish-button%402x.png 2x, https://substackc=
dn.com/image/fetch/$s_!KjtY!,w_405,c_limit,f_auto,q_auto:good,fl_progressiv=
e:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Femail%2Fpublish-button%403x.png =
3x" width=3D"135" alt=3D"Start writing" height=3D"40" style=3D"max-width: 5=
50px;border: none !important;vertical-align: middle;"></a></p></div></div><=
/td><td></td></tr></tbody></table><img src=3D"https://eotrx.substackcdn.com=
/open?token=3DeyJtIjoiPDIwMjUwODAxMTgwODM0LjMuNWE1NGQwOThhNGVkN2M5N0BtZzIuc=
3Vic3RhY2suY29tPiIsInUiOjMzNjQ0ODIyMywiciI6ImVpdGFuQGVpc2xhdy5jby5pbCIsImQi=
OiJtZzIuc3Vic3RhY2suY29tIiwicCI6MTY5NjcwMDMwLCJ0IjoibmV3c2xldHRlciIsImEiOiJ=
ldmVyeW9uZSIsInMiOjE0Mjg2NjcsImMiOiJwb3N0IiwiZiI6dHJ1ZSwicG9zaXRpb24iOiJib3=
R0b20iLCJpYXQiOjE3NTQwNzE3NDMsImV4cCI6MTc1NjY2Mzc0MywiaXNzIjoicHViLTAiLCJzd=
WIiOiJlbyJ9.Tme_PUG8iU-9c7qBO9jknFT_8Pv9mEH9hy9FuwU3tRc" alt=3D"" width=3D"=
1" height=3D"1" border=3D"0" style=3D"height:1px !important;width:1px !impo=
rtant;border-width:0 !important;margin-top:0 !important;margin-bottom:0 !im=
portant;margin-right:0 !important;margin-left:0 !important;padding-top:0 !i=
mportant;padding-bottom:0 !important;padding-right:0 !important;padding-lef=
t:0 !important;"><img width=3D"1" height=3D"1" alt=3D"" src=3D"https://emai=
l.mg2.substack.com/o/eJxEkEuO9CAMBk_TLCPCI5AFZ4kccPJbfwItMN2T24_6Ic22LJdKXw=
TGvdQr3EtjkYJJo7deYBidNdKNzliBJ9Cx7JixAmNagP-uepq0-Bf0Jr1GmVwydosW3Ky995tFr=
9YZrBMUlFRWejmOXnptBj1YsCbJ2YPB5OLsbkaeuxpaXxtD_D_Ecgpqy1bxHRC4dhSvzAV6IswR=
Az6wXiV_MaUwTvPkpNTyQ_i6Y8j4bAcyYxX3vi6xnGfPxNeCGdYD01fc14MiMJX8Fhnlp8mJGpA=
Y8s1IpHbAc4hloEO0vqZyAuXwA7T3S_BnwN6wvt61nozxSmnxCOo3AAD__yKRdM8"></body></=
html>=

--2f01162e9de12d1b406ea5774817f4a9378d6128010d961bb946b905b0eb--
