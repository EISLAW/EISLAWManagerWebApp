Received: from DB9PR01MB8957.eurprd01.prod.exchangelabs.com
 (2603:10a6:10:36f::20) by AS8PR01MB10319.eurprd01.prod.exchangelabs.com with
 HTTPS; Wed, 2 Jul 2025 13:09:34 +0000
Received: from AM8P189CA0014.EURP189.PROD.OUTLOOK.COM (2603:10a6:20b:218::19)
 by DB9PR01MB8957.eurprd01.prod.exchangelabs.com (2603:10a6:10:36f::20) with
 Microsoft SMTP Server (version=TLS1_2,
 cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id 15.20.8901.17; Wed, 2 Jul
 2025 13:09:28 +0000
Received: from AMS0EPF00000196.eurprd05.prod.outlook.com
 (2603:10a6:20b:218:cafe::6f) by AM8P189CA0014.outlook.office365.com
 (2603:10a6:20b:218::19) with Microsoft SMTP Server (version=TLS1_3,
 cipher=TLS_AES_256_GCM_SHA384) id 15.20.8880.32 via Frontend Transport; Wed,
 2 Jul 2025 13:09:27 +0000
Authentication-Results: spf=pass (sender IP is 159.112.244.48)
 smtp.mailfrom=mg1.substack.com; dkim=pass (signature was verified)
 header.d=mg1.substack.com;dmarc=pass action=none
 header.from=substack.com;compauth=pass reason=100
Received-SPF: Pass (protection.outlook.com: domain of mg1.substack.com
 designates 159.112.244.48 as permitted sender)
 receiver=protection.outlook.com; client-ip=159.112.244.48;
 helo=m244-48.mailgun.net; pr=C
Received: from m244-48.mailgun.net (159.112.244.48) by
 AMS0EPF00000196.mail.protection.outlook.com (10.167.16.217) with Microsoft
 SMTP Server (version=TLS1_2, cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id
 15.20.8901.15 via Frontend Transport; Wed, 2 Jul 2025 13:09:27 +0000
DKIM-Signature: a=rsa-sha256; v=1; c=relaxed/relaxed; d=mg1.substack.com; q=dns/txt; s=mailo; t=1751461766; x=1751468966;
 h=List-Unsubscribe-Post: List-Unsubscribe: List-Post: List-Id: List-Archive: List-Owner: Reply-To: In-Reply-To: References: sender: sender: Date: Message-Id: To: To: From: From: Subject: Subject: Content-Type: Mime-Version;
 bh=uQgebcT7ZHmOfJC4CnT8gFq4/6A3XTuWIKdDjvliuHg=;
 b=brFMDvraZiLKalgPq3clZZjzXAwI+NSaIK4AgoV7bxPeAwckCUdnFNP+5xTRz7NxSpS3j0HvEcz4cKqUk01DDvdowBc/TGMhPqa3mXBTVPsA14yJ7Tqcsf5uh1HYxum8JEaaAzxs7NIPZC1TZSayUr2bAecbK7WGNETlmNGS2To=
X-Mailgun-Sid: WyI3NTNmNCIsImVpdGFuQGVpc2xhdy5jby5pbCIsIjA3MmM3YiJd
Received: by d1785d1956aa with HTTP id 68652f7ee64bcc69adbd26d4; Wed, 02 Jul 2025
 13:09:18 GMT
X-Mailgun-Sending-Ip: 159.112.244.48
X-Mailgun-Batch-Id: 68652f7e64cbf3220acb593e
Content-Type: multipart/alternative;
 boundary="ebe98e78b0216adcedd09930bab97482a73c6559294177601adf396a187f"
Subject: RAG: The Complete Guide to Retrieval-Augmented Generation for AI
From: =?utf-8?b?TmF0ZSBmcm9tIE5hdGXigJlzIFN1YnN0YWNr?=
 <natesnewsletter@substack.com>
To: eitan@eislaw.co.il
X-Mailgun-Tag: post
X-Mailgun-Track-Clicks: false
Message-Id: <20250702130313.3.59107de89deb126f@mg1.substack.com>
Date: Wed, 2 Jul 2025 13:03:13 +0000
Feedback-ID: post-167307195:cat-post:pub-1373231:substack
sender: =?utf-8?b?TmF0ZSBmcm9tIE5hdGXigJlzIFN1YnN0YWNr?=
 <natesnewsletter@substack.com>
References: <post-167307195@substack.com>
In-Reply-To: <post-167307195@substack.com>
Reply-To: =?utf-8?b?TmF0ZSBmcm9tIE5hdGXigJlzIFN1YnN0YWNr?=
 <reply+2rlz23&5kb93z&&6a1312bb5675126e6da89d3c172117afcbdbc0c4c4cc57cdcc0503d400333fde@mg1.substack.com>
List-Owner: <mailto:natesnewsletter@substack.com>
List-URL: <https://natesnewsletter.substack.com/>
List-Archive: <https://natesnewsletter.substack.com/archive>
List-Id: <natesnewsletter.substack.com>
List-Post: <https://natesnewsletter.substack.com/p/rag-the-complete-guide-to-retrieval>
List-Unsubscribe: <https://natesnewsletter.substack.com/action/disable_email/disable?token=eyJ1c2VyX2lkIjozMzY0NDgyMjMsInBvc3RfaWQiOjE2NzMwNzE5NSwiaWF0IjoxNzUxNDYxNzU3LCJleHAiOjE3ODI5OTc3NTcsImlzcyI6InB1Yi0xMzczMjMxIiwic3ViIjoiZGlzYWJsZV9lbWFpbCJ9.9n2__jPdFqRfekPvDmfUqy712wRxvUw-3-j_vMW3_Tg&all_sections=true>
List-Unsubscribe-Post: List-Unsubscribe=One-Click
X-Mailgun-Variables: {"category": "post", "email_generated_at": "1751461758111", "post_audience":
 "only_paid", "post_id": "167307195", "post_type": "podcast",
 "pub_community_enabled": "true", "publication_id": "1373231", "subdomain":
 "natesnewsletter", "user_id": "336448223"}
Return-Path: bounce+bdbcee.072c7b-eitan=eislaw.co.il@mg1.substack.com
X-MS-Exchange-Organization-ExpirationStartTime: 02 Jul 2025 13:09:27.8481
 (UTC)
X-MS-Exchange-Organization-ExpirationStartTimeReason: OriginalSubmit
X-MS-Exchange-Organization-ExpirationInterval: 1:00:00:00.0000000
X-MS-Exchange-Organization-ExpirationIntervalReason: OriginalSubmit
X-MS-Exchange-Organization-Network-Message-Id: 42ddf78f-e9e3-410d-c7d2-08ddb969accf
X-EOPAttributedMessage: 0
X-EOPTenantAttributedMessage: 384c4129-e818-4ea7-8f8b-189d997170d1:0
X-MS-Exchange-Organization-MessageDirectionality: Incoming
X-MS-PublicTrafficType: Email
X-MS-TrafficTypeDiagnostic: AMS0EPF00000196:EE_|DB9PR01MB8957:EE_|AS8PR01MB10319:EE_
X-MS-Exchange-Organization-AuthSource: AMS0EPF00000196.eurprd05.prod.outlook.com
X-MS-Exchange-Organization-AuthAs: Anonymous
X-MS-Office365-Filtering-Correlation-Id: 42ddf78f-e9e3-410d-c7d2-08ddb969accf
X-MS-Exchange-Organization-SCL: 1
X-Microsoft-Antispam: BCL:3;ARA:13230040|2092899012|12012899012|29132699027|4022899009|69100299015|13003099007|8096899003;
X-Forefront-Antispam-Report: CIP:159.112.244.48;CTRY:US;LANG:en;SCL:1;SRV:;IPV:NLI;SFV:NSPM;H:m244-48.mailgun.net;PTR:m244-48.mailgun.net;CAT:NONE;SFS:(13230040)(2092899012)(12012899012)(29132699027)(4022899009)(69100299015)(13003099007)(8096899003);DIR:INB;
X-MS-Exchange-CrossTenant-OriginalArrivalTime: 02 Jul 2025 13:09:27.4191
 (UTC)
X-MS-Exchange-CrossTenant-Network-Message-Id: 42ddf78f-e9e3-410d-c7d2-08ddb969accf
X-MS-Exchange-CrossTenant-Id: 384c4129-e818-4ea7-8f8b-189d997170d1
X-MS-Exchange-CrossTenant-AuthSource: AMS0EPF00000196.eurprd05.prod.outlook.com
X-MS-Exchange-CrossTenant-AuthAs: Anonymous
X-MS-Exchange-CrossTenant-FromEntityHeader: Internet
X-MS-Exchange-Transport-CrossTenantHeadersStamped: DB9PR01MB8957
X-MS-Exchange-Transport-EndToEndLatency: 00:00:06.8486947
X-MS-Exchange-Processed-By-BccFoldering: 15.20.8901.017
X-Microsoft-Antispam-Mailbox-Delivery:
	ucf:0;jmr:0;auth:0;dest:I;ENG:(910005)(944506478)(944626604)(920097)(930097)(140003);
X-Microsoft-Antispam-Message-Info:
	=?utf-8?B?QlM0OCtPeUVsREk4Y2QzcHZXU2taSTlsaUpsMVd6OS9qV091aitESTVjOGth?=
 =?utf-8?B?ekpzcFE3N1RtUHVpL1QreW81TThuUytySFRGT1pvQWFEOGJuZkY1OXNvdnBn?=
 =?utf-8?B?VEg2TzJJQ09DbXdvZXBwTmJROVZldUFFaFpya3VEajRTTTFid3ROeWs3cWs3?=
 =?utf-8?B?QkRScTFjMHlqQ2xZYmpYTUdzMEUwUmVNcy9zZkdRTkxpZ1k5ZUczNllnQ2Mx?=
 =?utf-8?B?b3U2ZWV6djVEK05mWmdPeTlGVVhMUFcvL1FCSXlyeEZ0SFhTeHVqWCtEMDVL?=
 =?utf-8?B?cmVrWjVHTkxiOVcxTFBDeHRVT3p2VkdnWFlnRTBvRFZYZ3FPaStvOUprVEx5?=
 =?utf-8?B?a001Y3VCaUFURHYybk9yRkZLNFlzdGNKVGxBSGNWRDViYUd4SGJ1ZFc5ZHRH?=
 =?utf-8?B?V0grTjlpaUFBcG4zTWVadFlRZy9PcEo0NFdFZjhMS3UyenhFS1JsMCszUWFK?=
 =?utf-8?B?eXRvNnBvV0tkSExKZzhDby8rWmZQQ09PZGtlMERyN2hzc1VpNUxlTnVGcVlM?=
 =?utf-8?B?bjlTRVZLSUkzaUVZS1hMS1ZiaWlSRzdtQ3AxdmFvdGM2NGloT1JEY2MwZmlS?=
 =?utf-8?B?d2NkY1VnL0JjZzQvVjBDOHQ2RGpNL3V6KzR5dTYwL01MVktCb0RnRUtHWWRB?=
 =?utf-8?B?VXhIZDNJbkNGRkVjWU1INjRXWHJVaExGbE1jNFo2UlhmZEtpUmcxSVkxV2pC?=
 =?utf-8?B?YTIzMXBUVUtUaXFmSnV3TXNyZit3N1E4bUJ5S2cwTDBzNkIvSi9pYmhaWm5F?=
 =?utf-8?B?R1JGRm9aWWIyZTZVTFFUY2lDcXFVT3dER1RQM2Z2Zjl5RG9GbW1QaU1BY1lE?=
 =?utf-8?B?REZmVEdYbjhEN0lTYnhhYmZxZzI4dGtuNzEzNUU3UmtOQkhoZVdvSzZFdnNS?=
 =?utf-8?B?SDh0dEd3SEpyNFVtSGJJMTlEaVdveGdySncyWXQyRS9MbTIvdTVNdUtzV2NU?=
 =?utf-8?B?VDZZakJMTGRvdEpvTi9OOWlDdW9HbTV4Vy9ROUY5YU9SaGlDZWxLdTBoVktw?=
 =?utf-8?B?SG90dDdoTHNmbisxL0Z3b0JPTHgrbHpYSWw5STB4ZE9ISFlDTWZiRVRMeHJW?=
 =?utf-8?B?Z3hRd1NkUUNyU2ZFcnBiaUdlRzYvREdxa2FSL3hucnRqMFlpblNHRC9OVWRJ?=
 =?utf-8?B?a0owK29kTytFdnIzWFBvQnI1QTlqY0ovNjluL29TazBvWkdRckdGOEV4b0o0?=
 =?utf-8?B?amxTNlpjRGZPRnJrU3B1TG5LR2pHazFIc1EwclN1bGl5ZzVRa1JtWUZ3WFd1?=
 =?utf-8?B?Q0twY1Vac3NZOGNPYWQvMmxMcXZ6bkN1NVNsZjREbUNiVEpQem9PN0V2L2VP?=
 =?utf-8?B?bFN2TDRmQmNWVmpoMDM2QVQyS2QxeDhDTHRnelVEWVRrT1k5RCtWYUlpSjRH?=
 =?utf-8?B?NUVmTEZWQTdBVElsbDJzUE1CamEyT0V3TThFUDVHTjVaM2ExQXFlZVU1MS9M?=
 =?utf-8?B?VVFDTFNGRm9Kc0RLRDZpdWtJeVBiL1pNcEprT01nbVp5ZHUwWGh2aURJNCtt?=
 =?utf-8?B?RWtFZzRweWpxMjE0U282MklhQ3ZmKzVCQy9ZNU4yS3dacUh0RVZ0Sm5tMVha?=
 =?utf-8?B?eFQvdkpSc2plank4MmpUbFR4V2tFWUNzSXRCeFZlVUFyVDk1SFNsU0N6bW42?=
 =?utf-8?B?UjIyOXduTmYwY3MvR09zbkJoK01SYloxOFUyUUhURlEyMkhQaHFEK2JzWnY3?=
 =?utf-8?B?OHlSUm1xcUVSUDdaVGxNUmZ4VnY2cklxU3dPdU1TYmVlN053THkxWHk5TUcr?=
 =?utf-8?B?TjNHbUZKRFlDdzcvRjdMTkNicmh6NDJZN015VFcwVXN5SS9ZNHNROHhYQiti?=
 =?utf-8?B?Q2lncmlRZ2FkMS9YY0pjWU0xNjdmSHZ5RjQ1andaYlkwZ04zTXBiOWlIMzI3?=
 =?utf-8?B?N1ZvSkVBc1QzRmhjZW1qdzQ1RjdETUVOMllzVVQxa2FPbHdnWVNGZHJRZWl4?=
 =?utf-8?B?dlNvTFdNRWliTkF6OHJjRmt6MWZZOEtxL1NSL3Jlb2piaFJwaHZUYWg5cXl1?=
 =?utf-8?B?b2hqMGF0Q202VXBYNDJRUkdtZ3JwWUlZT2IrbzVWeXVWVUl0a2FlS2lKYTQ0?=
 =?utf-8?B?aTBCTEJ2cW5TbWpjY1h6YWFOKzA3MkpEdVkvUXFBWnJmWERyenYvRnFPY21W?=
 =?utf-8?B?eUpjRDBvUTRIajMxUHdUak5JZHZFYVVxNXFaUGVnMWsrd2NocVU4YnNwNUNY?=
 =?utf-8?B?Zjc5WFRBTHhxT29KYjVWV1hFbmZyVHJFM0Nhemo2OWsxazR6Y1E1aThmeEhu?=
 =?utf-8?B?RWs2Wkp3NTRnVXM0V0kyRWVFVGxFSkdNWGxFOEgyMCtZYXA0ZytLNnBqR0xB?=
 =?utf-8?B?Qkk0ejBWSEdxc1hZdkZxL0xIQTNKU3h2dnZSYkI4OE9CYk5ZZXVpZlFGTUpY?=
 =?utf-8?B?QXJpVnJqLzZXbHRCS0M5T0VjNHp1ZU1mV1NrRlVUWGlscVF4cGdMaERLRXg4?=
 =?utf-8?B?NVhUT3p3eUE2K3RFZjNSb0JIcGlHN3NQRzlBU01ZcUh1U241Z0ZIMEJFV3Fm?=
 =?utf-8?B?SEJCK2Z5MUs2RWFQSWZsY2RRQUhaY252K3ZFUnZTNGZmUTVIVFR2QUVuM2ZK?=
 =?utf-8?B?YUZsYU1EU0p3YTVMc0dUTUF1eUFmeTZSU0diZXM1Vno2SkY4MGV3NDZzTTkw?=
 =?utf-8?B?em42YXVjcDA0WVJlUDRaTmlIeVBuR3laOW16b2JiRkhzWDVYVDR1RUFRY0U1?=
 =?utf-8?B?eXFXN2N5bHZzcHVaejJzODFIeXVUYlVJcVJtT2ZlSHFWTmd5cDJERGczUWJD?=
 =?utf-8?Q?xY=3D?=
MIME-Version: 1.0

--ebe98e78b0216adcedd09930bab97482a73c6559294177601adf396a187f
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable

View this post on the web at https://natesnewsletter.substack.com/p/rag-the=
-complete-guide-to-retrieval

Imagine you meet someone brilliant=E2=80=94someone who seems to know absolu=
tely everything. Every answer they give feels sharp, insightful, even groun=
dbreaking. Now, picture this person having one fatal flaw: every so often, =
they confidently state something that=E2=80=99s totally wrong. Not just wro=
ng, mind you, but spectacularly incorrect=E2=80=94like insisting that Abrah=
am Lincoln was a professional skateboarder. Welcome to the current state of=
 Large Language Models (LLMs).
As fascinating and powerful as AI systems like ChatGPT and Claude have beco=
me, they still possess what I affectionately (and sometimes frustratingly) =
call a =E2=80=9Cfrozen brain problem.=E2=80=9D Their knowledge is permanent=
ly stuck at their last training cutoff, causing them to occasionally halluc=
inate answers=E2=80=94AI jargon for confidently stating nonsense. In my mor=
e forgiving moments, I compare it to asking a very smart student to ace an =
exam without any notes: impressive, yes, but prone to error and entirely re=
liant on memory.
That=E2=80=99s where Retrieval-Augmented Generation, or RAG, enters the cha=
t. RAG fundamentally reshapes what we thought possible from AI by handing t=
hese brilliant-but-flawed models a crucial upgrade: an external, dynamic me=
mory. Imagine giving our hypothetical brilliant person access to an extensi=
ve, always-up-to-date digital library=E2=80=94now every answer can be check=
ed, validated, and supported with actual data. It=E2=80=99s like turning th=
at closed-book exam into an open-book test, enabling real-time, accurate, a=
nd trustworthy answers.
The stakes couldn=E2=80=99t be higher. We=E2=80=99re moving quickly into a =
future where businesses, hospitals, law firms, and schools increasingly rel=
y on AI to handle complex information retrieval and decision-making tasks. =
According to recent market analyses, this isn=E2=80=99t a niche upgrade=E2=
=80=94it=E2=80=99s a seismic shift expected to catapult the RAG market from=
 $1.96 billion in 2025 to over $40 billion by 2035. Companies who fail to e=
mbrace RAG risk becoming like video rental stores in the Netflix era: quain=
t, nostalgic, but rapidly obsolete.
I=E2=80=99ve spent considerable time sifting through the noise, experimenti=
ng, succeeding, and occasionally stumbling with RAG. This document you=E2=
=80=99re holding=E2=80=94or, more realistically, scrolling through=E2=80=94=
is the distilled result: a 53-page guide that=E2=80=99s comprehensive, nuan=
ced, and occasionally humorous (I promise, there=E2=80=99s levity amidst th=
e deep dives into cosine similarity and chunking strategies). Whether you=
=E2=80=99re a curious novice or a seasoned practitioner, there=E2=80=99s go=
ld here for everyone.
Inside this guide, we=E2=80=99ll demystify exactly how RAG works=E2=80=94re=
trieval, embedding, generation, chunking, and all=E2=80=94using analogies c=
lear enough for dinner party conversations and precise enough for your next=
 team meeting. We=E2=80=99ll explore advanced techniques, including hybrid =
searches and multi-modal retrieval, to ensure you don=E2=80=99t just unders=
tand RAG=E2=80=94you master it. We=E2=80=99ll even examine some cautionary =
tales from companies who jumped in headfirst without checking the depth (sp=
oiler: they regret it).
Why should you read this? Because memory matters. In AI, memory isn=E2=80=
=99t a nice-to-have feature; it=E2=80=99s the essential backbone that trans=
forms impressive parlor tricks into reliable, transformative technology. If=
 you=E2=80=99re investing in AI, building products, or even just navigating=
 an AI-driven world, understanding RAG isn=E2=80=99t optional=E2=80=94it=E2=
=80=99s critical.
So, pour a coffee, settle in, and let=E2=80=99s tackle this together. You=
=E2=80=99re about to gain the keys to AI=E2=80=99s memory revolution, ensur=
ing your AI doesn=E2=80=99t just sound brilliant but actually knows its stu=
ff. Welcome to your next-level guide on Retrieval-Augmented Generation: AI=
=E2=80=99s long-awaited memory upgrade.
Subscribers get all these pieces!
From 0 to 5K: The Complete Simplified Guide to RAG (Retrieval-Augmented Gen=
eration)
Imagine if ChatGPT had perfect memory =E2=80=93 never hallucinating, and ab=
le to tap your company=E2=80=99s entire knowledge base in real time. That=
=E2=80=99s the promise of Retrieval-Augmented Generation (RAG), and it=E2=
=80=99s changing everything about how we build with AI. In this guide, we=
=E2=80=99ll demystify RAG from the ground up, transforming complex concepts=
 into an engaging, accessible journey for AI enthusiasts.
Why RAG Changes Everything
Picture this: you ask your AI assistant a question, and it instantly pulls =
up exactly the right facts from your company docs, giving a confident answe=
r with references. No more =E2=80=9Challucinated=E2=80=9D nonsense =E2=80=
=93 just accurate, up-to-date info. This isn=E2=80=99t sci-fi; it=E2=80=99s=
 RAG, and it=E2=80=99s big. Analysts project the RAG market to soar from ab=
out $1.96=E2=80=AFbillion in 2025 to over $40=E2=80=AFbillion by 2035. Comp=
anies are betting big on RAG because it tackles AI=E2=80=99s biggest weak s=
pots (memory and truthfulness) head-on.
Did you know? LinkedIn applied RAG (with a knowledge graph twist) to their =
customer support and slashed median ticket resolution time by 28.6%. And th=
ey=E2=80=99re not alone. Roughly 80% of enterprises are now using RAG appro=
aches (retrieval) over fine-tuning their models =E2=80=93 a massive shift i=
n strategy. Why? Because RAG gives AI real-time data access, and that=E2=80=
=99s gold. One survey found nearly 73% of companies are engaged with AI in =
some form , and providing those AI systems with current, relevant data is t=
he new race. In other words, the companies winning in 2025 aren=E2=80=99t t=
he ones with the biggest model =E2=80=93 they=E2=80=99re the ones whose AI =
knows their business inside and out.
So buckle up. By the end of this guide, you=E2=80=99ll see why RAG is the h=
ot topic (a $1.96B opportunity and growing ), how it=E2=80=99s delivering =
=E2=80=9Cwow moments=E2=80=9D like LinkedIn=E2=80=99s support success, and =
why 73%+ of orgs are scrambling to give their AI a real-time knowledge upgr=
ade. RAG changes everything by making AI both smart and knowledgeable =E2=
=80=93 and today, we=E2=80=99ll show you how to go from 0 to RAG hero in an=
 approachable, step-by-step narrative.
RAG Basics: Your AI Gets a Research Assistant
If a large language model (LLM) is like a brilliant student who studied eve=
rything up until 2023, then RAG gives that student a real-time library card=
=2E It=E2=80=99s like letting your AI take=
 an open-book exam instead of relyi=
ng on memory alone. How does that work? Think of RAG as giving your AI a re=
search assistant: when asked a question, the AI can Retrieve relevant info =
from a knowledge source, Embed that info into a form it understands, and th=
en Generate a final answer using both its built-in knowledge and the retrie=
ved facts.
Analogy alert: LLMs are like students; RAG lets them bring notes. As one en=
gineer quipped, =E2=80=9CLLMs don=E2=80=99t know =E2=80=93 they predict. Th=
eir memory is frozen. That=E2=80=99s where RAG changes the game. It=E2=80=
=99s like giving the model an open-book test: it still has to reason, but n=
ow it gets to reference something real=E2=80=9D. In practice, that means wh=
en an LLM gets a query, a RAG system will first fetch relevant text (from y=
our documents, websites, etc.), and supply those facts to the model so it c=
an formulate a grounded answer. The magic three-part process is:
Retrieval: Take the user=E2=80=99s question and search a knowledge source f=
or relevant info (just like Googling or querying a database).
Embedding: Behind the scenes, both the question and documents are converted=
 into numerical embeddings =E2=80=93 basically, turning words into vectors =
(imagine coordinates in a 1536-dimensional space for OpenAI=E2=80=99s ada-0=
02 model ) so that semantic similarity can be computed.
Generation: The LLM receives the question plus the retrieved context and ge=
nerates a final answer, augmented by these real-time facts.
Traditional LLMs have a fixed knowledge cutoff and often bluff when asked s=
omething outside their training. RAG makes the LLM=E2=80=99s knowledge dyna=
mic and verifiable. It=E2=80=99s the difference between a student taking a =
closed-book test (relying on possibly outdated memory) vs. an open-book tes=
t with the latest textbook in hand. For example, without RAG, an LLM is stu=
ck with whatever it learned in training =E2=80=93 ask it about something it=
 never saw, and it might just make up an answer. With RAG, we first retriev=
e the latest relevant info and feed it in, so the model=E2=80=99s answer ca=
n cite real data.
Here=E2=80=99s a simple diagram of a RAG workflow, which shows how a query =
flows through retrieval into generation:
Notice in the diagram: your AI isn=E2=80=99t just guessing from its frozen =
memory; it actively searches your knowledge base for context. This approach=
 virtually eliminates those =E2=80=9Csorry, I don=E2=80=99t have that info=
=E2=80=9D dead-ends and dramatically reduces hallucinations. Users gain tru=
st because the AI can show sources for its answers. In fact, RAG is often i=
ntroduced specifically to boost factual accuracy and up-to-dateness. One AW=
S expert described a base LLM as an =E2=80=9Cover-enthusiastic new employee=
 who refuses to stay informed=E2=80=9D =E2=80=93 RAG is how you get that em=
ployee to check the company wiki before answering !
But doesn=E2=80=99t adding retrieval make things slower? It=E2=80=99s a com=
mon concern that giving an LLM a =E2=80=9Cresearch step=E2=80=9D will add t=
oo much latency. In reality, modern RAG systems are incredibly snappy. Vect=
or search engines can fetch relevant chunks in tens of milliseconds, and ov=
erall RAG query times often land in the few-hundred-millisecond range. For =
instance, engineers report end-to-end RAG responses around 300=E2=80=93500=
=E2=80=AFms in practice =E2=80=93 essentially real-time for most apps. Even=
 complex multi-hop queries that pull lots of data might take a couple secon=
ds at most. So while vanilla ChatGPT might answer in ~1=E2=80=933 seconds, =
a well-tuned RAG might be 0.5=E2=80=935 seconds depending on complexity. In=
 conversational terms, that=E2=80=99s barely noticeable, and it=E2=80=99s a=
 small trade-off for answers grounded in truth. (And with some clever cachi=
ng and indexing, many RAG systems actually outpace humans hunting through d=
ocuments =E2=80=93 your support bot might answer in 500=E2=80=AFms what too=
k a human agent 5 minutes.)
Bottom line: RAG gives your AI =E2=80=9Cretrieval superpowers.=E2=80=9D Ins=
tead of being limited to what it memorized, it can search and cite fresh, r=
elevant information on the fly. It=E2=80=99s like upgrading your genius stu=
dent (LLM) with an always-available research librarian. In the next section=
s, we=E2=80=99ll dive deeper into how it works under the hood =E2=80=93 but=
 at its heart, RAG is the simple yet profound idea of augmenting generation=
 with retrieval. It turns out this one idea addresses a lot of AI=E2=80=99s=
 toughest challenges (hallucinations, stale knowledge, lack of trust). No w=
onder 73% of organizations are racing to implement AI with real-time data a=
ccess =E2=80=93 RAG makes AI not just smarter, but wisely informed. And tha=
t changes everything.
Under the Hood: How RAG Really Works
Let=E2=80=99s lift the hood on this RAG engine and see the mechanics in act=
ion. There are three technical concepts that make the RAG magic possible: e=
mbeddings (turning text into vectors), chunking (breaking text into retriev=
able pieces), and similarity search (finding which pieces are relevant). Do=
n=E2=80=99t worry =E2=80=93 we=E2=80=99ll break each concept down with simp=
le analogies and visuals so it all clicks.
The Journey from Text to Vector
In RAG, your words aren=E2=80=99t just words =E2=80=93 they=E2=80=99re coor=
dinates in a high-dimensional space. When we say we =E2=80=9Cembed=E2=80=9D=
 text, imagine plotting meanings on a giant star map with 1,536 dimensions.=
 For example, the phrase =E2=80=9Ccustomer refund policy=E2=80=9D might bec=
ome a vector like [0.23, -0.45, 0.67, =E2=80=A6] (with 1,536 numbers). What=
 do those numbers mean? Individually, not much to a human =E2=80=93 but col=
lectively they position the phrase in a semantic space where distance corre=
lates with meaning. Two pieces of text that mean similar things will end up=
 as vectors that are close together (small angle between them), even if the=
y don=E2=80=99t share any keywords. This is why embedding is so powerful: s=
imilar meanings cluster together in vector space.
The state-of-the-art embedding model many use is OpenAI=E2=80=99s text-embe=
dding-ada-002, which produces 1536-dimensional vectors and is remarkably go=
od general-purpose. Ada-002 was a milestone because it collapsed multiple e=
mbedding tasks into one uber-model and made it cheap and easy via API. But =
it=E2=80=99s not the only game in town. Companies like Cohere offer embeddi=
ng models (for instance, Cohere=E2=80=99s embed-english-v3 has its own dime=
nsionality and strengths ), and open-source models like E5 or InstructorXL =
are now rivaling the proprietary ones. In fact, recent leaderboards (like M=
TEB) show tiny open-source models can come within a few percentage points o=
f Ada=E2=80=99s accuracy. The takeaway: embedding models are evolving fast.=
 Ada=E2=80=99s 1536-d vector was cutting-edge in 2022, but by 2025 we have =
specialized embeddings for images, code, multi-lingual data, etc., and some=
 open models tuned for certain domains can outperform the general ones. The=
 good news is that the concept is the same =E2=80=93 whatever model you cho=
ose, it converts text into vectors such that meaningful similarity =3D math=
ematical closeness.
To visualize it, imagine each document chunk as a point in a cosmic galaxy.=
 All chunks about refund policies cluster in one nebula; all chunks about t=
echnical errors cluster elsewhere. When a query comes in (=E2=80=9CHow do I=
 process a refund?=E2=80=9D), we embed the query into this same space and s=
ee which document points are nearest. Those nearest neighbors are likely ta=
lking about refunds too, even if they don=E2=80=99t share the exact wording=
 of the question.
One mind-blowing fact: OpenAI=E2=80=99s 1536-dim embedding can capture incr=
edibly nuanced meaning. For instance, it will place =E2=80=9CApple pay refu=
nd=E2=80=9D close to =E2=80=9Creimbursing customers via Apple Pay=E2=80=9D =
even if the wording differs, because the core idea is the same. This semant=
ic clustering is something old keyword search couldn=E2=80=99t do =E2=80=93=
 it would miss synonyms or paraphrases =E2=80=93 but embeddings nail it. It=
=E2=80=99s like magic: the model somehow knows that =E2=80=9CNDA=E2=80=9D a=
nd =E2=80=9Cnon-disclosure agreement=E2=80=9D are related, or that a Jaguar=
 (animal) is different from Jaguar (car), based on context usage. Of course=
, the model doesn=E2=80=99t =E2=80=9Cknow=E2=80=9D in a human sense; it=E2=
=80=99s all statistical correlation from training. But the effect is a vect=
or space where related ideas gravitate together.
Before we move on: you=E2=80=99ll often hear about cosine similarity vs. do=
t product vs. Euclidean distance as ways to measure vector closeness. Here=
=E2=80=99s a quick cheat sheet: cosine similarity cares only about the angl=
e between vectors (essentially their orientation, ignoring magnitude). Dot =
product is like cosine but also scales with magnitude (two vectors in the s=
ame direction will register even more similar if they=E2=80=99re longer). E=
uclidean distance is the straight-line distance. Many systems use cosine or=
 dot (with normalized embeddings, dot and cosine become equivalent). Concep=
tually, you can think: cosine =3D how aligned are the meanings, dot =3D ali=
gned + confidence, Euclidean =3D literal distance considering all component=
s. The basic rule is actually simple: use whatever the embedding model was =
trained with. Ada was trained with cosine, so use cosine for Ada vectors. S=
ome newer models use dot product. As long as you match it up, you=E2=80=99r=
e golden. We won=E2=80=99t belabor the math =E2=80=93 just know these metri=
cs exist, and cosine is popular because it neatly ignores differences in le=
ngth and focuses on meaning direction.
The Art and Science of Chunking
Now our text is embedding-ready =E2=80=93 but wait, we can=E2=80=99t just e=
mbed a whole huge document in one go (context windows have limits). We need=
 to chunk documents into pieces. Chunking is an unsung art in RAG. Do it wr=
ong, and you =E2=80=9Cshred=E2=80=9D context and lose meaning; do it right,=
 and your retrieval is laser-precise. It=E2=80=99s said that bad chunking i=
s responsible for sinking up to 40% of RAG projects =E2=80=93 anecdotally, =
one expert looked at 10+ RAG implementations and found 80% had chunking tha=
t broke context. Ouch.
So what are the chunking strategies, and which actually work? Here are four=
 you should know, from simplest (and most dangerous) to most advanced:
Fixed-size chunking: Break text into equal-sized blocks (e.g. every 500 tok=
ens). It=E2=80=99s easy, but often dangerous. It can cut off in the middle =
of topics =E2=80=93 imagine a policy document where the chunk boundary spli=
ts a paragraph explaining a rule. The model might retrieve a chunk that say=
s =E2=80=9CExceptions: none.=E2=80=9D without the preceding chunk that expl=
ains the rule =E2=80=93 misleading! Fixed windows (no matter 500 tokens or =
1000) can break semantic units.
Sentence-based chunking: Split by sentences or paragraphs, respecting natur=
al boundaries. This is better for Q&A on prose, because each chunk is a sel=
f-contained thought. It=E2=80=99s commonly used in chat-style RAG: you ensu=
re each chunk is, say, <=3D 200 tokens but you only cut at sentence ends. F=
or conversational systems or FAQ docs, this often works well.
Semantic chunking: The =E2=80=9Csmart approach=E2=80=9D =E2=80=93 use an al=
gorithm to split text where topics change. For example, some tools use embe=
ddings themselves to decide split points (looking for where similarity betw=
een consecutive paragraphs drops). Others use heading structure in document=
s to keep subtopics together. Semantic chunking tries to keep each chunk ab=
out one main idea. It=E2=80=99s like an automatic outline parser.
Recursive chunking (hierarchical): When you have natural hierarchy (chapter=
s =E2=86=92 sections =E2=86=92 subsections), you chunk at multiple levels. =
E.g., first chunk into sections, but if a section is too long, further chun=
k it into paragraphs. This preserves the tree structure. Recursive chunking=
 ensures that if one chunk isn=E2=80=99t enough, you might retrieve multipl=
e from the same section (because their content is related). It=E2=80=99s us=
eful for things like books or multi-step instructions.
A huge tip from experience: overlap your chunks. By allowing, say, a 20% ov=
erlap between consecutive chunks, you ensure important context isn=E2=80=99=
t lost at boundaries. For instance, if chunk A ends with =E2=80=9CThe resul=
ts are shown in Table 5=E2=80=9D and chunk B begins with Table 5, an overla=
p would put the end of chunk A (=E2=80=9CThe results are shown in Table 5=
=E2=80=9D) also at the start of chunk B. Then if a query hits that transiti=
on, you won=E2=80=99t miss it. Many practitioners recommend overlaps around=
 10=E2=80=9320% of chunk size. NVIDIA=E2=80=99s research found ~15% optimal=
 for certain finance docs. The impact of overlap can be big: without it, yo=
u might get incomplete answers; with it, one study noted a significant boos=
t in accuracy (some internal tests saw ~35% relative improvement when using=
 overlapping chunks). The exact number isn=E2=80=99t magic, but some overla=
p is usually worth it.
Let=E2=80=99s illustrate chunking with a quick example. Suppose we have a 5=
-page HR policy PDF. Using fixed 300-word chunks, we=E2=80=99d just cut eve=
ry 300 words =E2=80=93 possibly slicing mid-paragraph. With sentence-based,=
 we might end up with 20 sentence-long chunks (better). Semantic chunking m=
ight yield chunks like =E2=80=9CVacation Policy Overview=E2=80=9D (chunk 1)=
, =E2=80=9CAccrual Rates=E2=80=9D (chunk 2), =E2=80=9CCarryover Rules=E2=80=
=9D (chunk 3), etc., aligning with headings. That=E2=80=99s ideal for targe=
ted Q&A: a question about carryover will likely retrieve the =E2=80=9CCarry=
over Rules=E2=80=9D chunk exactly. Recursive chunking would note that =E2=
=80=9CVacation Policy=E2=80=9D is part of =E2=80=9CBenefits Policies=E2=80=
=9D and keep that association, so a higher-level query about benefits might=
 retrieve multiple related chunks.
One more pro-tip: garbage in, garbage out. Clean your text before chunking.=
 Remove irrelevant headers/footers, deduplicate content, and consider addin=
g metadata (like section titles) to chunks. A chunk with metadata =E2=80=9C=
Section: Return Policy=E2=80=9D is far more informative to a retriever than=
 a naked block of text. We=E2=80=99ll cover data prep in detail later, but =
chunking is the stage where a lot of that happens =E2=80=93 splitting, labe=
ling, and indexing the knowledge.
Why chunking matters: If you chunk wrong, your RAG system might retrieve th=
e wrong pieces or miss the right ones entirely. It=E2=80=99s been called th=
e silent killer of RAG projects. But with the four strategies above and a b=
it of overlap, you can avoid the common pitfalls. A 20% overlap can improve=
 accuracy dramatically by ensuring context isn=E2=80=99t accidentally dropp=
ed. And focusing on semantic units keeps the signal-to-noise ratio high for=
 the LLM, which it loves. Think of chunking like making bite-sized snacks f=
or your AI =E2=80=93 not too big to chew, and each with a clear flavor.
Similarity Search Demystified
We=E2=80=99ve embedded our chunks and query, and we have our chunks nicely =
defined =E2=80=93 now comes the retrieval part: similarity search. This is =
where the vector database (or index) finds which chunks are most similar to=
 the query vector. Let=E2=80=99s demystify it.
First, what does =E2=80=9Cnearest neighbor in 1536D space=E2=80=9D even mea=
n? A simple analogy: imagine each document chunk is a point on a map, but i=
nstead of latitude/longitude, we have 1536 coordinates. When you ask a ques=
tion, you=E2=80=99re essentially dropping a pin in this 1536-D map, and say=
ing =E2=80=9Cfind me the closest points=E2=80=9D. The nearest neighbors wil=
l be chunks that have high cosine similarity (small angle) with the query v=
ector =E2=80=93 i.e., they talk about the same thing. Crucially, this finds=
 meaning, not just exact words. For example, if you ask =E2=80=9CHow do I r=
eimburse a customer?=E2=80=9D, the nearest neighbors might include chunks m=
entioning =E2=80=9Crefund process=E2=80=9D or =E2=80=9Cissue a credit to th=
e customer=E2=80=9D even if the word =E2=80=9Creimburse=E2=80=9D isn=E2=80=
=99t there. Vector search =E2=89=A0 keyword search =E2=80=93 it=E2=80=99s s=
earching by concept. This is why nearest neighbor in embedding space can fe=
el like magic: it retrieves relevant info even when wording differs.
The common metrics for similarity we already touched on (cosine, dot, etc.)=
=2E In practice, most vector database=
s let you choose one. The results =E2=80=
=93 nearest neighbors =E2=80=93 will be the same in terms of ranking if you=
 use the one the model expects (cosine for normalized vectors, etc.). Cosin=
e similarity is popular since it focuses purely on orientation (meaning). D=
ot product can be slightly more sensitive to frequency (longer text can hav=
e larger dot value even if semantically similar). Euclidean is less used fo=
r text embeddings but conceptually similar to dot for normalized vectors. T=
he key is: the vector DB returns a list of top-K chunks and their similarit=
y scores.
Now, the =E2=80=9Cre-ranking revolution=E2=80=9D. Basic vector search is gr=
eat, but researchers found you can push accuracy even higher by a second-st=
age rerank. One approach: retrieve, say, top 50 chunks by cosine, then use =
a more precise but slower model (like a cross-encoder or even the LLM itsel=
f) to rerank those 50 for true relevance. This two-tier system can take you=
 from maybe 70% relevant results to 90%+. Why? The cross-encoder actually l=
ooks at the query and chunk together (like =E2=80=9CWould this chunk answer=
 that question?=E2=80=9D) rather than just comparing embeddings. It=E2=80=
=99s more computationally expensive, hence only done on top-K candidates, b=
ut it substantially improves precision.
There=E2=80=99s also a simpler heuristic called Reciprocal Rank Fusion (RRF=
) which can merge results from different methods (like one from keyword sea=
rch, one from vector) and boosts final accuracy. RRF essentially says =E2=
=80=9Cif a document is high on any list, boost it in the final rank=E2=80=
=9D. It=E2=80=99s robust and often used in hybrid systems (which we=E2=80=
=99ll talk about soon).
For a visceral sense of similarity search, let=E2=80=99s do a quick =E2=80=
=9Clive=E2=80=9D example in narrative form: User asks: =E2=80=9CWhat is the=
 warranty period for product X if purchased in Europe?=E2=80=9D =E2=80=93 T=
he system embeds that query. It then computes similarity against thousands =
of chunks: Chunk 1: =E2=80=9C=E2=80=A6the standard warranty is 1 year in th=
e US and 2 years in the EU=E2=80=A6=E2=80=9D =E2=80=93 similarity 0.95 (ver=
y high, because it directly addresses warranty in EU). Chunk 2: =E2=80=9C=
=E2=80=A6product X comes with a limited warranty covering defects for 24 mo=
nths in Europe=E2=80=A6=E2=80=9D =E2=80=93 similarity 0.93 (also relevant w=
ording). Chunk 3: =E2=80=9C=E2=80=A6return policy for product X is 30 days=
=E2=80=A6=E2=80=9D =E2=80=93 similarity 0.5 (not very related, it=E2=80=99s=
 about returns vs warranty). The system would retrieve chunks 1 and 2 as to=
p hits. If we only looked for the keyword =E2=80=9Cwarranty=E2=80=9D, we=E2=
=80=99d have found them too perhaps =E2=80=93 but consider if the query was=
 phrased as =E2=80=9CHow long is support provided=E2=80=A6?=E2=80=9D and th=
e doc said =E2=80=9C24-month limited warranty=E2=80=9D. A pure keyword migh=
t miss that (no literal =E2=80=9Csupport=E2=80=9D word), but the embedding =
knows =E2=80=9Csupport period=E2=80=9D is semantically near =E2=80=9Cwarran=
ty period=E2=80=9D and still pulls it. That=E2=80=99s the power of nearest =
neighbor search in high dimensions =E2=80=93 it finds meaning, not just mat=
ching terms.
One more modern twist: Hybrid search, combining sparse (keyword) and dense =
(vector) searches. This can catch edge cases where one method alone might f=
ail. For example, exact codes or names (like error code =E2=80=9CGAN-404=E2=
=80=9D) are best found via keyword, while conceptual questions prefer vecto=
r. In a hybrid setup, you do both and merge results (maybe via RRF as menti=
oned). This often yields the best of both: semantic breadth and lexical pre=
cision. We=E2=80=99ll cover hybrid more in Advanced Patterns, but keep in m=
ind: vector search gets you 80-90% there; adding a sprinkle of keyword sear=
ch and re-ranking can push accuracy to the 95% range. In fact, our upcoming=
 case study will show an AI that went from below 60% accuracy to 94-95% by =
smart retrieval and agentic steps =E2=80=93 it=E2=80=99s not hype, it=E2=80=
=99s achievable with these techniques.
To summarize this =E2=80=9Csimilarity search=E2=80=9D stage: The query=E2=
=80=99s embedding is matched to chunk embeddings via a similarity metric. T=
he nearest chunks (those with highest cosine or dot) are fetched as relevan=
t context. Because of embeddings, this finds relevant info even when phrasi=
ng differs =E2=80=93 the AI is truly understanding the intent in vector for=
m. And by layering in re-ranking or hybrid methods, you ensure the most rel=
evant bits bubble up (even nailing tricky queries that a single method migh=
t fumble). It=E2=80=99s the secret sauce taking retrieval precision from go=
od (~70%) to great (90%+). All of this happens in a blink of an eye (millis=
econd-scale for vector math, maybe a couple hundred ms if re-ranking with a=
 smaller model). The result: your LLM gets a tidy packet of top-notch infor=
mation to work with. Next, we=E2=80=99ll see how we go from those retrieved=
 chunks to a full answer =E2=80=93 and how you can build this whole pipelin=
e yourself, step by step.
The Technical Journey: From Zero to Hero
Alright, time to roll up our sleeves and get practical. How do you go from =
zero (no RAG at all) to a hero-level implementation? We=E2=80=99ll walk thr=
ough building a simple RAG pipeline in minutes, then explore the rich ecosy=
stem of tools and stacks available, and finally outline the 5 levels of RAG=
 mastery you can aspire to. Don=E2=80=99t worry if you=E2=80=99re not a cod=
ing wizard =E2=80=93 we=E2=80=99ll keep it approachable. By the end, you mi=
ght just yell =E2=80=9CIt=E2=80=99s alive!=E2=80=9D as your first RAG syste=
m comes to life.
Starting Simple (The 10-minute RAG)
Can we build a basic RAG app in a few lines of code? Yes. Thanks to high-le=
vel frameworks like LlamaIndex and LangChain, a minimal example is surprisi=
ngly short. Here=E2=80=99s a tiny RAG setup using LlamaIndex (formerly GPT =
Index) that can load documents, create a vector index, and answer queries:
# Your first RAG in ~15 lines of code
from langchain import SimpleDirectoryReader, VectorStoreIndex
# 1. Load your data (all files in "your_data" folder)
docs =3D SimpleDirectoryReader("your_data").load_data()
# 2. Create a vector index from documents
index =3D VectorStoreIndex.from_documents(docs)
# 3. Query the index with a question
response =3D index.query("Your question here")
print(response)
That=E2=80=99s it! This example uses langchain and its integration of Llama=
Index. In step 1, it reads documents from a directory (using an out-of-the-=
box reader that handles text files). Step 2 creates an index =E2=80=93 unde=
r the hood it=E2=80=99s embedding those docs (likely with OpenAI=E2=80=99s =
Ada model by default) and storing vectors in a simple vector store. Step 3 =
sends a query; the library does the embedding of the query, similarity sear=
ch, and calls an LLM to generate an answer, returning a nice response objec=
t (which we print). With those few lines, you=E2=80=99ve built a basic doc-=
QA bot. Congrats! You just built your first RAG system. =F0=9F=8E=89 It=E2=
=80=99s pretty much =E2=80=9Cbatteries included.=E2=80=9D Of course, in a r=
eal app you=E2=80=99d add your API keys, maybe use ServiceContext to specif=
y which LLM to use (GPT-4, etc.), but the core flow remains that simple.
This toy example can be run on a small set of text files. If you had a fold=
er of policies or FAQs, it would work out of the box. The answer might look=
 like: =E2=80=9CThe warranty period is 2 years for EU purchases=E3=80=90sou=
rce.pdf=E3=80=91.=E2=80=9D (Yes, these frameworks even return source citati=
ons automatically in many cases!). Now, this simplicity is great for a prot=
otype, but as you scale up, you=E2=80=99ll want to make choices about your =
stack.
Choosing Your Stack (with personality-driven comparisons)
There=E2=80=99s an ever-growing landscape of RAG tooling. Let=E2=80=99s tal=
k about a few popular ones in a fun way =E2=80=93 imagine them as character=
s:
LangChain: =E2=80=9CThe Swiss Army knife=E2=80=9D =E2=80=93 LangChain is th=
e generalist that can do everything (sometimes too much!). It=E2=80=99s a f=
ramework with chains, agents, memory, integrations=E2=80=A6 you name it. Ne=
ed to plug in a vector DB, call an API, parse output =E2=80=93 LangChain ha=
s a module. This is great for complex apps that do more than just retrieval=
 (like multi-step reasoning). But the flip side is it can feel heavy or ove=
rly abstract for simple RAG. You=E2=80=99ll sometimes hear that LangChain i=
s too broad =E2=80=93 it=E2=80=99s like a Swiss Army knife with 50 attachme=
nts; fantastic, but you might only need 3 of them.
LlamaIndex: =E2=80=9CThe specialist=E2=80=9D =E2=80=93 LlamaIndex (GPT Inde=
x) is laser-focused on RAG. It shines in indexing and querying data with LL=
Ms. If you =E2=80=9Cwant RAG done right=E2=80=9D out of the box, LlamaIndex=
 is a great start. It handles chunking strategies, embeddings, and even has=
 neat tricks like Query Transformers and structured retrieval. It=E2=80=99s=
 not trying to orchestrate arbitrary tool use or agents =E2=80=93 it=E2=80=
=99s specifically the RAG specialist. Many find LlamaIndex simpler for pure=
 QA use cases, whereas LangChain is maybe better if you need to, say, do a =
RAG then an external calculation then chain another LLM call (i.e., more co=
mplex chain logic).
(Reality: LangChain and LlamaIndex often work together =E2=80=93 you can us=
e LlamaIndex as a retriever in LangChain =E2=80=93 but painting them as dis=
tinct personas helps clarify their emphases.) According to one StackOverflo=
w summary: =E2=80=9CYou=E2=80=99ll be fine with just LangChain, however Lla=
maIndex is optimized for indexing and retrieving data.=E2=80=9D. LangChain =
is like the big toolkit; LlamaIndex is the refined instrument for data-LLM =
integration.
Now, beyond those, you have alternatives: Haystack (an open-source framewor=
k from deepset) which is like an enterprise-ready QA system toolkit, and va=
rious proprietary solutions (Azure Cognitive Search, etc.). But LangChain a=
nd LlamaIndex have huge communities right now. Use LangChain if you need th=
at Swiss Army flexibility (chains, agents, lots of integrations). Use Llama=
Index if your focus is =E2=80=9Cfeed these docs to an LLM and get answers=
=E2=80=9D and you want it quick with sensible defaults. In practice, many s=
tart with LlamaIndex for a pilot, and as they add more complex flows, they =
might incorporate LangChain components.
Next, let=E2=80=99s compare vector databases with a similar personality fla=
ir:
Pinecone: =E2=80=9CThe reliable pro, but pricey=E2=80=9D =E2=80=93 Pinecone=
 is a cloud vector DB that=E2=80=99s fully managed and very easy to use. It=
=E2=80=99s known for high performance and reliability at scale. But like a =
seasoned pro, it comes with a price tag. Their Starter tier is free (up to =
~300K vectors), but beyond that, a standard 50K vector index costs ~$70/mon=
th , and pricing scales up with volume and QPS. Pinecone is great when you =
don=E2=80=99t want to worry about infrastructure and you have the budget fo=
r quality service. (Think: the BMW of vector DBs =E2=80=93 smooth ride, pre=
mium features, but you pay for it.)
Chroma: =E2=80=9CThe free spirit=E2=80=9D =E2=80=93 Chroma is open-source a=
nd you can self-host it for free. It=E2=80=99s super easy to get started (p=
ip install chromadb and you have a local DB in minutes). It=E2=80=99s not a=
s battle-tested for massive scale as Pinecone, but it=E2=80=99s improving f=
ast. If you=E2=80=99re a startup or hobbyist (or just cost-conscious), Chro=
ma =3D $0 (self-hosted) and often that=E2=80=99s enough for quite large pro=
jects. It=E2=80=99s like the trusty open-source toolkit =E2=80=93 freedom a=
nd flexibility, though you might have to get your hands a bit dirty on scal=
ing.
Qdrant: =E2=80=9CThe budget-friendly workhorse=E2=80=9D =E2=80=93 Qdrant is=
 another open-source vector DB that also offers a cloud service. It=E2=80=
=99s known for being efficient and having a friendly pricing model. One com=
parison found Qdrant=E2=80=99s cloud estimated around $9/month for 50K vect=
ors (versus Pinecone=E2=80=99s $70). So Qdrant is like the solid, economica=
l choice =E2=80=93 maybe not as fancy as Pinecone, but gets the job done an=
d keeps costs low. Performance-wise, Qdrant is quite good; it uses HNSW und=
er the hood like many others, and can handle millions of vectors too.
Other names include Weaviate (feature-rich, hybrid search support), Milvus =
(from Zilliz, high-performance, but heavier to manage). An insightful bench=
mark summarized: For 50K vectors, Qdrant=E2=80=99s ~$9 is hard to beat, Wea=
viate ~$25, Pinecone ~$70. Also Pinecone isn=E2=80=99t open-source (fully m=
anaged only), whereas Qdrant, Weaviate, Chroma are open or offer OSS versio=
ns.
In short: Pinecone if you value turnkey service and can pay; Chroma if you =
want free and local; Qdrant if you want cheap cloud with solid performance.=
 There=E2=80=99s no one-size-fits-all =E2=80=93 it depends on your needs (p=
rivacy? scale? budget?). Many prototyping with Chroma or LlamaIndex=E2=80=
=99s in-memory store, then move to Pinecone or Qdrant for production.
Finally, consider the LLM for generation. If using OpenAI, GPT-4 gives best=
 quality but at higher cost/latency; GPT-3.5 is faster/cheaper but may hall=
ucinate more if the retrieved context isn=E2=80=99t obviously relevant. The=
re=E2=80=99s also Cohere, Anthropic, and open models (like LLaMA 70B via AP=
I or self-hosted). Using a powerful model for final answer is important for=
 quality, but you can often get away with a smaller model if your retrieval=
 is very on-point (because then the model=E2=80=99s job is easier =E2=80=93=
 just summarize or lightly rephrase the facts in context).
This naturally leads to the idea of cascading models to optimize cost, whic=
h advanced users do (e.g., try answering with a cheap model first, and only=
 if unsure, call GPT-4). We=E2=80=99ll revisit that in Enterprise tips. But=
 for now, let=E2=80=99s outline the stages of RAG mastery you can progress =
through.
The 5 Levels of RAG Mastery
Basic RAG =E2=80=93 Simple Document Q&A: Level 1: You can feed documents an=
d get answers. This is where you likely are after writing those 15 lines ab=
ove. It handles questions like =E2=80=9CWhat is our refund policy for EU cu=
stomers?=E2=80=9D by retrieving a snippet from your policy docs and answeri=
ng. The system uses one strategy (vector search) and one data source. At th=
is level, you might occasionally get irrelevant context if the query is amb=
iguous, but generally it works for straightforward Q&A on your content.
Hybrid Search =E2=80=93 Combining Semantic + Keyword: Level 2: You enhance =
retrieval by using both vector similarity and traditional keyword (BM25) se=
arch. Why? Because certain queries need exact matches (e.g., codes, proper =
nouns) that vectors might miss. By combining results from both and merging =
(perhaps via that RRF method ), you cover both the =E2=80=9Cfuzzy meaning=
=E2=80=9D and =E2=80=9Cexact token=E2=80=9D bases. The result: higher accur=
acy and robustness, especially for edge cases. At this level your system ca=
n handle things like =E2=80=9Cerror code 500 out-of-memory=E2=80=9D (which =
needs exact code match) and =E2=80=9COOM error=E2=80=9D (which a vector mig=
ht link to the same thing). You=E2=80=99re mitigating the recall issues of =
vector or sparse alone.
Multi-modal RAG =E2=80=93 Text, Images, and Beyond: Level 3: Now your =E2=
=80=9Cdocuments=E2=80=9D aren=E2=80=99t just text =E2=80=93 they could be i=
mages, audio transcripts, even video. Multi-modal RAG means retrieving acro=
ss different data types. For example, Vimeo=E2=80=99s support might use RAG=
 to search not just their text docs but also transcripted tutorials or even=
 the content of videos (via image captions or OCR). Another scenario: in he=
althcare, a RAG system might pull a relevant medical diagram along with tex=
t. Technically, this involves embedding other modalities (e.g., using CLIP =
for images to get vectors). By mastering multi-modal RAG, your AI could ans=
wer a question like =E2=80=9CWhat does the workflow diagram look like for p=
rocess X?=E2=80=9D by retrieving an image of that diagram (converted to an =
embedding) plus some explanation text. It opens up a new world of use cases=
 =E2=80=93 chat about your PDFs and your slide decks and your videos.
Agentic RAG =E2=80=93 Self-improving Systems with Reasoning: Level 4: Here =
we blend RAG with agent-like behavior. Instead of a single retrieval step, =
the AI can iteratively plan and retrieve, or use tools, to answer more comp=
lex queries. For example, an agentic RAG might break down a tough question =
into sub-questions, retrieve answers for each, and then compose a final ans=
wer. It can also decide to do follow-up retrieval if the initial info wasn=
=E2=80=99t sufficient =E2=80=93 essentially a loop where the LLM says =E2=
=80=9CLet me dig deeper on XYZ=E2=80=9D and performs another retrieval. Thi=
s level often uses frameworks like LangChain agents or the ReAct pattern (L=
LM reasoning with retrieval actions). The system not only fetches facts, bu=
t can chain them or perform calculations, etc. It=E2=80=99s =E2=80=9Copen-b=
ook exam + reasoning=E2=80=9D. One cool example: an agentic RAG might take =
a customer query, retrieve some knowledge base articles, then notice it nee=
ds the latest sales figure, call an API to get that, and then answer =E2=80=
=93 all dynamically. It=E2=80=99s more complex but can tackle multi-step ta=
sks and even self-correct if initial info was misleading. This is where =E2=
=80=9CAI assistants=E2=80=9D live, going beyond pure Q&A.
Production RAG =E2=80=93 Enterprise-scale with Millisecond Latency: Level 5=
: The final boss level. Your RAG system serves thousands or millions of use=
rs, with perhaps 10 million+ queries a day, all under tight latency require=
ments (say <100ms for search). You=E2=80=99ve deployed indexes with million=
s of chunks, sharded across servers. Caching is employed (maybe an in-memor=
y cache for popular queries), and you monitor latency percentiles. This is =
where search engine tech meets RAG. Systems like Bing (with Sydney), or Goo=
gle=E2=80=99s search augmentation, or enterprise digital assistants fall he=
re. You=E2=80=99ve mastered vector index scaling, index updating without do=
wntime, and cost optimization (like using a cheaper model for 95% of querie=
s and only using GPT-4 for the hardest ones to save money). Also, productio=
n-ready means robust evaluation and fallback =E2=80=93 you likely integrate=
 feedback loops where if the AI is not confident, it might gracefully decli=
ne or escalate. It also involves security =E2=80=93 ensuring no data leakag=
e, compliance with things like HIPAA/GDPR if applicable. At Level 5, you ar=
e building RAG with the rigor of a mission-critical system. The payoff: use=
rs get instant, accurate answers at scale, and your company=E2=80=99s colle=
ctive knowledge truly becomes an =E2=80=9CAI brain=E2=80=9D that anyone can=
 tap.
Think of these levels as cumulative. Each builds on the previous. By the ti=
me you=E2=80=99re at Level 5, you=E2=80=99ve incorporated hybrid search, ma=
ybe multi-modal sources, perhaps some reasoning, and you=E2=80=99ve industr=
ialized it. But don=E2=80=99t be daunted =E2=80=93 you can get a lot of val=
ue at Level 1 and 2 already. Many internal tools live around Level 2 or 3: =
e.g., a chatbot that answers from company docs (text-only) with some hybrid=
 search is Level 2. That alone can reduce support tickets or onboard employ=
ees faster. The fancy stuff like agents and multi-modal come into play for =
advanced applications (like an AI that can troubleshoot software by reading=
 logs and viewing system graphs =E2=80=93 text + metrics + images, with rea=
soning).
Next, we=E2=80=99ll delve into how to get your data ready for these levels.=
 Because even the fanciest RAG pipeline fails if fed garbage data. It=E2=80=
=99s like having a brilliant chef but giving them rotten ingredients =E2=80=
=93 the dish won=E2=80=99t turn out well. So, let=E2=80=99s talk data prep!
The Data Preparation Pipeline: Garbage In, Genius Out
You=E2=80=99ve heard it a million times: =E2=80=9Cgarbage in, garbage out.=
=E2=80=9D Nowhere is this more true than in RAG. The smartest retrieval and=
 LLM won=E2=80=99t help if your documents are a mess =E2=80=93 imagine PDFs=
 with broken text, irrelevant boilerplate, or missing context. In this sect=
ion, we=E2=80=99ll cover how to turn raw data into RAG-ready gold. From par=
sing gnarly file formats to enriching with metadata, consider this your pre=
-flight checklist before launching your AI.
Document Parsing Secrets: Your data likely isn=E2=80=99t a neat collection =
of.txt files. You=E2=80=99ll have PDFs, Word docs, HTML pages, maybe spread=
sheets. The first step is parsing them into plain text (or structured text)=
=2E Each format has its quirks:
PDFs: Use reliable PDF parsers (like PyMuPDF/fitz or pdfplumber in Python).=
 Extract text but beware =E2=80=93 PDFs often have headers/footers on every=
 page, line breaks in weird places, etc. A secret: many PDFs are basically =
scanned images (like that one cursed legacy contract). For those, you=E2=80=
=99ll need OCR (optical character recognition) to get text. Tools like Tess=
eract or AWS Textract can OCR images in PDFs. Also, watch out for multi-col=
umn layouts (scientific papers) =E2=80=93 a naive parser might read across =
columns mixing content. Some libraries can detect columns or you might spli=
t by page and handle manually.
Word Docs (.docx): These are easier =E2=80=93 use python-docx or LibreOffic=
e command line to convert to text. Most formatting (bold, etc.) we don=E2=
=80=99t need, but we want to preserve structure like headings. A good strat=
egy: extract text and also output something like =E2=80=9C## Heading: [Head=
ing Text]=E2=80=9D lines so you know what was a heading.
HTML/Markdown: Likely documentation or web pages. Stripping HTML tags is st=
ep one (BeautifulSoup can help). But preserving some structure (like lists,=
 tables) is useful. You might convert HTML to markdown, which keeps bullet =
points and links in a readable way. Be careful to remove navigation menus, =
ads, etc., that aren=E2=80=99t part of main content (there are boilerplate =
removal tools for HTML).
Excel/CSV: If you have tabular data that=E2=80=99s relevant (maybe product =
price lists or error code tables), you can either embed those as text (e.g.=
, convert small tables to text lists) or handle them specially (some RAG sy=
stems store small tables as structured data and let the LLM access them via=
 a =E2=80=9Ctool=E2=80=9D). But often, converting each row into a sentence =
works (e.g., row with Product X =E2=80=93 Warranty 2 years becomes =E2=80=
=9CProduct X has a warranty period of 2 years.=E2=80=9D for embedding).
Essentially, use the right parser for each format, and verify the output. Y=
ou don=E2=80=99t want chunks full of gibberish because the parser mis-order=
ed the text. A quick manual skim of parsed output for a few files can save =
headaches.
Metadata Magic: Once you have the content, add metadata! Metadata is additi=
onal info about each chunk =E2=80=93 like source filename, document title, =
author, timestamp, section, etc. Why? It can 3=C3=97 your accuracy in retri=
eval and even generation. For example, if each chunk knows it came from =E2=
=80=9CFAQ.doc =E2=80=93 Section: Pricing=E2=80=9D, the retriever can use th=
at in relevance scoring (some vector DBs allow filtering or weighted fields=
). The LLM can also be instructed to cite the source or use section info to=
 format answer. In an evaluation at one company, adding metadata like docum=
ent category boosted relevant retrieval by a huge margin (one anecdote: +35=
% hit rate on correct doc). At minimum, store: source name, and if applicab=
le section headings and dates. Dates are important for time-sensitive info =
=E2=80=93 e.g., =E2=80=9CPolicy updated March 2023=E2=80=9D. You can store =
that so if a query asks =E2=80=9Cwhat=E2=80=99s the latest policy=E2=80=9D,=
 you might prefer newer chunks.
A neat trick: use metadata to filter. If your system supports it, you can t=
ag chunks by type (e.g., =E2=80=9Cinternal=E2=80=9D vs =E2=80=9Ccustomer-fa=
cing=E2=80=9D). Then if you build a chatbot for customers, you filter out i=
nternal-only docs entirely. This avoids embarrassing mistakes (like the AI =
revealing an internal memo because it was in the index).
Cleaning Strategies that Work: Before embedding, you want your text clean a=
nd useful:
Intelligent removal of headers/footers: Many docs have repeated boilerplate=
 (company name, page numbers, legal footers). These can pollute retrieval =
=E2=80=93 e.g., you don=E2=80=99t want a chunk of mostly footer text (=E2=
=80=9CACME Corp Confidential =E2=80=93 Page 5 of 10=E2=80=9D) to be retriev=
ed. You can detect these by frequency (if a line appears in every page, dro=
p it), or specific cues (if it matches regex like =E2=80=9CPage \d of \d=E2=
=80=9D or has the company name in ALLCAPS over and over). Removing or reduc=
ing this boilerplate improves the signal.
Handle tables and lists carefully: If a PDF parser outputs tables in a jumb=
led way (e.g., row data doesn=E2=80=99t line up), consider post-processing =
it. Sometimes it=E2=80=99s better to manually parse important tables (or us=
e a CSV export from the source). For lists, keep the bullets or numbering i=
f possible =E2=80=93 it gives structure. For example, an answer might list =
the 3 steps of a process; if your chunk preserved =E2=80=9C1. Do X 2. Do Y =
3. Do Z=E2=80=9D as separate lines, the LLM can more cleanly produce a numb=
ered answer.
Preserve context while removing noise: This is key. You want to trim the ju=
nk but not accidentally trim meaningful context. For instance, if a heading=
 says =E2=80=9C4.2 Refund Process=E2=80=9D and the next page starts mid-sen=
tence because of a page break, ensure the text is contiguous. One strategy =
is join text from page to page if there=E2=80=99s an obvious cut. Another i=
s to include the heading text as metadata or inline (like add a line =E2=80=
=9CRefund Process:=E2=80=9D before the paragraph text). That way the chunk =
is self-contained contextually.
Normalize text: fix OCR errors (common ones like =E2=80=9CO=E2=80=9D vs =E2=
=80=9C0=E2=80=9D or =E2=80=9Crn=E2=80=9D vs =E2=80=9Cm=E2=80=9D). Also, uni=
fy things like whitespace, remove weird characters. If the data has a lot o=
f unicode bullets or emojis not relevant, strip them. Consistency in text w=
ill help embeddings not get confused by artifacts.
Language and encoding: If you have multilingual docs, note language in meta=
data. Remove any Unicode BOMs or encoding issues (most libraries handle UTF=
-8 fine nowadays, but just be cautious if any documents are in different la=
nguages/scripts =E2=80=93 test a bit).
Think of cleaning like preparing a training dataset =E2=80=93 a bit of time=
 here yields a much smarter system. There=E2=80=99s a story of a startup sp=
ending weeks debugging why their RAG answers were off, only to realize the =
PDF parser scrambled columns so text read like word salad. A quick fix in p=
arsing and accuracy jumped. So, invest time here.
The preprocessing checklist: Every document should ideally go through these=
 10 steps:
Convert to text: (using appropriate parser for PDF, docx, etc.)
Split into paragraphs/sections: (don=E2=80=99t chunk yet, just logical sect=
ions)
Remove boilerplate: (headers, footers, legalese not needed)
Normalize whitespace and punctuation: (clean newlines, fix broken hyphenati=
ons where a word is split at line break)
Extract or insert section titles as needed: (to give context to each part)
Add metadata: (filename, section, date, etc.)
Chunk into pieces with overlap: (apply your chunk strategy here on the clea=
ned content)
Embed chunks and store in vector DB: (and store metadata alongside)
Verify sample chunks: (manually check a few: =E2=80=9CDoes this chunk make =
sense on its own? Does it have necessary context?=E2=80=9D)
Iterate if needed: (if something looked off, tweak parsing or chunking and =
re-run for that doc).
For a =E2=80=9Cmessy financial report=E2=80=9D example: say you have a 100-=
page annual report PDF with financial tables and text. The steps would be: =
parse text, detect that every page has a footer =E2=80=9CCompany =E2=80=93 =
Confidential=E2=80=9D and remove that line everywhere. Join lines that got =
broken in half by page breaks. For tables, maybe you notice they came out m=
isaligned =E2=80=93 you might manually copy the table as CSV, or at least e=
nsure each table row stays in a chunk so context isn=E2=80=99t lost. Add me=
tadata like =E2=80=9Csection: Balance Sheet=E2=80=9D for the section where =
the table is. Then chunk maybe by sub-sections or 512-token blocks with ove=
rlap, ensuring not to cut mid-table. The end result: a set of chunks like =
=E2=80=9CBalance Sheet: =E2=80=A6assets=E2=80=A6liabilities=E2=80=A6=E2=80=
=9D with perhaps the table values listed neatly. Now when a question asks =
=E2=80=9CWhat were the total assets in 2024?=E2=80=9D, the chunk with that =
info is retrievable and the model can answer accurately.
Remember, an hour spent cleaning data can save dozens of hours troubleshoot=
ing weird AI outputs later. When the AI gives a wrong or odd answer, 9 time=
s out of 10 the issue can be traced back to missing or poorly formatted con=
text in the chunks. Garbage in, garbage out is a law; but with clean, well-=
chunked data in, you get genius out.
Memory Magic: Short-term vs Long-term
One thing people often ask is, =E2=80=9CIf RAG gives the LLM external info,=
 do we even need the LLM=E2=80=99s own memory?=E2=80=9D Also, how do we han=
dle multi-turn conversations =E2=80=93 can the AI =E2=80=9Cremember=E2=80=
=9D what was said earlier? This is where memory comes in, and in RAG we dea=
l with two kinds: short-term (conversation context) and long-term (persiste=
nt knowledge). Let=E2=80=99s explore how to give your AI a memory like an e=
lephant (when needed), without blowing the context window.
The =E2=80=9Cconversation amnesia=E2=80=9D problem: If you=E2=80=99ve used =
ChatGPT, you know it can carry on a conversation remembering what you said =
earlier =E2=80=93 up to a limit. That limit is the context window (e.g., ~4=
K or 8K tokens for GPT-3.5, 32K or more for GPT-4). In a chat setting, the =
model doesn=E2=80=99t truly remember anything beyond what=E2=80=99s in the =
prompt each turn. If the convo exceeds the window, it =E2=80=9Cforgets=E2=
=80=9D the earliest parts unless we do something. This is conversation shor=
t-term memory issue. In a RAG chatbot, it=E2=80=99s similar: you want it to=
 remember what the user already asked and what answers it gave.
Episodic memory (like human memory, but better): One solution is to impleme=
nt episodic memory for the AI. Think of splitting the conversation into epi=
sodes or chunks and summarizing past ones. For example, after 10 turns, you=
 generate a summary of the conversation so far (or the important points) an=
d use that going forward instead of the full history. This is akin to how h=
umans remember key points of a long discussion, not every sentence. There a=
re known strategies:
Summary memory: Every few turns, produce a concise summary and include that=
 in the prompt instead of raw transcript.
Message retrieval memory: This is cool =E2=80=93 treat past dialogue as kno=
wledge chunks, embed them, and when context is needed, retrieve relevant pa=
st utterances (yes, RAG on the conversation itself). So if 30 turns ago the=
 user mentioned something now relevant, the system can fetch that line rath=
er than hoping it=E2=80=99s still in context. This is like context-aware me=
mory retrieval.
At its core, you can maintain a vector store for conversation history. We c=
all this a =E2=80=9Clong-term memory=E2=80=9D module. As the chat goes on, =
store each user and assistant message embedding. When new question comes, y=
ou can retrieve past messages that seem related to the new query and prepen=
d them as context. This way, even if the conversation spans 100 turns, the =
AI can recall specific details from earlier by retrieving them as needed. I=
t=E2=80=99s like an AI having selective photographic memory: it doesn=E2=80=
=99t hold everything in immediate view, but it can search its memory for re=
levant bits.
Context window hacks: Besides retrieval-based memory, there are some hacks =
to fit more into the context window:
Truncation strategy: Always drop the oldest turns once you near the limit (=
not great if user refers back to something older).
Prioritize important content: e.g., keep system instructions and last user =
question, maybe summary of rest.
Use larger context models for summarizing smaller context models: For insta=
nce, use GPT-4 32K to manage summarization that GPT-3.5 can=E2=80=99t hold,=
 etc. But that=E2=80=99s advanced interplay.
With Anthropic=E2=80=99s models boasting 100K token context now and likely =
million-token contexts on the horizon , one might say =E2=80=9Cwhy bother s=
ummarizing, just use a bigger model!=E2=80=9D True, bigger windows alleviat=
e some need for memory tricks =E2=80=93 but they aren=E2=80=99t infinite an=
d come with higher cost and slower performance. So memory techniques will r=
emain useful.
A real example: DoorDash (just a hypothetical scenario to illustrate) =E2=
=80=93 suppose DoorDash has a support chatbot that helps with live customer=
 orders. They want it to remember your entire conversation (=E2=80=9CDid th=
e agent already ask for my order ID five messages ago?=E2=80=9D). They migh=
t use an episodic memory: each time you provide info like order ID, it=E2=
=80=99s stored in a slot; if you mention a restaurant name earlier and late=
r say =E2=80=9Cthe restaurant messed up my order=E2=80=9D, the bot should r=
ecall which restaurant =E2=80=93 that could be done by simply including pre=
vious user messages in context until it can=E2=80=99t, then summarizing =E2=
=80=9CUser=E2=80=99s order from McDonald=E2=80=99s had an issue=E2=80=A6=E2=
=80=9D. Anecdotally, an AI support bot forgetting earlier details leads to =
repetition and frustration, so solving conversation memory is crucial. Many=
 production systems use a combination: keep recent turns verbatim (for rece=
ncy), use a summary for older turns, and even incorporate retrieval for spe=
cific facts from the dialogue history.
Short-term vs Long-term memory: In human terms: short-term is like the scra=
tchpad of what=E2=80=99s actively being discussed (the last few exchanges),=
 long-term is everything that happened before that you might need to recall=
 if context shifts back. RAG gives an LLM a form of long-term memory by hoo=
king into an external knowledge base. We can extend that concept to convers=
ation =E2=80=93 the knowledge base in this case is the conversation transcr=
ipt itself. In fact, some research works use the same RAG pipeline for conv=
ersation: treat the entire dialogue as a growing document. But more practic=
ally, we maintain separate memory vector indexes for conversation history v=
s. general knowledge.
Context window optimization analogy: =E2=80=9CFitting an encyclopedia on a =
Post-it note.=E2=80=9D If you only have a 4K-token Post-it (context), you c=
an=E2=80=99t fit the whole company handbook. But RAG lets you include just =
the relevant parts (like copying the needed lines from the encyclopedia ont=
o the Post-it). For conversation, memory management is like continuously up=
dating that Post-it with the most pertinent facts from the chat so far. Sum=
maries and retrieval act as our compression techniques.
One approach known in literature is Hierarchical Memory: maintain multiple =
levels of abstraction (immediate last utterances full text, then a summary =
of older stuff, etc.).
Don=E2=80=99t forget, the model=E2=80=99s own weights are a kind of long-te=
rm memory too =E2=80=93 the base knowledge from pretraining. It =E2=80=9Ckn=
ows=E2=80=9D common sense and some general facts. RAG complements that with=
 specific data. So when we say short vs long-term memory in AI:
Long-term (in context of RAG) =3D persistent knowledge base (could be docum=
ents, or conversation history stored externally).
Short-term =3D the active context window content.
DoorDash example extended: The bot remembers entire history: customer says =
at turn 2, =E2=80=9CMy order #123 was missing fries.=E2=80=9D At turn 15, t=
hey say =E2=80=9CI never got one item=E2=80=9D. The bot should recall =E2=
=80=9Cfries were missing=E2=80=9D =E2=80=93 ideally it does. If the convers=
ation is long, that initial statement might have dropped out of context. Bu=
t if the bot stored that fact, it can retrieve it. One can imagine a memory=
 retrieval: user asks =E2=80=9CCan I get a refund for that missing item now=
?=E2=80=9D The bot=E2=80=99s system retrieves earlier message about =E2=80=
=9Cmissing fries=E2=80=9D and then answers: =E2=80=9CYes, I see your fries =
were missing =E2=80=93 I=E2=80=99ve issued a refund for those.=E2=80=9D Thi=
s level of continuity is achievable with RAG-memory.
In summary, RAG isn=E2=80=99t only for external knowledge, it can be for th=
e conversation itself. Proper memory management (short-term by window, long=
-term by retrieval) ensures your AI doesn=E2=80=99t suffer dementia in long=
 chats. The result: dialogues that feel coherent and contextually aware fro=
m start to finish. And as context window sizes grow (hey GPT-4-32k, and Cla=
ude=E2=80=99s 100k), we get more breathing room =E2=80=93 but good memory t=
echniques will always improve efficiency and capability, especially as we p=
ush toward multi-hour or continuous conversations (think AI personal assist=
ants that chat with you over weeks).
Next, how do we know our RAG system is actually working well? We need to ev=
aluate and test it =E2=80=93 that=E2=80=99s our next section.
Evaluation and Testing: Measuring What Matters
So you=E2=80=99ve built a RAG system. How do you know it=E2=80=99s good? We=
 can=E2=80=99t just trust our gut or a few anecdotal successes =E2=80=93 we=
 need solid evaluation. This section covers the key metrics that predict su=
ccess, how to create an evaluation dataset, and strategies like A/B testing=
 and feedback loops to continuously improve your RAG.
The 4 metrics that predict success:
Relevance (Recall at K): Are we retrieving the right stuff? This is usually=
 measured by something like Recall@K =E2=80=93 the percentage of queries fo=
r which the relevant document is in the top K retrieved. If your knowledge =
base has ground-truth answers, you check if those were included. For exampl=
e, if a user asks =E2=80=9CWhat=E2=80=99s the refund period?=E2=80=9D, and =
the correct doc chunk about refunds was retrieved in top 3 results, that=E2=
=80=99s a hit. You want high recall so the needed info is almost always fet=
ched. If your retriever isn=E2=80=99t getting relevant content, the generat=
or can=E2=80=99t answer correctly. Thus, relevance of retrieval is metric #=
1.
Faithfulness (Groundedness): Is the answer actually based on the retrieved =
sources, or is the model hallucinating extra details? An answer is faithful=
 if every claim it makes can be traced to provided context. One way to meas=
ure: have human raters label answers as =E2=80=9Csupported by source vs. no=
t=E2=80=9D. Or automatically, one can check if answer sentences overlap eno=
ugh with source text. Faithfulness is crucial =E2=80=93 a RAG system that r=
etrieves correctly but then the LLM ignores it and makes something up fails=
 the point. Metrics like =E2=80=9CSelf-Consistency=E2=80=9D or using a smal=
ler model to verify facts can be used. In research, sometimes they measure =
the percentage of answers with at least one correct citation (as a proxy).
Answer quality (Usefulness/Accuracy): This is the end-to-end quality =E2=80=
=93 would a human judge the answer as correct, complete, and directly answe=
ring the question? This is a bit subjective, but you can operationalize it =
via a test set: e.g., 100 questions with gold answers (could be written by =
experts or from an FAQ). Run the system and compare answers to gold =E2=80=
=93 measure accuracy or use F1 if partial. Another angle: have human evalua=
tors rate answers 1-5 on satisfaction. This metric is holistic =E2=80=93 it=
 factors in retrieval and generation and readability.
Latency: All the goodness above doesn=E2=80=99t matter if it=E2=80=99s too =
slow for users. Latency is critical for real user experience. We often look=
 at p90 or p95 latency (the 90th/95th percentile response time) =E2=80=93 m=
eaning the slowest typical responses. If p95 latency is, say, 4 seconds, th=
at means 19 out of 20 responses come in under 4s, but 5% of queries take lo=
nger (maybe a big retrieval or an agent loop). Depending on your use case, =
you might target sub-1s for live chat, or maybe a few seconds is acceptable=
 for complex analysis. Either way, monitor it. There=E2=80=99s also cost (n=
ot a =E2=80=9Cuser metric=E2=80=9D but business metric) =E2=80=93 if your R=
AG calls an expensive model, you measure cost/query. Latency and cost often=
 trade off with using bigger models or doing reranking.
Other useful metrics include Precision of retrieval (are retrieved docs act=
ually relevant vs. bringing some irrelevant stuff), Hallucination rate (inv=
erse of faithfulness =E2=80=93 how often does it make unsupported claims), =
and Coverage (if a question has multiple points, did answer cover them all?=
). But to keep it simple, the four above capture major aspects: did we get =
the info, did we ground the answer, was the answer good, and was it fast.
Now, how to actually measure these? You need an evaluation dataset. Ideally=
, a set of sample questions (representative of what users will ask) along w=
ith ground-truth references or expected answers. Often this is ~50=E2=80=93=
100 questions for a small system, but for robust eval maybe a few hundred. =
Building this =E2=80=9Cgold set=E2=80=9D is an investment, but hugely worth=
 it. It=E2=80=99s your yardstick.
Building your evaluation dataset =E2=80=93 the 100-question gold standard: =
Start by collecting real queries if available (like search logs, customer q=
uestions). If none exist, brainstorm likely questions or have domain expert=
s generate them. Then for each question, determine what the correct answer =
or document is. For instance, if question is =E2=80=9CHow many days do I ha=
ve to return a product?=E2=80=9D, the gold reference might be =E2=80=9CRetu=
rns are accepted within 30 days of purchase=E2=80=9D from ReturnPolicy.doc.=
 If you can, actually have a human write the ideal answer (with sources not=
ed). If not, at least mark which document/section contains the answer. This=
 way you can evaluate retrieval (did it retrieve ReturnPolicy.doc?), and ge=
neration (did it say 30 days?).
It=E2=80=99s often helpful to include some tricky cases: ambiguous question=
s, multi-part questions, etc., to see how system handles them. Once you hav=
e this dataset, run your RAG system on it and measure:
Retrieval recall: what % of answers had the correct doc in top 3 retrieved?=
 (Use your gold references).
Answer accuracy: compare the system=E2=80=99s answer to the gold answer (co=
uld be exact match for factual, or BLEU score or a simple correctness check=
).
If possible, manual review a subset to label hallucinations or irrelevant c=
ontent.
A/B Testing RAG: When you make improvements, you=E2=80=99ll want to A/B tes=
t to prove ROI. For example, you launch RAG bot version A (maybe the old sy=
stem was a static FAQ or a non-RAG model) vs. version B (with RAG). Define =
success metrics =E2=80=93 maybe deflection rate (questions answered without=
 human agent), user satisfaction (collected via thumbs-up/thumbs-down), or =
simply accuracy on sample queries. Run both versions (e.g., route a portion=
 of traffic to each) and compare. Say LinkedIn implemented RAG and found su=
pport resolution time dropped 28.6% =E2=80=93 that=E2=80=99s a concrete ROI=
 metric. To A/B test internally, you can simulate by splitting your evaluat=
ion questions and answering them with old vs new system, have judges blind-=
score which answers are better. If 9/10 times RAG=E2=80=99s answer is bette=
r, you have a winner.
A/B testing is not just for accuracy, but also can test latency/cost trade-=
offs. E.g., =E2=80=9CIs using GPT-4 (which is slower) giving significantly =
better answers than GPT-3.5 for our domain?=E2=80=9D You might run eval wit=
h both and find maybe a slight quality uptick but double latency =E2=80=93 =
then you decide if that=E2=80=99s worth it. Only good evaluation data and t=
esting will tell.
The feedback loop: Using production data to improve continuously. Once your=
 system is live, you=E2=80=99ll get real interactions. Put hooks to capture=
 useful signals:
Let users rate answers (thumbs up/down or =E2=80=9CDid this answer your que=
stion? Yes/No=E2=80=9D). This is invaluable supervised data. Every thumbs-d=
own with the conversation and answer is a training example of what went wro=
ng. Did retrieval fail or did the model hallucinate? You can label these an=
d use them to refine either component (e.g., fine-tune the LLM to be more c=
autious, or improve indexing).
Track what users do after the answer. If they immediately rephrase the ques=
tion or go browsing the document link, maybe the answer wasn=E2=80=99t sati=
sfactory or complete. That can be an implicit signal.
Log all queries that got =E2=80=9CI don=E2=80=99t know=E2=80=9D or low-conf=
idence answers. Later, check if perhaps answers exist in your data but were=
 missed. If so, why missed? Maybe add that phrasing to document text or adj=
ust embedding parameters.
Teams often set up a regular review of failed cases. For instance, Notion=
=E2=80=99s team might have taken transcripts of cases their AI missed and t=
hen updated their index or prompts accordingly. Over a few months, this ite=
rative loop improved their RAG accuracy from, say, 72% to 94% (hypothetical=
 numbers, but plausible given iterative improvements).
One case study: Notion (per reports) improved their answer accuracy dramati=
cally by analyzing where the chatbot failed to retrieve correct pages and a=
dding better metadata and new training for those cases. They basically trea=
ted it like model fine-tuning, but in retrieval space =E2=80=93 adjusting d=
ata when the system was off. The result was an increase from mediocre perfo=
rmance to near expert-level answer quality over a quarter. The moral: syste=
matic evaluation + feedback loop =3D rapid improvement.
To implement feedback: use a tool like RAGAS (Retrieval-Augmented Generatio=
n Assessment System) or custom scripts to evaluate logs. RAGAS provides met=
rics like answer accuracy and hallucination detection with LLM judges. Some=
 companies pipe conversation logs into an evaluation pipeline nightly =E2=
=80=93 e.g., run an LLM offline to score each answer for correctness when p=
ossible, to identify issues proactively.
Finally, to prove ROI to stakeholders, tie these metrics to business outcom=
es. E.g., after deploying RAG, support ticket volume dropped by X%, custome=
r satisfaction on help answers rose Y points. Or employees find info 2x fas=
ter (maybe measure by a before/after experiment). These concrete wins justi=
fy the investment and guide further funding.
In summary, evaluate early, evaluate often. Use a diverse set of metrics: r=
etrieval quality, answer faithfulness, user-level success. Build a gold tes=
t set of at least 100 Q&As =E2=80=93 it will act as your compass. Then cont=
inuously A/B test improvements and incorporate real user feedback. With thi=
s discipline, you=E2=80=99ll avoid falling into the trap of anecdotal =E2=
=80=9Cit seems to work=E2=80=9D and instead know how well it works and wher=
e to improve. That=E2=80=99s how you push from a decent prototype to a reli=
able production system. Next, let=E2=80=99s talk about scaling that system =
up to enterprise level =E2=80=93 where millions of dollars might be on the =
line.
Enterprise RAG: When Millions Are on the Line
Building a small RAG demo is one thing; deploying it across a Fortune 500 e=
nterprise is another beast entirely. In an enterprise setting, stakes are h=
igh =E2=80=93 mistakes can cost millions or tarnish reputation. Let=E2=80=
=99s explore the challenges (and solutions) that arise when scaling RAG to =
enterprise level: horror stories to avoid, how to handle millions of querie=
s, keeping data secure/compliant, and optimizing costs.
The =E2=80=9CFrankenstein RAG=E2=80=9D horror story (and how to avoid it): =
Picture a patchwork of AI components: one team built a vector index, anothe=
r wired up a chatbot UI, a consultant added a custom reranker, and nobody t=
hought through the overall architecture. The result? A monster that=E2=80=
=99s hard to maintain, with inconsistent answers and mysterious failures =
=E2=80=93 a Frankenstein RAG. One enterprise recounted how their first atte=
mpt at an AI assistant integrated 5 different services (some on-prem, some =
cloud) with brittle connections. It worked in demos, but under load it coll=
apsed =E2=80=93 context dropouts, timeouts between components, and absolute=
ly no single source of truth for debugging. To avoid this, design an end-to=
-end architecture early. Decide: will you use an all-in-one solution (some =
vendors now offer full RAG platforms), or a carefully orchestrated pipeline=
? Document how data flows and where each piece lives. Ensure observability =
=E2=80=93 logs at each stage (retrieval logs, LLM input/output logs). Frank=
enstein systems often die because no one can tell which stitch ripped when =
it fails.
Scaling from 10 to 10 million queries: When your query volume grows, watch =
out for two bottlenecks: the vector database and the LLM API. Vector DBs, i=
f self-hosted, might need sharding or upgrades =E2=80=93 e.g., going from a=
 single node to a cluster. Many solutions can scale to millions of vectors =
(Milvus, Elastic, etc., scale horizontally), but queries per second (QPS) i=
s the real kicker. If you expect, say, 100 QPS, ensure your DB can handle t=
hat with <50ms each. Sometimes that means adding replicas (multiple instanc=
es serving the same index) to share query load. Meanwhile, LLM throughput m=
ight be a limit =E2=80=93 calling an API like OpenAI for each query might g=
et expensive or slow. Enterprises often implement caching: if the same ques=
tion gets asked often, cache the answer. Or use a smaller local model for s=
ome queries. One advanced approach is a cascading model deployment: try ans=
wering with a fine-tuned 7B model internally; if it=E2=80=99s confident, re=
spond, if not, fall back to GPT-4. This saved one company 30% of costs, for=
 example. Another scaling aspect is monitoring performance =E2=80=93 with m=
illions of queries, even a 99% accuracy means thousands of bad answers. Log=
ging and automated alerts (like if accuracy on a sliding window of queries =
dips below X, flag it) can catch issues quickly.
Security deep-dive: Enterprises care deeply about data security and complia=
nce (HIPAA for health data, SOC2, GDPR in EU, etc.). RAG systems must be de=
signed so that sensitive data doesn=E2=80=99t leak. Some considerations:
Access controls: If different users should only see certain data, the RAG n=
eeds to filter retrieval by permissions. For example, an employee asking ab=
out HR policy can see internal policies, but a client using a chatbot shoul=
d not retrieve those. That means integrating your vector store with an ACL =
(access control list) =E2=80=93 for instance, include user roles in metadat=
a and filter query results accordingly.
PII scrubbing: If logs might contain personal data (names, addresses), you =
should either avoid logging full text or have a process to scrub them for a=
nalysis. Similarly, when feeding content to external APIs (OpenAI, etc.), y=
ou might need to avoid sending truly sensitive info unless you have agreeme=
nts in place. Solutions include using on-prem LLMs for highly sensitive dat=
a, or at least encrypting certain fields.
Retention and Right to be Forgotten (GDPR): If a user deletes their data, a=
nd that data was in the knowledge base, you have to remove it from the inde=
x too. This means building a mechanism to update or delete vectors. Many ve=
ctor DBs support deletion by ID =E2=80=93 so track which chunk IDs correspo=
nd to, say, a user=E2=80=99s data, and be able to wipe them. Also re-chunk =
and re-index periodically for content updates.
Auditability: Who asked what and got what answer? Enterprises might need lo=
gs that show the chain: user query -> docs retrieved -> answer. If an answe=
r is challenged legally (=E2=80=9CYour AI gave wrong financial advice!=E2=
=80=9D), you need to reproduce what it saw and why it answered that way. St=
oring query and retrieval traces with timestamps is thus important.
Model filtering: Use the model=E2=80=99s tools too =E2=80=93 e.g., OpenAI h=
as moderation APIs; you can run the final answer through that to ensure no =
disallowed content. If building your own model, have toxicity filters etc.,=
 in place. This prevents an malicious user from getting the AI to spill sec=
rets or produce harassment by carefully poisoning a query.
A big reassurance: RAG can actually help with compliance compared to raw LL=
Ms. Because the AI is constrained to use provided data, it=E2=80=99s less l=
ikely to wander into areas it shouldn=E2=80=99t. Also, you can ensure that,=
 say, medical AI only references approved medical literature =E2=80=93 redu=
cing risk of non-compliant advice.
Cost optimization: RAG can reduce costs by answering with smaller models or=
 fewer API calls, but it can also introduce its own costs (vector DB hostin=
g, embedding generation, etc.). An interesting case: one company was using =
GPT-4 for everything at maybe $0.06 per query. They realized many queries a=
re simple FAQs that GPT-3.5 or even a fine-tuned smaller model could handle=
=2E They implemented a two-tier system:=
 attempt answer with GPT-3.5 (cost $0.=
002) along with retrieved context. Only if certain uncertainty triggers are=
 hit (like low similarity score or user follow-up suggests dissatisfaction)=
 do they escalate to GPT-4 for a refined answer or second attempt. This sav=
ed about 90% of their previous costs, which over millions of queries was ab=
out $2M/year in savings, while keeping answer quality high (the trick was t=
uning the handoff threshold carefully). This is akin to a =E2=80=9Ccascadin=
g model deployment=E2=80=9D I mentioned.
Another tactic: optimize embeddings. Calling OpenAI=E2=80=99s embed API for=
 each new doc chunk can add up. Some open-source embedding models (like Ins=
tructorXL) can be run one-time to embed all docs in-house, saving money at =
the expense of requiring some GPU compute. For ongoing usage, ensure you on=
ly embed new/changed content, not re-embed everything needlessly.
Case study: RBC revolutionizing banking support with RAG. The Royal Bank of=
 Canada (RBC) developed Arcane, an internal Retrieval-Augmented Generation =
(RAG) system designed to help specialists quickly locate relevant investmen=
t policies across the bank=E2=80=99s internal platforms. Arcane indexes pol=
icy documents and other semi-structured data, such as PDFs, HTML, and XML, =
and uses advanced embedding models to enable precise retrieval of informati=
on. This system significantly improves productivity by allowing specialists=
 to find complex policy details in seconds, streamlining access to informat=
ion that previously took years of experience to master. The development of =
Arcane involved addressing challenges related to document parsing, context =
retention, and security, including robust privacy and safety measures to pr=
otect proprietary financial information. The system=E2=80=99s success demon=
strates how AI can enhance decision-making and operational efficiency in la=
rge financial institutions
This illustrates how an enterprise might integrate RAG internally first (fo=
r employees). Many do that to mitigate risk vs. a public-facing bot. Then g=
radually, as confidence grows, they roll out to customers. Morgan Stanley, =
for instance, built a GPT-4 RAG on their wealth management knowledge base (=
internal) to assist advisors =E2=80=93 a similar idea of revolutionizing su=
pport with verified info.
In enterprise, also think about fallbacks: If the AI isn=E2=80=99t confiden=
t, have a graceful handoff (e.g., =E2=80=9CI=E2=80=99m not sure, let me con=
nect you to a human.=E2=80=9D). Not every query should be answered by AI if=
 it=E2=80=99s risky.
Finally, consider that enterprise RAG is a team sport: involve IT for data =
pipelines, involve legal for compliance, involve domain experts to curate c=
ontent. It=E2=80=99s not just a lab experiment =E2=80=93 it touches many fa=
cets of the business. With careful design, you=E2=80=99ll avoid the Franken=
stein and instead get a robust, scalable, secure RAG deployment that your w=
hole company trusts.
Next up: we=E2=80=99ll compare RAG to another rising approach =E2=80=93 Age=
ntic search =E2=80=93 and discuss when to use which, and how they might con=
verge.
RAG vs Agentic Search: The $40 Billion Question
There=E2=80=99s a hot debate in the AI world: should you use Retrieval-Augm=
ented Generation (RAG) or an AI Agent that can search and reason on its own=
 (often called Agentic search, like AutoGPT or similar systems)? They have =
different strengths. Let=E2=80=99s break down the fundamental difference, s=
ome performance considerations (85% vs 95% accuracy type of trade-off), and=
 a decision framework for when to use each =E2=80=93 and glimpse into a fut=
ure where they merge.
RAG retrieves and answers; Agents think, plan, then answer. In essence, a R=
AG system is like a smart lookup combined with an answer generator. It=E2=
=80=99s relatively straightforward: given a query, it retrieves relevant in=
fo and produces an answer grounded in that info. An agentic approach (like =
the ReAct pattern or say a tool-using agent) will treat the query as a task=
 it needs to solve possibly through multiple steps: it might search (multip=
le times even), analyze intermediate results, maybe use a calculator or cal=
l an API, and finally give an answer. The agent has a kind of mini-planner =
built in (often the LLM itself does the planning via prompts like =E2=80=9C=
Thought: I should search X=E2=80=A6 Action: search=E2=80=A6 Observing resul=
ts=E2=80=A6 Thought: now I got this, final answer=E2=80=A6=E2=80=9D).
So difference: RAG =3D single retrieval round then answer; Agent =3D could =
be multiple retrievals + reasoning steps. Agents shine when a query is comp=
licated or requires combining info from different places. Example: =E2=80=
=9CCompare the growth of Apple vs Microsoft in the last quarter and give me=
 a trend=E2=80=9D =E2=80=93 an agent might do two searches, get data for Ap=
ple, get data for Microsoft, maybe do a quick calculation or summary, then =
answer. A single-shot RAG might struggle to gather all that in one go (unle=
ss the info is conveniently in one doc).
Performance shootout: 85% vs 95% (but at what cost?). Let=E2=80=99s say on =
a certain set of complex questions, a basic RAG system gets ~85% accuracy =
=E2=80=93 it fails on multi-hop or when reasoning is needed across docs. An=
 agentic approach (which can plan, do multi-hop retrievals, use a calculato=
r etc.) might achieve 95% accuracy on those because it can do more steps. H=
owever, the cost is typically latency and complexity: that 95% might come w=
ith, say, an average of 5 tool calls (searches or calculations), each addin=
g latency. So maybe the agent=E2=80=99s answers take 10 seconds instead of =
1 second. Also each step might call an API (cost), and it=E2=80=99s harder =
to guarantee what the agent will do (could go down a rabbit hole or get stu=
ck).
There=E2=80=99s also a reliability factor =E2=80=93 RAG is relatively deter=
ministic (retrieve best match and answer), whereas agents have more moving =
parts that can go wrong (like choosing a wrong search query and never findi=
ng the right info, or looping). Some academic evaluations have noted that w=
hile agents can theoretically solve more, they sometimes fail in unpredicta=
ble ways, making their overall reliability not clearly higher than a well-t=
uned RAG on simpler tasks. Think of it like: RAG is a bicycle =E2=80=93 sim=
ple, robust; an Agent is a car =E2=80=93 can go further and faster, but mor=
e ways to break down.
When to use each approach:
Use RAG when your queries are well-covered by existing knowledge bases and =
typically only need one round of retrieval. If the task is primarily Q&A or=
 straightforward decision support where the needed info is easily identifie=
d with keywords, RAG is efficient and less error-prone. RAG is also usually=
 easier to implement and cheaper to run. For instance, a documentation chat=
bot or a legal assistant that fetches relevant laws =E2=80=93 those work gr=
eat with RAG because one query =3D find relevant clause =3D answer.
Use an Agentic approach when tasks are more complex, like those involving:
Multi-step reasoning: e.g., =E2=80=9CFirst find X, then use X to get Y=E2=
=80=9D.
Tool use beyond text retrieval: e.g., need to call an API, do math, interac=
t with the environment. Agents can interface with calculators, databases, o=
r even execute code.
Exploratory search: where the query isn=E2=80=99t well-defined. An agent ca=
n search iteratively, refining the query like a human researcher might.
Planning tasks: not just answering a question, but deciding a sequence of a=
ctions (like booking travel given constraints =E2=80=93 search flights, com=
pare, etc.).
However, if you can achieve the goal with a simpler RAG, do it =E2=80=93 be=
cause agents bring overhead.
Hybrid future =E2=80=93 Agentic RAG: It=E2=80=99s very likely that we won=
=E2=80=99t be choosing RAG vs Agents as mutually exclusive in the future, b=
ut rather combining them. For example, you might have an agent whose availa=
ble =E2=80=9Ctools=E2=80=9D include a vector search (RAG) and other APIs. I=
t can then reason (=E2=80=9CTool: use knowledge_base_search for query X=E2=
=80=9D) to fetch something, then reason more, etc. This is already happenin=
g in some frameworks (LangChain agents often use a vector search as one of =
the tools).
2026 might indeed be the year of Agentic RAG, where autonomous agents are a=
ugmented with retrieval. Anthropic=E2=80=99s prediction that AI might surpa=
ss Nobel laureates by 2026 probably assumes systems that can both recall va=
st knowledge (via retrieval) and reason through novel problems (via plannin=
g).
We asked: RAG vs Agent =E2=80=93 it=E2=80=99s like asking screwdriver vs po=
wer drill. One is manual but simple (RAG), the other is powerful but requir=
es more care and power (Agents). For a $40B question (the forecast size of =
this AI orchestration market), the answer is: both will be used, often toge=
ther, depending on the task complexity. If accuracy needs are ~85% and resp=
onse must be instant, lean on RAG. If you need 95%+ and can afford some sec=
onds, incorporate agent capabilities.
One more angle: agents often use RAG internally anyway. For example, an age=
nt might decide to use a Bing search tool =E2=80=93 that=E2=80=99s basicall=
y retrieval, just unstructured. Some research combined ReAct (reasoning) wi=
th RAG and got excellent results =E2=80=93 e.g., the medical QA study where=
 GPT-4 with RAG and some agentic steps hit 95% accuracy. Pure RAG was 94% w=
ith Llama 70B, interestingly, showing that a strong model with retrieval ca=
n nearly match an agentic GPT-4 on that task. But the agentic GPT-4 edged i=
t out to 95%.
So you see, differences can be subtle. It often comes down to diminishing r=
eturns: maybe going from 85 to 95% requires quadruple the effort (multi-ste=
p, bigger model, etc.). If that last 10% is mission-critical (like diagnosi=
ng a patient correctly vs missing something), it=E2=80=99s worth it. If not=
, a simpler approach might suffice.
Decision framework in practice:
Start with RAG (simple, one-shot). Evaluate performance on task.
If you see frequent failures that are multi-hop in nature, consider adding =
an agent loop to handle those (maybe as a fallback).
If numerical accuracy is an issue, consider adding a calculation tool to th=
e chain (agent style).
Evaluate again. Always weigh complexity vs gain.
If domain requires using external services (like checking inventory, sendin=
g an email), you=E2=80=99ll need an agent style anyway (RAG alone can=E2=80=
=99t interact with environment).
Finally, think of user experience: For a chat interface, an agent taking 10=
 seconds might be okay if it=E2=80=99s a heavy question. For real-time sear=
ch queries, 10s is too slow. So use RAG where immediacy matters.
In 2026 and beyond, I predict we=E2=80=99ll see Agentic RAG systems becomin=
g standard: LLMs that automatically retrieve info as needed and also perfor=
m multi-step reasoning. They won=E2=80=99t call it two separate things; it=
=E2=80=99ll just be =E2=80=9Cautonomous AI agents=E2=80=9D that have a know=
ledge base backbone. And as mentioned in Future Shock, context windows goin=
g 1M+ will blur the lines =E2=80=93 if you can stuff an entire knowledge ba=
se in the context, is that RAG or just huge memory? It becomes a continuum.
But until then, decide case by case. You have a powerful hammer in RAG and =
a Swiss Army tool in Agents. Use the right one for the job =E2=80=93 or com=
bine them for maximum effect. And keep an eye on that $40B market =E2=80=93=
 a lot of it will be won by those who figure out the optimal mix of retriev=
al and reasoning in their AI solutions.
Next, we venture into some advanced patterns =E2=80=93 the secret sauce tha=
t cutting-edge RAG practitioners are using to push towards 99% accuracy and=
 beyond.
When RAG fails: Real-world limitations of retrieval-augmented generation
Retrieval-Augmented Generation (RAG) faces significant real-world limitatio=
ns that have led companies to choose simpler alternatives, with documented =
failures showing latency increases from 15 to 180 seconds under load, compl=
iance barriers in regulated industries preventing implementation entirely, =
and cost-benefit analyses revealing that fine-tuning can be more economical=
 for stable, high-volume applications despite RAG's 36% lower annual costs =
in some scenarios.
While RAG promises to enhance large language models with external knowledge=
, extensive research reveals critical failure modes and inappropriate use c=
ases. Academic studies document seven distinct failure points in RAG system=
s, from missing content to incomplete answers. Industry practitioners repor=
t RAG being "brittle" with "no science to it" after year-long implementatio=
ns. Performance benchmarks show RAG doubles time-to-first-token latency fro=
m 495ms to 965ms, making it unsuitable for voice applications requiring sub=
-150ms responses. In regulated sectors, HIPAA compliance, attorney-client p=
rivilege, and SOC2 requirements create insurmountable barriers, forcing org=
anizations like Morgan Stanley to build custom on-premises solutions. Cost =
analyses reveal that while RAG offers lower upfront investment, operational=
 expenses can exceed $10,000 monthly for enterprise deployments, leading co=
mpanies with stable knowledge bases to choose fine-tuning despite higher in=
itial costs of $100,000+ for small models.
The seven ways RAG systems fail in production
Academic research from Scott Barnett and colleagues at the 3rd Internationa=
l Conference on AI Engineering identified seven distinct failure points thr=
ough analysis of three case studies across research, education, and biomedi=
cal domains. Their study, using the BioASQ dataset with 15,000 documents an=
d 1,000 Q&A pairs, revealed systematic problems that plague RAG implementat=
ions:
Missing content hallucination - Questions cannot be answered from available=
 documents, yet the system provides hallucinated responses instead of admit=
ting uncertainty.
Missed the top-K - Correct answers exist in the corpus but rank too low in =
retrieval results, falling outside the top-K threshold and never reaching t=
he language model.
Bundled up wrong - Retrieved documents containing answers get filtered out =
during the aggregation phase due to poor scoring mechanisms in consolidatio=
n strategies.
Extraction failure - When relevant content does reach the model, extraction=
 failures occur because the LLM struggles with noisy or conflicting informa=
tion in the provided context.
Wrong format - Systems generate correct content but fail to match expected =
structures, rendering the information unusable for downstream applications.
Incorrect specificity - Responses suffer from incorrect specificity levels,=
 providing either overly general answers when precision is needed or inappr=
opriately detailed responses to broad questions.
Incomplete - Systems often generate incomplete answers despite having all n=
ecessary information in the retrieved context, suggesting fundamental issue=
s with how RAG systems process and synthesize information.
Real-time applications abandon RAG due to prohibitive latency
Performance benchmarks reveal RAG's fundamental incompatibility with real-t=
ime applications, where retrieval overhead accounts for 41-47% of total lat=
ency. Systems-level characterization studies show RAG nearly doubles time-t=
o-first-token latency from 495ms to 965ms compared to baseline language mod=
els, with P99 latency showing 50ms additional overhead for retrieval stages=
 alone.
Voice applications face the most severe constraints, with ITU-T standards r=
ecommending 100ms latency for interactive tasks and 150ms for conversationa=
l use cases. Industry experts confirm "achieving a one-way latency of 150ms=
 is almost impossible with a RAG-like architecture where several components=
 are involved in voice processing." Even optimized systems struggle to achi=
eve 500ms response times for voice-to-voice interactions, far exceeding use=
r tolerance thresholds.
High-frequency trading systems represent the extreme end of latency sensiti=
vity, requiring microsecond-level responses that make RAG architectures com=
pletely unsuitable. Financial firms report that "milliseconds can make the =
difference between profit and loss," leading them to implement custom FPGA-=
based solutions instead of any AI-based retrieval systems. Gaming applicati=
ons similarly avoid RAG for real-time character interactions, preferring tr=
aditional scripted responses that guarantee predictable performance.
Under production loads, performance degradation becomes catastrophic. Engin=
eering teams report average execution time increasing from 15 seconds to 18=
0 seconds when scaling from 5 to 50 concurrent users. Naive re-retrieval im=
plementations can push end-to-end latency to nearly 30 seconds, "precluding=
 production deployment" according to academic studies. Even with aggressive=
 optimization, including semantic caching achieving 100ms response times fo=
r cache hits, the 30% cache hit rate assumption in real-world scenarios fai=
ls to meet the stringent requirements of real-time applications.
Regulatory barriers prevent RAG adoption in sensitive sectors
Healthcare organizations face insurmountable compliance challenges when imp=
lementing RAG systems, with HIPAA's Protected Health Information requiremen=
ts creating fundamental conflicts with vector database architectures. The c=
ore issue stems from embeddings potentially being reverse-engineered to rev=
eal original patient data, while most cloud-based RAG providers fail to off=
er HIPAA-compliant Business Associate Agreements.
The healthcare sector's struggles extend beyond basic compliance. HIPAA's "=
minimum necessary" rule directly conflicts with RAG systems' need for exten=
sive context to improve accuracy. Vector databases storing medical embeddin=
gs lack sufficient access controls for compliance, while third-party embedd=
ing providers like OpenAI and Cohere provide inadequate protections for PHI=
=2E Audit trail requirements for health=
care data access prove difficult to ma=
intain in distributed RAG architectures, forcing organizations to abandon i=
mplementations or build costly custom solutions.
Legal firms encounter similar barriers with attorney-client privilege creat=
ing absolute requirements for data confidentiality. The American Bar Associ=
ation Model Rules require lawyers to ensure non-lawyers, including AI syste=
ms, comply with professional conduct rules. A mid-sized law firm with 150+ =
staff reported "significant challenges" implementing GenAI, with only 25% o=
f attorneys actively using AI tools due to compliance concerns. Major AmLaw=
100 firms have resorted to developing custom, on-premises RAG solutions rat=
her than risk client data exposure through cloud services.
Financial services face a complex web of regulations including SOC2 Type II=
 requirements demonstrating control effectiveness over time, which proves c=
hallenging with rapidly evolving RAG systems. One financial institution aba=
ndoned RAG implementation entirely, returning to relational databases after=
 discovering vector databases lacked required compliance controls. The comb=
ination of PCI DSS for payment data, Basel III for risk assessment, and SEC=
 requirements for explainable AI creates an environment where traditional d=
atabase architectures remain the only viable option for many use cases.
Cost reality: When fine-tuning beats RAG economics
Detailed cost analyses reveal RAG's economic proposition varies dramaticall=
y based on use case and scale, with enterprise deployments reaching $10,000=
+ monthly for infrastructure alone. While RAG offers lower upfront investme=
nt compared to fine-tuning's $100,000+ for small models, operational expens=
es quickly accumulate through multiple cost centers.
RAG implementation incurs ongoing charges of $0.10 per million tokens for e=
mbedding generation using OpenAI's ada-2 model, $120 monthly for vector dat=
abase storage of approximately 5 million tokens on Pinecone, and $500 month=
ly for standard AWS EC2 instances. High-volume production systems processin=
g 10 million input tokens plus 3 million output tokens daily face $480 in A=
PI costs alone. Hidden expenses multiply through network bandwidth at $0.09=
 per GB, monitoring at $0.30 per GB for logs, and data engineering at $100 =
per hour for maintenance.
Fine-tuning presents a contrasting cost structure with high initial investm=
ent but lower operational expenses. LLAMA 2 fine-tuning totaled $723 over a=
 15-day period processing 10 million tokens, compared to $1,186 for equival=
ent RAG usage. Once deployed, fine-tuned models eliminate per-query retriev=
al costs and API dependencies, providing predictable pricing crucial for bu=
dgeting. Companies with stable knowledge bases updating yearly or less freq=
uently find fine-tuning's one-time cost preferable to RAG's ongoing infrast=
ructure expenses.
Real-world implementations demonstrate these trade-offs clearly. An e-comme=
rce company achieved 36% annual cost savings using RAG over traditional AI =
approaches through improved query efficiency, reaching break-even within th=
ree months. Conversely, organizations with high-volume, low-latency require=
ments report fine-tuned models delivering better economics at scale due to =
eliminated retrieval overhead and lower per-query costs. The decision ultim=
ately depends on data update frequency, query volume, latency requirements,=
 and whether source citations justify RAG's additional complexity and cost.
The hidden selection bias in RAG success stories
Perhaps the most revealing finding from extensive research across engineeri=
ng blogs, conference proceedings, and technical forums is the conspicuous a=
bsence of documented RAG rejection cases. Despite searching major tech comp=
any engineering blogs from Uber, Airbnb, Netflix, and Spotify, along with M=
edium publications and GitHub discussions, researchers found virtually no p=
ublic documentation of companies explicitly choosing simpler solutions over=
 RAG.
This silence speaks volumes about selection bias in the AI implementation l=
andscape. Companies successfully implementing RAG actively share their expe=
riences through blog posts and conference talks, while those encountering i=
nsurmountable challenges either never attempt implementation after initial =
assessment or quietly abandon failed projects without public disclosure. Th=
e few practitioners willing to discuss failures anonymously report RAG bein=
g "more of a problem than a solution" after year-long implementations, citi=
ng brittleness and lack of systematic approaches to hyperparameter tuning.
Community discussions reveal widespread frustration hidden beneath the vene=
er of published success stories. Reddit's r/MachineLearning community docum=
ents common issues including performance degradation as document volumes in=
crease, retrieved chunks getting "pushed down" in rankings, and no sustaina=
ble solution for optimizing top-K thresholds. HackerNews engineering discus=
sions expose sequential processing bottlenecks in popular frameworks like L=
angChain, with developers discovering double billing in ConversationalRetri=
evalChain due to automatic query rephrasing.
The absence of formal "RAG rejection" documentation suggests companies choo=
sing simpler solutions frame decisions as "right-sizing" rather than failur=
e, avoiding potential embarrassment or competitive disadvantage from public=
ly acknowledging limitations. This creates a distorted view of RAG's applic=
ability, where published literature overwhelmingly features success stories=
 while failures remain buried in private Slack channels and closed-door eng=
ineering meetings. The AI community would benefit significantly from transp=
arent sharing of implementation decisions, including cases where prompt eng=
ineering or fine-tuning proved superior to complex retrieval systems.
Wrapping up =E2=80=94 when not to use RAG
The documented failures, performance limitations, compliance barriers, and =
economic realities of RAG systems reveal a technology often misapplied to p=
roblems better solved through simpler approaches. While RAG excels for spec=
ific use cases requiring fresh external knowledge and source citations, the=
 seven failure points identified by academic research, latency overhead mak=
ing real-time applications impossible, regulatory barriers in sensitive ind=
ustries, and complex cost structures limiting economic viability demonstrat=
e why many organizations ultimately choose alternatives. The striking absen=
ce of published RAG rejection stories suggests a broader industry challenge=
 with transparent failure documentation, leaving practitioners to rediscove=
r these limitations through costly trial and error. Success with RAG requir=
es careful evaluation of whether its benefits justify the documented comple=
xity, with many finding that prompt engineering for simple queries, fine-tu=
ning for stable domains, or traditional databases for regulated industries =
provide more practical solutions.
Advanced Patterns: The Secret Sauce
By now, you know the fundamentals of RAG. Ready to go further down the rabb=
it hole? In this section, we=E2=80=99ll explore advanced patterns that can =
supercharge your RAG system =E2=80=93 the kind of techniques at the bleedin=
g edge of research and industry practice. Buckle up for GraphRAG, Recursive=
 RAG, Multi-modal mastery, MCP (Model-Context Protocol), and Hybrid search =
deep-dive. These are the secret sauce ingredients that can take accuracy fr=
om great to jaw-dropping, handle complex data types, and make your AI an ev=
en smarter data hound. We=E2=80=99ll also include code snippets and simple =
diagrams to clarify some of these concepts.
GraphRAG: Microsoft=E2=80=99s 90%+ Accuracy Breakthrough
What if your knowledge base isn=E2=80=99t just unstructured text, but also =
relationships =E2=80=93 like a knowledge graph? Enter GraphRAG, an approach=
 pioneered by Microsoft that blends knowledge graphs with RAG. In a GraphRA=
G, you use a structured graph of entities and their relations to inform ret=
rieval. For example, LinkedIn=E2=80=99s support RAG built a graph of relate=
d support tickets (linking similar issues, or linking a ticket to a product=
 feature). The result: instead of retrieving isolated text chunks, the syst=
em could retrieve a whole subgraph of connected information. This preserved=
 context and relationships that plain chunking would lose. LinkedIn reporte=
d a 77.6% improvement in retrieval accuracy (MRR) and a 28.6% reduction in =
median resolution time after incorporating knowledge graph relations =E2=80=
=93 effectively reaching new heights in relevant results and efficiency.
Numbers reported vary but hover between 90-93%? Why this high? Because Grap=
hRAG can ensure that if a question involves multiple entities or steps, the=
 graph guides the retrieval to the right linked pieces. Microsoft Research =
showed demos where GraphRAG answered broad analytical questions with astoni=
shing precision because it could traverse a graph of concepts rather than j=
ust do keyword matching. Imagine asking =E2=80=9CHow are enzymes X and Y re=
lated in disease Z?=E2=80=9D =E2=80=93 a GraphRAG system might have a biosc=
ience knowledge graph linking X -> pathway -> Y in context of Z, retrieve t=
hat subgraph, and give a highly accurate answer pulling those connections, =
whereas plain RAG might retrieve separate facts and risk missing the link.
How do you implement GraphRAG? Typically:
Build or integrate a knowledge graph (e.g., from existing database or by en=
tity extraction from text).
When a query comes, identify key entities (you might use an NER model or a =
heuristic).
Use the graph to find related entities or relevant nodes. This gives you a =
set of candidate nodes.
Retrieve text associated with those nodes (could be definitions or document=
s connected to them).
Feed that into the LLM to generate answer.
It=E2=80=99s like giving the LLM a map of the knowledge landscape instead o=
f just a list of documents. The wow moment here: GraphRAG enabled near-perf=
ect answers in domains like technical support by preserving relationships t=
hat text chunking lost. It also makes answers more explainable: since you h=
ave a graph, you can visualize the chain of reasoning (like =E2=80=9CIssue =
A is related to B, which causes C =E2=80=93 thus solution is =E2=80=A6=E2=
=80=9D).
A quick code concept (not real, just illustrative):
# Pseudo-code for GraphRAG retrieval
entities =3D entity_extraction(query) # e.g., ["enzyme X", "enzyme Y", "dis=
ease Z"]
subgraph =3D graph.get_neighbors(entities) # get connected nodes and edges
related_docs =3D []
for node in subgraph.nodes:
related_docs +=3D text_index.search(node.name) # find docs about that entit=
y
# Now we have related_docs from graph context
answer =3D llm.generate(query, context=3Dcombine(related_docs))
This is simplified =E2=80=93 a real one might use graph traversal algorithm=
s and also embed graph node descriptions. But it shows the idea.
GraphRAG is especially useful in enterprise where you often have structured=
 data (like product catalogs, org charts, etc.). Instead of flattening ever=
ything to text, use that structure to inform retrieval. Microsoft has an in=
ternal system (=E2=80=9CProject Discovery=E2=80=9D) doing this =E2=80=93 re=
sults were so good they integrated it into some of their products.
Recursive RAG: When One Retrieval Isn=E2=80=99t Enough
Sometimes one round of retrieval doesn=E2=80=99t cut it. Perhaps the initia=
l query is high-level, and only after reading an initial doc can the AI for=
m a more precise follow-up query. Recursive RAG is about doing retrieval in=
 multiple iterations. It=E2=80=99s like an agent, but specifically focusing=
 on retrieval.
For example, a user asks: =E2=80=9CWhat were the key findings of the health=
 inspector=E2=80=99s report for the restaurant I visited last night?=E2=80=
=9D =E2=80=93 The system might first retrieve something about who/when (may=
be identify the restaurant and find a link to an inspector report). That do=
cument is lengthy and has codes. The AI might then ask itself, =E2=80=9CHmm=
, what does the user really want? Probably summary of violations.=E2=80=9D =
It then formulates a sub-query: =E2=80=9CSummarize violations from Inspecto=
r Report ID 123.=E2=80=9D And retrieves sections of the report about violat=
ions. Then it gives the final answer.
In practice, implementing recursive RAG could mean:
The LLM is prompted to decide if more info is needed. If yes, have it outpu=
t a refined query.
Use that query to retrieve again, then either answer or even loop once more=
 if needed.
This is basically an internal Q&A: use RAG to answer parts of the question =
which feed into answering the main question. Tools like LlamaIndex have que=
ry transforms that can do something akin to this (e.g., query an index, use=
 result to query another index).
Under the Hood example:
User asks a question involving a chain of reasoning: =E2=80=9CIs the device=
 from Order #12345 still under warranty and how to claim it?=E2=80=9D The a=
ssistant might:
Recognize it needs order details -> retrieve Order #12345 info (which inclu=
des device model and purchase date).
From that info, figure out purchase date, then query warranty policy for th=
at device/model and date.
Get answer that warranty valid or not, plus procedure.
Finally, compose answer.
This chain is a recursive retrieval: first get order data, then use that to=
 get policy data. This could be done with an agent approach naturally. But =
if you constrain it within RAG, you might pre-index different data types se=
parately (orders vs policies) and then orchestrate queries. Possibly an age=
nt is easier here, but sometimes domain-specific logic (like linking an ord=
er to a policy) can be hard-coded or handled with simple recursion.
Benefits: This iterative retrieval can dramatically improve accuracy on com=
plex queries because it ensures the context the LLM gets is highly relevant=
 at each step. It also breaks a big problem into chunks which is easier on =
the model (no need to jam everything in one huge context). The drawback is =
increased latency (multiple search calls).
Real example =E2=80=93 Bing=E2=80=99s multi-hop QA: Bing (with GPT-4) often=
 does this: it will search something, then from the results, search another=
 related thing, etc., before answering =E2=80=93 effectively a recursive RA=
G with an agent.
Takeaway: If you find your RAG failing on questions that involve multiple p=
ieces of info from disparate sources, consider a recursive retrieval strate=
gy. It can be as straightforward as doing 2 passes: broad retrieval for can=
didates, then specific retrieval focusing on one candidate.
Multi-modal Mastery: Processing Invoices, Diagrams, and Videos
Who says RAG is only for text? Multi-modal RAG is about bringing in images,=
 audio, and beyond into the retrieval-generation loop. Imagine processing a=
n invoice PDF that contains a company logo (image), line items (table), and=
 terms (text). A multi-modal RAG system could index the text AND perhaps an=
 extracted table structure or images of signatures, etc.
Use cases:
Invoices and receipts: Use OCR to extract text (for RAG on text), but also =
possibly embed the image of the receipt for visual details (like a handwrit=
ten note maybe).
Diagrams: Suppose you have an architecture diagram image and an AI has to a=
nswer questions about system architecture. You could use an image embedding=
 model to allow retrieving the diagram image when relevant, and maybe gener=
ate a caption for it as context.
Videos: A support knowledge base might include how-to videos. You can trans=
cribe the audio (making it text, then index). For an image (frame) that con=
tains crucial info, you could use image captions or tags as metadata. If us=
er asks =E2=80=9CWhere in the video do they mention resetting the router?=
=E2=80=9D, you could even retrieve the timestamp via searching the transcri=
pt.
There=E2=80=99s also the concept of multi-modal queries =E2=80=93 user migh=
t input an image and ask a question about it combined with text. For exampl=
e, =E2=80=9CIs this component (image) compatible with the product described=
 in spec X?=E2=80=9D A multi-modal RAG would need to identify what=E2=80=99=
s in the image (maybe with image recognition), find relevant product info f=
rom text, then answer.
How to implement: Most vector DBs now support storing vectors of different =
modalities if they are same dimensionality. You could use OpenAI=E2=80=99s =
CLIP or CLIP-like models to embed images, and store those vectors with meta=
data. If user query is text about an image, you might not directly combine,=
 but if query includes an image, you embed the image and search the image i=
ndex, etc. Alternatively, you can convert images to text (e.g., =E2=80=9Cdi=
agram of supply chain=E2=80=9D) via captioning and treat it as extra text d=
oc.
Example code snippet:
# Pseudocode: Indexing images and text together
image_embeddings =3D []
for img_path in image_files:
vec =3D image_embedder.embed_image(img_path)
image_embeddings.append({"vec": vec, "metadata": {"type": "image", "file": =
img_path}})
# assume text_docs is list of texts
text_embeddings =3D []
for doc in text_docs:
vec =3D text_embedder.embed_text(doc)
text_embeddings.append({"vec": vec, "metadata": {"type": "text", "content":=
 doc[:100]}})
# store both in one index
index =3D VectorIndex(image_embeddings + text_embeddings)
At query time, if query is text, you might search text index primarily but =
could allow cross-modal search if appropriate (=E2=80=9Cdiagram=E2=80=9D in=
 query might boost image search).
An inspiring case: A legal RAG system that processed millions of pages of d=
ocuments and also diagrams (like patents often have drawings). They found t=
hat including the figure captions in the index and having the ability to re=
trieve the figure image by caption reference improved user satisfaction =E2=
=80=93 attorneys could quickly get the relevant figure when asking about a =
specific concept in the patent.
Another advanced pattern in multi-modal: Audio RAG. Suppose support calls a=
re recorded =E2=80=93 you can transcribe them and use RAG on that so your A=
I can recall what was said in a previous call (=E2=80=9CThe customer said l=
ast week their internet was intermittent.=E2=80=9D and the AI uses that con=
text this week).
So, multi-modal mastery is about expanding the knowledge sources beyond pla=
in text, which can broaden the AI=E2=80=99s capabilities. It=E2=80=99s espe=
cially crucial as enterprises often have important info locked in PDFs with=
 charts, or manuals with images, etc. The good news: the retrieval part mos=
tly stays the same =E2=80=93 it=E2=80=99s about extracting and embedding th=
ose modalities appropriately.
The MCP Revolution: How Anthropic=E2=80=99s Protocol Changes Everything
Earlier we mentioned Anthropic=E2=80=99s Model-Context Protocol (MCP) . Thi=
nk of MCP as a standardized way to connect an AI model to external data sou=
rces (web, databases, etc.) =E2=80=93 essentially a formalization of RAG an=
d tool use. Anthropic dubs it the =E2=80=9CUSB-C for AI=E2=80=9D because it=
=E2=80=99s an open protocol aiming to make hooking up any data to any model=
 plug-and-play.
Why is this a big deal? Today, building RAG or agent systems is somewhat cu=
stom: you wire specific calls in code. MCP aims to define a common interfac=
e: an AI can say (in a structured way) =E2=80=9CHey, I need data from X=E2=
=80=9D, and any MCP-compatible data source can respond. This two-way connec=
tion allows AI to maintain context across systems seamlessly . It also emph=
asizes security and standardization =E2=80=93 rather than each dev worrying=
 about leaking data to the model, MCP will have guidelines and methods to s=
afely transmit only what=E2=80=99s needed.
Imagine a future where, instead of building custom retrieval code, you simp=
ly point your AI at an MCP server which exposes, say, your company=E2=80=99=
s Confluence wiki. The AI can then query it at will (with auth and all hand=
led by MCP). It decouples the model from the data integration. Companies li=
ke Slack, Google Drive, databases, etc., could all have MCP adaptors . As a=
 developer, you then might write prompts that trigger these calls implicitl=
y.
For example:
User asks: =E2=80=9CWhat=E2=80=99s the latest sales figure for product X th=
is quarter?=E2=80=9D Under the hood, an Anthropic Claude model might have a=
n MCP client that knows how to fetch data from a connected database or CSV =
of sales. The model=E2=80=99s prompt might include something like <MCP: que=
ry sales_db for X Q3 sales> and the server returns 42,000 units which the m=
odel then uses to answer. All standardized =E2=80=93 you don=E2=80=99t dire=
ctly write that code; the model (via prompt engineering) does it.
This changes RAG in that the retrieval step becomes part of a broader conte=
xt-sharing protocol. It=E2=80=99s not just search + prompt; it=E2=80=99s a =
conversation between model and data sources, orchestrated by this protocol.=
 If widely adopted, it will accelerate building AI apps =E2=80=93 no more b=
espoke integration for each dataset; just spin up an MCP server for your da=
ta and voila, AI can use it.
From Anthropic=E2=80=99s announcements and what we saw:
MCP is open-source, open standard.
Already integrated with Claude (Anthropic=E2=80=99s model) for some early p=
artners (Block, etc., as mentioned).
It supports things like real-time updates (so data can stream if needed) an=
d presumably bidirectional (AI can write via MCP, not just read).
In simpler terms: RAG currently often requires we bolt on a vector database=
 and ask the model to read those results. MCP could make that a native abil=
ity of models =E2=80=93 retrieving context on the fly as needed. It might e=
ven handle routing: if the answer is in a vector DB or in a SQL DB or via a=
n API, the AI doesn=E2=80=99t care =E2=80=93 it asks via MCP and the right =
connector answers.
This can change everything by making any AI app easier to build and more re=
liable (since the integration is standardized, fewer errors). Also, for pri=
vacy, companies can run MCP servers internally so the model only accesses a=
llowed data and does so securely .
So the MCP revolution is about universal AI-data connectivity. In the timel=
ine of this guide, by 2025 it=E2=80=99s just starting. By 2026=E2=80=932027=
, it could be as ubiquitous as HTTP for web. If RAG is one approach now, MC=
P might generalize that to all context (structured and unstructured) =E2=80=
=93 essentially fulfilling the promise of letting AI know everything you wa=
nt it to know, when it needs to know it, without retraining.
For someone building RAG now, keep an eye on MCP. If you adopt its pattern =
early, your system might easily plug into future models supporting it. It=
=E2=80=99s the direction the industry=E2=80=99s moving for sure =E2=80=93 b=
oth OpenAI and Anthropic are eyeing such protocols (OpenAI with plugins whi=
ch is similar concept, Anthropic with MCP, etc.).
Hybrid Search Deep-Dive: Best of Both Worlds
We touched on hybrid search earlier, but let=E2=80=99s go deep: combining B=
M25 (sparse keyword search) with vector search can significantly boost perf=
ormance, especially in domains with technical terms, proper nouns, or when =
you have to ensure no relevant doc is missed.
Recap: Vector search finds semantic matches; BM25 finds lexical matches. Th=
ey often retrieve overlapping but not identical sets of results . For examp=
le, query =E2=80=9CCOVID transmission aerosol study=E2=80=9D:
Vector search might find a research paper discussing airborne transmission =
in general (even if it doesn=E2=80=99t have exact word =E2=80=9Caerosol=E2=
=80=9D), because semantically it=E2=80=99s similar.
BM25 might find a specific document that has the exact phrase =E2=80=9Caero=
sol transmission of COVID=E2=80=9D even if that doc is otherwise small or n=
ot well-written (so embedding might not rank it as high).
By doing both and merging, you get broader coverage.
There are a few ways to do hybrid:
Score fusion: e.g., normalized BM25 score + alpha * vector similarity score=
, then rank. This requires tuning that alpha. If alpha=3D0, purely BM25; if=
 high, more weight to semantic.
RRF (Reciprocal Rank Fusion) : doesn=E2=80=99t need heavy tuning. It takes =
the rank positions from each method and combines such that if either method=
 ranks a doc high, it gets boosted.
Two-stage retrieval: e.g., use BM25 to filter a large set to, say, 1000 can=
didates, then use vector similarity to get top 5 from those. Or vice-versa =
(vector first, then within those apply BM25 to refine).
Index combination: Some vector DBs (Weaviate, Pinecone, Qdrant with =E2=80=
=9Chybrid search=E2=80=9D features) allow adding a sparse vector to the den=
se vector for each doc and performing a single combined similarity search. =
For example, Qdrant introduced a =E2=80=9Cpayload boost=E2=80=9D algorithm =
that basically accounts for keyword overlap (BM25-like) along with vector d=
istance .
When implementing, you=E2=80=99ll need a sparse index of some kind. This co=
uld be Elasticsearch or just Lucene. If you already use something like Open=
Search, you can actually index vectors there too and do a combined query. O=
r use two systems side by side.
Visual analogy: Think of vector search as a wide net that catches things by=
 meaning, and BM25 as a spear that precisely hits documents with matching k=
eywords. Hybrid means you fish with a net and spear simultaneously =E2=80=
=93 you won=E2=80=99t miss much.
Why 2025 loves hybrid: Because many users complain pure vector search somet=
imes gives =E2=80=9Cfuzzy=E2=80=9D results that are on topic but don=E2=80=
=99t contain the answer, whereas keyword search might directly find a doc w=
ith the exact answer text (but might miss synonyms). Combining them often y=
ields an answer in top 1-2 results that either method alone would have as, =
say, result 5 or 6.
A concrete example from our experiences: Searching a database of DevOps inc=
idents for =E2=80=9CDNS error EAI_AGAIN solution=E2=80=9D.
BM25 finds maybe a post containing =E2=80=9CEAI_AGAIN=E2=80=9D exactly.
Vector finds a troubleshooting guide that doesn=E2=80=99t mention that code=
 but talks about =E2=80=9Cnetwork DNS resolution issues=E2=80=9D.
 The actual best answer was a forum thread that had both the code and gener=
al discussion. Hybrid brought that thread to rank 1, whereas BM25 had it ra=
nk 3 (below some code snippet page), and vector had it rank 4.
RAG usage: In the retrieval step of RAG, you can implement hybrid by:
bm25_results =3D bm25_index.search(query, k=3D10)
vec_results =3D vector_index.search(query_vec, k=3D10)
final_results =3D fuse(bm25_results, vec_results)
Then pass final_results (top few) to LLM.
Reranking revolution (I=E2=80=99ll tie back here): After hybrid, you might =
still apply an LLM-based reranker (like feed query + snippet to a smaller c=
ross-attention model to judge relevance). In effect, hybrid gave you a bett=
er set of 10 candidates, and reranker picks the best 3. This stacking can y=
ield extremely high precision =E2=80=93 as high as 95%+ relevant on first c=
hunk for well-formed queries. Microsoft noted something similar in their Su=
perlinked article: hybrid + semantic rerank gave big gains .
At the cost of some complexity, you get the best of both worlds. Most state=
-of-the-art QA systems do use a hybrid approach under the hood now. Even Op=
enAI=E2=80=99s WebGPT (older) combined information retrieval with searches =
that effectively were sparse lookups (search queries) and then reading page=
s.
In summary: Don=E2=80=99t pick sides in the dense vs sparse debate; use bot=
h. It=E2=80=99s often not either-or. If you incorporate hybrid search in yo=
ur RAG, you=E2=80=99ll likely see fewer missed answers and more robust perf=
ormance, especially on queries with rare terms (product codes, error IDs, n=
ames) and queries that need conceptual match.
These advanced patterns =E2=80=93 GraphRAG, recursive retrieval, multi-moda=
lity, MCP integration, and hybrid search =E2=80=93 are like tools in the ex=
pert chef=E2=80=99s kitchen. You don=E2=80=99t always need all of them, but=
 knowing they exist and when to apply them can elevate your RAG system to g=
ourmet level. They represent how the field is pushing towards higher accura=
cy, broader capability, and easier integration.
As we near the end of our epic guide, let=E2=80=99s ensure we also learn fr=
om the pitfalls others have faced, and then we=E2=80=99ll gaze into the fut=
ure (2025=E2=80=932027) to see what=E2=80=99s coming (spoiler: autonomous a=
gents and massive context windows). But first =E2=80=93 the pitfall graveya=
rd, to avoid ending up there.
The Pitfall Graveyard
Even seasoned practitioners have horror stories of RAG projects that went a=
wry. In this section, we=E2=80=99ll visit the 7 ways RAG projects die (so y=
ou can avoid each), from chunking disasters to embedding mismatches. We=E2=
=80=99ll shine light on the infamous =E2=80=9Clost in the middle=E2=80=9D p=
roblem, share hallucination horror stories and how to prevent them, warn ab=
out a $500K mistake a startup made with a wrong vector DB choice, and disse=
ct the newest pitfall: the embedding model mismatch fiasco. Consider this a=
 tour of the graveyard so your project doesn=E2=80=99t end up buried here.
1. Chunking Gone Wrong (The Context Shredder): We=E2=80=99ve harped on chun=
king because it=E2=80=99s that important. Many RAG projects die early becau=
se the team didn=E2=80=99t respect context boundaries =E2=80=93 they arbitr=
arily chopped docs into pieces that made no sense. The result: retrieval fe=
tched chunks that were irrelevant or incomplete, leading the LLM to give wr=
ong answers. One startup had a legal QA system that kept failing to cite th=
e correct clause. Post-mortem found they chunked contracts by fixed 1000-ch=
aracter windows, often splitting clauses in half. The answer chunk would ha=
ve =E2=80=9C=E2=80=A6except as provided in Section 5(b)=E2=80=9D but Sectio=
n 5(b) was in the next chunk, which wasn=E2=80=99t retrieved =E2=80=93 ouch=
=2E This killed user trust. The fix: re=
-chunk by clause or paragraph, and all=
ow overlaps. Lesson: Don=E2=80=99t kill context coherence. If your logs sho=
w queries retrieving chunks that seem off, inspect if chunking is to blame.
2. The =E2=80=9CLost in the Middle=E2=80=9D Problem: This one=E2=80=99s sne=
aky =E2=80=93 even if you chunk well and retrieve the right chunk, the answ=
er might be in the middle of a long chunk and the model might overlook or s=
ummarize incorrectly. LLMs (especially transformer models) have known biase=
s: they pay a bit more attention to the start and end of their input, somet=
imes less to the middle . If the crucial detail is buried in the middle of =
a 500-word chunk, there=E2=80=99s a chance the model misses it or =E2=80=9C=
hallucinates=E2=80=9D around it. A story: an AI assistant was reading a pro=
duct manual chunk that listed limitations in the middle. The user asked abo=
ut a limitation; the relevant text was there but mid-chunk, so the model, p=
erhaps pattern-matching, gave a generic answer missing the specific detail =
(which was in lines it didn=E2=80=99t focus on). Users caught that it wasn=
=E2=80=99t specific. The solution can be to chunk smaller or highlight the =
answer if you can pre-process. One trick: when retrieving, you might bold o=
r mark the exact sentence in the prompt (some do <highlight>sentence about =
limitation</highlight> around it). Or use an extractive model to pull the s=
entence out as an answer candidate. Either way, be aware that long chunks c=
an dilute focus. Ensure key info isn=E2=80=99t lost in the middle.
3. Hallucination Horrors: Perhaps the scariest pitfall =E2=80=93 your AI so=
unds confident but is spewing nonsense not supported by any doc. Hallucinat=
ions in RAG usually happen when retrieval fails (no good info) but the mode=
l feels it must answer anyway, or when it tries to stitch together partial =
info and fills gaps with guesswork. One prevention is always instruct the m=
odel to say =E2=80=9CI don=E2=80=99t know=E2=80=9D or something if unsure, =
but models sometimes ignore that if they think they can =E2=80=9Cbe helpful=
=2E=E2=80=9D Real tale: a customer asked=20=
a RAG bot about a policy that didn=
=E2=80=99t exist in the docs (it was a trick question). The bot confidently=
 fabricated a policy clause, complete with a fake quote and citation to a d=
ocument =E2=80=93 which freaked out the legal team. They nearly scrapped th=
e project thinking it could create legal liabilities. The fix was multi-pro=
nged: (a) improve retrieval so at least a relevant doc is found or if none,=
 a flag is set; (b) add a final check where the model=E2=80=99s answer is c=
ompared against sources =E2=80=93 if low overlap, replace answer with a =E2=
=80=9Ccannot find info=E2=80=9D response (some use an LLM judge for this, o=
r a simpler heuristic like =E2=80=9Cif answer has facts not in retrieved te=
xt, then caution=E2=80=9D). Hallucinations can kill a project=E2=80=99s cre=
dibility in one stroke, so have guardrails: either clear refusals for unkno=
wn answers or at least an apologetic =E2=80=9CI=E2=80=99m not certain=E2=80=
=9D rather than confident lies.
4. Misusing the Wrong Vector Database (the $500K Mistake): Choosing tech wi=
thout due diligence can be costly. Imagine spending months and $$$ on a vec=
tor DB that promises enterprise scale, only to discover at scale it has a m=
emory leak or it doesn=E2=80=99t support your needed feature. One startup s=
pent over $500K on licensing and deploying a certain vector search solution=
 that was hyped (won=E2=80=99t name names). They pumped in millions of embe=
ddings =E2=80=93 it worked until queries got slow as data grew. They later =
realized an open-source alternative (free) performed better for their use c=
ase, but migrating was non-trivial and that money was sunk cost. Moral: ben=
chmark and start small. For most, open-source like Chroma or Qdrant suffice=
; only move to pricey Pinecone or managed if you confirmed need. And even t=
hen, try their free tier or PoC. Also pay attention to how well the DB inte=
grates with your stack (some DBs might not have the best client libraries f=
or your language, etc.). The wrong choice can burn money and time.
5. Overlooking Data Refresh (Stale Knowledge): Another grave: some RAG syst=
ems fail to update their index as knowledge changes. It=E2=80=99s easy to i=
ndex a snapshot of data and forget it. Then users ask about the latest info=
 and get outdated answers. In fields like finance or regulations, that=E2=
=80=99s dangerous. Picture an AI advisor giving a tax law from 2022 that go=
t amended in 2023 because the index wasn=E2=80=99t updated =E2=80=93 the us=
er acts on wrong info. Regular updates (or use a dynamic retrieval that alw=
ays pulls from source in real-time if possible) are critical. It=E2=80=99s =
a pitfall when teams treat RAG like a static model (=E2=80=9Cwe indexed onc=
e, done=E2=80=9D). You need a pipeline for ingestion of new/changed docs, a=
nd possibly an expiration for old ones. Some solve it by indexing on the fl=
y each query (like retrieve via API rather than vector DB, but that=E2=80=
=99s slower). At least schedule re-embedding periodically. The pitfall isn=
=E2=80=99t a one-time failure but a slow death of usefulness as content dri=
fts. Avoid by setting up processes from day 1 for continuous data maintenan=
ce.
6. Security/PII Leaks (Cautionary Tale): Some projects died on compliance r=
eview because they accidentally allowed sensitive data exposure. One intern=
al chatbot was shut down because it retrieved a chunk containing another cl=
ient=E2=80=99s info to answer a query for a different client =E2=80=93 a bi=
g no-no (lack of permission filtering). Another got axed because logs of us=
er queries containing personal data were sent to a third-party API without =
proper agreements. These are pitfalls of negligence more than tech, but the=
y can kill a project via legal. Always think: what data is being indexed? D=
oes it contain PII? Should it be anonymized or segmented? Who can query wha=
t? Implement filtering by user roles (if possible, e.g., separate vector in=
dexes per client or attribute-based access). And ensure if using external L=
LM APIs, you=E2=80=99re not violating any privacy requirement (OpenAI=E2=80=
=99s policy now says they don=E2=80=99t train on your data by default, but =
still don=E2=80=99t send what you shouldn=E2=80=99t). One preventative meas=
ure: have a red-teaming phase =E2=80=93 purposely try to get the bot to rev=
eal something it shouldn=E2=80=99t (like ask =E2=80=9CShow me data on clien=
t B=E2=80=9D as client A, etc.). If it does, fix before real launch.
7. Embedding Model Mismatch Disaster (New Pitfall): This is a more technica=
l gotcha that some teams recently encountered. It happens when your documen=
t embeddings and query embeddings are not compatible =E2=80=93 e.g., you em=
bed documents with one model and queries with another (accidentally or due =
to an update), so similarity search breaks. Or if you upgraded your embeddi=
ng model (say from Ada-001 to Ada-002) without re-embedding your corpus, th=
e vectors live in different spaces now. One team updated their code to use =
a new embedding model version, not realizing they had 100k old vectors in t=
he DB from the old model. Suddenly retrieval quality plummeted (because the=
 new query embeddings didn=E2=80=99t align with old doc embeddings). They s=
pent weeks debugging poor results until noticing the model name mismatch. T=
o avoid, maintain metadata on which embedding model and version was used fo=
r the index. If you change, re-index everything. Similarly, mixing differen=
t dimensionalities is obvious but I=E2=80=99ve seen someone try to concaten=
ate two embedding vectors from different models for docs, then queries only=
 with one model =E2=80=93 also ineffective. Another variant: using multilin=
gual embeddings for docs but English-only for queries or vice versa; if mod=
el wasn=E2=80=99t aligned across languages, retrieval fails cross-language.=
 Always ensure embedding alignment. If using open-source models, know if th=
e query vs doc embedding model are separate (Cohere had separate ones for e=
xample search vs doc). Use as intended.
These pitfalls claim victims regularly. But armed with foreknowledge, you c=
an steer clear. In short:
Keep chunks coherent and overlapping.
Be mindful of LLM limitations (like mid-chunk info) and mitigate.
Strangle hallucinations with instruction and verification.
Choose tech carefully (and cheaply until proven).
Keep knowledge updated.
Enforce security and privacy from the start.
Manage embeddings rigorously (versions and types).
If you do all that, you=E2=80=99ll avoid the graveyard where failed RAG pro=
jects lie. Instead, your project will live on to see the bright future of A=
I that we=E2=80=99ll discuss next!
The Future Shock: 2025-2027
What does the future hold for RAG and AI in the next few years? In a word: =
mind-blowing advancements. We=E2=80=99re standing on the edge of some game-=
changing developments =E2=80=93 things that sound like science fiction but =
are just around the corner. Let=E2=80=99s peer into 2025-2027:
Anthropic=E2=80=99s Prediction: AI Surpassing Nobel Laureates by 2026 =E2=
=80=93 Bold, but not unfounded. This suggests that AI (powered by retrieval=
 and reasoning) will excel in specialized domains to a degree that matches =
or exceeds top human experts. Think about it: an AI with access to all scie=
ntific literature (via RAG) and a reasoning engine could potentially propos=
e novel solutions and insights at a pace humans can=E2=80=99t. We already s=
ee early hints: an AI agent that read tens of thousands of chemistry papers=
 and suggested a new material that human researchers hadn=E2=80=99t thought=
 of. By 2026, such feats might be common =E2=80=93 an AI doctor diagnosing =
ultra-rare diseases by cross-referencing millions of cases (something even =
Nobel-winning doctors might not do in real-time). The key drivers are massi=
ve context and knowledge integration (RAG providing memory), plus improved =
reasoning algorithms.
1M+ Token Context Windows Change Everything =E2=80=93 Context size has been=
 ballooning: OpenAI went from 4k to 32k, Anthropic to 100k tokens. We have =
million token context windows (on paper) now. I expect multi-million contex=
t windows to be common by 2026. That=E2=80=99s essentially entire books or =
multiple books at once. A million tokens is roughly 750k words (about 1500 =
pages). Imagine feeding an entire corporate wiki, or decades of legal case =
law, or all of Wikipedia on a topic, directly into the model prompt. The bo=
undaries between retrieval and prompting blur here: you might not need an e=
xternal vector DB for moderately sized corpora; you could just stuff everyt=
hing into the prompt (with clever compression). One engineer I know joked, =
=E2=80=9CIf we get 1M tokens, I=E2=80=99ll just feed the model our whole da=
tabase schema and docs and ask questions directly.=E2=80=9D Of course, more=
 context means slower processing, and we=E2=80=99ll still use RAG to select=
 relevant parts, but the flexibility is huge =E2=80=93 conversation history=
 can be basically unlimited, models can do deeper analysis without truncati=
ng context. Also, multi-step reasoning could happen internally without exte=
rnal calls if the model can =E2=80=9Cremember=E2=80=9D all intermediate ste=
ps within its giant scratchpad.
MCP Integration: Universal AI-Data Connectivity =E2=80=93 As we discussed, =
the Model-Context Protocol by Anthropic aims to be a standard pipe connecti=
ng AI to data. This is already becoming widely adopted and by 2027, I suspe=
ct this will be near-universal. That means when you launch a new AI system,=
 you won=E2=80=99t spend time on writing custom retrieval code; you=E2=80=
=99ll spin up connectors (to your databases, websites, tools) and the model=
 just knows how to talk to them. It=E2=80=99s analogous to how in the early=
 web days you had to manually code a lot to connect to a database, but then=
 ORMs and APIs standardized that. This will accelerate new AI application d=
evelopment drastically (taking weeks instead of months to integrate sources=
). It also means AI assistants (like your personal AI or company=E2=80=99s =
AI) can continuously and securely fetch needed info.
 Effect on RAG: It might shift the concept =E2=80=93 =E2=80=9Cretrieval aug=
mented=E2=80=9D might become just standard =E2=80=9Ccontext retrieval=E2=80=
=9D feature of all LLMs (like how we don=E2=80=99t talk about =E2=80=9Cinte=
rnet-augmented smartphone=E2=80=9D because internet connectivity is assumed=
). Once any AI can seamlessly pull data via MCP or similar, RAG is no longe=
r an optional add-on, but a given. The winners will be those who leverage i=
t best (with quality data and sources).
Why the RAG Market will hit $40B by 2030 =E2=80=93 We saw projections earli=
er. As more orgs adopt AI with retrieval (since pure end-to-end training is=
 impractical for each org=E2=80=99s data), RAG tools and infrastructure bec=
ome a huge business. Vector DB companies, enterprise search integration, AI=
 assistants for knowledge work =E2=80=93 all these fall under that umbrella=
=2E The workforce will likely includ=
e thousands of =E2=80=9CAI knowledge engi=
neers=E2=80=9D whose job is to manage corpora, tune retrieval, etc. By 2030=
, nearly every enterprise app might have an AI copilot that relies on RAG t=
o stay current. If you think of verticals: law, medicine, finance, customer=
 support, programming =E2=80=93 all have specialized knowledge that RAG hel=
ps inject into general AI. $40B might even be conservative if we include ha=
rdware and services around it.
Your Career in the Age of Autonomous AI Agents =E2=80=93 People often ask, =
=E2=80=9CWill AI (with RAG) automate me out of a job?=E2=80=9D If you know =
this blog you know I don=E2=80=99t think that way. But I might frame it dif=
ferently: =E2=80=9CAI won=E2=80=99t replace you, but a person using AI migh=
t.=E2=80=9D Those who embrace these tools early will have a huge edge. The =
nature of many jobs will shift to supervising AI, verifying outputs, and ha=
ndling the non-automatable nuance. If AI becomes as smart as top experts in=
 certain domains, human roles might evolve to more creative, strategic, or =
interpersonal tasks (things AI is far from mastering).
 But there=E2=80=99s also new career prospects: AI trainers, AI content cur=
ators, ethicists, etc. The age of autonomous agents (AutoGPT-like systems t=
hat can perform goals relatively independently) means we=E2=80=99ll need =
=E2=80=9CAI wranglers=E2=80=9D =E2=80=93 people who define objectives, moni=
tor agent performance, and intervene when needed. Think of it like managing=
 employees =E2=80=93 except these employees are AI agents working 24/7 and =
scouring data. It=E2=80=99s likely by 2027 or so that every knowledge worke=
r will have some AI agent assistance (imagine a =E2=80=9Cjunior AI analyst=
=E2=80=9D assigned to each person).
 The ones who thrive will be those who learn to ask the right questions and=
 validate AI outputs. With RAG, a lot of factual grunt work is handled, so =
humans can focus on interpretation and decision-making. That Nobel-level AI=
? Perhaps it will partner with Nobel scientists to accelerate discoveries, =
not just do it alone.
In summary, expect:
AI that knows more and forgets less, thanks to big context and integration =
=E2=80=93 drastically improving capabilities.
Standards making AI integration plug-and-play, leading to an explosion of A=
I-augmented applications in every field.
A shifting job landscape, where working alongside smart AI (and occasionall=
y reigning it in) is the norm. Those who start adapting now (e.g., learning=
 to use RAG tools, understanding limitations like hallucinations) will be t=
he leaders of tomorrow.
One could say, companies winning in 2025 aren=E2=80=99t those with just the=
 biggest models, but those whose AI truly knows their business (via RAG). B=
y 2027, that will be even more pronounced: it=E2=80=99s not about who has A=
I, but who has integrated AI deeply and responsibly into their operations.
Exciting times ahead =E2=80=93 equal parts exhilarating and challenging. Th=
e next and final section will equip you with resources to ride this wave =
=E2=80=93 a toolkit and action plan to become a RAG master in the coming 90=
 days and beyond. Let=E2=80=99s gear you up for that future.
Your RAG Toolkit: Resources That Matter
As we wrap up, I want to leave you with a toolkit =E2=80=93 the best resour=
ces to continue your RAG journey. Whether you=E2=80=99re a beginner looking=
 to start from scratch (and ideally spend nothing), a growing company needi=
ng to scale up, or an enterprise dealing with compliance and heavy load, we=
=E2=80=99ve got suggestions for each. Plus, I=E2=80=99ll point to community=
 goldmines and courses to deepen your expertise. Finally, I challenge you t=
o a 30-day RAG sprint to go from novice to practitioner.
The =E2=80=9CStart Here=E2=80=9D Stack for Beginners (Free Tier Everything)
If you=E2=80=99re just getting your feet wet, here=E2=80=99s a recommended =
stack that won=E2=80=99t cost you a dime:
LLM: OpenAI=E2=80=99s GPT-3.5 (old, but free via their API with trial credi=
t, or use the free ChatGPT UI for experimentation). Alternatively, Hugging =
Face=E2=80=99s HuggingChat or Google Colab with a smaller model (like flan-=
t5 or a 7B LLaMA derivative) can be free. But GPT-3.5 is easiest to convers=
e with initially.
Vector DB: ChromaDB (open source, pip install chromadb). It=E2=80=99s simpl=
e and runs locally. Or use a local FAISS index if you prefer just an in-mem=
ory approach. Both are free.
Library: LlamaIndex (GPT Index) or LangChain =E2=80=93 both are open source=
 and quite friendly. LlamaIndex maybe simpler for pure QA, LangChain if you=
 want to tinker with chain logic. They have great docs and examples.
Data source: Whatever docs you have =E2=80=93 but if you need sample data, =
there are open datasets. For example, Wikipedia (you can grab a few pages),=
 or some public domain texts (Project Gutenberg for literature). Or use you=
r own notes/markdown files to make it personally interesting.
Dev environment: Jupyter Notebooks (free) =E2=80=93 great for iterative dev=
elopment. Or VS Code with the Python extension.
With that, you can build a prototype Q&A bot or documentation assistant wit=
hout paying for anything (assuming you stay within free API limits or use l=
ocal models). Actually, OpenAI=E2=80=99s $5-10 free credit might get you th=
ousands of queries on gpt-3.5, which is plenty to play with.
For a quickstart, check out LangChain=E2=80=99s beginner tutorial [ https:/=
/substack.com/redirect/c851b32a-c238-44e2-9938-c7f44a95d690?j=3DeyJ1IjoiNWt=
iOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0 ] or LlamaIndex=E2=80=
=99s Getting Started [ https://substack.com/redirect/e8f3fb78-a951-4e77-a95=
0-168f9542b146?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAj=
MRMPOw0 ] =E2=80=93 they walk through loading data and querying it . Also, =
Chroma=E2=80=99s docs show how to do basic insert and query.
The =E2=80=9CScale Up=E2=80=9D Stack for Growing Companies
Okay, you=E2=80=99ve proven the concept, now your startup or team needs to =
deploy something for real usage. Cost is a concern, but you can spend a bit=
, and reliability matters:
LLM: Consider using OpenAI=E2=80=99s GPT-4 for higher quality if needed, bu=
t GPT-3.5 may suffice. Also look at Cohere or Anthropic Claude if they have=
 a better pricing or context for your needs (Claude has that 100k context v=
ersion). Some companies fine-tune a smaller open model to their data to sav=
e per-call costs =E2=80=93 e.g., fine-tuning LLaMA 2 13B on your corpus. Th=
at has upfront cost but then queries are virtually free if self-hosted. A c=
ommon approach is a tiered LLM strategy: use GPT-3.5 for most, GPT-4 for co=
mplex queries or final verification.
Vector DB: If you outgrew Chroma on a single machine, you could move to Chr=
oma Cloud (they have managed service), or Qdrant Cloud which is quite affor=
dable (remember the ~$9 for 50k vectors estimate ). Weaviate also offers a =
hybrid search and is free up to some limit. Many go with Pinecone starter (=
$) at this stage for ease. If you have under ~1M embeddings, even a Postgre=
s with pgvector extension might suffice (then you reuse your existing DB in=
fra).
Orchestration: LangChain or LlamaIndex will still serve, but maybe now impl=
ement robust error handling/logging around it. You might also containerize =
the app for deployment (Docker).
Authentication & front-end: If exposing to users, set up an interface. Coul=
d be a simple React app calling a backend, or even a Slack/Discord bot. Use=
 API keys or auth to protect it. Many companies roll out in Slack first for=
 internal Q&A.
Monitoring: Use something like Streamlit or Gradio for quick dashboard if n=
eeded, and definitely log interactions. You can use LangChain=E2=80=99s bui=
lt-in tracing or just your own logging to a file/DB. Monitor usage and have=
 a way to turn it off if something goes haywire (feature flag).
Cost control: Set up usage limits (like don=E2=80=99t let one user spam 100=
0 queries/min). Also track OpenAI API usage; you can use their usage APIs o=
r just your logs to gauge spend. Possibly implement caching of LLM response=
s for identical queries to save.
Growing companies at this stage might also invest in prompt engineering =E2=
=80=93 e.g., creating a few prompt templates for different styles of questi=
ons. And unit tests for your RAG: provide it some known queries and check i=
t returns acceptable answers (to catch regressions when you change somethin=
g).
The =E2=80=9CEnterprise=E2=80=9D Stack for Fortune 500s
Now we=E2=80=99re talking heavy duty: requirements include security, high a=
vailability, compliance, and potentially millions of knowledge items and us=
ers.
LLM: Likely a combination of self-hosted models for data-sensitive content =
and limited use of external APIs for general knowledge. Enterprises might d=
eploy Azure OpenAI (which is OpenAI models in their Azure cloud, with data =
not leaving), or AWS Bedrock (which offers Jurassic, Anthropic, etc. with e=
nterprise-friendly terms). Some might even run GPT-4 on-prem if and when av=
ailable (or a comparable big model). Also consider Google=E2=80=99s Gemini =
API via GCP, depending on partnership. The key is enterprise agreements (pr=
ivacy, SLA).
Vector DB: At this scale, probably managed services or on-prem clusters. Pi=
necone Enterprise, Weaviate Enterprise, or using something like ElasticSear=
ch with vector-capability if they already have ELK stack. Enterprises often=
 prefer well-supported tools: e.g., Microsoft Cognitive Search (which now s=
upports vectors) for those in MS ecosystem. Or OpenSearch for those already=
 using AWS search solutions. They will care about features like RBAC (role-=
based access control), encryption at rest, etc. Many vector DBs now offer t=
hose enterprise features (e.g., Pinecone and Weaviate have RBAC, etc. ).
Orchestration & Integration: Likely a microservice architecture. They might=
 integrate RAG into existing platforms (say, an internal SharePoint plugin,=
 or a CRM assistant). LangChain might be used under the hood, or a custom s=
olution for more control. Observability is key =E2=80=93 so integrate with =
Splunk or AppDynamics for logging, Datadog for monitoring performance. Poss=
ibly use OpenTelemetry if custom solution to trace calls.
Compliance & Security: This stack includes things like Data loss prevention=
 (DLP) checks on AI output (prevent it from spitting out something it shoul=
dn=E2=80=99t), audit logs of who asked what, etc. Possibly behind the scene=
s every AI response goes through an approval step or a human-in-the-loop fo=
r certain sensitive domains.
Scalability: It will be deployed across regions, with fallback models if on=
e service fails. For vector DB, maybe multi-region replication. Also they=
=E2=80=99ll have a retraining/indexing pipeline that continuously updates (=
with proper CI/CD =E2=80=93 maybe nightly builds of the index or streaming =
updates).
User Interface: could be deeply integrated (not a separate chat UI, but emb=
edded in existing tools like Office 365 via plugins, etc.). Or for customer=
 support, integrated with their support portal as a chat assistant. The UI =
might need to handle handoff to human agents seamlessly when AI can=E2=80=
=99t help (so integration with their ticketing system).
Tooling & Knowledge Management: Enterprises often have existing knowledge m=
anagement workflows. The RAG system might tie into that =E2=80=93 e.g., whe=
n a new policy doc is published on Confluence, automatically chunk & index =
it. That means connectors to internal data sources (file shares, intranets,=
 DBs) =E2=80=93 possibly using MCP in the future or current enterprise sear=
ch connectors.
Testing & Evaluation: Formal testing with domain experts. Possibly running =
the AI in shadow mode (giving suggestions to human agents, but not directly=
 to customers, until it proves good enough). Ensuring it handles domain-spe=
cific vocabulary correctly (maybe even fine-tuning embeddings or the model =
on domain data to improve).
This stack is not one-size-fits-all, but at enterprise scale, the emphasis =
is on robustness, compliance, integration. They=E2=80=99d rather a slightly=
 weaker model that is secure than a powerful one that might leak data. For =
example, some banks disabled direct internet search for their AI assistant =
because they can=E2=80=99t allow unpredictable external info.
Community Goldmines: Discords, GitHub Repos, Courses
You=E2=80=99re not alone in this journey. The RAG/LLM community is vibrant =
and sharing knowledge daily:
Discord servers:
LangChain=E2=80=99s Discord =E2=80=93 great for Q&A and seeing what issues =
others face.
LlamaIndex Discord =E2=80=93 developers and users share tips, plus the devs=
 often answer.
Vector database Discords (Pinecone, Weaviate, Qdrant each have communities)=
=2E
Hugging Face Discord =E2=80=93 for general transformer/LLM discussions, inc=
luding retrieval techniques.
These are good to lurk in and search history =E2=80=93 often your question =
has been asked by someone.
GitHub Repositories:
awesome-rag [ https://substack.com/redirect/563dc5dc-f2ca-411e-af1f-f67397b=
f35af?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0 ]
LangChain Hub [ https://substack.com/redirect/fb297555-ff34-4983-a61d-5a582=
30ebdb3?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0=
 ] and Examples =E2=80=93 many example scripts for various tasks.
OpenAI Cookbook [ https://substack.com/redirect/28059eb0-1afe-45f5-8006-354=
5b2e16a15?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPO=
w0 ] (GitHub: openai/openai-cookbook) =E2=80=93 although not RAG-specific, =
it has sections on retrieval augmentation and plenty of relevant examples.
InstructorEmbedding [ https://substack.com/redirect/7b705ba3-3efd-45b9-9222=
-378f305020e1?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjM=
RMPOw0 ] or sentence-transformers repos =E2=80=93 if exploring custom embed=
ding models.
And of course, papers: e.g., the original RAG paper [ https://substack.com/=
redirect/efcf8b38-a757-49b7-94d8-03d17c0e9f14?j=3DeyJ1IjoiNWtiOTN6In0.zdzy8=
8YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0 ] by Lewis et al. 2020 is on GitHub =
for code, etc.
Also Microsoft=E2=80=99s Guidance repo shows prompt strategies, including r=
etrieval.
Courses and Tutorials:
Andrew Ng=E2=80=99s DeepLearning.AI [ https://substack.com/redirect/d87dc4f=
d-c38d-4e7c-ae81-2044a380d277?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgU=
C7BDnPG6RxQ4tAjMRMPOw0 ] short course on LangChain (on Coursera) =E2=80=93 =
hands-on building chains and agents (he has other RAG courses too)
Full Stack Deep Learning [ https://substack.com/redirect/40aac557-05cb-477d=
-a0ba-398022a17157?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ=
4tAjMRMPOw0 ] has some modules on deploying LLMs with RAG.
Hugging Face Courses [ https://substack.com/redirect/704ee7da-20f8-4a42-810=
7-c91fbabb061b?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAj=
MRMPOw0 ] =E2=80=93 they have a new course on LLMs and might cover retrieva=
l augmentation.
OpenAI=E2=80=99s ChatGPT prompt engineering for developers [ https://substa=
ck.com/redirect/f60733c4-3705-4080-a596-6a81fc758795?j=3DeyJ1IjoiNWtiOTN6In=
0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0 ] (free resource) =E2=80=93 t=
ouches on how to instruct the model (useful when you do RAG prompts).
Many YouTubers also have RAG content =E2=80=93 e.g., Sam Witteveen, James B=
riggs, etc., showing building QA bots.
Reading:
Research papers =E2=80=93 =E2=80=9CRetrieval-Augmented Generation (RAG) [ h=
ttps://substack.com/redirect/efcf8b38-a757-49b7-94d8-03d17c0e9f14?j=3DeyJ1I=
joiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0 ]=E2=80=9D, =E2=
=80=9CREALM [ https://substack.com/redirect/b0a27444-0676-49d0-b77e-3dc4d97=
77868?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0 ]=
=E2=80=9D, =E2=80=9CReAct [ https://substack.com/redirect/a2b6dd91-5d20-4a4=
a-9eab-5a8503f3dbeb?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6Rx=
Q4tAjMRMPOw0 ]=E2=80=9D, =E2=80=9CGraphRAG survey [ https://substack.com/re=
direct/d0116e4b-5194-4cec-97c4-8efa54754c37?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88Y=
FxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0 ] =E2=80=9D, etc. Even if mathy, skip =
to discussion for ideas.
Company blogs (with caution as user said): e.g., OpenAI, Anthropic, Cohere =
blogs =E2=80=93 they sometimes discuss use cases and best practices.
The article you=E2=80=99re reading now (=F0=9F=98=89) can serve as a mini r=
eference too, given the citations we=E2=80=99ve sprinkled.
The 30-Day RAG Challenge: From Novice to Practitioner
Ready to apply everything and build something real? Here=E2=80=99s a rough =
plan for 30 days (modify as fits your schedule):
Week 1-2: Environment setup and first RAG =E2=80=93 Set up your dev environ=
ment (maybe a notebook or simple app). Pick a small domain (e.g., use 10 Wi=
kipedia articles on a topic you like). Day 1-2: Load and index data with Ll=
amaIndex, do simple queries. Day 3-5: Try LangChain, experiment with differ=
ent retrievers (maybe FAISS vs Chroma). By end of Week 1, have a basic QA b=
ot working locally. Week 2: Increase complexity =E2=80=93 add multiple file=
s, try some multi-turn conversation with memory. Also join a community (dis=
cord) and ask at least one question. Read two articles/papers on RAG to dee=
pen understanding. Checkpoint at Day 14: You should be comfortable with bas=
ic RAG pipeline code and have a small demo.
Week 3: Production-ready prototype =E2=80=93 Focus on robustness. Implement=
 caching of answers, add logging. Try hybrid search if you haven=E2=80=99t:=
 integrate a keyword search (maybe just Python whoosh or Elastic if you can=
) along with vector. Compare results quality. Also work on prompt tuning =
=E2=80=93 e.g., test different prompt wording (=E2=80=9CUse the provided co=
ntext to answer=E2=80=A6=E2=80=9D vs. =E2=80=9CAnswer concisely based on in=
fo above.=E2=80=9D). See what yields better factual accuracy. Around Day 18=
, purposely break it =E2=80=93 ask something outside the knowledge base =E2=
=80=93 see if it says =E2=80=9CI don=E2=80=99t know.=E2=80=9D If not, refin=
e instructions. Checkpoint Day 21: Your bot should be much more robust: les=
s hallucination, and you should have a good handle on tuning it.
Week 4: Advanced patterns and optimization =E2=80=93 Now incorporate one ad=
vanced concept: maybe GraphRAG-lite (even just linking sections by title), =
or multi-modal (throw an image in and handle it if applicable), or implemen=
t a second retrieval step for a complex query. This is stretch learning =E2=
=80=93 pick what interests you. Also, measure performance: how fast is quer=
y? Try to speed it up (maybe reduce embedding size or use batching). If cos=
t is an issue, maybe deploy a local model for retrieval or generation and m=
easure quality vs API. In parallel, start packaging your project: container=
ize it or deploy on a free service (like Streamlit Sharing or Hugging Face =
Spaces) so others can try. Checkpoint Day 28: You=E2=80=99ve implemented so=
mething non-trivial beyond basics and have a sharable prototype.
Days 29-30: Scale and integrate =E2=80=93 Think bigger: if this were used b=
y 1000 people, what would you need? Perhaps set up a Pinecone trial and ind=
ex more data (if you have). Or integrate it into a simple UI (a chat web in=
terface). Basically stress test and refine. Day 30: reflect on what you=E2=
=80=99ve learned, post a summary on a forum or LinkedIn =E2=80=93 teaching =
solidifies learning.
This challenge covers building, tuning, and scaling aspects in a condensed =
way. Adjust as needed =E2=80=93 the goal is to touch on each important aspe=
ct at least briefly (data, retrieval variants, prompting, eval, deployment)=
=2E
By the end of these 30 days, you should feel like a RAG practitioner: able =
to build a custom QA system, aware of pitfalls, and ready to apply these sk=
ills in a project or job. And importantly, you=E2=80=99ll have a deeper int=
uition for why RAG works the way it does and how to get the most out of it.
That concludes your toolkit =E2=80=93 but one more thing before we sign off=
: a dose of inspiration and urgency in our concluding words.
Real Stories from the Trenches
Let=E2=80=99s ground this in reality with some rapid-fire case studies =E2=
=80=93 real stories of RAG in action that show what=E2=80=99s possible, alo=
ng with metrics and lessons learned from each.
Vimeo=E2=80=99s Video Chat Revolution: Vimeo integrated RAG to help users s=
earch within their video content. Think of it as a =E2=80=9Cvideo chat=E2=
=80=9D =E2=80=93 a user asks about a particular tutorial video (=E2=80=9CHo=
w do I add music to my project in Video X?=E2=80=9D) and the chatbot, using=
 RAG, retrieves the transcript section where that is explained, and answers=
 with reference to the timestamp. In testing, they found users could get to=
 the info 3=C3=97 faster than scrubbing through videos manually. The wow mo=
ment was when a user asked a vague question and the bot answered, =E2=80=9C=
At 2:13 in the video, the host explains how to add music=E2=80=A6=E2=80=9D,=
 providing a direct link. This boosted user engagement with tutorial videos=
 by an estimated 20% (because users weren=E2=80=99t dropping off frustrated=
). Lesson: multi-modal RAG (transcripts as data) can unlock content that wa=
s otherwise hard to navigate. And users loved the time-specific answers.
Legal Firm Processes 1M Documents in 24 Hours: A large law firm dealing wit=
h litigation had to comb through a million documents (emails, PDFs) for rel=
evant evidence =E2=80=93 a classic e-discovery nightmare. They deployed a R=
AG pipeline with a combination of keyword filtering and vector search. With=
in 24 hours, the system (running on a beefy cloud setup) indexed all docs a=
nd allowed attorneys to ask questions like =E2=80=9CFind discussions of pro=
ject Thunderbolt budget overruns.=E2=80=9D The RAG system retrieved key ema=
ils and memos in seconds. One attorney said it was like having a team of 50=
 paralegals working overnight. The firm reported they found crucial evidenc=
e in hours instead of weeks, potentially saving $200,000 in billable time. =
Lesson: RAG at scale + domain expertise =3D massive efficiency gains. Also,=
 they learned to trust but verify =E2=80=93 every AI-found doc was double-c=
hecked by a human, which was still faster than humans finding it in the fir=
st place.
Hospital Reduces Misdiagnosis by 30%: A hospital implemented a RAG-powered =
support tool for doctors. It indexed medical literature, patient histories,=
 and guidelines. During diagnosis, a doctor could quietly query, say, =E2=
=80=9Cpatient with X symptoms and Y lab results =E2=80=93 possible conditio=
ns?=E2=80=9D The AI would retrieve similar case studies and relevant guidel=
ine excerpts. Over 6 months, in a pilot, the tool flagged several cases whe=
re the initial human diagnosis missed a rare disease =E2=80=93 suggesting f=
urther tests which confirmed the rarer condition. Hospital data showed a 30=
% reduction in diagnostic errors in departments where the tool was used. Do=
ctors noted it was like getting a second opinion from an encyclopedia that =
actually understood context. One key metric: malpractice incidents in that =
period dropped (though need long-term data). Lesson: RAG can act as a safet=
y net in high-stakes fields, but it=E2=80=99s critical to have up-to-date, =
vetted data in the index. Also, doctors had to be trained to use the tool e=
ffectively; those that did saw noticeable improvements.
Financial Firm=E2=80=99s Fraud Detection Transformation: A fintech company =
used RAG to enhance fraud investigations. They have tons of transaction dat=
a and profiles of known fraud patterns. Their new system let analysts ask q=
uestions like =E2=80=9CShow me any connection between user A=E2=80=99s tran=
sactions and these suspicious accounts=E2=80=9D =E2=80=93 behind the scenes=
, it retrieved relevant logs and even generated a graph visualization of co=
nnections (GraphRAG in action). What used to take an analyst days of SQL qu=
eries and cross-referencing was done in minutes. In one case, this system i=
dentified a fraud ring of 12 accounts that had eluded earlier detection rul=
es. The firm estimated they prevented $1M in fraud in that quarter thanks t=
o quicker, deeper analysis by the AI assistant. Analysts noted that the AI =
could surface non-obvious links (like matching phone numbers or device fing=
erprints across accounts) that they might have missed. Lesson: RAG can augm=
ent human pattern-finding, and combining structured data retrieval with uns=
tructured (like support tickets content) provided a holistic view.
Across these stories, common threads:
The metrics (faster by X%, errors down Y%, cost saved $Z) build the busines=
s case for RAG.
Implementation lessons (like doctors needing training, or law firm verifyin=
g AI results) show that it=E2=80=99s not just plug-and-play; process integr=
ation matters.
Timeline: many achieved significant results in months, not years, once data=
 and tools were in place.
User acceptance: Initially, some professionals were skeptical (e.g., lawyer=
s hesitant to trust AI suggestions), but success cases converted many into =
proponents. Key was keeping them in control (AI suggests, human confirms).
These real-world successes hopefully spark ideas for your context. Whether =
it=E2=80=99s speeding up content access (Vimeo), supercharging analysis (le=
gal/finance), or acting as a diagnostic safety net (medical), RAG is making=
 a tangible impact. Think about your field: what information overload or de=
lay could be tackled with these techniques? The examples above were once ju=
st wishful thinking, now they are proven.
Lastly, before we finish, let=E2=80=99s chart out an action plan for you to=
 get from here to your own success story in the next 90 days.
Your Action Plan: Next 90 Days
Ready to build the future? Here=E2=80=99s a clear 90-day roadmap to go from=
 theory to impact, whether you=E2=80=99re implementing RAG in your company =
or building your own project. We=E2=80=99ll break it down by weeks with con=
crete goals and success metrics at each checkpoint.
Weeks 1-2: Environment Setup & First Build
Goal: Set up infrastructure and create a basic RAG application.
Tasks:
Assemble your =E2=80=9CStart Here=E2=80=9D stack (as mentioned in Toolkit).=
 Install libraries (LangChain, etc.), get API keys if needed.
Pick a small set of data relevant to your domain (maybe 10-20 documents).
Build a simple retrieval + LLM script to answer questions from that data.
Experiment with a few prompts and questions to ensure it works end-to-end.
Success Metric: By end of Week 2, you should be able to ask a question and =
get a reasonable answer with a source citation from your data. Essentially,=
 a prototype Q&A chatbot is functioning on a small scale.
Checkpoint assessment: Do a demo to a colleague or friend. If they ask a qu=
estion from the doc and get a correct answer, you=E2=80=99re on track. If n=
ot, troubleshoot (likely issues: parsing errors, poor prompt, etc. =E2=80=
=93 fix those now while scope is small).
Weeks 3-4: Production-Ready Prototype
Goal: Scale up data and robustness; integrate a front-end if needed.
Tasks:
Increase your dataset size (if you ultimately need 1000 docs, try indexing =
a few hundred this week).
Implement necessary chunking, metadata, and possibly hybrid search if queri=
es are complex.
Add a user interface or integrate into your app environment (e.g., a simple=
 web UI or Slack bot).
Start logging queries and answers for review.
Define =E2=80=9CI don=E2=80=99t know=E2=80=9D behavior: decide how the syst=
em should respond when unsure (and implement that guard).
Success Metric: By end of Week 4, your prototype should handle the full bre=
adth of your use-case questions with, say, >80% accuracy/relevance in testi=
ng. Also, non-experts should be able to use it via the UI and find it usefu=
l.
Checkpoint assessment: Conduct a small user test (could be colleagues from =
different teams). Give them 5-10 sample questions to try. If majority of an=
swers are correct and users find the interface easy, you pass. Note any fai=
lures for improvement.
Month 2 (Weeks 5-8): Advanced Patterns & Optimization
Goal: Enhance system intelligence and efficiency; address any failures from=
 testing.
Tasks:
If you found patterns in misses (e.g., multi-hop questions failing), implem=
ent recursive retrieval or agent steps for those.
If certain info was missing or outdated, update your index pipeline (maybe =
link it to the source of truth for auto-updates).
Optimize latency: perhaps introduce caching for repeated queries, or use a =
faster embedding model if embedding time is slow.
Security check: implement basic auth if needed, and ensure no sensitive dat=
a leaks (e.g., mask PII in responses if applicable).
Scale dry-run: simulate or actually run, say, 1000 queries and see if syste=
m holds up (both accuracy and performance).
Success Metric: By end of Week 8, the system=E2=80=99s accuracy should impr=
ove (target >90% on known evaluation set). Latency should be within accepta=
ble range for users (e.g., <2 seconds per query for interactive use). And t=
he system should handle a moderate concurrent load (if relevant).
Checkpoint assessment: Re-run your earlier user test (and maybe expand it).=
 If previously it got 80% right, see if now it=E2=80=99s 90%+. If latency w=
as an issue, see if users now feel it=E2=80=99s snappy. Also do an internal=
 stress test: run a script to send, say, 50 queries in a short burst =E2=80=
=93 does it still respond correctly and quickly? If any fail or slow down m=
assively, address that (maybe need concurrency handling or rate limiting).
Month 3 (Weeks 9-12): Scale & Integrate for Real-world Use
Goal: Deploy at full scale and integrate into business workflow; establish =
monitoring and continuous improvement loop.
Tasks:
Deploy the system to production environment (cloud or on-prem). Index all r=
equired data (full corpus).
Integrate with existing systems: e.g., link it on your website, or enable i=
t for support agents in their console, etc., as appropriate.
Set up monitoring dashboards for usage, accuracy signals (like user feedbac=
k), latency, and costs. Use real user feedback mechanism (thumbs up/down).
Train users/staff as needed (=E2=80=9CHere=E2=80=99s how to ask the AI, her=
e=E2=80=99s what it can/can=E2=80=99t do=E2=80=9D).
Create an evaluation schedule: e.g., review logs weekly to catch any bad an=
swers and feed that back (either by adjusting data or prompt or adding thos=
e cases to a training set).
Success Metric: By end of 90 days, you have a live RAG-powered feature with=
 actual users. Key metrics could be: X daily active users interacting with =
it, Y% of feedback is positive, Z minutes saved on average per query (if me=
asurable). Essentially, a measurable positive impact on whatever process yo=
u targeted.
Checkpoint assessment: After 2-4 weeks of production use, produce a brief r=
eport (even if informal): how often is it used, what are outcomes? For exam=
ple, =E2=80=9COur support bot deflected 50% of tier-1 questions in the firs=
t month, freeing up 100 hours of agent time=E2=80=9D or =E2=80=9CInternal t=
ool answered 200 queries with 95% accuracy as rated by staff, saving numero=
us email exchanges.=E2=80=9D If the metrics align with success criteria set=
 by stakeholders, congrats =E2=80=93 you=E2=80=99ve delivered. If not, iden=
tify why: is accuracy still lacking on some edge cases? Are users not adopt=
ing it (maybe need to improve UX or training)? Use these insights to iterat=
e further.
This 90-day plan is aggressive but realistic for many scenarios. The key is=
 iterative development and constant feedback. Don=E2=80=99t aim for perfect=
 out of the gate; get something usable, then refine. Each checkpoint ensure=
s you=E2=80=99re not going down a wrong path too long without correction.
By following this plan, in 3 months you=E2=80=99ll not only have a working =
solution but also the confidence of stakeholders (seeing progress and metri=
cs) and the foundation for continuous improvement. RAG projects aren=E2=80=
=99t =E2=80=9Cset and forget=E2=80=9D =E2=80=93 but after 90 days, you=E2=
=80=99ll have the infrastructure to keep making it better and the success t=
o justify that effort.
And with that, you=E2=80=99re equipped to build the future, one retrieval-a=
ugmented step at a time.
Congratulations! You=E2=80=99ve journeyed from 0 to RAG (and maybe to 5K an=
d beyond). We=E2=80=99ve covered why it matters, how it works, how to build=
 it, and where it=E2=80=99s all headed. The companies winning in this new e=
ra aren=E2=80=99t necessarily those with the biggest models, but those who =
best harness their own knowledge with AI. RAG is how you give your AI that =
=E2=80=9Cperfect memory=E2=80=9D and tie it into your world.
The question now isn=E2=80=99t whether to start using these tools, but whet=
her you=E2=80=99ll start before your competition does. So grab this guide, =
assemble your toolkit, and start building. Let your AI know your business i=
nside and out =E2=80=93 make it your smartest team member.
Ready to join hundreds of builders mastering RAG and other technologies? Fe=
el free to hop into the Nate=E2=80=99s Newsletter Discord [ https://substac=
k.com/redirect/d6462442-aab1-47fa-924d-1c50661df655?j=3DeyJ1IjoiNWtiOTN6In0=
=2Ezdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4=
tAjMRMPOw0 ] and let=E2=80=99s build the=20=
future together. The future is knocking =E2=80=93 and now you have the keys=
=2E
For more on AI, subscribe and share!
Google Doc with Links for this article is here [ https://substack.com/redir=
ect/86534399-6e57-4a8b-b494-6347518f018f?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxP=
mLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0 ]

Unsubscribe https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9uYXRlc25ld3N=
sZXR0ZXIuc3Vic3RhY2suY29tL2FjdGlvbi9kaXNhYmxlX2VtYWlsP3Rva2VuPWV5SjFjMlZ5WD=
Jsa0lqb3pNelkwTkRneU1qTXNJbkJ2YzNSZmFXUWlPakUyTnpNd056RTVOU3dpYVdGMElqb3hOe=
lV4TkRZeE56VTNMQ0psZUhBaU9qRTNPREk1T1RjM05UY3NJbWx6Y3lJNkluQjFZaTB4TXpjek1q=
TXhJaXdpYzNWaUlqb2laR2x6WVdKc1pWOWxiV0ZwYkNKOS45bjJfX2pQZEZxUmZla1B2RG1mVXF=
5NzEyd1J4dlV3LTMtal92TVczX1RnIiwicCI6MTY3MzA3MTk1LCJzIjoxMzczMjMxLCJmIjpmYW=
xzZSwidSI6MzM2NDQ4MjIzLCJpYXQiOjE3NTE0NjE3NTcsImV4cCI6MjA2NzAzNzc1NywiaXNzI=
joicHViLTAiLCJzdWIiOiJsaW5rLXJlZGlyZWN0In0.LNzLtWbJtsha5vdnyC5gk4nLA_vqphwH=
Ng4KQvULLjw?
--ebe98e78b0216adcedd09930bab97482a73c6559294177601adf396a187f
Content-Type: text/html; charset="utf-8"
Content-Transfer-Encoding: quoted-printable

<html style=3D"scrollbar-width: thin;scrollbar-color: rgb(219,219,219)rgb(2=
55,255,255);"><head>
<meta http-equiv=3D"Content-Type" content=3D"text/html; charset=3Dutf-8"><t=
itle>RAG: The Complete Guide to Retrieval-Augmented Generation for AI</titl=
e><style>
@media all and (-ms-high-contrast: none), (-ms-high-contrast: active) {
  .typography .markup table.image-wrapper img,
  .typography.editor .markup table.image-wrapper img,
  .typography .markup table.kindle-wrapper img,
  .typography.editor .markup table.kindle-wrapper img {
    max-width: 550px;
  }
}
@media (max-width: 1024px) {
  .typography,
  .typography.editor {
    /* Disable offset on mobile/tablet */
  }
  .typography .captioned-image-container figure:has(> a.image2-align-left),
  .typography.editor .captioned-image-container figure:has(> a.image2-align=
-left),
  .typography .captioned-image-container figure:has(> a.image2-align-right)=
,
  .typography.editor .captioned-image-container figure:has(> a.image2-align=
-right) {
    float: none;
    margin: 1em auto;
    max-width: 100%;
    width: auto;
    padding: 0;
  }
  .typography .captioned-image-container figure:has(> a.image2-align-left.t=
hefp),
  .typography.editor .captioned-image-container figure:has(> a.image2-align=
-left.thefp),
  .typography .captioned-image-container figure:has(> a.image2-align-right.=
thefp),
  .typography.editor .captioned-image-container figure:has(> a.image2-align=
-right.thefp) {
    margin: 1em auto;
  }
  .typography .captioned-image-container figure:has(> a.image2-offset-left)=
,
  .typography.editor .captioned-image-container figure:has(> a.image2-offse=
t-left),
  .typography .captioned-image-container figure:has(> a.image2-offset-right=
),
  .typography.editor .captioned-image-container figure:has(> a.image2-offse=
t-right) {
    margin: 1em auto;
  }
  .typography .captioned-image-container figure:has(> a.image2-align-left) =
.image2-inset,
  .typography.editor .captioned-image-container figure:has(> a.image2-align=
-left) .image2-inset,
  .typography .captioned-image-container figure:has(> a.image2-align-right)=
 .image2-inset,
  .typography.editor .captioned-image-container figure:has(> a.image2-align=
-right) .image2-inset {
    display: block;
    justify-content: initial;
  }
}
@media (max-width: 768px) {
  .typography .markup div.sponsorship-campaign-embed,
  .typography.editor .markup div.sponsorship-campaign-embed {
    margin-top: 24px;
    margin-bottom: 24px;
  }
  .typography .markup div.sponsorship-campaign-embed:first-child,
  .typography.editor .markup div.sponsorship-campaign-embed:first-child {
    margin-top: 0px;
  }
}
@media screen and (max-width: 650px) {
  .typography .markup div.youtube-overlay,
  .typography.editor .markup div.youtube-overlay,
  .typography .markup div.vimeo-overlay,
  .typography.editor .markup div.vimeo-overlay {
    display: none !important;
  }
}
@media screen and (max-width: 370px) {
  .typography .markup div.tiktok-wrap,
  .typography.editor .markup div.tiktok-wrap {
    width: calc(95vw - 32px);
    height: calc((95vw - 32px - 2px) / 0.485714);
  }
}
@media screen and (max-width: 650px) {
  .typography .markup div.embedded-publication-wrap .embedded-publication.s=
how-subscribe,
  .typography.editor .markup div.embedded-publication-wrap .embedded-public=
ation.show-subscribe {
    padding: 24px;
  }
}
@media screen and (max-width: 650px) {
  .typography .markup div.subscription-widget-wrap .subscription-widget.sho=
w-subscribe,
  .typography.editor .markup div.subscription-widget-wrap .subscription-wid=
get.show-subscribe,
  .typography .markup div.subscription-widget-wrap-editor .subscription-wid=
get.show-subscribe,
  .typography.editor .markup div.subscription-widget-wrap-editor .subscript=
ion-widget.show-subscribe,
  .typography .markup div.captioned-button-wrap .subscription-widget.show-s=
ubscribe,
  .typography.editor .markup div.captioned-button-wrap .subscription-widget=
.show-subscribe {
    padding: 0px 24px;
  }
}
@media screen and (max-width: 650px) {
  .typography .markup div.subscription-widget-wrap .subscription-widget.sho=
w-subscribe .subscription-widget-subscribe .button,
  .typography.editor .markup div.subscription-widget-wrap .subscription-wid=
get.show-subscribe .subscription-widget-subscribe .button,
  .typography .markup div.subscription-widget-wrap-editor .subscription-wid=
get.show-subscribe .subscription-widget-subscribe .button,
  .typography.editor .markup div.subscription-widget-wrap-editor .subscript=
ion-widget.show-subscribe .subscription-widget-subscribe .button,
  .typography .markup div.captioned-button-wrap .subscription-widget.show-s=
ubscribe .subscription-widget-subscribe .button,
  .typography.editor .markup div.captioned-button-wrap .subscription-widget=
.show-subscribe .subscription-widget-subscribe .button {
    padding: 10px 12px;
    min-width: 110px;
  }
}
@media (max-width: 650px) {
  .typography .markup .tweet,
  .typography.editor .markup .tweet {
    padding: 12px;
  }
}
@media (max-width: 650px) {
  .typography .markup .tweet .tweet-text,
  .typography.editor .markup .tweet .tweet-text {
    font-size: 14px;
    line-height: 20px;
  }
}
@media (max-width: 650px) {
  .typography .markup .tweet .tweet-photos-container.two,
  .typography.editor .markup .tweet .tweet-photos-container.two,
  .typography .markup .tweet .tweet-photos-container.three,
  .typography.editor .markup .tweet .tweet-photos-container.three,
  .typography .markup .tweet .tweet-photos-container.four,
  .typography.editor .markup .tweet .tweet-photos-container.four {
    height: 200px;
  }
}
@media (max-width: 650px) {
  .typography .markup .tweet a.expanded-link .expanded-link-img,
  .typography.editor .markup .tweet a.expanded-link .expanded-link-img {
    max-height: 180px;
  }
}
@media (max-width: 650px) {
  .typography .markup .tweet a.expanded-link .expanded-link-description,
  .typography.editor .markup .tweet a.expanded-link .expanded-link-descript=
ion {
    display: none;
  }
}
@media screen and (max-width: 650px) {
  .typography .markup .apple-podcast-container,
  .typography.editor .markup .apple-podcast-container {
    width: unset;
  }
}
@media (max-width: 420px) {
  .typography .markup .install-substack-app-embed img.install-substack-app-=
embed-img,
  .typography.editor .markup .install-substack-app-embed img.install-substa=
ck-app-embed-img {
    margin: 0 auto 16px auto;
  }
}
@media (max-width: 420px) {
  .typography .markup .install-substack-app-embed .install-substack-app-emb=
ed-text,
  .typography.editor .markup .install-substack-app-embed .install-substack-=
app-embed-text {
    margin: 0 0 12px 0;
    max-width: 100%;
    width: auto;
    text-align: center;
  }
}
@media (max-width: 420px) {
  .typography .markup .install-substack-app-embed .install-substack-app-emb=
ed-link,
  .typography.editor .markup .install-substack-app-embed .install-substack-=
app-embed-link {
    display: flex;
    justify-content: center;
  }
}
@media screen and (min-width: 481px) {
  .share-button-container {
    height: 38px;
  }
}
@media screen and (min-width: 481px) {
  .share-button-container a.comment {
    height: 38px;
    line-height: 38px;
    padding-right: 10px;
  }
}
@media screen and (max-width: 480px) {
  .share-button-container .separator {
    display: block;
    margin: 0;
    height: 8px;
    border-left: none;
  }
}
@media screen and (max-width: 480px) {
  .share-button-container a.share.first img {
    padding-left: 0;
  }
}
@media screen and (min-width: 481px) {
  .share-button-container a.mobile {
    display: none !important;
  }
}
@media screen and (min-width: 541px) {
  .settings-add-pub-modal-wrapper .container .add-recommending-pub-modal-co=
ntainer {
    padding: 36px;
    height: 680px;
  }
}
@media screen and (min-width: 541px) {
  .settings-add-pub-modal-wrapper .container .add-recommending-pub-modal-co=
ntainer .footer {
    position: absolute;
    bottom: 36px;
    margin: 0px;
  }
}
@media screen and (max-width: 650px) {
  .header-anchor-parent {
    display: none;
  }
}
@media screen and (max-width: 768px) {
  .post {
    padding: 16px 0 0 0;
  }
}
@media screen and (max-width: 650px) {
  .post .post-header .post-label {
    margin-top: 8px;
  }
}
@media screen and (max-width: 650px) {
  .post .post-header .meta-author-wrap.alternative-meta .meta-right-column =
.post-meta {
    margin-top: 6px;
  }
}
@media screen and (max-width: 650px) {
  .post .footer-facepile-container {
    height: 64px;
    padding: 0 16px;
    display: flex;
    align-items: center;
    justify-content: flex-start;
    width: 100%;
  }
}
@media screen and (max-width: 650px) {
  .post .post-footer.use-separators {
    justify-content: center;
  }
}
@media screen and (max-width: 650px) {
  .post .post-footer.next-prev {
    height: 64px;
    justify-content: space-between;
    box-sizing: border-box;
  }
}
@media screen and (max-width: 650px) {
  .post-contributor-footer .post-contributor-bio-table {
    display: block;
  }
  .post-contributor-footer .post-contributor-bio-table-row {
    display: flex;
    flex-direction: row;
  }
  .post-contributor-footer .post-contributor-bio-userhead-cell,
  .post-contributor-footer .post-contributor-bio-body-cell {
    display: block;
  }
  .post-contributor-footer .post-contributor-bio-body-cell {
    flex-grow: 1;
  }
  .post-contributor-footer .post-contributor-bio-body-table {
    display: block;
  }
  .post-contributor-footer .post-contributor-bio-body-table-row {
    display: block;
  }
  .post-contributor-footer .post-contributor-bio-copy-cell,
  .post-contributor-footer .post-contributor-bio-controls-cell {
    display: block;
  }
  .post-contributor-footer .post-contributor-bio-copy-cell {
    margin: 0 0 16px 0;
  }
  .post-contributor-footer .post-contributor-bio-controls-cell {
    width: auto;
  }
  .post-contributor-footer .post-contributor-bio-controls {
    margin: auto;
  }
  .post-contributor-footer .post-contributor-bio-controls .button.primary {
    width: 100%;
  }
  .post-contributor-footer .post-contributor-bio-text {
    font-size: 14px;
  }
}
@media screen and (min-width: 768px) {
  .post-silhouette {
    padding: 32px 0;
  }
}
@media screen and (max-width: 650px) {
  .post-silhouette .post-silhouette-title {
    margin-top: 10.44225025px;
    height: 120px;
  }
}
@media screen and (max-width: 650px) {
  .post-silhouette .post-silhouette-meta {
    width: 75%;
  }
}
@media screen and (max-width: 650px) {
  .post-silhouette .post-silhouette-meta.with-byline-image {
    margin: 20px 0;
  }
}
@media screen and (max-width: 650px) {
  .use-theme-bg .post-meta.alternative-meta .post-meta-item,
  .post-meta.alternative-meta .post-meta-item {
    padding-right: 16px;
  }
}
@media screen and (max-width: 370px) {
  .use-theme-bg .post-meta.alternative-meta .post-meta-item,
  .post-meta.alternative-meta .post-meta-item {
    font-size: 14px;
  }
}
@media screen and (max-width: 650px) {
  .use-theme-bg .post-meta.alternative-meta .post-meta-item.guest-author-pu=
blication,
  .post-meta.alternative-meta .post-meta-item.guest-author-publication {
    display: none;
  }
}
@media screen and (max-width: 370px) {
  .post-meta .post-meta-item .post-meta-button {
    height: 36px !important;
    /* important to override in-line height style on emails */
  }
  .post-meta .post-meta-item .post-meta-button .meta-button-label {
    display: none;
  }
  .post-meta .post-meta-item .post-meta-button > svg {
    margin-right: 0;
  }
}
@media screen and (max-width: 370px) {
  .post-meta .post-meta-item {
    font-size: 12px;
  }
}
@media screen and (max-width: 650px) {
  .post .floating-subscribe-button {
    bottom: 20px;
    right: 20px;
  }
}
@media all and (-ms-high-contrast: none), (-ms-high-contrast: active) {
  body .markup table.image-wrapper img,
  body .markup table.kindle-wrapper img {
    max-width: 550px;
  }
}
@media (max-width: 1024px) {
  body {
    /* Disable offset on mobile/tablet */
  }
  body .captioned-image-container figure:has(> a.image2-align-left),
  body .captioned-image-container figure:has(> a.image2-align-right) {
    float: none;
    margin: 1em auto;
    max-width: 100%;
    width: auto;
    padding: 0;
  }
  body .captioned-image-container figure:has(> a.image2-align-left.thefp),
  body .captioned-image-container figure:has(> a.image2-align-right.thefp) =
{
    margin: 1em auto;
  }
  body .captioned-image-container figure:has(> a.image2-offset-left),
  body .captioned-image-container figure:has(> a.image2-offset-right) {
    margin: 1em auto;
  }
  body .captioned-image-container figure:has(> a.image2-align-left) .image2=
-inset,
  body .captioned-image-container figure:has(> a.image2-align-right) .image=
2-inset {
    display: block;
    justify-content: initial;
  }
}
@media (max-width: 768px) {
  body .markup div.sponsorship-campaign-embed {
    margin-top: 24px;
    margin-bottom: 24px;
  }
  body .markup div.sponsorship-campaign-embed:first-child {
    margin-top: 0px;
  }
}
@media screen and (max-width: 650px) {
  body .markup div.youtube-overlay,
  body .markup div.vimeo-overlay {
    display: none !important;
  }
}
@media screen and (max-width: 370px) {
  body .markup div.tiktok-wrap {
    width: calc(95vw - 32px);
    height: calc((95vw - 32px - 2px) / 0.485714);
  }
}
@media screen and (max-width: 650px) {
  body .markup div.embedded-publication-wrap .embedded-publication.show-sub=
scribe {
    padding: 24px;
  }
}
@media screen and (max-width: 650px) {
  body .markup div.subscription-widget-wrap .subscription-widget.show-subsc=
ribe,
  body .markup div.subscription-widget-wrap-editor .subscription-widget.sho=
w-subscribe,
  body .markup div.captioned-button-wrap .subscription-widget.show-subscrib=
e {
    padding: 0px 24px;
  }
}
@media screen and (max-width: 650px) {
  body .markup div.subscription-widget-wrap .subscription-widget.show-subsc=
ribe .subscription-widget-subscribe .button,
  body .markup div.subscription-widget-wrap-editor .subscription-widget.sho=
w-subscribe .subscription-widget-subscribe .button,
  body .markup div.captioned-button-wrap .subscription-widget.show-subscrib=
e .subscription-widget-subscribe .button {
    padding: 10px 12px;
    min-width: 110px;
  }
}
@media (max-width: 650px) {
  body .markup .tweet {
    padding: 12px;
  }
}
@media (max-width: 650px) {
  body .markup .tweet .tweet-text {
    font-size: 14px;
    line-height: 20px;
  }
}
@media (max-width: 650px) {
  body .markup .tweet .tweet-photos-container.two,
  body .markup .tweet .tweet-photos-container.three,
  body .markup .tweet .tweet-photos-container.four {
    height: 200px;
  }
}
@media (max-width: 650px) {
  body .markup .tweet a.expanded-link .expanded-link-img {
    max-height: 180px;
  }
}
@media (max-width: 650px) {
  body .markup .tweet a.expanded-link .expanded-link-description {
    display: none;
  }
}
@media screen and (max-width: 650px) {
  body .markup .apple-podcast-container {
    width: unset;
  }
}
@media (max-width: 420px) {
  body .markup .install-substack-app-embed img.install-substack-app-embed-i=
mg {
    margin: 0 auto 16px auto;
  }
}
@media (max-width: 420px) {
  body .markup .install-substack-app-embed .install-substack-app-embed-text=
 {
    margin: 0 0 12px 0;
    max-width: 100%;
    width: auto;
    text-align: center;
  }
}
@media (max-width: 420px) {
  body .markup .install-substack-app-embed .install-substack-app-embed-link=
 {
    display: flex;
    justify-content: center;
  }
}
@media screen and (min-width: 500px) {
  body .header a.logo {
    width: 42px;
    height: 42px;
    border-radius: 12px;
  }
}
@media screen and (max-width: 420px) {
  body .subscription-receipt table:first-of-type .subscription-amount .subs=
cription-discount {
    width: 72px !important;
  }
}
@media screen and (min-width: 481px) {
  body .share-button-container {
    height: auto;
  }
}
@media screen and (max-width: 480px) {
  body .share-button-container .separator {
    display: block !important;
    margin: 0 !important;
    height: 8px !important;
    border-left: none !important;
  }
}
@media screen and (max-width: 650px) {
  .digest .item .post-meta-item.audience {
    display: none;
  }
}
@media screen and (min-width: 500px) {
  .digest-publication .logo img {
    width: 42px;
    height: 42px;
    border-radius: 8px;
  }
}
@media screen and (max-width: 650px) {
  .comments-page .container .comment-list .collapsed-reply {
    margin-left: calc(10 + 32px - 24px);
  }
}
@media screen and (max-width: 650px) {
  .comment > .comment-list {
    padding-left: 24px;
  }
}
@media screen and (max-width: 650px) {
  .finish-magic-login-modal .modal-content .container {
    padding: 24px 0;
  }
}
@media (max-width: 650px) {
  .reader2-text-b3 {
    line-height: 24px;
  }
}
@media screen and (max-width: 650px) {
  .reader2-text-h4 {
    line-height: 24px;
  }
}
@media screen and (min-width: 541px) {
  .user-profile-modal {
    padding-left: 12px;
    padding-right: 12px;
  }
}
@media screen and (max-width: 650px) {
  .subscribe-widget form.form .sideBySideWrap button.rightButton {
    padding: 10px 12px;
  }
}
@media screen and (min-width: 541px) {
  .pub-icon:hover .logo-hover,
  .feed-item-icon:hover .logo-hover {
    display: block;
  }
}
@media screen and (max-width: 650px) {
  .post-ufi.single-full-width-button .post-ufi-button-wrapper {
    width: 100%;
    padding: 16px;
  }
  .post-ufi.single-full-width-button .post-ufi-button-wrapper:empty {
    display: none;
  }
  .post-ufi.single-full-width-button .post-ufi-button {
    width: 100%;
    justify-content: center;
  }
}
@media screen and (max-width: 768px) {
  .file-embed-wrapper {
    padding: 0;
  }
}
@media screen and (max-width: 768px) {
  .file-embed-wrapper-editor {
    padding: 0;
  }
}
@media screen and (max-width: 768px) {
  .file-embed-wrapper-editor:active {
    padding: 0;
  }
}
@media only screen and (max-width: 650px) {
  .file-embed-button.wide,
  .file-embed-error-button.wide {
    display: none;
  }
}
@media only screen and (min-width: 630px) {
  .file-embed-button.narrow,
  .file-embed-error-button.narrow {
    display: none;
  }
}
@media screen and (min-width: 541px) {
  .audio-player-wrapper .audio-player {
    min-width: 500px;
  }
}
@media screen and (max-width: 650px) {
  .audio-player-wrapper .audio-player .audio-player-progress {
    border-left-width: 16px;
    border-right-width: 16px;
  }
}
@media screen and (max-width: 650px) {
  .audio-player-wrapper .audio-player .audio-player-progress .audio-player-=
progress-bar .audio-player-progress-bar-popup {
    top: -54px;
  }
}
@media screen and (max-width: 650px) {
  .audio-player-wrapper-fancy .audio-player .audio-player-progress {
    border-left-width: 16px;
    border-right-width: 16px;
  }
}
@media screen and (max-width: 650px) {
  .audio-player-wrapper-fancy .audio-player .audio-player-progress .audio-p=
layer-progress-bar .audio-player-progress-bar-popup {
    top: -54px;
  }
}
@media (min-width: 250px) {
  .audio-player-wrapper-fancy .audio-player {
    padding: 32px;
  }
  .audio-player-wrapper-fancy .audio-player .btn-group {
    display: flex;
  }
  .audio-player-wrapper-fancy .audio-player .btn-group .button:last-of-type=
 {
    display: block;
  }
}
@media (min-width: 300px) {
  .audio-player-wrapper-fancy .audio-player .btn-group {
    display: block;
  }
  .audio-player-wrapper-fancy .audio-player .btn-group .button:first-of-typ=
e {
    display: block;
  }
}
@media (min-width: 350px) {
  .audio-player-wrapper-fancy .audio-player .audio-player-substack-logo {
    display: block;
  }
  .audio-player-wrapper-fancy .audio-player .audio-player-title {
    margin-top: 16px;
  }
  .audio-player-wrapper-fancy .audio-player .audio-player-hero-image-contai=
ner {
    padding-top: 15%;
    width: 15%;
    display: block;
  }
  .audio-player-wrapper-fancy .audio-player .btn-group .button:first-of-typ=
e {
    display: block;
  }
  .audio-player-wrapper-fancy .audio-player .audio-player-substack-logo {
    display: block;
  }
}
@media (min-width: 350px) {
  .audio-player-wrapper-fancy .audio-player .audio-player-hero-image-contai=
ner {
    padding-top: 25%;
    width: 25%;
    display: block;
  }
  .audio-player-wrapper-fancy .audio-player .btn-group {
    display: flex;
  }
  .audio-player-wrapper-fancy .audio-player .btn-group .button:first-of-typ=
e {
    display: block;
  }
}
@media (min-width: 400px) {
  .audio-player-wrapper-fancy .audio-player .audio-player-hero-image-contai=
ner {
    padding-top: 40%;
    width: 40%;
  }
}
@media (max-width: 400px) {
  .audio-player-wrapper-fancy .audio-player .btn-group {
    margin-top: 12px;
  }
  .audio-player-wrapper-fancy .audio-player .btn-group .button {
    font-size: 13px;
    padding: 6px 12px;
    height: auto;
    margin-top: 10px;
  }
}
@media (min-width: 600px) {
  .audio-player-wrapper-fancy .audio-player .audio-player-hero-image-contai=
ner {
    padding-top: 55%;
    width: 55%;
  }
}
@media (max-width: 650px) {
  .poll-editor-modal {
    min-width: calc(100% - 20px);
  }
}
@media (max-width: 750px) {
  .poll-embed .poll-anchor-target .poll-anchor-copy-button {
    left: 8px;
    top: 45px;
  }
}
@media screen and (min-width: 541px) {
  .poll-embed .poll-wrapper.poll-web .poll-dialog .modal-table .modal-row .=
modal-content > .container {
    width: 552px;
    padding: 26px 24px;
  }
}
@media screen and (max-width: 650px) {
  .poll-embed .poll-wrapper.poll-web .poll-dialog .modal-table .modal-row .=
modal-content > .container {
    padding: 40px 0;
  }
}
@media screen and (max-width: 650px) {
  .poll-embed .poll-wrapper.poll-web .poll-dialog .modal-row .modal-cell .m=
odal-exit-btn {
    margin-right: -20px;
  }
}</style></head><body class=3D"email-body" style=3D"font-kerning: auto;--im=
age-offset-margin: -117px;"><img src=3D"https://eotrx.substackcdn.com/open?=
token=3DeyJtIjoiPDIwMjUwNzAyMTMwMzEzLjMuNTkxMDdkZTg5ZGViMTI2ZkBtZzEuc3Vic3R=
hY2suY29tPiIsInUiOjMzNjQ0ODIyMywiciI6ImVpdGFuQGVpc2xhdy5jby5pbCIsImQiOiJtZz=
Euc3Vic3RhY2suY29tIiwicCI6MTY3MzA3MTk1LCJ0IjoicG9kY2FzdCIsImEiOiJvbmx5X3Bha=
WQiLCJzIjoxMzczMjMxLCJjIjoicG9zdCIsImYiOmZhbHNlLCJwb3NpdGlvbiI6InRvcCIsImlh=
dCI6MTc1MTQ2MTc1OCwiZXhwIjoxNzU0MDUzNzU4LCJpc3MiOiJwdWItMCIsInN1YiI6ImVvIn0=
.GqYoCQY6RMgUZIUvicdAsFoKppwfhBjJ7N-GvO_HGz0" alt=3D"" width=3D"1" height=
=3D"1" border=3D"0" style=3D"height:1px !important;width:1px !important;bor=
der-width:0 !important;margin-top:0 !important;margin-bottom:0 !important;m=
argin-right:0 !important;margin-left:0 !important;padding-top:0 !important;=
padding-bottom:0 !important;padding-right:0 !important;padding-left:0 !impo=
rtant;"><div class=3D"preview" style=3D"display:none;font-size:1px;color:#3=
33333;line-height:1px;max-height:0px;max-width:0px;opacity:0;overflow:hidde=
n;">Watch now (23 mins) | Master Retrieval-Augmented Generation=E2=80=94end=
 ChatGPT hallucinations, access live data, and see why 80% of enterprises b=
ack this $40 billion AI upgrade with this guide</div><div class=3D"preview"=
 style=3D"display:none;font-size:1px;color:#333333;line-height:1px;max-heig=
ht:0px;max-width:0px;opacity:0;overflow:hidden;">=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =
=C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &n=
bsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD</div><table class=3D"em=
ail-body-container" role=3D"presentation" width=3D"100%" border=3D"0" cells=
pacing=3D"0" cellpadding=3D"0"><tbody><tr><td></td><td class=3D"content" wi=
dth=3D"550"></td><td></td></tr><tr><td></td><td class=3D"content" width=3D"=
550" align=3D"left"><div style=3D"font-size: 16px;line-height: 26px;max-wid=
th: 550px;width: 100%;margin: 0 auto;overflow-wrap: break-word;"><table rol=
e=3D"presentation" width=3D"100%" border=3D"0" cellspacing=3D"0" cellpaddin=
g=3D"0"><tbody><tr><td align=3D"right" style=3D"height:20px;"><table role=
=3D"presentation" width=3D"auto" border=3D"0" cellspacing=3D"0" cellpadding=
=3D"0"><tbody><tr><td style=3D"vertical-align:middle;"><span class=3D"pencr=
aft pc-reset reset-IxiVJZ tw-font-body tw-text-ssm tw-text-substack-seconda=
ry" style=3D"font-family: SF Pro Text, -apple-system, system-ui, BlinkMacSy=
stemFont, Inter, Segoe UI, Roboto, Helvetica, Arial, sans-serif, Apple Colo=
r Emoji, Segoe UI Emoji, Segoe UI Symbol;font-size: 13px;color: unset;list-=
style: none;text-decoration: unset;margin: 0;"><a class=3D"tw-text-substack=
-secondary tw-underline" href=3D"https://substack.com/redirect/2/eyJlIjoiaH=
R0cHM6Ly9uYXRlc25ld3NsZXR0ZXIuc3Vic3RhY2suY29tL3AvcmFnLXRoZS1jb21wbGV0ZS1nd=
WlkZS10by1yZXRyaWV2YWw_dXRtX2NhbXBhaWduPWVtYWlsLXBvc3Qmcj01a2I5M3omdG9rZW49=
ZXlKMWMyVnlYMmxrSWpvek16WTBORGd5TWpNc0luQnZjM1JmYVdRaU9qRTJOek13TnpFNU5Td2l=
hV0YwSWpveE56VXhORFl4TnpVM0xDSmxlSEFpT2pFM05UUXdOVE0zTlRjc0ltbHpjeUk2SW5CMV=
lpMHhNemN6TWpNeElpd2ljM1ZpSWpvaWNHOXpkQzF5WldGamRHbHZiaUo5LkRFYkxWYW1tcWU2T=
FptN2dOT1dEVTdyZUhzbXo4SmpFLV9XNlAyV0tPUDAiLCJwIjoxNjczMDcxOTUsInMiOjEzNzMy=
MzEsImYiOmZhbHNlLCJ1IjozMzY0NDgyMjMsImlhdCI6MTc1MTQ2MTc1NywiZXhwIjoyMDY3MDM=
3NzU3LCJpc3MiOiJwdWItMCIsInN1YiI6ImxpbmstcmVkaXJlY3QifQ.ccKUClg0u7IXwpUNiQv=
gVhjwRbJS8Z3OzTzTDTvmrxY?" style=3D"color: rgb(119,119,119);-webkit-text-de=
coration-line: underline;text-decoration-line: underline;">View in browser<=
/a></span></td></tr></tbody></table></td></tr></tbody></table><table role=
=3D"presentation" width=3D"auto" border=3D"0" cellspacing=3D"0" cellpadding=
=3D"0" style=3D"width:100%;"><tbody><tr><td style=3D"vertical-align:middle;=
width:100%;"><a href=3D"https://substack.com/app-link/post?publication_id=
=3D1373231&amp;post_id=3D167307195&amp;utm_source=3Dpodcast-email&amp;play_=
audio=3Dtrue&amp;r=3D5kb93z&amp;utm_campaign=3Demail-play-on-substack&amp;t=
oken=3DeyJ1c2VyX2lkIjozMzY0NDgyMjMsInBvc3RfaWQiOjE2NzMwNzE5NSwiaWF0IjoxNzUx=
NDYxNzU3LCJleHAiOjE3NTQwNTM3NTcsImlzcyI6InB1Yi0xMzczMjMxIiwic3ViIjoicG9zdC1=
yZWFjdGlvbiJ9.DEbLVammqe6LZm7gNOWDU7reHsmz8JjE-_W6P2WKOP0&amp;utm_content=
=3Dwatch_now_gif"><img src=3D"https://substackcdn.com/image/fetch/$s_!OIDw!=
,w_1138,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubst=
ack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F971084fa-f436-425d-8749=
-d98c4ef10255_320x180.gif" alt=3D"RAG-complete-Substack.mp4" style=3D"max-w=
idth: 550px;border: none;vertical-align: middle;width: 100%;border-radius: =
12px"></a></td></tr></tbody></table><table role=3D"presentation" width=3D"a=
uto" border=3D"0" cellspacing=3D"0" cellpadding=3D"0" style=3D"width:100%;"=
><tbody><tr><td><div style=3D"font-size: 16px;line-height: 26px;height: 16p=
x">&nbsp;</div></td></tr><tr height=3D"8"><td></td></tr><tr><td><table role=
=3D"presentation" width=3D"auto" border=3D"0" cellspacing=3D"0" cellpadding=
=3D"0" style=3D"width:100%;"><tbody><tr><td style=3D"vertical-align:middle;=
width:100%;"><table class=3D"fullWidth-mgXGs7" role=3D"presentation" width=
=3D"auto" border=3D"0" cellspacing=3D"0" cellpadding=3D"0" style=3D"box-siz=
ing: border-box;width: 100%;min-height: 40px;padding-left: 0;padding-right:=
 0;"><tbody><tr><td class=3D"fullWidth-mgXGs7 emailButtonTd-o2ymya priority=
_primary-vWRHI0" align=3D"center" style=3D"box-sizing: border-box;width: 10=
0%;min-height: 40px;padding-left: 0;padding-right: 0;border-radius: 8px;bac=
kground-color: #45D800;"><a class=3D"fullWidth-mgXGs7 emailButtonA-Ktpg7h" =
href=3D"https://substack.com/app-link/post?publication_id=3D1373231&amp;pos=
t_id=3D167307195&amp;utm_source=3Dpodcast-email&amp;play_audio=3Dtrue&amp;r=
=3D5kb93z&amp;utm_campaign=3Demail-play-on-substack&amp;token=3DeyJ1c2VyX2l=
kIjozMzY0NDgyMjMsInBvc3RfaWQiOjE2NzMwNzE5NSwiaWF0IjoxNzUxNDYxNzU3LCJleHAiOj=
E3NTQwNTM3NTcsImlzcyI6InB1Yi0xMzczMjMxIiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.DEbL=
Vammqe6LZm7gNOWDU7reHsmz8JjE-_W6P2WKOP0&amp;utm_content=3Dwatch_now_button"=
 style=3D"box-sizing: border-box;width: 100%;min-height: 40px;padding-left:=
 0;padding-right: 0;font-family: system-ui,-apple-system,BlinkMacSystemFont=
,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI=
 Emoji','Segoe UI Symbol';font-size: 14px;font-weight: 600;letter-spacing: =
-.15px;border-radius: 8px;padding: 12px 24px;line-height: 1;text-decoration=
: none;display: inline-block;color: #ffffff;border: none;"><img src=3D"http=
s://substackcdn.com/image/fetch/$s_!JR-4!,w_26.046511627906977,c_scale,f_pn=
g,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FPlay=
Icon%3Fv%3D4%26height%3D32%26fill%3D%2523ffffff%26stroke%3D%2523ffffff%26st=
rokeWidth%3D2" width=3D"13.023255813953488" height=3D"16" style=3D"border: =
none;vertical-align: middle;max-width: 13.023255813953488px" alt=3D""><span=
 style=3D"margin-left:8px;vertical-align:middle;">Watch now</span></a></td>=
</tr></tbody></table></td></tr></tbody></table></td></tr><tr height=3D"8"><=
td></td></tr><tr><td><div style=3D"font-size: 16px;line-height: 26px;height=
: 16px">&nbsp;</div></td></tr></tbody></table><div class=3D"post typography=
" dir=3D"auto" style=3D"--image-offset-margin: -117px;padding: 32px 0 0 0;f=
ont-size: 16px;line-height: 26px;"><div class=3D"post-header" role=3D"regio=
n" aria-label=3D"Post header" style=3D"font-size: 16px;line-height: 26px;">=
<h1 class=3D"post-title published" style=3D"color: rgb(54,55,55);font-famil=
y: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMac=
SystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji'=
,'Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoothin=
g: antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearance: opt=
imizelegibility;-moz-appearance: optimizelegibility;appearance: optimizeleg=
ibility;margin: 0;line-height: 36px;font-size: 32px;"><a href=3D"https://su=
bstack.com/app-link/post?publication_id=3D1373231&amp;post_id=3D167307195&a=
mp;utm_source=3Dpost-email-title&amp;utm_campaign=3Demail-post-title&amp;is=
Freemail=3Dfalse&amp;r=3D5kb93z&amp;token=3DeyJ1c2VyX2lkIjozMzY0NDgyMjMsInB=
vc3RfaWQiOjE2NzMwNzE5NSwiaWF0IjoxNzUxNDYxNzU3LCJleHAiOjE3NTQwNTM3NTcsImlzcy=
I6InB1Yi0xMzczMjMxIiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.DEbLVammqe6LZm7gNOWDU7re=
Hsmz8JjE-_W6P2WKOP0" style=3D"color: rgb(54,55,55);text-decoration: none;">=
RAG: The Complete Guide to Retrieval-Augmented Generation for AI</a></h1><h=
3 class=3D"subtitle" style=3D"font-family: 'SF Pro Display',-apple-system-h=
eadline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvet=
ica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol'=
;font-weight: normal;-webkit-font-smoothing: antialiased;-moz-osx-font-smoo=
thing: antialiased;-webkit-appearance: optimizelegibility;-moz-appearance: =
optimizelegibility;appearance: optimizelegibility;margin: 4px 0 0;color: #7=
77777;line-height: 24px;font-size: 18px;margin-top: 12px;">Master Retrieval=
-Augmented Generation=E2=80=94end ChatGPT hallucinations, access live data,=
 and see why 80% of enterprises back this $40 billion AI upgrade with this =
guide</h3><table class=3D"post-meta" role=3D"presentation" width=3D"100%" b=
order=3D"0" cellspacing=3D"0" cellpadding=3D"0" style=3D"margin: 1em 0;heig=
ht: 20px;align-items: center;"><tbody><tr><td><table role=3D"presentation" =
width=3D"auto" border=3D"0" cellspacing=3D"0" cellpadding=3D"0"><tbody><tr>=
<td><table role=3D"presentation" width=3D"auto" border=3D"0" cellspacing=3D=
"0" cellpadding=3D"0"><tbody><tr><td style=3D"vertical-align:middle;"><div =
class=3D"pencraft pc-reset color-primary-zABazT line-height-20-t4M0El font-=
meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq =
reset-IxiVJZ meta-EgzBVA custom-css-email-post-author" style=3D"list-style:=
 none;font-size: 11px;line-height: 20px;text-decoration: unset;color: rgb(5=
4,55,55);margin: 0;font-family: 'SF Compact',-apple-system,system-ui,-apple=
-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Ap=
ple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: 500;text-t=
ransform: uppercase;letter-spacing: .2px;"><a class=3D"pencraft pc-reset co=
lor-primary-zABazT line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx we=
ight-medium-fw81nC transform-uppercase-yKDgcq reset-IxiVJZ meta-EgzBVA" sty=
le=3D"list-style: none;color: rgb(54,55,55);margin: 0;font-size: 11px;line-=
height: 20px;font-family: 'SF Compact',-apple-system,system-ui,-apple-syste=
m,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Co=
lor Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: 500;text-transfo=
rm: uppercase;letter-spacing: .2px;text-decoration: none" href=3D"https://s=
ubstack.com/@natesnewsletter">Nate</a></div></td></tr></tbody></table></td>=
</tr><tr><td><table role=3D"presentation" width=3D"auto" border=3D"0" cells=
pacing=3D"0" cellpadding=3D"0"><tbody><tr><td style=3D"vertical-align:middl=
e;"><div class=3D"pencraft pc-reset color-secondary-ls1g8s line-height-20-t=
4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-upperc=
ase-yKDgcq reset-IxiVJZ meta-EgzBVA" style=3D"list-style: none;font-size: 1=
1px;line-height: 20px;text-decoration: unset;color: rgb(119,119,119);margin=
: 0;font-family: 'SF Compact',-apple-system,system-ui,-apple-system,BlinkMa=
cSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji=
','Segoe UI Emoji','Segoe UI Symbol';font-weight: 500;text-transform: upper=
case;letter-spacing: .2px;"><time datetime=3D"2025-07-02T13:06:15.529Z">Jul=
 2</time></div></td><td width=3D"4" style=3D"min-width: 4px"></td><td style=
=3D"vertical-align:middle;"><table role=3D"presentation" width=3D"auto" bor=
der=3D"0" cellspacing=3D"0" cellpadding=3D"0"><tbody><tr><td style=3D"verti=
cal-align:middle;"><div class=3D"pencraft pc-reset color-secondary-ls1g8s l=
ine-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC t=
ransform-uppercase-yKDgcq reset-IxiVJZ meta-EgzBVA" style=3D"list-style: no=
ne;font-size: 11px;line-height: 20px;text-decoration: unset;color: rgb(119,=
119,119);margin: 0;font-family: 'SF Compact',-apple-system,system-ui,-apple=
-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Ap=
ple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: 500;text-t=
ransform: uppercase;letter-spacing: .2px;">=E2=88=99</div></td><td width=3D=
"4" style=3D"min-width: 4px"></td><td style=3D"vertical-align:middle;"><div=
 class=3D"pencraft pc-reset color-paid-LmY0EP line-height-20-t4M0El font-me=
ta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq re=
set-IxiVJZ meta-EgzBVA" translated=3D"" style=3D"list-style: none;font-size=
: 11px;line-height: 20px;text-decoration: unset;color: rgb(94,73,217);margi=
n: 0;font-family: 'SF Compact',-apple-system,system-ui,-apple-system,BlinkM=
acSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoj=
i','Segoe UI Emoji','Segoe UI Symbol';font-weight: 500;text-transform: uppe=
rcase;letter-spacing: .2px;">Paid</div></td></tr></tbody></table></td></tr>=
</tbody></table></td></tr></tbody></table></td><td align=3D"right"><table r=
ole=3D"presentation" width=3D"auto" border=3D"0" cellspacing=3D"0" cellpadd=
ing=3D"0"><tbody><tr><td style=3D"vertical-align:middle;"><a href=3D"https:=
//substack.com/@natesnewsletter"><img class=3D"custom-css-email-avatar avat=
ar-QIQ5yR" src=3D"https://substackcdn.com/image/fetch/$s_!AgBy!,f_auto,q_au=
to:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws=
.com%2Fpublic%2Fimages%2Fa37385e3-0387-487a-9f2c-e13aa963da4c_1080x1080.png=
" style=3D"box-sizing: border-box;border-radius: 500000px;max-width: 550px;=
border: none;vertical-align: middle;width: 40px;height: 40px;min-width: 40p=
x;min-height: 40px;object-fit: cover;margin: 0px;display: inline" width=3D"=
40" height=3D"40"></a></td></tr></tbody></table></td></tr></tbody></table><=
table class=3D"email-ufi-2-top" role=3D"presentation" width=3D"100%" border=
=3D"0" cellspacing=3D"0" cellpadding=3D"0" style=3D"border-top: 1px solid r=
gb(0,0,0,.1);border-bottom: 1px solid rgb(0,0,0,.1);min-width: 100%;"><tbod=
y><tr height=3D"16"><td height=3D"16" style=3D"font-size:0px;line-height:0;=
">&nbsp;</td></tr><tr><td><table role=3D"presentation" width=3D"100%" borde=
r=3D"0" cellspacing=3D"0" cellpadding=3D"0"><tbody><tr><td><table role=3D"p=
resentation" width=3D"auto" border=3D"0" cellspacing=3D"0" cellpadding=3D"0=
"><tbody><tr><td style=3D"vertical-align:middle;"><table role=3D"presentati=
on" width=3D"38" border=3D"0" cellspacing=3D"0" cellpadding=3D"0"><tbody><t=
r><td align=3D"center"><a class=3D"email-icon-button" href=3D"https://subst=
ack.com/app-link/post?publication_id=3D1373231&amp;post_id=3D167307195&amp;=
utm_source=3Dsubstack&amp;isFreemail=3Dfalse&amp;submitLike=3Dtrue&amp;toke=
n=3DeyJ1c2VyX2lkIjozMzY0NDgyMjMsInBvc3RfaWQiOjE2NzMwNzE5NSwicmVhY3Rpb24iOiL=
inaQiLCJpYXQiOjE3NTE0NjE3NTcsImV4cCI6MTc1NDA1Mzc1NywiaXNzIjoicHViLTEzNzMyMz=
EiLCJzdWIiOiJyZWFjdGlvbiJ9.4puYxFsBJs1W8tgxGFGUIA5oGWQbYWmbwn34V6zF3-I&amp;=
utm_medium=3Demail&amp;utm_campaign=3Demail-reaction&amp;r=3D5kb93z" style=
=3D"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Robo=
to,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe U=
I Symbol';display: inline-block;font-weight: 500;border: 1px solid rgb(0,0,=
0,.1);border-radius: 9999px;text-transform: uppercase;font-size: 12px;line-=
height: 1;padding: 9px 0;text-decoration: none;color: rgb(119,119,119);min-=
width: 38px;box-sizing: border-box;width: 38px"><img class=3D"icon" src=3D"=
https://substackcdn.com/image/fetch/$s_!PeVs!,w_36,c_scale,f_png,q_auto:goo=
d,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideHeart%3Fv%=
3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2" w=
idth=3D"18" height=3D"18" style=3D"border: none;vertical-align: middle;max-=
width: 18px" alt=3D""></a></td></tr></tbody></table></td><td width=3D"8" st=
yle=3D"min-width: 8px"></td><td style=3D"vertical-align:middle;"><table rol=
e=3D"presentation" width=3D"38" border=3D"0" cellspacing=3D"0" cellpadding=
=3D"0"><tbody><tr><td align=3D"center"><a class=3D"email-icon-button" href=
=3D"https://substack.com/app-link/post?publication_id=3D1373231&amp;post_id=
=3D167307195&amp;utm_source=3Dsubstack&amp;utm_medium=3Demail&amp;isFreemai=
l=3Dfalse&amp;comments=3Dtrue&amp;token=3DeyJ1c2VyX2lkIjozMzY0NDgyMjMsInBvc=
3RfaWQiOjE2NzMwNzE5NSwiaWF0IjoxNzUxNDYxNzU3LCJleHAiOjE3NTQwNTM3NTcsImlzcyI6=
InB1Yi0xMzczMjMxIiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.DEbLVammqe6LZm7gNOWDU7reHs=
mz8JjE-_W6P2WKOP0&amp;r=3D5kb93z&amp;utm_campaign=3Demail-half-magic-commen=
ts&amp;action=3Dpost-comment&amp;utm_source=3Dsubstack&amp;utm_medium=3Dema=
il" style=3D"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe=
 UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji'=
,'Segoe UI Symbol';display: inline-block;font-weight: 500;border: 1px solid=
 rgb(0,0,0,.1);border-radius: 9999px;text-transform: uppercase;font-size: 1=
2px;line-height: 1;padding: 9px 0;text-decoration: none;color: rgb(119,119,=
119);min-width: 38px;box-sizing: border-box;width: 38px"><img class=3D"icon=
" src=3D"https://substackcdn.com/image/fetch/$s_!x1tS!,w_36,c_scale,f_png,q=
_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideC=
omments%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26stroke=
Width%3D2" width=3D"18" height=3D"18" style=3D"border: none;vertical-align:=
 middle;max-width: 18px" alt=3D""></a></td></tr></tbody></table></td><td wi=
dth=3D"8" style=3D"min-width: 8px"></td><td style=3D"vertical-align:middle;=
"><table role=3D"presentation" width=3D"38" border=3D"0" cellspacing=3D"0" =
cellpadding=3D"0"><tbody><tr><td align=3D"center"><a class=3D"email-icon-bu=
tton" href=3D"https://substack.com/app-link/post?publication_id=3D1373231&a=
mp;post_id=3D167307195&amp;utm_source=3Dsubstack&amp;utm_medium=3Demail&amp=
;utm_content=3Dshare&amp;utm_campaign=3Demail-share&amp;action=3Dshare&amp;=
triggerShare=3Dtrue&amp;isFreemail=3Dfalse&amp;r=3D5kb93z&amp;token=3DeyJ1c=
2VyX2lkIjozMzY0NDgyMjMsInBvc3RfaWQiOjE2NzMwNzE5NSwiaWF0IjoxNzUxNDYxNzU3LCJl=
eHAiOjE3NTQwNTM3NTcsImlzcyI6InB1Yi0xMzczMjMxIiwic3ViIjoicG9zdC1yZWFjdGlvbiJ=
9.DEbLVammqe6LZm7gNOWDU7reHsmz8JjE-_W6P2WKOP0" style=3D"font-family: system=
-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans=
-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';display: inli=
ne-block;font-weight: 500;border: 1px solid rgb(0,0,0,.1);border-radius: 99=
99px;text-transform: uppercase;font-size: 12px;line-height: 1;padding: 9px =
0;text-decoration: none;color: rgb(119,119,119);min-width: 38px;box-sizing:=
 border-box;width: 38px"><img class=3D"icon" src=3D"https://substackcdn.com=
/image/fetch/$s_!_L14!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/=
https%3A%2F%2Fsubstack.com%2Ficon%2FLucideShare2%3Fv%3D4%26height%3D36%26fi=
ll%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2" width=3D"18" height=3D"=
18" style=3D"border: none;vertical-align: middle;max-width: 18px" alt=3D"">=
</a></td></tr></tbody></table></td><td width=3D"8" style=3D"min-width: 8px"=
></td><td style=3D"vertical-align:middle;"><table role=3D"presentation" wid=
th=3D"38" border=3D"0" cellspacing=3D"0" cellpadding=3D"0"><tbody><tr><td a=
lign=3D"center"><a class=3D"email-icon-button" href=3D"https://substack.com=
/redirect/2/eyJlIjoiaHR0cHM6Ly9vcGVuLnN1YnN0YWNrLmNvbS9wdWIvbmF0ZXNuZXdzbGV=
0dGVyL3AvcmFnLXRoZS1jb21wbGV0ZS1ndWlkZS10by1yZXRyaWV2YWw_dXRtX3NvdXJjZT1zdW=
JzdGFjayZ1dG1fbWVkaXVtPWVtYWlsJnV0bV9jYW1wYWlnbj1lbWFpbC1yZXN0YWNrLWNvbW1lb=
nQmYWN0aW9uPXJlc3RhY2stY29tbWVudCZyPTVrYjkzeiZ0b2tlbj1leUoxYzJWeVgybGtJam96=
TXpZME5EZ3lNak1zSW5CdmMzUmZhV1FpT2pFMk56TXdOekU1TlN3aWFXRjBJam94TnpVeE5EWXh=
OelUzTENKbGVIQWlPakUzTlRRd05UTTNOVGNzSW1semN5STZJbkIxWWkweE16Y3pNak14SWl3aW=
MzVmlJam9pY0c5emRDMXlaV0ZqZEdsdmJpSjkuREViTFZhbW1xZTZMWm03Z05PV0RVN3JlSHNte=
jhKakUtX1c2UDJXS09QMCIsInAiOjE2NzMwNzE5NSwicyI6MTM3MzIzMSwiZiI6ZmFsc2UsInUi=
OjMzNjQ0ODIyMywiaWF0IjoxNzUxNDYxNzU3LCJleHAiOjIwNjcwMzc3NTcsImlzcyI6InB1Yi0=
wIiwic3ViIjoibGluay1yZWRpcmVjdCJ9._sxeD-y3fAwpqDsrfTG7bL5N6G0jfo6fEm0z295Wo=
G4?&amp;utm_source=3Dsubstack&amp;utm_medium=3Demail" style=3D"font-family:=
 system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Ari=
al,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';displa=
y: inline-block;font-weight: 500;border: 1px solid rgb(0,0,0,.1);border-rad=
ius: 9999px;text-transform: uppercase;font-size: 12px;line-height: 1;paddin=
g: 9px 0;text-decoration: none;color: rgb(119,119,119);min-width: 38px;box-=
sizing: border-box;width: 38px"><img class=3D"icon" src=3D"https://substack=
cdn.com/image/fetch/$s_!5EGt!,w_36,c_scale,f_png,q_auto:good,fl_progressive=
:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FNoteForwardIcon%3Fv%3D4%26height=
%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2" width=3D"18" =
height=3D"18" style=3D"border: none;vertical-align: middle;max-width: 18px"=
 alt=3D""></a></td></tr></tbody></table></td></tr></tbody></table></td><td =
align=3D"right"><table role=3D"presentation" width=3D"auto" border=3D"0" ce=
llspacing=3D"0" cellpadding=3D"0"><tbody><tr><td style=3D"vertical-align:mi=
ddle;"><table role=3D"presentation" width=3D"auto" border=3D"0" cellspacing=
=3D"0" cellpadding=3D"0"><tbody><tr><td align=3D"center"><a class=3D"email-=
button-outline" href=3D"https://open.substack.com/pub/natesnewsletter/p/rag=
-the-complete-guide-to-retrieval?utm_source=3Demail&amp;redirect=3Dapp-stor=
e&amp;utm_campaign=3Demail-read-in-app" style=3D"font-family: system-ui,-ap=
ple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,=
'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';display: inline-bloc=
k;font-weight: 500;border: 1px solid rgb(0,0,0,.1);border-radius: 9999px;te=
xt-transform: uppercase;font-size: 12px;line-height: 12px;padding: 9px 14px=
;text-decoration: none;color: rgb(119,119,119);"><div class=3D"email-button=
-spacer" style=3D"font-size: 16px;line-height: 26px;display: inline-block;v=
ertical-align: middle;max-width: 0;min-height: 18px;"></div><span class=3D"=
email-button-text" style=3D"vertical-align: middle;margin-right: 4px">READ =
IN APP</span><img class=3D"icon text-icon" src=3D"https://substackcdn.com/i=
mage/fetch/$s_!ET-_!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/ht=
tps%3A%2F%2Fsubstack.com%2Ficon%2FLucideArrowUpRight%3Fv%3D4%26height%3D36%=
26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2" width=3D"18" height=
=3D"18" style=3D"min-width: 18px;min-height: 18px;border: none;vertical-ali=
gn: middle;margin-right: 0;margin-left: 0;max-width: 18px" alt=3D""></a></t=
d></tr></tbody></table></td></tr></tbody></table></td></tr></tbody></table>=
</td></tr><tr height=3D"16"><td height=3D"16" style=3D"font-size:0px;line-h=
eight:0;">&nbsp;</td></tr></tbody></table></div></div><div class=3D"post ty=
pography" dir=3D"auto" style=3D"--image-offset-margin: -117px;padding: 32px=
 0 0 0;font-size: 16px;line-height: 26px;"><div class=3D"body markup" dir=
=3D"auto" style=3D"text-align: initial;font-size: 16px;line-height: 26px;wi=
dth: 100%;word-break: break-word;margin-bottom: 16px;"><p style=3D"margin: =
0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;margin-to=
p: 0;"><em>Imagine you meet someone brilliant=E2=80=94someone who seems to =
know absolutely everything. Every answer they give feels sharp, insightful,=
 even groundbreaking. Now, picture this person having one fatal flaw: every=
 so often, they confidently state something that=E2=80=99s totally wrong. N=
ot just wrong, mind you, but spectacularly incorrect=E2=80=94like insisting=
 that Abraham Lincoln was a professional skateboarder. Welcome to the curre=
nt state of Large Language Models (LLMs).</em></p><p style=3D"margin: 0 0 2=
0px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><em>As fasci=
nating and powerful as AI systems like ChatGPT and Claude have become, they=
 still possess what I affectionately (and sometimes frustratingly) call a =
=E2=80=9Cfrozen brain problem.=E2=80=9D Their knowledge is permanently stuc=
k at their last training cutoff, causing them to occasionally hallucinate a=
nswers=E2=80=94AI jargon for confidently stating nonsense. In my more forgi=
ving moments, I compare it to asking a very smart student to ace an exam wi=
thout any notes: impressive, yes, but prone to error and entirely reliant o=
n memory.</em></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-=
height: 26px;font-size: 16px;"><em>That=E2=80=99s where Retrieval-Augmented=
 Generation, or RAG, enters the chat. RAG fundamentally reshapes what we th=
ought possible from AI by handing these brilliant-but-flawed models a cruci=
al upgrade: an external, dynamic memory. Imagine giving our hypothetical br=
illiant person access to an extensive, always-up-to-date digital library=E2=
=80=94now every answer can be checked, validated, and supported with actual=
 data. It=E2=80=99s like turning that closed-book exam into an open-book te=
st, enabling real-time, accurate, and trustworthy answers.</em></p><p style=
=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16=
px;"><em>The stakes couldn=E2=80=99t be higher. We=E2=80=99re moving quickl=
y into a future where businesses, hospitals, law firms, and schools increas=
ingly rely on AI to handle complex information retrieval and decision-makin=
g tasks. According to recent market analyses, this isn=E2=80=99t a niche up=
grade=E2=80=94it=E2=80=99s a seismic shift expected to catapult the RAG mar=
ket from $1.96 billion in 2025 to over $40 billion by 2035. Companies who f=
ail to embrace RAG risk becoming like video rental stores in the Netflix er=
a: quaint, nostalgic, but rapidly obsolete.</em></p><p style=3D"margin: 0 0=
 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><em>I=E2=
=80=99ve spent considerable time sifting through the noise, experimenting, =
succeeding, and occasionally stumbling with RAG. This document you=E2=80=99=
re holding=E2=80=94or, more realistically, scrolling through=E2=80=94is the=
 distilled result: a 53-page guide that=E2=80=99s comprehensive, nuanced, a=
nd occasionally humorous (I promise, there=E2=80=99s levity amidst the deep=
 dives into cosine similarity and chunking strategies). Whether you=E2=80=
=99re a curious novice or a seasoned practitioner, there=E2=80=99s gold her=
e for everyone.</em></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55)=
;line-height: 26px;font-size: 16px;"><em>Inside this guide, we=E2=80=99ll d=
emystify exactly how RAG works=E2=80=94retrieval, embedding, generation, ch=
unking, and all=E2=80=94using analogies clear enough for dinner party conve=
rsations and precise enough for your next team meeting. We=E2=80=99ll explo=
re advanced techniques, including hybrid searches and multi-modal retrieval=
, to ensure you don=E2=80=99t just understand RAG=E2=80=94you master it. We=
=E2=80=99ll even examine some cautionary tales from companies who jumped in=
 headfirst without checking the depth (spoiler: they regret it).</em></p><p=
 style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-si=
ze: 16px;"><em>Why should you read this? Because memory matters. In AI, mem=
ory isn=E2=80=99t a nice-to-have feature; it=E2=80=99s the essential backbo=
ne that transforms impressive parlor tricks into reliable, transformative t=
echnology. If you=E2=80=99re investing in AI, building products, or even ju=
st navigating an AI-driven world, understanding RAG isn=E2=80=99t optional=
=E2=80=94it=E2=80=99s critical.</em></p><p style=3D"margin: 0 0 20px 0;colo=
r: rgb(54,55,55);line-height: 26px;font-size: 16px;"><em>So, pour a coffee,=
 settle in, and let=E2=80=99s tackle this together. You=E2=80=99re about to=
 gain the keys to AI=E2=80=99s memory revolution, ensuring your AI doesn=E2=
=80=99t just sound brilliant but actually knows its stuff. Welcome to your =
next-level guide on Retrieval-Augmented Generation: AI=E2=80=99s long-await=
ed memory upgrade.</em></p><div class=3D"subscription-widget-wrap" style=3D=
"font-size: 16px;line-height: 26px;"><div class=3D"subscription-widget show=
-subscribe" style=3D"font-size: 16px;direction: ltr !important;font-weight:=
 400;text-decoration: none;font-family: system-ui,-apple-system,BlinkMacSys=
temFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','S=
egoe UI Emoji','Segoe UI Symbol';color: #363737;line-height: 1.5;max-width:=
 560px;margin: 24px auto;align-items: flex-start;display: block;text-align:=
 center;padding: 0px 32px;"><div class=3D"preamble" style=3D"margin-top: 16=
px;font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Robot=
o,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI=
 Symbol';font-size: 18px;max-width: 384px;width: fit-content;line-height: 2=
2px;display: flex;align-items: center;text-align: center;font-weight: 400;m=
argin-left: auto;margin-right: auto;"><p style=3D"margin: 0 0 20px 0;color:=
 rgb(54,55,55);line-height: 26px;font-size: 16px;">Subscribers get all thes=
e pieces!</p></div><div class=3D"subscribe-widget is-signed-up is-fully-sub=
scribed" data-component-name=3D"SubscribeWidget" style=3D"margin: 0 0 1em;d=
irection: ltr;font-size: 16px;line-height: 26px;"><div class=3D"pencraft pc=
-reset button-wrapper" style=3D"text-decoration: unset;list-style: none;fon=
t-size: 16px;line-height: 26px;text-align: center;cursor: pointer;border-ra=
dius: 4px;"><a class=3D"button subscribe-btn outline" href=3D"https://subst=
ack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9uYXRlc25ld3NsZXR0ZXIuc3Vic3RhY2suY29t=
L2FjY291bnQiLCJwIjoxNjczMDcxOTUsInMiOjEzNzMyMzEsImYiOmZhbHNlLCJ1IjozMzY0NDg=
yMjMsImlhdCI6MTc1MTQ2MTc1NywiZXhwIjoyMDY3MDM3NzU3LCJpc3MiOiJwdWItMCIsInN1Yi=
I6ImxpbmstcmVkaXJlY3QifQ.u4Q2luvhcIBo8GK1n8VRPu9gEqe37IlJaISpFzNaVJg?" styl=
e=3D"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Rob=
oto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe =
UI Symbol';display: inline-block;box-sizing: border-box;cursor: pointer;bor=
der-radius: 8px;font-size: 14px;line-height: 20px;font-weight: 600;text-ali=
gn: center;background-color: transparent;opacity: 1;outline: none;white-spa=
ce: nowrap;text-decoration: none !important;border: 1px solid #45d800;margi=
n: 0 auto;background: transparent;color: #45d800;padding: 12px 20px;height:=
 auto;"><img class=3D"check-icon static" src=3D"https://substackcdn.com/ima=
ge/fetch/$s_!3t53!,w_40,c_scale,f_png,q_auto:good,fl_progressive:steep/http=
s%3A%2F%2Fsubstack.com%2Ficon%2FLucideCheck%3Fv%3D4%26height%3D40%26fill%3D=
transparent%26stroke%3D%252345D800%26strokeWidth%3D3.6" width=3D"20" height=
=3D"20" style=3D"border: none;vertical-align: middle;-ms-interpolation-mode=
: bicubic;height: auto;display: inline-block;margin: -2px 8px 0 0;max-width=
: 20px" alt=3D""><span style=3D"text-decoration: none;">Subscribed</span></=
a></div></div></div></div><div class=3D"paywall-jump" data-component-name=
=3D"PaywallToDOM" style=3D"font-size: 16px;line-height: 26px;display: none;=
"></div><h1 class=3D"header-anchor-post" style=3D"position: relative;font-f=
amily: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,Blin=
kMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Em=
oji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoo=
thing: antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearance:=
 optimizelegibility;-moz-appearance: optimizelegibility;appearance: optimiz=
elegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16e=
m;font-size: 2em;"><strong>From 0 to 5K: The Complete Simplified Guide to R=
AG (Retrieval-Augmented Generation)</strong></h1><p style=3D"margin: 0 0 20=
px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>Imagi=
ne if ChatGPT had perfect memory =E2=80=93 never hallucinating, and able to=
 tap your company=E2=80=99s entire knowledge base in real time.</strong><sp=
an> That=E2=80=99s the promise of Retrieval-Augmented Generation (RAG), and=
 it=E2=80=99s changing everything about how we build with AI. In this guide=
, we=E2=80=99ll demystify RAG from the ground up, transforming complex conc=
epts into an engaging, accessible journey for AI enthusiasts.</span></p><h2=
 class=3D"header-anchor-post" style=3D"position: relative;font-family: 'SF =
Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemF=
ont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe=
 UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoothing: anti=
aliased;-moz-osx-font-smoothing: antialiased;-webkit-appearance: optimizele=
gibility;-moz-appearance: optimizelegibility;appearance: optimizelegibility=
;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-size=
: 1.625em;"><strong>Why RAG Changes Everything</strong></h2><p style=3D"mar=
gin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><s=
pan>Picture this: you ask your AI assistant a question, and it instantly pu=
lls up exactly the right facts from your company docs, giving a confident a=
nswer </span><em>with references</em><span>. No more =E2=80=9Challucinated=
=E2=80=9D nonsense =E2=80=93 just accurate, up-to-date info. This isn=E2=80=
=99t sci-fi; it=E2=80=99s RAG, and it=E2=80=99s </span><strong>big</strong>=
<span>. Analysts project the RAG market to soar from about </span><strong>$=
1.96=E2=80=AFbillion in 2025 to over $40=E2=80=AFbillion by 2035</strong><s=
pan>. Companies are betting big on RAG because it tackles AI=E2=80=99s bigg=
est weak spots (memory and truthfulness) head-on.</span></p><p style=3D"mar=
gin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><s=
trong>Did you know?</strong><span> LinkedIn applied RAG (with a knowledge g=
raph twist) to their customer support and slashed median ticket resolution =
time by </span><strong>28.6%</strong><span>. And they=E2=80=99re not alone.=
 Roughly </span><strong>80% of enterprises are now using RAG approaches (re=
trieval) over fine-tuning</strong><span> their models =E2=80=93 a massive s=
hift in strategy. Why? Because RAG gives AI real-time data access, and that=
=E2=80=99s gold. One survey found nearly </span><strong>73% of companies ar=
e engaged with AI</strong><span> in some form , and providing those AI syst=
ems with current, relevant data is the new race. In other words, the compan=
ies winning in 2025 aren=E2=80=99t the ones with the biggest model =E2=80=
=93 they=E2=80=99re the ones whose </span><strong>AI knows their business i=
nside and out</strong><span>.</span></p><p style=3D"margin: 0 0 20px 0;colo=
r: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>So buckle up. By=
 the end of this guide, you=E2=80=99ll see why RAG is </span><em>the</em><s=
pan> hot topic (a $1.96B opportunity and growing ), how it=E2=80=99s delive=
ring =E2=80=9Cwow moments=E2=80=9D like LinkedIn=E2=80=99s support success,=
 and why </span><strong>73%+ of orgs are scrambling to give their AI a real=
-time knowledge upgrade</strong><span>. RAG changes everything by making AI=
 both smart </span><em>and</em><span> knowledgeable =E2=80=93 and today, we=
=E2=80=99ll show you how to go from 0 to RAG hero in an approachable, step-=
by-step narrative.</span></p><h2 class=3D"header-anchor-post" style=3D"posi=
tion: relative;font-family: 'SF Pro Display',-apple-system-headline,system-=
ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-=
serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: b=
old;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing: antialiase=
d;-webkit-appearance: optimizelegibility;-moz-appearance: optimizelegibilit=
y;appearance: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,5=
5);line-height: 1.16em;font-size: 1.625em;"><strong>RAG Basics: Your AI Get=
s a Research Assistant</strong></h2><p style=3D"margin: 0 0 20px 0;color: r=
gb(54,55,55);line-height: 26px;font-size: 16px;"><span>If a large language =
model (LLM) is like a brilliant student who studied everything up until 202=
3, then </span><strong>RAG gives that student a real-time library card.</st=
rong><span> It=E2=80=99s like letting your AI take an open-book exam instea=
d of relying on memory alone. </span><em>How does that work?</em><span> Thi=
nk of RAG as giving your AI a </span><strong>research assistant</strong><sp=
an>: when asked a question, the AI can </span><strong>Retrieve</strong><spa=
n> relevant info from a knowledge source, </span><strong>Embed</strong><spa=
n> that info into a form it understands, and then </span><strong>Generate</=
strong><span> a final answer using both its built-in knowledge and the retr=
ieved facts.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);=
line-height: 26px;font-size: 16px;"><strong>Analogy alert:</strong><span> <=
/span><em>LLMs are like students; RAG lets them bring notes.</em><span> As =
one engineer quipped, =E2=80=9CLLMs don=E2=80=99t </span><em>know</em><span=
> =E2=80=93 they predict. Their memory is frozen. That=E2=80=99s where RAG =
changes the game. It=E2=80=99s like giving the model an open-book test: it =
still has to reason, but now it gets to reference something real=E2=80=9D. =
In practice, that means when an LLM gets a query, a RAG system will first f=
etch relevant text (from your documents, websites, etc.), and supply those =
facts to the model so it can formulate a grounded answer. The magic three-p=
art process is:</span></p><ol style=3D"margin-top: 0;padding: 0;"><li style=
=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line-height: 26=
px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16p=
x;margin: 0;"><strong>Retrieval:</strong><span> Take the user=E2=80=99s que=
stion and search a knowledge source for relevant info (just like Googling o=
r querying a database).</span></p></li><li style=3D"margin: 8px 0 0 32px;">=
<p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-siz=
ing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Embed=
ding:</strong><span> Behind the scenes, both the question and documents are=
 converted into numerical </span><strong>embeddings</strong><span> =E2=80=
=93 basically, turning words into vectors (imagine coordinates in a 1536-di=
mensional space for OpenAI=E2=80=99s ada-002 model ) so that semantic simil=
arity can be computed.</span></p></li><li style=3D"margin: 8px 0 0 32px;"><=
p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizi=
ng: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Genera=
tion:</strong><span> The LLM receives the question </span><em>plus</em><spa=
n> the retrieved context and generates a final answer, </span><em>augmented=
</em><span> by these real-time facts.</span></p></li></ol><p style=3D"margi=
n: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><spa=
n>Traditional LLMs have a fixed knowledge cutoff and often bluff when asked=
 something outside their training. RAG makes the LLM=E2=80=99s knowledge </=
span><strong>dynamic and verifiable</strong><span>. It=E2=80=99s the differ=
ence between a student taking a closed-book test (relying on possibly outda=
ted memory) vs. an open-book test with the latest textbook in hand. For exa=
mple, without RAG, an LLM is stuck with whatever it learned in training =E2=
=80=93 ask it about something it never saw, and it might just make up an an=
swer. With RAG, we first retrieve the latest relevant info and feed it in, =
so the model=E2=80=99s answer can cite real data.</span></p><p style=3D"mar=
gin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">He=
re=E2=80=99s a simple diagram of a RAG workflow, which shows how a query fl=
ows through retrieval into generation:</p><div class=3D"captioned-image-con=
tainer-static" style=3D"font-size: 16px;line-height: 26px;margin: 32px auto=
;"><figure style=3D"width: 100%;margin: 0 auto;"><table class=3D"image-wrap=
per" width=3D"100%" border=3D"0" cellspacing=3D"0" cellpadding=3D"0" data-c=
omponent-name=3D"Image2ToDOMStatic" style=3D"mso-padding-alt: 1em 0 1.6em;"=
><tbody><tr><td style=3D"text-align: center;"></td><td class=3D"content" al=
ign=3D"left" width=3D"1456" style=3D"text-align: center;"><a class=3D"image=
-link" target=3D"_blank" href=3D"https://substack.com/redirect/b9a259d9-0fc=
3-4d55-ba86-fefb80ed41ee?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDn=
PG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"position: relative;flex-direction: col=
umn;align-items: center;padding: 0;width: auto;height: auto;border: none;te=
xt-decoration: none;display: block;margin: 0;"><img class=3D"wide-image" da=
ta-attrs=3D"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws=
.com/public/images/a21f4f1d-068c-4690-94ce-9be5785454a1_2015x3840.png&quot;=
,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSiz=
e&quot;:null,&quot;height&quot;:2775,&quot;width&quot;:1456,&quot;resizeWid=
th&quot;:null,&quot;bytes&quot;:311685,&quot;alt&quot;:null,&quot;title&quo=
t;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;=
belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&q=
uot;:&quot;https://natesnewsletter.substack.com/i/167307195?img=3Dhttps%3A%=
2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa21f4f1d-068c=
-4690-94ce-9be5785454a1_2015x3840.png&quot;,&quot;isProcessing&quot;:false,=
&quot;align&quot;:null,&quot;offset&quot;:false}" alt=3D"" width=3D"550" he=
ight=3D"1048.2486263736264" src=3D"https://substackcdn.com/image/fetch/$s_!=
NmNX!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2F=
substack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa21f4f1d-068c-4690=
-94ce-9be5785454a1_2015x3840.png" style=3D"border: none !important;vertical=
-align: middle;display: block;-ms-interpolation-mode: bicubic;height: auto;=
margin-bottom: 0;width: auto !important;max-width: 100% !important;margin: =
0 auto;"></a></td><td style=3D"text-align: center;"></td></tr></tbody></tab=
le></figure></div><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-=
height: 26px;font-size: 16px;"><span>Notice in the diagram: your AI isn=E2=
=80=99t just guessing from its frozen memory; it actively </span><em>search=
es</em><span> your knowledge base for context. This approach virtually elim=
inates those =E2=80=9Csorry, I don=E2=80=99t have that info=E2=80=9D dead-e=
nds and dramatically reduces hallucinations. </span><strong>Users gain trus=
t</strong><span> because the AI can show sources for its answers. In fact, =
RAG is often introduced specifically to boost factual accuracy and up-to-da=
teness. One AWS expert described a base LLM as an </span><em>=E2=80=9Cover-=
enthusiastic new employee who refuses to stay informed=E2=80=9D</em><span> =
=E2=80=93 RAG is how you get that employee to check the company wiki before=
 answering !</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);=
line-height: 26px;font-size: 16px;"><strong>But doesn=E2=80=99t adding retr=
ieval make things slower?</strong><span> It=E2=80=99s a common concern that=
 giving an LLM a =E2=80=9Cresearch step=E2=80=9D will add too much latency.=
 In reality, modern RAG systems are incredibly snappy. Vector search engine=
s can fetch relevant chunks in tens of milliseconds, and overall RAG query =
times often land in the few-hundred-millisecond range. For instance, engine=
ers report end-to-end RAG responses around </span><strong>300=E2=80=93500=
=E2=80=AFms</strong><span> in practice =E2=80=93 essentially real-time for =
most apps. Even complex multi-hop queries that pull lots of data might take=
 a couple seconds at most. So while vanilla ChatGPT might answer in ~1=E2=
=80=933 seconds, a well-tuned RAG might be 0.5=E2=80=935 seconds depending =
on complexity. In conversational terms, that=E2=80=99s barely noticeable, a=
nd it=E2=80=99s a small trade-off for </span><strong>answers grounded in tr=
uth</strong><span>. (And with some clever caching and indexing, many RAG sy=
stems actually </span><em>outpace</em><span> humans hunting through documen=
ts =E2=80=93 your support bot might answer in 500=E2=80=AFms what took a hu=
man agent 5 minutes.)</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(5=
4,55,55);line-height: 26px;font-size: 16px;"><strong>Bottom line:</strong><=
span> RAG gives your AI </span><strong>=E2=80=9Cretrieval superpowers.=E2=
=80=9D</strong><span> Instead of being limited to what it </span><em>memori=
zed</em><span>, it can search and cite fresh, relevant information on the f=
ly. It=E2=80=99s like upgrading your genius student (LLM) with an always-av=
ailable research librarian. In the next sections, we=E2=80=99ll dive deeper=
 into </span><em>how</em><span> it works under the hood =E2=80=93 but at it=
s heart, RAG is the simple yet profound idea of </span><strong>augmenting g=
eneration with retrieval</strong><span>. It turns out this one idea address=
es a lot of AI=E2=80=99s toughest challenges (hallucinations, stale knowled=
ge, lack of trust). No wonder </span><strong>73% of organizations are racin=
g to implement AI with real-time data access</strong><span> =E2=80=93 RAG m=
akes AI not just smarter, but </span><em>wisely informed</em><span>. And th=
at changes everything.</span></p><h2 class=3D"header-anchor-post" style=3D"=
position: relative;font-family: 'SF Pro Display',-apple-system-headline,sys=
tem-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,s=
ans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weigh=
t: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing: antial=
iased;-webkit-appearance: optimizelegibility;-moz-appearance: optimizelegib=
ility;appearance: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,=
55,55);line-height: 1.16em;font-size: 1.625em;"><strong>Under the Hood: How=
 RAG Really Works</strong></h2><p style=3D"margin: 0 0 20px 0;color: rgb(54=
,55,55);line-height: 26px;font-size: 16px;"><span>Let=E2=80=99s lift the ho=
od on this RAG engine and see the mechanics in action. There are three tech=
nical concepts that make the RAG magic possible: </span><strong>embeddings<=
/strong><span> (turning text into vectors), </span><strong>chunking</strong=
><span> (breaking text into retrievable pieces), and </span><strong>similar=
ity search</strong><span> (finding which pieces are relevant). Don=E2=80=99=
t worry =E2=80=93 we=E2=80=99ll break each concept down with simple analogi=
es and visuals so it all clicks.</span></p><h3 class=3D"header-anchor-post"=
 style=3D"position: relative;font-family: 'SF Pro Display',-apple-system-he=
adline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helveti=
ca,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';=
font-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothi=
ng: antialiased;-webkit-appearance: optimizelegibility;-moz-appearance: opt=
imizelegibility;appearance: optimizelegibility;margin: 1em 0 0.625em 0;colo=
r: rgb(54,55,55);line-height: 1.16em;font-size: 1.375em;"><strong>The Journ=
ey from Text to Vector</strong></h3><p style=3D"margin: 0 0 20px 0;color: r=
gb(54,55,55);line-height: 26px;font-size: 16px;"><span>In RAG, </span><em>y=
our words aren=E2=80=99t just words =E2=80=93 they=E2=80=99re coordinates i=
n a high-dimensional space.</em><span> When we say we =E2=80=9Cembed=E2=80=
=9D text, imagine plotting meanings on a giant star map with 1,536 dimensio=
ns. For example, the phrase </span><strong>=E2=80=9Ccustomer refund policy=
=E2=80=9D</strong><span> might become a vector like </span><strong>[0.23, -=
0.45, 0.67, =E2=80=A6]</strong><span> (with 1,536 numbers). What do those n=
umbers mean? Individually, not much to a human =E2=80=93 but collectively t=
hey position the phrase in a semantic space where distance correlates with =
meaning. Two pieces of text that mean similar things will end up as vectors=
 that are close together (small angle between them), even if they don=E2=80=
=99t share any keywords. This is why embedding is so powerful: </span><stro=
ng>similar meanings cluster together in vector space</strong><span>.</span>=
</p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;f=
ont-size: 16px;"><span>The state-of-the-art embedding model many use is Ope=
nAI=E2=80=99s text-embedding-ada-002, which produces 1536-dimensional vecto=
rs and is remarkably good general-purpose. Ada-002 was a milestone because =
it collapsed multiple embedding tasks into one uber-model and made it cheap=
 and easy via API. But it=E2=80=99s not the only game in town. Companies li=
ke Cohere offer embedding models (for instance, Cohere=E2=80=99s embed-engl=
ish-v3 has its own dimensionality and strengths ), and open-source models l=
ike </span><strong>E5</strong><span> or </span><strong>InstructorXL</strong=
><span> are now rivaling the proprietary ones. In fact, recent leaderboards=
 (like MTEB) show tiny open-source models can come within a few percentage =
points of Ada=E2=80=99s accuracy. The takeaway: </span><strong>embedding mo=
dels are evolving fast</strong><span>. Ada=E2=80=99s 1536-d vector was cutt=
ing-edge in 2022, but by 2025 we have specialized embeddings for images, co=
de, multi-lingual data, etc., and some open models tuned for certain domain=
s can outperform the general ones. The good news is that the concept is the=
 same =E2=80=93 whatever model you choose, it converts text into vectors su=
ch that </span><em>meaningful similarity =3D mathematical closeness</em><sp=
an>.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-hei=
ght: 26px;font-size: 16px;">To visualize it, imagine each document chunk as=
 a point in a cosmic galaxy. All chunks about refund policies cluster in on=
e nebula; all chunks about technical errors cluster elsewhere. When a query=
 comes in (=E2=80=9CHow do I process a refund?=E2=80=9D), we embed the quer=
y into this same space and see which document points are nearest. Those nea=
rest neighbors are likely talking about refunds too, even if they don=E2=80=
=99t share the exact wording of the question.</p><div class=3D"captioned-im=
age-container-static" style=3D"font-size: 16px;line-height: 26px;margin: 32=
px auto;"><figure style=3D"width: 100%;margin: 0 auto;"><table class=3D"ima=
ge-wrapper" width=3D"100%" border=3D"0" cellspacing=3D"0" cellpadding=3D"0"=
 data-component-name=3D"Image2ToDOMStatic" style=3D"mso-padding-alt: 1em 0 =
1.6em;"><tbody><tr><td style=3D"text-align: center;"></td><td class=3D"cont=
ent" align=3D"left" width=3D"1456" style=3D"text-align: center;"><a class=
=3D"image-link" target=3D"_blank" href=3D"https://substack.com/redirect/483=
8e106-fd97-41d2-abef-ca87635e772b?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9h=
shgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"position: relative;flex-direc=
tion: column;align-items: center;padding: 0;width: auto;height: auto;border=
: none;text-decoration: none;display: block;margin: 0;"><img class=3D"wide-=
image" data-attrs=3D"{&quot;src&quot;:&quot;https://substack-post-media.s3.=
amazonaws.com/public/images/4c313e1c-e22b-485d-813b-d321dfb824d2_3840x1292.=
png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot=
;imageSize&quot;:null,&quot;height&quot;:490,&quot;width&quot;:1456,&quot;r=
esizeWidth&quot;:null,&quot;bytes&quot;:245632,&quot;alt&quot;:null,&quot;t=
itle&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:nul=
l,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRe=
direct&quot;:&quot;https://natesnewsletter.substack.com/i/167307195?img=3Dh=
ttps%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c313=
e1c-e22b-485d-813b-d321dfb824d2_3840x1292.png&quot;,&quot;isProcessing&quot=
;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt=3D"" width=3D=
"550" height=3D"185.09615384615384" src=3D"https://substackcdn.com/image/fe=
tch/$s_!NBFq!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%=
3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c313e1c-e=
22b-485d-813b-d321dfb824d2_3840x1292.png" style=3D"border: none !important;=
vertical-align: middle;display: block;-ms-interpolation-mode: bicubic;heigh=
t: auto;margin-bottom: 0;width: auto !important;max-width: 100% !important;=
margin: 0 auto;"></a></td><td style=3D"text-align: center;"></td></tr></tbo=
dy></table></figure></div><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,5=
5);line-height: 26px;font-size: 16px;"><span>One </span><strong>mind-blowin=
g fact</strong><span>: OpenAI=E2=80=99s 1536-dim embedding can capture incr=
edibly nuanced meaning. For instance, it will place </span><em>=E2=80=9CApp=
le pay refund=E2=80=9D</em><span> close to </span><em>=E2=80=9Creimbursing =
customers via Apple Pay=E2=80=9D</em><span> even if the wording differs, be=
cause the core idea is the same. This semantic clustering is something old =
keyword search couldn=E2=80=99t do =E2=80=93 it would miss synonyms or para=
phrases =E2=80=93 but embeddings nail it. It=E2=80=99s like magic: </span><=
em>the model somehow knows</em><span> that =E2=80=9CNDA=E2=80=9D and =E2=80=
=9Cnon-disclosure agreement=E2=80=9D are related, or that a Jaguar (animal)=
 is different from Jaguar (car), based on context usage. Of course, the mod=
el doesn=E2=80=99t =E2=80=9Cknow=E2=80=9D in a human sense; it=E2=80=99s al=
l statistical correlation from training. But the effect is a vector space w=
here </span><strong>related ideas gravitate together</strong><span>.</span>=
</p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;f=
ont-size: 16px;"><span>Before we move on: you=E2=80=99ll often hear about c=
osine similarity vs. dot product vs. Euclidean distance as ways to measure =
vector closeness. Here=E2=80=99s a quick cheat sheet: </span><strong>cosine=
 similarity</strong><span> cares only about the </span><em>angle</em><span>=
 between vectors (essentially their orientation, ignoring magnitude). </spa=
n><strong>Dot product</strong><span> is like cosine but also scales with ma=
gnitude (two vectors in the same direction will register even more similar =
if they=E2=80=99re longer). </span><strong>Euclidean distance</strong><span=
> is the straight-line distance. Many systems use cosine or dot (with norma=
lized embeddings, dot and cosine become equivalent). Conceptually, you can =
think: </span><em>cosine =3D how aligned are the meanings</em><span>, </spa=
n><em>dot =3D aligned + confidence</em><span>, </span><em>Euclidean =3D lit=
eral distance considering all components</em><span>. The </span><strong>bas=
ic rule</strong><span> is actually simple: use whatever the embedding model=
 was trained with. Ada was trained with cosine, so use cosine for Ada vecto=
rs. Some newer models use dot product. As long as you match it up, you=E2=
=80=99re golden. We won=E2=80=99t belabor the math =E2=80=93 just know thes=
e metrics exist, and cosine is popular because it neatly ignores difference=
s in length and focuses on meaning direction.</span></p><h3 class=3D"header=
-anchor-post" style=3D"position: relative;font-family: 'SF Pro Display',-ap=
ple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',R=
oboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Sego=
e UI Symbol';font-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx=
-font-smoothing: antialiased;-webkit-appearance: optimizelegibility;-moz-ap=
pearance: optimizelegibility;appearance: optimizelegibility;margin: 1em 0 0=
.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-size: 1.375em;"><str=
ong>The Art and Science of Chunking</strong></h3><p style=3D"margin: 0 0 20=
px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>Now our=
 text is embedding-ready =E2=80=93 but wait, we can=E2=80=99t just embed a =
whole huge document in one go (context windows have limits). We need to </s=
pan><strong>chunk</strong><span> documents into pieces. Chunking is an unsu=
ng art in RAG. Do it wrong, and you </span><em>=E2=80=9Cshred=E2=80=9D</em>=
<span> context and lose meaning; do it right, and your retrieval is laser-p=
recise. It=E2=80=99s said that </span><strong>bad chunking is responsible f=
or sinking up to 40% of RAG projects</strong><span> =E2=80=93 anecdotally, =
one expert looked at 10+ RAG implementations and found </span><strong>80% h=
ad chunking that broke context</strong><span>. Ouch.</span></p><div class=
=3D"captioned-image-container-static" style=3D"font-size: 16px;line-height:=
 26px;margin: 32px auto;"><figure style=3D"width: 100%;margin: 0 auto;"><ta=
ble class=3D"image-wrapper" width=3D"100%" border=3D"0" cellspacing=3D"0" c=
ellpadding=3D"0" data-component-name=3D"Image2ToDOMStatic" style=3D"mso-pad=
ding-alt: 1em 0 1.6em;"><tbody><tr><td style=3D"text-align: center;"></td><=
td class=3D"content" align=3D"left" width=3D"1456" style=3D"text-align: cen=
ter;"><a class=3D"image-link" target=3D"_blank" href=3D"https://substack.co=
m/redirect/44bfa8a2-b5cf-405c-a661-ce6cfa8304d9?j=3DeyJ1IjoiNWtiOTN6In0.zdz=
y88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"position: relat=
ive;flex-direction: column;align-items: center;padding: 0;width: auto;heigh=
t: auto;border: none;text-decoration: none;display: block;margin: 0;"><img =
class=3D"wide-image" data-attrs=3D"{&quot;src&quot;:&quot;https://substack-=
post-media.s3.amazonaws.com/public/images/e48e7551-a96d-439a-9eb4-58416e771=
a0d_3840x2081.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&qu=
ot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:789,&quot;width&quot=
;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:555895,&quot;alt&quot=
;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;=
href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&q=
uot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/167=
307195?img=3Dhttps%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2F=
images%2Fe48e7551-a96d-439a-9eb4-58416e771a0d_3840x2081.png&quot;,&quot;isP=
rocessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt=
=3D"" width=3D"550" height=3D"298.0425824175824" src=3D"https://substackcdn=
.com/image/fetch/$s_!Qx0c!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive=
:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages=
%2Fe48e7551-a96d-439a-9eb4-58416e771a0d_3840x2081.png" style=3D"border: non=
e !important;vertical-align: middle;display: block;-ms-interpolation-mode: =
bicubic;height: auto;margin-bottom: 0;width: auto !important;max-width: 100=
% !important;margin: 0 auto;"></a></td><td style=3D"text-align: center;"></=
td></tr></tbody></table><figcaption class=3D"image-caption" style=3D"box-si=
zing: content-box;color: #777777;font-size: 14px;line-height: 20px;font-wei=
ght: 400;letter-spacing: -.15px;margin-top: 8px;width: 70%;padding-left: 15=
%;padding-right: 15%;text-align: center;"><em><span>It=E2=80=99s an eye-cha=
rt, so I </span><a href=3D"https://substack.com/redirect/3b60e935-b821-4872=
-9cc1-e16f463fca09?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ=
4tAjMRMPOw0" rel=3D"" style=3D"text-decoration: underline;">made a Notion</=
a><span> where you can zoom in.</span></em></figcaption></figure></div><p s=
tyle=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size=
: 16px;">So what are the chunking strategies, and which actually work? Here=
 are four you should know, from simplest (and most dangerous) to most advan=
ced:</p><ul style=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 =
0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-h=
eight: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-=
size: 16px;margin: 0;"><strong>Fixed-size chunking:</strong><span> Break te=
xt into equal-sized blocks (e.g. every 500 tokens). It=E2=80=99s easy, but =
often </span><em>dangerous</em><span>. It can cut off in the middle of topi=
cs =E2=80=93 imagine a policy document where the chunk boundary splits a pa=
ragraph explaining a rule. The model might retrieve a chunk that says =E2=
=80=9CExceptions: none.=E2=80=9D without the preceding chunk that explains =
the rule =E2=80=93 misleading! Fixed windows (no matter 500 tokens or 1000)=
 can break semantic units.</span></p></li><li style=3D"margin: 8px 0 0 32px=
;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height:=
 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: =
16px;margin: 0;"><strong>Sentence-based chunking:</strong><span> Split by s=
entences or paragraphs, respecting natural boundaries. This is better for Q=
&amp;A on prose, because each chunk is a self-contained thought. It=E2=80=
=99s commonly used in chat-style RAG: you ensure each chunk is, say, &lt;=
=3D 200 tokens but you only cut at sentence ends. For conversational system=
s or FAQ docs, this often works well.</span></p></li><li style=3D"margin: 8=
px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);l=
ine-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;=
font-size: 16px;margin: 0;"><strong>Semantic chunking:</strong><span> The =
=E2=80=9Csmart approach=E2=80=9D =E2=80=93 use an algorithm to split text w=
here </span><em>topics</em><span> change. For example, some tools use embed=
dings themselves to decide split points (looking for where similarity betwe=
en consecutive paragraphs drops). Others use heading structure in documents=
 to keep subtopics together. Semantic chunking tries to keep each chunk abo=
ut </span><strong>one main idea</strong><span>. It=E2=80=99s like an automa=
tic outline parser.</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-sp=
ecial-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;m=
argin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;ma=
rgin: 0;"><strong>Recursive chunking (hierarchical):</strong><span> When yo=
u have natural hierarchy (chapters =E2=86=92 sections =E2=86=92 subsections=
), you chunk at multiple levels. E.g., first chunk into sections, but if a =
section is too long, further chunk it into paragraphs. This preserves the t=
ree structure. </span><em>Recursive chunking</em><span> ensures that if one=
 chunk isn=E2=80=99t enough, you might retrieve multiple from the same sect=
ion (because their content is related). It=E2=80=99s useful for things like=
 books or multi-step instructions.</span></p></li></ul><p style=3D"margin: =
0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>A=
 huge tip from experience: </span><strong>overlap your chunks.</strong><spa=
n> By allowing, say, a 20% overlap between consecutive chunks, you ensure i=
mportant context isn=E2=80=99t lost at boundaries. For instance, if chunk A=
 ends with =E2=80=9CThe results are shown in Table 5=E2=80=9D and chunk B b=
egins with Table 5, an overlap would put the end of chunk A (=E2=80=9CThe r=
esults are shown in Table 5=E2=80=9D) also at the start of chunk B. Then if=
 a query hits that transition, you won=E2=80=99t miss it. Many practitioner=
s recommend overlaps around 10=E2=80=9320% of chunk size. NVIDIA=E2=80=99s =
research found ~15% optimal for certain finance docs. The </span><em>impact=
</em><span> of overlap can be big: without it, you might get incomplete ans=
wers; with it, one study noted a significant boost in accuracy (some intern=
al tests saw ~35% relative improvement when using overlapping chunks). The =
exact number isn=E2=80=99t magic, but </span><strong>some overlap is usuall=
y worth it</strong><span>.</span></p><p style=3D"margin: 0 0 20px 0;color: =
rgb(54,55,55);line-height: 26px;font-size: 16px;">Let=E2=80=99s illustrate =
chunking with a quick example. Suppose we have a 5-page HR policy PDF. Usin=
g fixed 300-word chunks, we=E2=80=99d just cut every 300 words =E2=80=93 po=
ssibly slicing mid-paragraph. With sentence-based, we might end up with 20 =
sentence-long chunks (better). Semantic chunking might yield chunks like =
=E2=80=9CVacation Policy Overview=E2=80=9D (chunk 1), =E2=80=9CAccrual Rate=
s=E2=80=9D (chunk 2), =E2=80=9CCarryover Rules=E2=80=9D (chunk 3), etc., al=
igning with headings. That=E2=80=99s ideal for targeted Q&amp;A: a question=
 about carryover will likely retrieve the =E2=80=9CCarryover Rules=E2=80=9D=
 chunk exactly. Recursive chunking would note that =E2=80=9CVacation Policy=
=E2=80=9D is part of =E2=80=9CBenefits Policies=E2=80=9D and keep that asso=
ciation, so a higher-level query about benefits might retrieve multiple rel=
ated chunks.</p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-he=
ight: 26px;font-size: 16px;"><span>One more pro-tip: </span><strong>garbage=
 in, garbage out.</strong><span> Clean your text before chunking. Remove ir=
relevant headers/footers, deduplicate content, and consider adding </span><=
strong>metadata</strong><span> (like section titles) to chunks. A chunk wit=
h metadata =E2=80=9CSection: </span><em>Return Policy</em><span>=E2=80=9D i=
s far more informative to a retriever than a naked block of text. We=E2=80=
=99ll cover data prep in detail later, but chunking is the stage where a lo=
t of that happens =E2=80=93 splitting, labeling, and indexing the knowledge=
.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height=
: 26px;font-size: 16px;"><strong>Why chunking matters:</strong><span> If yo=
u chunk wrong, your RAG system might retrieve the wrong pieces or miss the =
right ones entirely. It=E2=80=99s been called the silent killer of RAG proj=
ects. But with the four strategies above and a bit of overlap, you can avoi=
d the common pitfalls. A </span><strong>20% overlap</strong><span> can impr=
ove accuracy dramatically by ensuring context isn=E2=80=99t accidentally dr=
opped. And focusing on semantic units keeps the </span><strong>signal-to-no=
ise ratio</strong><span> high for the LLM, which it loves. Think of chunkin=
g like making bite-sized snacks for your AI =E2=80=93 not too big to chew, =
and each with a clear flavor.</span></p><h3 class=3D"header-anchor-post" st=
yle=3D"position: relative;font-family: 'SF Pro Display',-apple-system-headl=
ine,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,=
Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';fon=
t-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing:=
 antialiased;-webkit-appearance: optimizelegibility;-moz-appearance: optimi=
zelegibility;appearance: optimizelegibility;margin: 1em 0 0.625em 0;color: =
rgb(54,55,55);line-height: 1.16em;font-size: 1.375em;"><strong>Similarity S=
earch Demystified</strong></h3><p style=3D"margin: 0 0 20px 0;color: rgb(54=
,55,55);line-height: 26px;font-size: 16px;"><span>We=E2=80=99ve embedded ou=
r chunks and query, and we have our chunks nicely defined =E2=80=93 now com=
es the </span><em>retrieval</em><span> part: </span><strong>similarity sear=
ch</strong><span>. This is where the vector database (or index) finds which=
 chunks are most </span><em>similar</em><span> to the query vector. Let=E2=
=80=99s demystify it.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(5=
4,55,55);line-height: 26px;font-size: 16px;"><span>First, what does =E2=80=
=9Cnearest neighbor in 1536D space=E2=80=9D even mean? A simple analogy: im=
agine each document chunk is a point on a map, but instead of latitude/long=
itude, we have 1536 coordinates. When you ask a question, you=E2=80=99re es=
sentially dropping a pin in this 1536-D map, and saying =E2=80=9Cfind me th=
e closest points=E2=80=9D. The </span><strong>nearest neighbors</strong><sp=
an> will be chunks that have high cosine similarity (small angle) with the =
query vector =E2=80=93 i.e., they talk about the same thing. Crucially, thi=
s finds </span><em>meaning</em><span>, not just exact words. For example, i=
f you ask =E2=80=9CHow do I reimburse a customer?=E2=80=9D, the nearest nei=
ghbors might include chunks mentioning =E2=80=9Crefund process=E2=80=9D or =
=E2=80=9Cissue a credit to the customer=E2=80=9D even if the word =E2=80=9C=
reimburse=E2=80=9D isn=E2=80=99t there. </span><strong>Vector search =E2=89=
=A0 keyword search</strong><span> =E2=80=93 it=E2=80=99s searching by conce=
pt. This is why nearest neighbor in embedding space can feel like </span><e=
m>magic</em><span>: it retrieves relevant info even when wording differs.</=
span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 2=
6px;font-size: 16px;"><span>The common metrics for similarity we already to=
uched on (cosine, dot, etc.). In practice, most vector databases let you ch=
oose one. The results =E2=80=93 nearest neighbors =E2=80=93 will be the sam=
e in terms of ranking if you use the one the model expects (cosine for norm=
alized vectors, etc.). </span><strong>Cosine similarity</strong><span> is p=
opular since it focuses purely on orientation (meaning). </span><strong>Dot=
 product</strong><span> can be slightly more sensitive to frequency (longer=
 text can have larger dot value even if semantically similar). </span><stro=
ng>Euclidean</strong><span> is less used for text embeddings but conceptual=
ly similar to dot for normalized vectors. The key is: the vector DB returns=
 a list of top-K chunks and their similarity scores.</span></p><p style=3D"=
margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"=
><span>Now, the </span><em>=E2=80=9Cre-ranking revolution=E2=80=9D</em><spa=
n>. Basic vector search is great, but researchers found you can push accura=
cy even higher by a second-stage rerank. One approach: retrieve, say, top 5=
0 chunks by cosine, then use a more precise but slower model (like a cross-=
encoder or even the LLM itself) to rerank those 50 for true relevance. This=
 two-tier system can take you from maybe 70% relevant results to 90%+. Why?=
 The cross-encoder actually looks at the query and chunk together (like =E2=
=80=9CWould this chunk answer that question?=E2=80=9D) rather than just com=
paring embeddings. It=E2=80=99s more computationally expensive, hence only =
done on top-K candidates, but it </span><strong>substantially improves prec=
ision</strong><span>.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(5=
4,55,55);line-height: 26px;font-size: 16px;"><span>There=E2=80=99s also a s=
impler heuristic called </span><strong>Reciprocal Rank Fusion (RRF)</strong=
><span> which can merge results from different methods (like one from keywo=
rd search, one from vector) and boosts final accuracy. RRF essentially says=
 =E2=80=9Cif a document is high on any list, boost it in the final rank=E2=
=80=9D. It=E2=80=99s robust and often used in hybrid systems (which we=E2=
=80=99ll talk about soon).</span></p><p style=3D"margin: 0 0 20px 0;color: =
rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>For a visceral sens=
e of similarity search, let=E2=80=99s do a quick =E2=80=9Clive=E2=80=9D exa=
mple in narrative form: </span><em>User asks:</em><span> =E2=80=9CWhat is t=
he warranty period for product X if purchased in Europe?=E2=80=9D =E2=80=93=
 The system embeds that query. It then computes similarity against thousand=
s of chunks: </span><em>Chunk 1:</em><span> =E2=80=9C=E2=80=A6the standard =
warranty is 1 year in the US and 2 years in the EU=E2=80=A6=E2=80=9D =E2=80=
=93 similarity 0.95 (very high, because it directly addresses warranty in E=
U). </span><em>Chunk 2:</em><span> =E2=80=9C=E2=80=A6product X comes with a=
 limited warranty covering defects for 24 months in Europe=E2=80=A6=E2=80=
=9D =E2=80=93 similarity 0.93 (also relevant wording). </span><em>Chunk 3:<=
/em><span> =E2=80=9C=E2=80=A6return policy for product X is 30 days=E2=80=
=A6=E2=80=9D =E2=80=93 similarity 0.5 (not very related, it=E2=80=99s about=
 returns vs warranty). The system would retrieve chunks 1 and 2 as top hits=
. If we only looked for the keyword =E2=80=9Cwarranty=E2=80=9D, we=E2=80=99=
d have found them too perhaps =E2=80=93 but consider if the query was phras=
ed as =E2=80=9CHow long is support provided=E2=80=A6?=E2=80=9D and the doc =
said =E2=80=9C24-month limited warranty=E2=80=9D. A pure keyword might miss=
 that (no literal =E2=80=9Csupport=E2=80=9D word), but the embedding knows =
=E2=80=9Csupport period=E2=80=9D is semantically near =E2=80=9Cwarranty per=
iod=E2=80=9D and still pulls it. </span><em>That=E2=80=99s</em><span> the p=
ower of nearest neighbor search in high dimensions =E2=80=93 it finds meani=
ng, not just matching terms.</span></p><p style=3D"margin: 0 0 20px 0;color=
: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>One more modern t=
wist: </span><strong>Hybrid search</strong><span>, combining sparse (keywor=
d) and dense (vector) searches. This can catch edge cases where one method =
alone might fail. For example, exact codes or names (like error code =E2=80=
=9CGAN-404=E2=80=9D) are best found via keyword, while conceptual questions=
 prefer vector. In a hybrid setup, you do both and merge results (maybe via=
 RRF as mentioned). This often yields the best of both: semantic breadth </=
span><em>and</em><span> lexical precision. We=E2=80=99ll cover hybrid more =
in </span><em>Advanced Patterns</em><span>, but keep in mind: </span><stron=
g>vector search gets you 80-90% there; adding a sprinkle of keyword search =
and re-ranking can push accuracy to the 95% range</strong><span>. In fact, =
our upcoming case study will show an AI that went from below 60% accuracy t=
o </span><strong>94-95%</strong><span> by smart retrieval and agentic steps=
 =E2=80=93 it=E2=80=99s not hype, it=E2=80=99s achievable with these techni=
ques.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-he=
ight: 26px;font-size: 16px;"><span>To summarize this =E2=80=9Csimilarity se=
arch=E2=80=9D stage: The query=E2=80=99s embedding is matched to chunk embe=
ddings via a similarity metric. The nearest chunks (those with highest cosi=
ne or dot) are fetched as relevant context. Because of embeddings, this fin=
ds relevant info even when phrasing differs =E2=80=93 </span><strong>the AI=
 is truly understanding the intent</strong><span> in vector form. And by la=
yering in re-ranking or hybrid methods, you ensure the </span><em>most</em>=
<span> relevant bits bubble up (even nailing tricky queries that a single m=
ethod might fumble). It=E2=80=99s the secret sauce taking retrieval precisi=
on from good (~70%) to great (90%+). All of this happens in a blink of an e=
ye (millisecond-scale for vector math, maybe a couple hundred ms if re-rank=
ing with a smaller model). The result: your LLM gets a tidy packet of top-n=
otch information to work with. Next, we=E2=80=99ll see how we go from those=
 retrieved chunks to a full answer =E2=80=93 and how you can build this who=
le pipeline yourself, step by step.</span></p><h2 class=3D"header-anchor-po=
st" style=3D"position: relative;font-family: 'SF Pro Display',-apple-system=
-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helv=
etica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbo=
l';font-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoo=
thing: antialiased;-webkit-appearance: optimizelegibility;-moz-appearance: =
optimizelegibility;appearance: optimizelegibility;margin: 1em 0 0.625em 0;c=
olor: rgb(54,55,55);line-height: 1.16em;font-size: 1.625em;"><strong>The Te=
chnical Journey: From Zero to Hero</strong></h2><p style=3D"margin: 0 0 20p=
x 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>Alright,=
 time to roll up our sleeves and get practical. How do you go from zero (no=
 RAG at all) to a hero-level implementation? We=E2=80=99ll walk through bui=
lding a simple RAG pipeline in minutes, then explore the rich ecosystem of =
tools and stacks available, and finally outline the </span><strong>5 levels=
 of RAG mastery</strong><span> you can aspire to. Don=E2=80=99t worry if yo=
u=E2=80=99re not a coding wizard =E2=80=93 we=E2=80=99ll keep it approachab=
le. By the end, you might just yell =E2=80=9C</span><strong>It=E2=80=99s al=
ive!</strong><span>=E2=80=9D as your first RAG system comes to life.</span>=
</p><h3 class=3D"header-anchor-post" style=3D"position: relative;font-famil=
y: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMac=
SystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji'=
,'Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoothin=
g: antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearance: opt=
imizelegibility;-moz-appearance: optimizelegibility;appearance: optimizeleg=
ibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;fo=
nt-size: 1.375em;"><strong>Starting Simple (The 10-minute RAG)</strong></h3=
><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font=
-size: 16px;"><span>Can we build a basic RAG app in a few lines of code? </=
span><strong>Yes.</strong><span> Thanks to high-level frameworks like Llama=
Index and LangChain, a minimal example is surprisingly short. Here=E2=80=99=
s a tiny RAG setup using LlamaIndex (formerly GPT Index) that can load docu=
ments, create a vector index, and answer queries:</span></p><p style=3D"mar=
gin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"># =
Your first RAG in ~15 lines of code</p><p style=3D"margin: 0 0 20px 0;color=
: rgb(54,55,55);line-height: 26px;font-size: 16px;">from langchain import S=
impleDirectoryReader, VectorStoreIndex</p><p style=3D"margin: 0 0 20px 0;co=
lor: rgb(54,55,55);line-height: 26px;font-size: 16px;"># 1. Load your data =
(all files in &quot;your_data&quot; folder)</p><p style=3D"margin: 0 0 20px=
 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">docs =3D Simple=
DirectoryReader(&quot;your_data&quot;).load_data()</p><p style=3D"margin: 0=
 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"># 2. Cre=
ate a vector index from documents</p><p style=3D"margin: 0 0 20px 0;color: =
rgb(54,55,55);line-height: 26px;font-size: 16px;">index =3D VectorStoreInde=
x.from_documents(docs)</p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,5=
5);line-height: 26px;font-size: 16px;"># 3. Query the index with a question=
</p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;f=
ont-size: 16px;">response =3D index.query(&quot;Your question here&quot;)</=
p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;fon=
t-size: 16px;">print(response)</p><p style=3D"margin: 0 0 20px 0;color: rgb=
(54,55,55);line-height: 26px;font-size: 16px;"><span>That=E2=80=99s it! Thi=
s example uses langchain and its integration of LlamaIndex. In step 1, it r=
eads documents from a directory (using an out-of-the-box reader that handle=
s text files). Step 2 creates an index =E2=80=93 under the hood it=E2=80=99=
s embedding those docs (likely with OpenAI=E2=80=99s Ada model by default) =
and storing vectors in a simple vector store. Step 3 sends a query; the lib=
rary does the embedding of the query, similarity search, and calls an LLM t=
o generate an answer, returning a nice response object (which we print). Wi=
th those few lines, you=E2=80=99ve built a basic doc-QA bot. </span><strong=
>Congrats! You just built your first RAG system.</strong><span> &#127881; I=
t=E2=80=99s pretty much =E2=80=9Cbatteries included.=E2=80=9D Of course, in=
 a real app you=E2=80=99d add your API keys, maybe use ServiceContext to sp=
ecify which LLM to use (GPT-4, etc.), but the core flow remains that simple=
.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height=
: 26px;font-size: 16px;">This toy example can be run on a small set of text=
 files. If you had a folder of policies or FAQs, it would work out of the b=
ox. The answer might look like: =E2=80=9CThe warranty period is 2 years for=
 EU purchases=E3=80=90source.pdf=E3=80=91.=E2=80=9D (Yes, these frameworks =
even return source citations automatically in many cases!). Now, this simpl=
icity is great for a prototype, but as you scale up, you=E2=80=99ll want to=
 make choices about your stack.</p><h3 class=3D"header-anchor-post" style=
=3D"position: relative;font-family: 'SF Pro Display',-apple-system-headline=
,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Ari=
al,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-w=
eight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing: an=
tialiased;-webkit-appearance: optimizelegibility;-moz-appearance: optimizel=
egibility;appearance: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb=
(54,55,55);line-height: 1.16em;font-size: 1.375em;"><strong>Choosing Your S=
tack (with personality-driven comparisons)</strong></h3><p style=3D"margin:=
 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">There=
=E2=80=99s an ever-growing landscape of RAG tooling. Let=E2=80=99s talk abo=
ut a few popular ones in a fun way =E2=80=93 imagine them as characters:</p=
><ul style=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;=
mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: =
26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 1=
6px;margin: 0;"><strong>LangChain: =E2=80=9CThe Swiss Army knife=E2=80=9D</=
strong><span> =E2=80=93 LangChain is the generalist that can do everything =
(sometimes too much!). It=E2=80=99s a framework with chains, agents, memory=
, integrations=E2=80=A6 you name it. Need to plug in a vector DB, call an A=
PI, parse output =E2=80=93 LangChain has a module. This is great for comple=
x apps that do more than just retrieval (like multi-step reasoning). But th=
e flip side is it can feel heavy or overly abstract for simple RAG. You=E2=
=80=99ll sometimes hear that LangChain is too broad =E2=80=93 it=E2=80=99s =
like a Swiss Army knife with 50 attachments; fantastic, but you might only =
need 3 of them.</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-specia=
l-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margi=
n-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin=
: 0;"><strong>LlamaIndex: =E2=80=9CThe specialist=E2=80=9D</strong><span> =
=E2=80=93 LlamaIndex (GPT Index) is laser-focused on RAG. It shines in inde=
xing and querying data with LLMs. If you =E2=80=9Cwant RAG done right=E2=80=
=9D out of the box, LlamaIndex is a great start. It handles chunking strate=
gies, embeddings, and even has neat tricks like </span><strong>Query Transf=
ormers</strong><span> and structured retrieval. It=E2=80=99s not trying to =
orchestrate arbitrary tool use or agents =E2=80=93 it=E2=80=99s specificall=
y </span><strong>the RAG specialist</strong><span>. Many find LlamaIndex si=
mpler for pure QA use cases, whereas LangChain is maybe better if you need =
to, say, do a RAG then an external calculation then chain another LLM call =
(i.e., more complex chain logic).</span></p></li></ul><p style=3D"margin: 0=
 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><em>(Rea=
lity: LangChain and LlamaIndex often work together =E2=80=93 you can use Ll=
amaIndex as a retriever in LangChain =E2=80=93 but painting them as distinc=
t personas helps clarify their emphases.)</em><span> According to one Stack=
Overflow summary: </span><em>=E2=80=9CYou=E2=80=99ll be fine with just Lang=
Chain, however LlamaIndex is optimized for indexing and retrieving data.=E2=
=80=9D</em><span>. LangChain is like the big toolkit; LlamaIndex is the ref=
ined instrument for data-LLM integration.</span></p><p style=3D"margin: 0 0=
 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">Now, beyon=
d those, you have alternatives: Haystack (an open-source framework from dee=
pset) which is like an enterprise-ready QA system toolkit, and various prop=
rietary solutions (Azure Cognitive Search, etc.). But LangChain and LlamaIn=
dex have huge communities right now. Use LangChain if you need that Swiss A=
rmy flexibility (chains, agents, lots of integrations). Use LlamaIndex if y=
our focus is =E2=80=9Cfeed these docs to an LLM and get answers=E2=80=9D an=
d you want it quick with sensible defaults. In practice, many start with Ll=
amaIndex for a pilot, and as they add more complex flows, they might incorp=
orate LangChain components.</p><p style=3D"margin: 0 0 20px 0;color: rgb(54=
,55,55);line-height: 26px;font-size: 16px;">Next, let=E2=80=99s compare vec=
tor databases with a similar personality flair:</p><ul style=3D"margin-top:=
 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-special-format: bulle=
t;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box=
-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>P=
inecone: =E2=80=9CThe reliable pro, but pricey=E2=80=9D</strong><span> =E2=
=80=93 Pinecone is a cloud vector DB that=E2=80=99s fully managed and very =
easy to use. It=E2=80=99s known for high performance and reliability at sca=
le. But like a seasoned pro, it comes with a price tag. Their Starter tier =
is free (up to ~300K vectors), but beyond that, a </span><strong>standard 5=
0K vector index costs ~$70/month</strong><span> , and pricing scales up wit=
h volume and QPS. Pinecone is great when you don=E2=80=99t want to worry ab=
out infrastructure and you have the budget for quality service. (Think: the=
 BMW of vector DBs =E2=80=93 smooth ride, premium features, but you pay for=
 it.)</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: =
bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: =
0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><str=
ong>Chroma: =E2=80=9CThe free spirit=E2=80=9D</strong><span> =E2=80=93 Chro=
ma is open-source and you can self-host it for free. It=E2=80=99s super eas=
y to get started (pip install chromadb and you have a local DB in minutes).=
 It=E2=80=99s not as battle-tested for massive scale as Pinecone, but it=E2=
=80=99s improving fast. If you=E2=80=99re a startup or hobbyist (or just co=
st-conscious), </span><strong>Chroma =3D $0 (self-hosted)</strong><span> an=
d often that=E2=80=99s enough for quite large projects. It=E2=80=99s like t=
he trusty open-source toolkit =E2=80=93 freedom and flexibility, though you=
 might have to get your hands a bit dirty on scaling.</span></p></li><li st=
yle=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color:=
 rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;pa=
dding-left: 4px;font-size: 16px;margin: 0;"><strong>Qdrant: =E2=80=9CThe bu=
dget-friendly workhorse=E2=80=9D</strong><span> =E2=80=93 Qdrant is another=
 open-source vector DB that also offers a cloud service. It=E2=80=99s known=
 for being efficient and having a friendly pricing model. One comparison fo=
und </span><strong>Qdrant=E2=80=99s cloud estimated around $9/month for 50K=
 vectors</strong><span> (versus Pinecone=E2=80=99s $70). So Qdrant is like =
the solid, economical choice =E2=80=93 maybe not as fancy as Pinecone, but =
gets the job done and keeps costs low. Performance-wise, Qdrant is quite go=
od; it uses HNSW under the hood like many others, and can handle millions o=
f vectors too.</span></p></li></ul><p style=3D"margin: 0 0 20px 0;color: rg=
b(54,55,55);line-height: 26px;font-size: 16px;"><span>Other names include W=
eaviate (feature-rich, hybrid search support), Milvus (from Zilliz, high-pe=
rformance, but heavier to manage). An insightful benchmark summarized: </sp=
an><em>For 50K vectors, Qdrant=E2=80=99s ~$9 is hard to beat, Weaviate ~$25=
, Pinecone ~$70</em><span>. Also Pinecone isn=E2=80=99t open-source (fully =
managed only), whereas Qdrant, Weaviate, Chroma are open or offer OSS versi=
ons.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-hei=
ght: 26px;font-size: 16px;"><span>In short: </span><strong>Pinecone if you =
value turnkey service and can pay; Chroma if you want free and local; Qdran=
t if you want cheap cloud with solid performance.</strong><span> There=E2=
=80=99s no one-size-fits-all =E2=80=93 it depends on your needs (privacy? s=
cale? budget?). Many prototyping with Chroma or LlamaIndex=E2=80=99s in-mem=
ory store, then move to Pinecone or Qdrant for production.</span></p><p sty=
le=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: =
16px;">Finally, consider the LLM for generation. If using OpenAI, GPT-4 giv=
es best quality but at higher cost/latency; GPT-3.5 is faster/cheaper but m=
ay hallucinate more if the retrieved context isn=E2=80=99t obviously releva=
nt. There=E2=80=99s also Cohere, Anthropic, and open models (like LLaMA 70B=
 via API or self-hosted). Using a powerful model for final answer is import=
ant for quality, but you can often get away with a smaller model if your re=
trieval is very on-point (because then the model=E2=80=99s job is easier =
=E2=80=93 just summarize or lightly rephrase the facts in context).</p><p s=
tyle=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size=
: 16px;"><span>This naturally leads to the idea of </span><strong>cascading=
 models to optimize cost</strong><span>, which advanced users do (e.g., try=
 answering with a cheap model first, and only if unsure, call GPT-4). We=E2=
=80=99ll revisit that in Enterprise tips. But for now, let=E2=80=99s outlin=
e the stages of RAG mastery you can progress through.</span></p><div class=
=3D"captioned-image-container-static" style=3D"font-size: 16px;line-height:=
 26px;margin: 32px auto;"><figure style=3D"width: 100%;margin: 0 auto;"><ta=
ble class=3D"image-wrapper" width=3D"100%" border=3D"0" cellspacing=3D"0" c=
ellpadding=3D"0" data-component-name=3D"Image2ToDOMStatic" style=3D"mso-pad=
ding-alt: 1em 0 1.6em;"><tbody><tr><td style=3D"text-align: center;"></td><=
td class=3D"content" align=3D"left" width=3D"995" style=3D"text-align: cent=
er;"><a class=3D"image-link" target=3D"_blank" href=3D"https://substack.com=
/redirect/548e0fbe-3bf3-4829-9e26-def16e5acc79?j=3DeyJ1IjoiNWtiOTN6In0.zdzy=
88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"position: relati=
ve;flex-direction: column;align-items: center;padding: 0;width: auto;height=
: auto;border: none;text-decoration: none;display: block;margin: 0;"><img c=
lass=3D"wide-image" data-attrs=3D"{&quot;src&quot;:&quot;https://substack-p=
ost-media.s3.amazonaws.com/public/images/23ec37b2-4a6f-435f-92e2-2dd1cc4bb2=
17_995x3840.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot=
;:null,&quot;imageSize&quot;:null,&quot;height&quot;:3840,&quot;width&quot;=
:995,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:272401,&quot;alt&quot;:=
null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;hr=
ef&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quo=
t;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/16730=
7195?img=3Dhttps%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fim=
ages%2F23ec37b2-4a6f-435f-92e2-2dd1cc4bb217_995x3840.png&quot;,&quot;isProc=
essing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt=3D=
"" width=3D"550" height=3D"2122.613065326633" src=3D"https://substackcdn.co=
m/image/fetch/$s_!LZrW!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:st=
eep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F=
23ec37b2-4a6f-435f-92e2-2dd1cc4bb217_995x3840.png" style=3D"border: none !i=
mportant;vertical-align: middle;display: block;-ms-interpolation-mode: bicu=
bic;height: auto;margin-bottom: 0;width: auto !important;max-width: 100% !i=
mportant;margin: 0 auto;"></a></td><td style=3D"text-align: center;"></td><=
/tr></tbody></table></figure></div><h3 class=3D"header-anchor-post" style=
=3D"position: relative;font-family: 'SF Pro Display',-apple-system-headline=
,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Ari=
al,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-w=
eight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing: an=
tialiased;-webkit-appearance: optimizelegibility;-moz-appearance: optimizel=
egibility;appearance: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb=
(54,55,55);line-height: 1.16em;font-size: 1.375em;"><strong>The 5 Levels of=
 RAG Mastery</strong></h3><ol style=3D"margin-top: 0;padding: 0;"><li style=
=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line-height: 26=
px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16p=
x;margin: 0;"><strong>Basic RAG =E2=80=93 Simple Document Q&amp;A:</strong>=
<span> </span><em>Level 1:</em><span> You can feed documents and get answer=
s. This is where you likely are after writing those 15 lines above. It hand=
les questions like =E2=80=9CWhat is our refund policy for EU customers?=E2=
=80=9D by retrieving a snippet from your policy docs and answering. The sys=
tem uses one strategy (vector search) and one data source. At this level, y=
ou might occasionally get irrelevant context if the query is ambiguous, but=
 generally it works for straightforward Q&amp;A on your content.</span></p>=
</li><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);l=
ine-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;=
font-size: 16px;margin: 0;"><strong>Hybrid Search =E2=80=93 Combining Seman=
tic + Keyword:</strong><span> </span><em>Level 2:</em><span> You enhance re=
trieval by using both vector similarity and traditional keyword (BM25) sear=
ch. Why? Because certain queries need exact matches (e.g., codes, proper no=
uns) that vectors might miss. By combining results from both and merging (p=
erhaps via that RRF method ), you cover both the =E2=80=9Cfuzzy meaning=E2=
=80=9D and =E2=80=9Cexact token=E2=80=9D bases. The result: higher accuracy=
 and robustness, especially for edge cases. At this level your system can h=
andle things like =E2=80=9Cerror code 500 out-of-memory=E2=80=9D (which nee=
ds exact code match) </span><em>and</em><span> =E2=80=9COOM error=E2=80=9D =
(which a vector might link to the same thing). You=E2=80=99re mitigating th=
e recall issues of vector or sparse alone.</span></p></li><li style=3D"marg=
in: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margi=
n-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin=
: 0;"><strong>Multi-modal RAG =E2=80=93 Text, Images, and Beyond:</strong><=
span> </span><em>Level 3:</em><span> Now your =E2=80=9Cdocuments=E2=80=9D a=
ren=E2=80=99t just text =E2=80=93 they could be images, audio transcripts, =
even video. Multi-modal RAG means retrieving across different data types. F=
or example, Vimeo=E2=80=99s support might use RAG to search not just their =
text docs but also transcripted tutorials or even the content of videos (vi=
a image captions or OCR). Another scenario: in healthcare, a RAG system mig=
ht pull a relevant medical diagram along with text. Technically, this invol=
ves embedding other modalities (e.g., using CLIP for images to get vectors)=
. By mastering multi-modal RAG, your AI could answer a question like =E2=80=
=9CWhat does the workflow diagram look like for process X?=E2=80=9D by retr=
ieving an image of that diagram (converted to an embedding) plus some expla=
nation text. It opens up a new world of use cases =E2=80=93 chat about your=
 PDFs </span><em>and</em><span> your slide decks </span><em>and</em><span> =
your videos.</span></p></li><li style=3D"margin: 8px 0 0 32px;"><p style=3D=
"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border=
-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Agentic RAG =E2=
=80=93 Self-improving Systems with Reasoning:</strong><span> </span><em>Lev=
el 4:</em><span> Here we blend RAG with agent-like behavior. Instead of a s=
ingle retrieval step, the AI can iteratively plan and retrieve, or use tool=
s, to answer more complex queries. For example, an agentic RAG might break =
down a tough question into sub-questions, retrieve answers for each, and th=
en compose a final answer. It can also decide to do follow-up retrieval if =
the initial info wasn=E2=80=99t sufficient =E2=80=93 essentially a loop whe=
re the LLM says =E2=80=9CLet me dig deeper on XYZ=E2=80=9D and performs ano=
ther retrieval. This level often uses frameworks like LangChain agents or t=
he ReAct pattern (LLM reasoning with retrieval actions). The system not onl=
y fetches facts, but can chain them or perform calculations, etc. It=E2=80=
=99s =E2=80=9Copen-book exam + reasoning=E2=80=9D. One cool example: an age=
ntic RAG might take a customer query, retrieve some knowledge base articles=
, then notice it needs the latest sales figure, call an API to get that, an=
d then answer =E2=80=93 all dynamically. It=E2=80=99s more complex but can =
tackle multi-step tasks and even self-correct if initial info was misleadin=
g. This is where =E2=80=9CAI assistants=E2=80=9D live, going beyond pure Q&=
amp;A.</span></p></li><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color=
: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;p=
adding-left: 4px;font-size: 16px;margin: 0;"><strong>Production RAG =E2=80=
=93 Enterprise-scale with Millisecond Latency:</strong><span> </span><em>Le=
vel 5:</em><span> The final boss level. Your RAG system serves thousands or=
 millions of users, with perhaps </span><strong>10 million+ queries a day</=
strong><span>, all under tight latency requirements (say &lt;100ms for sear=
ch). You=E2=80=99ve deployed indexes with millions of chunks, sharded acros=
s servers. Caching is employed (maybe an in-memory cache for popular querie=
s), and you monitor latency percentiles. This is where search engine tech m=
eets RAG. Systems like Bing (with Sydney), or Google=E2=80=99s search augme=
ntation, or enterprise digital assistants fall here. You=E2=80=99ve mastere=
d vector index scaling, index updating without downtime, and cost optimizat=
ion (like using a cheaper model for 95% of queries and only using GPT-4 for=
 the hardest ones to save money). Also, production-ready means robust evalu=
ation and fallback =E2=80=93 you likely integrate feedback loops where if t=
he AI is not confident, it might gracefully decline or escalate. It also in=
volves security =E2=80=93 ensuring no data leakage, compliance with things =
like HIPAA/GDPR if applicable. At Level 5, you are building RAG with the ri=
gor of a mission-critical system. The payoff: users get instant, accurate a=
nswers at scale, and your company=E2=80=99s collective knowledge truly beco=
mes an =E2=80=9CAI brain=E2=80=9D that anyone can tap.</span></p></li></ol>=
<p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-=
size: 16px;"><span>Think of these levels as cumulative. Each builds on the =
previous. By the time you=E2=80=99re at Level 5, you=E2=80=99ve incorporate=
d hybrid search, maybe multi-modal sources, perhaps some reasoning, and you=
=E2=80=99ve industrialized it. But don=E2=80=99t be daunted =E2=80=93 </spa=
n><strong>you can get a lot of value at Level 1 and 2 already.</strong><spa=
n> Many internal tools live around Level 2 or 3: e.g., a chatbot that answe=
rs from company docs (text-only) with some hybrid search is Level 2. That a=
lone can reduce support tickets or onboard employees faster. The fancy stuf=
f like agents and multi-modal come into play for advanced applications (lik=
e an AI that can troubleshoot software by reading logs and viewing system g=
raphs =E2=80=93 text + metrics + images, with reasoning).</span></p><p styl=
e=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 1=
6px;"><span>Next, we=E2=80=99ll delve into how to get your </span><strong>d=
ata ready</strong><span> for these levels. Because even the fanciest RAG pi=
peline fails if fed garbage data. It=E2=80=99s like having a brilliant chef=
 but giving them rotten ingredients =E2=80=93 the dish won=E2=80=99t turn o=
ut well. So, let=E2=80=99s talk data prep!</span></p><h2 class=3D"header-an=
chor-post" style=3D"position: relative;font-family: 'SF Pro Display',-apple=
-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Robo=
to,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe U=
I Symbol';font-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-fo=
nt-smoothing: antialiased;-webkit-appearance: optimizelegibility;-moz-appea=
rance: optimizelegibility;appearance: optimizelegibility;margin: 1em 0 0.62=
5em 0;color: rgb(54,55,55);line-height: 1.16em;font-size: 1.625em;"><strong=
>The Data Preparation Pipeline: Garbage In, Genius Out</strong></h2><p styl=
e=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 1=
6px;"><span>You=E2=80=99ve heard it a million times: </span><em>=E2=80=9Cga=
rbage in, garbage out.=E2=80=9D</em><span> Nowhere is this more true than i=
n RAG. The smartest retrieval and LLM won=E2=80=99t help if your documents =
are a mess =E2=80=93 imagine PDFs with broken text, irrelevant boilerplate,=
 or missing context. In this section, we=E2=80=99ll cover how to turn raw d=
ata into RAG-ready gold. From parsing gnarly file formats to enriching with=
 metadata, consider this your pre-flight checklist before launching your AI=
.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height=
: 26px;font-size: 16px;"><strong>Document Parsing Secrets:</strong><span> Y=
our data likely isn=E2=80=99t a neat collection of.txt files. You=E2=80=99l=
l have PDFs, Word docs, HTML pages, maybe spreadsheets. The first step is <=
/span><strong>parsing</strong><span> them into plain text (or structured te=
xt). Each format has its quirks:</span></p><ul style=3D"margin-top: 0;paddi=
ng: 0;"><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p s=
tyle=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing:=
 border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>PDFs:</st=
rong><span> Use reliable PDF parsers (like PyMuPDF/fitz or pdfplumber in Py=
thon). Extract text but beware =E2=80=93 PDFs often have headers/footers on=
 every page, line breaks in weird places, etc. A secret: many PDFs are basi=
cally scanned images (like that one cursed legacy contract). For those, you=
=E2=80=99ll need OCR (optical character recognition) to get text. Tools lik=
e Tesseract or AWS Textract can OCR images in PDFs. Also, watch out for mul=
ti-column layouts (scientific papers) =E2=80=93 a naive parser might read a=
cross columns mixing content. Some libraries can detect columns or you migh=
t split by page and handle manually.</span></p></li><li style=3D"margin: 8p=
x 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);li=
ne-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;f=
ont-size: 16px;margin: 0;"><strong>Word Docs (.docx):</strong><span> These =
are easier =E2=80=93 use python-docx or LibreOffice command line to convert=
 to text. Most formatting (bold, etc.) we don=E2=80=99t need, but we want t=
o preserve structure like headings. A good strategy: extract text and also =
output something like =E2=80=9C## Heading: [Heading Text]=E2=80=9D lines so=
 you know what was a heading.</span></p></li><li style=3D"margin: 8px 0 0 3=
2px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-heig=
ht: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-siz=
e: 16px;margin: 0;"><strong>HTML/Markdown:</strong><span> Likely documentat=
ion or web pages. Stripping HTML tags is step one (BeautifulSoup can help).=
 But preserving some structure (like lists, tables) is useful. You might co=
nvert HTML to markdown, which keeps bullet points and links in a readable w=
ay. Be careful to remove navigation menus, ads, etc., that aren=E2=80=99t p=
art of main content (there are boilerplate removal tools for HTML).</span><=
/p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p s=
tyle=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing:=
 border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Excel/CSV=
:</strong><span> If you have tabular data that=E2=80=99s relevant (maybe pr=
oduct price lists or error code tables), you can either embed those as text=
 (e.g., convert small tables to text lists) or handle them specially (some =
RAG systems store small tables as structured data and let the LLM access th=
em via a =E2=80=9Ctool=E2=80=9D). But often, converting each row into a sen=
tence works (e.g., row with Product X =E2=80=93 Warranty 2 years becomes =
=E2=80=9CProduct X has a warranty period of 2 years.=E2=80=9D for embedding=
).</span></p></li></ul><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);=
line-height: 26px;font-size: 16px;"><span>Essentially, </span><strong>use t=
he right parser for each format, and verify the output</strong><span>. You =
don=E2=80=99t want chunks full of gibberish because the parser mis-ordered =
the text. A quick manual skim of parsed output for a few files can save hea=
daches.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-=
height: 26px;font-size: 16px;"><strong>Metadata Magic:</strong><span> Once =
you have the content, add metadata! Metadata is additional info about each =
chunk =E2=80=93 like source filename, document title, author, timestamp, se=
ction, etc. Why? It can </span><strong>3=C3=97 your accuracy</strong><span>=
 in retrieval and even generation. For example, if each chunk knows it came=
 from =E2=80=9CFAQ.doc =E2=80=93 Section: Pricing=E2=80=9D, the retriever c=
an use that in relevance scoring (some vector DBs allow filtering or weight=
ed fields). The LLM can also be instructed to cite the source or use sectio=
n info to format answer. In an evaluation at one company, adding metadata l=
ike document category boosted relevant retrieval by a huge margin (one anec=
dote: +35% hit rate on correct doc). At minimum, store: </span><strong>sour=
ce name</strong><span>, and if applicable </span><strong>section headings</=
strong><span> and </span><strong>dates</strong><span>. Dates are important =
for time-sensitive info =E2=80=93 e.g., =E2=80=9CPolicy updated March 2023=
=E2=80=9D. You can store that so if a query asks =E2=80=9Cwhat=E2=80=99s th=
e latest policy=E2=80=9D, you might prefer newer chunks.</span></p><p style=
=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16=
px;"><span>A neat trick: use metadata to </span><strong>filter</strong><spa=
n>. If your system supports it, you can tag chunks by type (e.g., =E2=80=9C=
internal=E2=80=9D vs =E2=80=9Ccustomer-facing=E2=80=9D). Then if you build =
a chatbot for customers, you filter out internal-only docs entirely. This a=
voids embarrassing mistakes (like the AI revealing an internal memo because=
 it was in the index).</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(=
54,55,55);line-height: 26px;font-size: 16px;"><strong>Cleaning Strategies t=
hat Work:</strong><span> Before embedding, you want your text clean and use=
ful:</span></p><ul style=3D"margin-top: 0;padding: 0;"><li style=3D"margin:=
 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55)=
;line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4p=
x;font-size: 16px;margin: 0;"><strong>Intelligent removal of headers/footer=
s:</strong><span> Many docs have repeated boilerplate (company name, page n=
umbers, legal footers). These can pollute retrieval =E2=80=93 e.g., you don=
=E2=80=99t want a chunk of mostly footer text (=E2=80=9CACME Corp Confident=
ial =E2=80=93 Page 5 of 10=E2=80=9D) to be retrieved. You can detect these =
by frequency (if a line appears in every page, drop it), or specific cues (=
if it matches regex like =E2=80=9CPage \d of \d=E2=80=9D or has the company=
 name in ALLCAPS over and over). Removing or reducing this boilerplate impr=
oves the signal.</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-speci=
al-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;marg=
in-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margi=
n: 0;"><strong>Handle tables and lists carefully:</strong><span> If a PDF p=
arser outputs tables in a jumbled way (e.g., row data doesn=E2=80=99t line =
up), consider post-processing it. Sometimes it=E2=80=99s better to manually=
 parse important tables (or use a CSV export from the source). For lists, k=
eep the bullets or numbering if possible =E2=80=93 it gives structure. For =
example, an answer might list the 3 steps of a process; if your chunk prese=
rved =E2=80=9C1. Do X 2. Do Y 3. Do Z=E2=80=9D as separate lines, the LLM c=
an more cleanly produce a numbered answer.</span></p></li><li style=3D"marg=
in: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,=
55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left:=
 4px;font-size: 16px;margin: 0;"><strong>Preserve context while removing no=
ise:</strong><span> This is key. You want to trim the junk but not accident=
ally trim meaningful context. For instance, if a heading says =E2=80=9C4.2 =
Refund Process=E2=80=9D and the next page starts mid-sentence because of a =
page break, ensure the text is contiguous. One strategy is </span><em>join<=
/em><span> text from page to page if there=E2=80=99s an obvious cut. Anothe=
r is to include the heading text as metadata or inline (like add a line =E2=
=80=9CRefund Process:=E2=80=9D before the paragraph text). That way the chu=
nk is self-contained contextually.</span></p></li><li style=3D"margin: 8px =
0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line=
-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;fon=
t-size: 16px;margin: 0;"><strong>Normalize text</strong><span>: fix OCR err=
ors (common ones like =E2=80=9CO=E2=80=9D vs =E2=80=9C0=E2=80=9D or =E2=80=
=9Crn=E2=80=9D vs =E2=80=9Cm=E2=80=9D). Also, unify things like whitespace,=
 remove weird characters. If the data has a lot of unicode bullets or emoji=
s not relevant, strip them. Consistency in text will help embeddings not ge=
t confused by artifacts.</span></p></li><li style=3D"margin: 8px 0 0 32px;m=
so-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 2=
6px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16=
px;margin: 0;"><strong>Language and encoding</strong><span>: If you have mu=
ltilingual docs, note language in metadata. Remove any Unicode BOMs or enco=
ding issues (most libraries handle UTF-8 fine nowadays, but just be cautiou=
s if any documents are in different languages/scripts =E2=80=93 test a bit)=
.</span></p></li></ul><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);l=
ine-height: 26px;font-size: 16px;">Think of cleaning like preparing a train=
ing dataset =E2=80=93 a bit of time here yields a much smarter system. Ther=
e=E2=80=99s a story of a startup spending weeks debugging why their RAG ans=
wers were off, only to realize the PDF parser scrambled columns so text rea=
d like word salad. A quick fix in parsing and accuracy jumped. So, invest t=
ime here.</p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-heigh=
t: 26px;font-size: 16px;"><strong>The preprocessing checklist:</strong><spa=
n> Every document should ideally go through these 10 steps:</span></p><ol s=
tyle=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;"><p s=
tyle=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing:=
 border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Convert t=
o text:</strong><span> (using appropriate parser for PDF, docx, etc.)</span=
></p></li><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,=
55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left:=
 4px;font-size: 16px;margin: 0;"><strong>Split into paragraphs/sections:</s=
trong><span> (don=E2=80=99t chunk yet, just logical sections)</span></p></l=
i><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line=
-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;fon=
t-size: 16px;margin: 0;"><strong>Remove boilerplate:</strong><span> (header=
s, footers, legalese not needed)</span></p></li><li style=3D"margin: 8px 0 =
0 32px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: =
0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><str=
ong>Normalize whitespace and punctuation:</strong><span> (clean newlines, f=
ix broken hyphenations where a word is split at line break)</span></p></li>=
<li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line-h=
eight: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-=
size: 16px;margin: 0;"><strong>Extract or insert section titles as needed:<=
/strong><span> (to give context to each part)</span></p></li><li style=3D"m=
argin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;ma=
rgin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;mar=
gin: 0;"><strong>Add metadata:</strong><span> (filename, section, date, etc=
.)</span></p></li><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rg=
b(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;paddi=
ng-left: 4px;font-size: 16px;margin: 0;"><strong>Chunk into pieces with ove=
rlap:</strong><span> (apply your chunk strategy here on the cleaned content=
)</span></p></li><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb=
(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;paddin=
g-left: 4px;font-size: 16px;margin: 0;"><strong>Embed chunks and store in v=
ector DB:</strong><span> (and store metadata alongside)</span></p></li><li =
style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line-heigh=
t: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size=
: 16px;margin: 0;"><strong>Verify sample chunks:</strong><span> (manually c=
heck a few: =E2=80=9CDoes this chunk make sense on its own? Does it have ne=
cessary context?=E2=80=9D)</span></p></li><li style=3D"margin: 8px 0 0 32px=
;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-=
sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>It=
erate if needed:</strong><span> (if something looked off, tweak parsing or =
chunking and re-run for that doc).</span></p></li></ol><p style=3D"margin: =
0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">For a =
=E2=80=9Cmessy financial report=E2=80=9D example: say you have a 100-page a=
nnual report PDF with financial tables and text. The steps would be: parse =
text, detect that every page has a footer =E2=80=9CCompany =E2=80=93 Confid=
ential=E2=80=9D and remove that line everywhere. Join lines that got broken=
 in half by page breaks. For tables, maybe you notice they came out misalig=
ned =E2=80=93 you might manually copy the table as CSV, or at least ensure =
each table row stays in a chunk so context isn=E2=80=99t lost. Add metadata=
 like =E2=80=9Csection: Balance Sheet=E2=80=9D for the section where the ta=
ble is. Then chunk maybe by sub-sections or 512-token blocks with overlap, =
ensuring not to cut mid-table. The end result: a set of chunks like =E2=80=
=9CBalance Sheet: =E2=80=A6assets=E2=80=A6liabilities=E2=80=A6=E2=80=9D wit=
h perhaps the table values listed neatly. Now when a question asks =E2=80=
=9CWhat were the total assets in 2024?=E2=80=9D, the chunk with that info i=
s retrievable and the model can answer accurately.</p><p style=3D"margin: 0=
 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>Re=
member, an hour spent cleaning data can save dozens of hours troubleshootin=
g weird AI outputs later. When the AI gives a wrong or odd answer, 9 times =
out of 10 the issue can be traced back to missing or poorly formatted conte=
xt in the chunks. </span><strong>Garbage in, garbage out</strong><span> is =
a law; but with clean, well-chunked data in, you get </span><em>genius out<=
/em><span>.</span></p><h2 class=3D"header-anchor-post" style=3D"position: r=
elative;font-family: 'SF Pro Display',-apple-system-headline,system-ui,-app=
le-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'=
Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-we=
bkit-font-smoothing: antialiased;-moz-osx-font-smoothing: antialiased;-webk=
it-appearance: optimizelegibility;-moz-appearance: optimizelegibility;appea=
rance: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line=
-height: 1.16em;font-size: 1.625em;"><strong>Memory Magic: Short-term vs Lo=
ng-term</strong></h2><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);li=
ne-height: 26px;font-size: 16px;"><span>One thing people often ask is, =E2=
=80=9CIf RAG gives the LLM external info, do we even need the LLM=E2=80=99s=
 own memory?=E2=80=9D Also, how do we handle multi-turn conversations =E2=
=80=93 can the AI =E2=80=9Cremember=E2=80=9D what was said earlier? This is=
 where </span><em>memory</em><span> comes in, and in RAG we deal with two k=
inds: short-term (conversation context) and long-term (persistent knowledge=
). Let=E2=80=99s explore how to give your AI a memory like an elephant (whe=
n needed), without blowing the context window.</span></p><p style=3D"margin=
: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><stro=
ng>The =E2=80=9Cconversation amnesia=E2=80=9D problem:</strong><span> If yo=
u=E2=80=99ve used ChatGPT, you know it can carry on a conversation remember=
ing what you said earlier =E2=80=93 up to a limit. That limit is the contex=
t window (e.g., ~4K or 8K tokens for GPT-3.5, 32K or more for GPT-4). In a =
chat setting, the model doesn=E2=80=99t truly </span><em>remember</em><span=
> anything beyond what=E2=80=99s in the prompt each turn. If the convo exce=
eds the window, it =E2=80=9Cforgets=E2=80=9D the earliest parts unless we d=
o something. This is conversation short-term memory issue. In a RAG chatbot=
, it=E2=80=99s similar: you want it to remember what the user already asked=
 and what answers it gave.</span></p><p style=3D"margin: 0 0 20px 0;color: =
rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>Episodic memory (=
like human memory, but better):</strong><span> One solution is to implement=
 </span><em>episodic memory</em><span> for the AI. Think of splitting the c=
onversation into episodes or chunks and summarizing past ones. For example,=
 after 10 turns, you generate a summary of the conversation so far (or the =
important points) and use that going forward instead of the full history. T=
his is akin to how humans remember key points of a long discussion, not eve=
ry sentence. There are known strategies:</span></p><ul style=3D"margin-top:=
 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-special-format: bulle=
t;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box=
-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>S=
ummary memory:</strong><span> Every few turns, produce a concise summary an=
d include that in the prompt instead of raw transcript.</span></p></li><li =
style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"colo=
r: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;=
padding-left: 4px;font-size: 16px;margin: 0;"><strong>Message retrieval mem=
ory:</strong><span> This is cool =E2=80=93 treat past dialogue as knowledge=
 chunks, embed them, and when context is needed, retrieve relevant past utt=
erances (yes, RAG on the conversation itself). So if 30 turns ago the user =
mentioned something now relevant, the system can fetch that line rather tha=
n hoping it=E2=80=99s still in context. This is like context-aware memory r=
etrieval.</span></p></li></ul><p style=3D"margin: 0 0 20px 0;color: rgb(54,=
55,55);line-height: 26px;font-size: 16px;"><span>At its core, you can maint=
ain a vector store for conversation history. We call this a =E2=80=9Clong-t=
erm memory=E2=80=9D module. As the chat goes on, store each user and assist=
ant message embedding. When new question comes, you can retrieve past messa=
ges that seem related to the new query and prepend them as context. This wa=
y, even if the conversation spans 100 turns, the AI can recall specific det=
ails from earlier by retrieving them as needed. It=E2=80=99s like an AI hav=
ing selective photographic memory: it doesn=E2=80=99t hold everything in im=
mediate view, but it can </span><em>search its memory</em><span> for releva=
nt bits.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line=
-height: 26px;font-size: 16px;"><strong>Context window hacks:</strong><span=
> Besides retrieval-based memory, there are some hacks to fit more into the=
 context window:</span></p><ul style=3D"margin-top: 0;padding: 0;"><li styl=
e=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: r=
gb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padd=
ing-left: 4px;font-size: 16px;margin: 0;"><strong>Truncation strategy:</str=
ong><span> Always drop the oldest turns once you near the limit (not great =
if user refers back to something older).</span></p></li><li style=3D"margin=
: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55=
);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4=
px;font-size: 16px;margin: 0;"><strong>Prioritize important content:</stron=
g><span> e.g., keep system instructions and last user question, maybe summa=
ry of rest.</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-special-fo=
rmat: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bo=
ttom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;=
"><strong>Use larger context models for summarizing smaller context models:=
</strong><span> For instance, use GPT-4 32K to manage summarization that GP=
T-3.5 can=E2=80=99t hold, etc. But that=E2=80=99s advanced interplay.</span=
></p></li></ul><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-hei=
ght: 26px;font-size: 16px;">With Anthropic=E2=80=99s models boasting 100K t=
oken context now and likely million-token contexts on the horizon , one mig=
ht say =E2=80=9Cwhy bother summarizing, just use a bigger model!=E2=80=9D T=
rue, bigger windows alleviate some need for memory tricks =E2=80=93 but the=
y aren=E2=80=99t infinite and come with higher cost and slower performance.=
 So memory techniques will remain useful.</p><p style=3D"margin: 0 0 20px 0=
;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>A </span><e=
m>real example</em><span>: DoorDash (just a hypothetical scenario to illust=
rate) =E2=80=93 suppose DoorDash has a support chatbot that helps with live=
 customer orders. They want it to remember your entire conversation (=E2=80=
=9CDid the agent already ask for my order ID five messages ago?=E2=80=9D). =
They might use an </span><strong>episodic memory</strong><span>: each time =
you provide info like order ID, it=E2=80=99s stored in a slot; if you menti=
on a restaurant name earlier and later say =E2=80=9Cthe restaurant messed u=
p my order=E2=80=9D, the bot should recall which restaurant =E2=80=93 that =
could be done by simply including previous user messages in context until i=
t can=E2=80=99t, then summarizing =E2=80=9CUser=E2=80=99s order from McDona=
ld=E2=80=99s had an issue=E2=80=A6=E2=80=9D. Anecdotally, an AI support bot=
 forgetting earlier details leads to repetition and frustration, so solving=
 conversation memory is crucial. Many production systems use a combination:=
 keep recent turns verbatim (for recency), use a summary for older turns, a=
nd even incorporate retrieval for specific facts from the dialogue history.=
</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height:=
 26px;font-size: 16px;"><strong>Short-term vs Long-term memory:</strong><sp=
an> In human terms: short-term is like the scratchpad of what=E2=80=99s act=
ively being discussed (the last few exchanges), long-term is everything tha=
t happened before that you might need to recall if context shifts back. RAG=
 gives an LLM a form of long-term memory by hooking into an external knowle=
dge base. We can extend that concept to conversation =E2=80=93 the knowledg=
e base in this case </span><em>is the conversation transcript itself</em><s=
pan>. In fact, some research works use the same RAG pipeline for conversati=
on: treat the entire dialogue as a growing document. But more practically, =
we maintain separate memory vector indexes for conversation history vs. gen=
eral knowledge.</span></p><div class=3D"captioned-image-container-static" s=
tyle=3D"font-size: 16px;line-height: 26px;margin: 32px auto;"><figure style=
=3D"width: 100%;margin: 0 auto;"><table class=3D"image-wrapper" width=3D"10=
0%" border=3D"0" cellspacing=3D"0" cellpadding=3D"0" data-component-name=3D=
"Image2ToDOMStatic" style=3D"mso-padding-alt: 1em 0 1.6em;"><tbody><tr><td =
style=3D"text-align: center;"></td><td class=3D"content" align=3D"left" wid=
th=3D"1456" style=3D"text-align: center;"><a class=3D"image-link" target=3D=
"_blank" href=3D"https://substack.com/redirect/f72162e7-23ad-41fa-8d1c-dcca=
9d3d834d?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw=
0" rel=3D"" style=3D"position: relative;flex-direction: column;align-items:=
 center;padding: 0;width: auto;height: auto;border: none;text-decoration: n=
one;display: block;margin: 0;"><img class=3D"wide-image" data-attrs=3D"{&qu=
ot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/imag=
es/9afc658e-fbbf-4a2c-9209-3a67becc8948_3643x3840.png&quot;,&quot;srcNoWate=
rmark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&qu=
ot;height&quot;:1535,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&q=
uot;bytes&quot;:690392,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;ty=
pe&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quo=
t;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https=
://natesnewsletter.substack.com/i/167307195?img=3Dhttps%3A%2F%2Fsubstack-po=
st-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9afc658e-fbbf-4a2c-9209-3a67b=
ecc8948_3643x3840.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot=
;:null,&quot;offset&quot;:false}" alt=3D"" width=3D"550" height=3D"579.8420=
32967033" src=3D"https://substackcdn.com/image/fetch/$s_!c9km!,w_1100,c_lim=
it,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-medi=
a.s3.amazonaws.com%2Fpublic%2Fimages%2F9afc658e-fbbf-4a2c-9209-3a67becc8948=
_3643x3840.png" style=3D"border: none !important;vertical-align: middle;dis=
play: block;-ms-interpolation-mode: bicubic;height: auto;margin-bottom: 0;w=
idth: auto !important;max-width: 100% !important;margin: 0 auto;"></a></td>=
<td style=3D"text-align: center;"></td></tr></tbody></table></figure></div>=
<p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-=
size: 16px;"><strong>Context window optimization analogy:</strong><span> =
=E2=80=9CFitting an encyclopedia on a Post-it note.=E2=80=9D If you only ha=
ve a 4K-token Post-it (context), you can=E2=80=99t fit the whole company ha=
ndbook. But RAG lets you include just the relevant parts (like copying the =
needed lines from the encyclopedia onto the Post-it). For conversation, mem=
ory management is like continuously updating that Post-it with the most per=
tinent facts from the chat so far. Summaries and retrieval act as our compr=
ession techniques.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,5=
5,55);line-height: 26px;font-size: 16px;"><span>One approach known in liter=
ature is </span><strong>Hierarchical Memory</strong><span>: maintain multip=
le levels of abstraction (immediate last utterances full text, then a summa=
ry of older stuff, etc.).</span></p><p style=3D"margin: 0 0 20px 0;color: r=
gb(54,55,55);line-height: 26px;font-size: 16px;">Don=E2=80=99t forget, the =
model=E2=80=99s own weights are a kind of long-term memory too =E2=80=93 th=
e base knowledge from pretraining. It =E2=80=9Cknows=E2=80=9D common sense =
and some general facts. RAG complements that with specific data. So when we=
 say short vs long-term memory in AI:</p><ul style=3D"margin-top: 0;padding=
: 0;"><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p sty=
le=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: b=
order-box;padding-left: 4px;font-size: 16px;margin: 0;">Long-term (in conte=
xt of RAG) =3D persistent knowledge base (could be documents, or conversati=
on history stored externally).</p></li><li style=3D"margin: 8px 0 0 32px;ms=
o-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26=
px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16p=
x;margin: 0;">Short-term =3D the active context window content.</p></li></u=
l><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;fon=
t-size: 16px;"><strong>DoorDash example extended:</strong><span> The bot re=
members entire history: customer says at turn 2, =E2=80=9CMy order #123 was=
 missing fries.=E2=80=9D At turn 15, they say =E2=80=9CI never got one item=
=E2=80=9D. The bot should recall =E2=80=9Cfries were missing=E2=80=9D =E2=
=80=93 ideally it does. If the conversation is long, that initial statement=
 might have dropped out of context. But if the bot stored that fact, it can=
 retrieve it. One can imagine a memory retrieval: user asks =E2=80=9CCan I =
get a refund for that missing item now?=E2=80=9D The bot=E2=80=99s system r=
etrieves earlier message about =E2=80=9Cmissing fries=E2=80=9D and then ans=
wers: =E2=80=9CYes, I see your fries were missing =E2=80=93 I=E2=80=99ve is=
sued a refund for those.=E2=80=9D This level of continuity is achievable wi=
th RAG-memory.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55=
);line-height: 26px;font-size: 16px;"><span>In summary, </span><strong>RAG =
isn=E2=80=99t only for external knowledge, it can be for the conversation i=
tself.</strong><span> Proper memory management (short-term by window, long-=
term by retrieval) ensures your AI doesn=E2=80=99t suffer dementia in long =
chats. The result: dialogues that feel coherent and contextually aware from=
 start to finish. And as context window sizes grow (hey GPT-4-32k, and Clau=
de=E2=80=99s 100k), we get more breathing room =E2=80=93 but good memory te=
chniques will </span><em>always</em><span> improve efficiency and capabilit=
y, especially as we push toward multi-hour or continuous conversations (thi=
nk AI personal assistants that chat with you over weeks).</span></p><p styl=
e=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 1=
6px;"><span>Next, how do we know our RAG system is actually </span><em>work=
ing</em><span> well? We need to </span><strong>evaluate and test</strong><s=
pan> it =E2=80=93 that=E2=80=99s our next section.</span></p><h2 class=3D"h=
eader-anchor-post" style=3D"position: relative;font-family: 'SF Pro Display=
',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe =
UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji',=
'Segoe UI Symbol';font-weight: bold;-webkit-font-smoothing: antialiased;-mo=
z-osx-font-smoothing: antialiased;-webkit-appearance: optimizelegibility;-m=
oz-appearance: optimizelegibility;appearance: optimizelegibility;margin: 1e=
m 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-size: 1.625em;"=
><strong>Evaluation and Testing: Measuring What Matters</strong></h2><p sty=
le=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: =
16px;">So you=E2=80=99ve built a RAG system. How do you know it=E2=80=99s g=
ood? We can=E2=80=99t just trust our gut or a few anecdotal successes =E2=
=80=93 we need solid evaluation. This section covers the key metrics that p=
redict success, how to create an evaluation dataset, and strategies like A/=
B testing and feedback loops to continuously improve your RAG.</p><p style=
=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16=
px;"><strong>The 4 metrics that predict success:</strong></p><ol style=3D"m=
argin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;"><p style=3D"c=
olor: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-b=
ox;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Relevance (Recall =
at K):</strong><span> Are we retrieving the right stuff? This is usually me=
asured by something like Recall@K =E2=80=93 the percentage of queries for w=
hich the relevant document is in the top K retrieved. If your knowledge bas=
e has ground-truth answers, you check if those were included. For example, =
if a user asks =E2=80=9CWhat=E2=80=99s the refund period?=E2=80=9D, and the=
 correct doc chunk about refunds was retrieved in top 3 results, that=E2=80=
=99s a hit. You want high recall so the needed info is almost always fetche=
d. If your retriever isn=E2=80=99t getting relevant content, the generator =
can=E2=80=99t answer correctly. Thus, </span><strong>relevance of retrieval=
</strong><span> is metric #1.</span></p></li><li style=3D"margin: 8px 0 0 3=
2px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;b=
ox-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong=
>Faithfulness (Groundedness):</strong><span> Is the answer actually based o=
n the retrieved sources, or is the model hallucinating extra details? An an=
swer is </span><em>faithful</em><span> if every claim it makes can be trace=
d to provided context. One way to measure: have human raters label answers =
as =E2=80=9Csupported by source vs. not=E2=80=9D. Or automatically, one can=
 check if answer sentences overlap enough with source text. Faithfulness is=
 crucial =E2=80=93 a RAG system that retrieves correctly but then the LLM i=
gnores it and makes something up fails the point. Metrics like </span><stro=
ng>=E2=80=9CSelf-Consistency=E2=80=9D</strong><span> or using a smaller mod=
el to verify facts can be used. In research, sometimes they measure the per=
centage of answers with at least one correct citation (as a proxy).</span><=
/p></li><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55=
);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4=
px;font-size: 16px;margin: 0;"><strong>Answer quality (Usefulness/Accuracy)=
:</strong><span> This is the end-to-end quality =E2=80=93 would a human jud=
ge the answer as correct, complete, and directly answering the question? Th=
is is a bit subjective, but you can operationalize it via a test set: e.g.,=
 100 questions with gold answers (could be written by experts or from an FA=
Q). Run the system and compare answers to gold =E2=80=93 measure accuracy o=
r use F1 if partial. Another angle: have human evaluators rate answers 1-5 =
on satisfaction. This metric is holistic =E2=80=93 it factors in retrieval =
and generation and readability.</span></p></li><li style=3D"margin: 8px 0 0=
 32px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0=
;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><stro=
ng>Latency:</strong><span> All the goodness above doesn=E2=80=99t matter if=
 it=E2=80=99s too slow for users. Latency is critical for real user experie=
nce. We often look at p90 or p95 latency (the 90th/95th percentile response=
 time) =E2=80=93 meaning the slowest typical responses. If p95 latency is, =
say, 4 seconds, that means 19 out of 20 responses come in under 4s, but 5% =
of queries take longer (maybe a big retrieval or an agent loop). Depending =
on your use case, you might target sub-1s for live chat, or maybe a few sec=
onds is acceptable for complex analysis. Either way, monitor it. There=E2=
=80=99s also cost (not a =E2=80=9Cuser metric=E2=80=9D but business metric)=
 =E2=80=93 if your RAG calls an expensive model, you measure cost/query. La=
tency and cost often trade off with using bigger models or doing reranking.=
</span></p></li></ol><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);li=
ne-height: 26px;font-size: 16px;"><strong>Other useful metrics</strong><spa=
n> include </span><strong>Precision</strong><span> of retrieval (are retrie=
ved docs actually relevant vs. bringing some irrelevant stuff), </span><str=
ong>Hallucination rate</strong><span> (inverse of faithfulness =E2=80=93 ho=
w often does it make unsupported claims), and </span><strong>Coverage</stro=
ng><span> (if a question has multiple points, did answer cover them all?). =
But to keep it simple, the four above capture major aspects: did we get the=
 info, did we ground the answer, was the answer good, and was it fast.</spa=
n></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px=
;font-size: 16px;"><span>Now, how to actually measure these? You need an </=
span><strong>evaluation dataset</strong><span>. Ideally, a set of sample qu=
estions (representative of what users will ask) along with ground-truth ref=
erences or expected answers. Often this is ~50=E2=80=93100 questions for a =
small system, but for robust eval maybe a few hundred. Building this =E2=80=
=9Cgold set=E2=80=9D is an investment, but hugely worth it. It=E2=80=99s yo=
ur yardstick.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55)=
;line-height: 26px;font-size: 16px;"><strong>Building your evaluation datas=
et =E2=80=93 the 100-question gold standard:</strong><span> Start by collec=
ting real queries if available (like search logs, customer questions). If n=
one exist, brainstorm likely questions or have domain experts generate them=
. Then for each question, determine what the correct answer or document is.=
 For instance, if question is =E2=80=9CHow many days do I have to return a =
product?=E2=80=9D, the gold reference might be =E2=80=9CReturns are accepte=
d within 30 days of purchase=E2=80=9D from ReturnPolicy.doc. If you can, ac=
tually have a human write the ideal answer (with sources noted). If not, at=
 least mark which document/section contains the answer. This way you can ev=
aluate retrieval (did it retrieve ReturnPolicy.doc?), and generation (did i=
t say 30 days?).</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,=
55);line-height: 26px;font-size: 16px;">It=E2=80=99s often helpful to inclu=
de some tricky cases: ambiguous questions, multi-part questions, etc., to s=
ee how system handles them. Once you have this dataset, run your RAG system=
 on it and measure:</p><ul style=3D"margin-top: 0;padding: 0;"><li style=3D=
"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(5=
4,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-=
left: 4px;font-size: 16px;margin: 0;">Retrieval recall: what % of answers h=
ad the correct doc in top 3 retrieved? (Use your gold references).</p></li>=
<li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"=
color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-=
box;padding-left: 4px;font-size: 16px;margin: 0;">Answer accuracy: compare =
the system=E2=80=99s answer to the gold answer (could be exact match for fa=
ctual, or BLEU score or a simple correctness check).</p></li><li style=3D"m=
argin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,=
55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-le=
ft: 4px;font-size: 16px;margin: 0;">If possible, manual review a subset to =
label hallucinations or irrelevant content.</p></li></ul><p style=3D"margin=
: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><stro=
ng>A/B Testing RAG:</strong><span> When you make improvements, you=E2=80=99=
ll want to A/B test to prove ROI. For example, you launch RAG bot version A=
 (maybe the old system was a static FAQ or a non-RAG model) vs. version B (=
with RAG). Define success metrics =E2=80=93 maybe deflection rate (question=
s answered without human agent), user satisfaction (collected via thumbs-up=
/thumbs-down), or simply accuracy on sample queries. Run both versions (e.g=
., route a portion of traffic to each) and compare. Say LinkedIn implemente=
d RAG and found support resolution time dropped 28.6% =E2=80=93 that=E2=80=
=99s a concrete ROI metric. To A/B test internally, you can simulate by spl=
itting your evaluation questions and answering them with old vs new system,=
 have judges blind-score which answers are better. If 9/10 times RAG=E2=80=
=99s answer is better, you have a winner.</span></p><p style=3D"margin: 0 0=
 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">A/B testin=
g is not just for accuracy, but also can test latency/cost trade-offs. E.g.=
, =E2=80=9CIs using GPT-4 (which is slower) giving significantly better ans=
wers than GPT-3.5 for our domain?=E2=80=9D You might run eval with both and=
 find maybe a slight quality uptick but double latency =E2=80=93 then you d=
ecide if that=E2=80=99s worth it. Only good evaluation data and testing wil=
l tell.</p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height:=
 26px;font-size: 16px;"><strong>The feedback loop: Using production data to=
 improve continuously.</strong><span> Once your system is live, you=E2=80=
=99ll get real interactions. Put hooks to capture useful signals:</span></p=
><ul style=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;=
mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: =
26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 1=
6px;margin: 0;">Let users rate answers (thumbs up/down or =E2=80=9CDid this=
 answer your question? Yes/No=E2=80=9D). This is invaluable supervised data=
. Every thumbs-down with the conversation and answer is a training example =
of what went wrong. Did retrieval fail or did the model hallucinate? You ca=
n label these and use them to refine either component (e.g., fine-tune the =
LLM to be more cautious, or improve indexing).</p></li><li style=3D"margin:=
 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55)=
;line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4p=
x;font-size: 16px;margin: 0;">Track what users do after the answer. If they=
 immediately rephrase the question or go browsing the document link, maybe =
the answer wasn=E2=80=99t satisfactory or complete. That can be an implicit=
 signal.</p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bull=
et;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;bo=
x-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">Log all =
queries that got =E2=80=9CI don=E2=80=99t know=E2=80=9D or low-confidence a=
nswers. Later, check if perhaps answers exist in your data but were missed.=
 If so, why missed? Maybe add that phrasing to document text or adjust embe=
dding parameters.</p></li></ul><p style=3D"margin: 0 0 20px 0;color: rgb(54=
,55,55);line-height: 26px;font-size: 16px;">Teams often set up a regular re=
view of failed cases. For instance, Notion=E2=80=99s team might have taken =
transcripts of cases their AI missed and then updated their index or prompt=
s accordingly. Over a few months, this iterative loop improved their RAG ac=
curacy from, say, 72% to 94% (hypothetical numbers, but plausible given ite=
rative improvements).</p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55=
);line-height: 26px;font-size: 16px;"><span>One case study: Notion (per rep=
orts) improved their answer accuracy dramatically by analyzing where the ch=
atbot failed to retrieve correct pages and adding better metadata and new t=
raining for those cases. They basically treated it like model fine-tuning, =
but in retrieval space =E2=80=93 adjusting data when the system was off. Th=
e result was an increase from mediocre performance to near expert-level ans=
wer quality over a quarter. The moral: </span><strong>systematic evaluation=
 + feedback loop =3D rapid improvement</strong><span>.</span></p><p style=
=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16=
px;">To implement feedback: use a tool like RAGAS (Retrieval-Augmented Gene=
ration Assessment System) or custom scripts to evaluate logs. RAGAS provide=
s metrics like answer accuracy and hallucination detection with LLM judges.=
 Some companies pipe conversation logs into an evaluation pipeline nightly =
=E2=80=93 e.g., run an LLM offline to score each answer for correctness whe=
n possible, to identify issues proactively.</p><p style=3D"margin: 0 0 20px=
 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>Finally, =
to </span><strong>prove ROI to stakeholders</strong><span>, tie these metri=
cs to business outcomes. E.g., after deploying RAG, support ticket volume d=
ropped by X%, customer satisfaction on help answers rose Y points. Or emplo=
yees find info 2x faster (maybe measure by a before/after experiment). Thes=
e concrete wins justify the investment and guide further funding.</span></p=
><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font=
-size: 16px;"><span>In summary, </span><strong>evaluate early, evaluate oft=
en</strong><span>. Use a diverse set of metrics: retrieval quality, answer =
faithfulness, user-level success. Build a gold test set of at least 100 Q&a=
mp;As =E2=80=93 it will act as your compass. Then continuously A/B test imp=
rovements and incorporate real user feedback. With this discipline, you=E2=
=80=99ll avoid falling into the trap of anecdotal =E2=80=9Cit seems to work=
=E2=80=9D and instead know </span><em>how well</em><span> it works and </sp=
an><em>where</em><span> to improve. That=E2=80=99s how you push from a dece=
nt prototype to a reliable production system. Next, let=E2=80=99s talk abou=
t scaling that system up to enterprise level =E2=80=93 where millions of do=
llars might be on the line.</span></p><h2 class=3D"header-anchor-post" styl=
e=3D"position: relative;font-family: 'SF Pro Display',-apple-system-headlin=
e,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Ar=
ial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-=
weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing: a=
ntialiased;-webkit-appearance: optimizelegibility;-moz-appearance: optimize=
legibility;appearance: optimizelegibility;margin: 1em 0 0.625em 0;color: rg=
b(54,55,55);line-height: 1.16em;font-size: 1.625em;"><strong>Enterprise RAG=
: When Millions Are on the Line</strong></h2><p style=3D"margin: 0 0 20px 0=
;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">Building a small =
RAG demo is one thing; deploying it across a Fortune 500 enterprise is anot=
her beast entirely. In an enterprise setting, stakes are high =E2=80=93 mis=
takes can cost millions or tarnish reputation. Let=E2=80=99s explore the ch=
allenges (and solutions) that arise when scaling RAG to enterprise level: h=
orror stories to avoid, how to handle millions of queries, keeping data sec=
ure/compliant, and optimizing costs.</p><p style=3D"margin: 0 0 20px 0;colo=
r: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>The =E2=80=9CF=
rankenstein RAG=E2=80=9D horror story (and how to avoid it):</strong><span>=
 Picture a patchwork of AI components: one team built a vector index, anoth=
er wired up a chatbot UI, a consultant added a custom reranker, and nobody =
thought through the overall architecture. The result? A monster that=E2=80=
=99s hard to maintain, with inconsistent answers and mysterious failures =
=E2=80=93 a </span><em>Frankenstein RAG</em><span>. One enterprise recounte=
d how their first attempt at an AI assistant integrated 5 different service=
s (some on-prem, some cloud) with brittle connections. It worked in demos, =
but under load it collapsed =E2=80=93 context dropouts, timeouts between co=
mponents, and absolutely </span><strong>no single source of truth</strong><=
span> for debugging. To avoid this, design an end-to-end architecture early=
. Decide: will you use an all-in-one solution (some vendors now offer full =
RAG platforms), or a carefully orchestrated pipeline? Document how data flo=
ws and where each piece lives. Ensure observability =E2=80=93 logs at each =
stage (retrieval logs, LLM input/output logs). Frankenstein systems often d=
ie because no one can tell which stitch ripped when it fails.</span></p><p =
style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-siz=
e: 16px;"><strong>Scaling from 10 to 10 million queries:</strong><span> Whe=
n your query volume grows, watch out for two bottlenecks: the vector databa=
se and the LLM API. Vector DBs, if self-hosted, might need sharding or upgr=
ades =E2=80=93 e.g., going from a single node to a cluster. Many solutions =
can scale to millions of vectors (Milvus, Elastic, etc., scale horizontally=
), but queries per second (QPS) is the real kicker. If you expect, say, 100=
 QPS, ensure your DB can handle that with &lt;50ms each. Sometimes that mea=
ns adding replicas (multiple instances serving the same index) to share que=
ry load. Meanwhile, LLM throughput might be a limit =E2=80=93 calling an AP=
I like OpenAI for each query might get expensive or slow. Enterprises often=
 implement caching: if the same question gets asked often, cache the answer=
. Or use a smaller local model for some queries. One advanced approach is a=
 </span><strong>cascading model</strong><span> deployment: try answering wi=
th a fine-tuned 7B model internally; if it=E2=80=99s confident, respond, if=
 not, fall back to GPT-4. This saved one company 30% of costs, for example.=
 Another scaling aspect is monitoring performance =E2=80=93 with millions o=
f queries, even a 99% accuracy means thousands of bad answers. Logging and =
automated alerts (like if accuracy on a sliding window of queries dips belo=
w X, flag it) can catch issues quickly.</span></p><p style=3D"margin: 0 0 2=
0px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>Secu=
rity deep-dive:</strong><span> Enterprises care deeply about data security =
and compliance (HIPAA for health data, SOC2, GDPR in EU, etc.). RAG systems=
 must be designed so that sensitive data doesn=E2=80=99t leak. Some conside=
rations:</span></p><ul style=3D"margin-top: 0;padding: 0;"><li style=3D"mar=
gin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55=
,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left=
: 4px;font-size: 16px;margin: 0;"><strong>Access controls:</strong><span> I=
f different users should only see certain data, the RAG needs to filter ret=
rieval by permissions. For example, an employee asking about HR policy can =
see internal policies, but a client using a chatbot should not retrieve tho=
se. That means integrating your vector store with an ACL (access control li=
st) =E2=80=93 for instance, include user roles in metadata and filter query=
 results accordingly.</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-=
special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px=
;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;=
margin: 0;"><strong>PII scrubbing:</strong><span> If logs might contain per=
sonal data (names, addresses), you should either avoid logging full text or=
 have a process to scrub them for analysis. Similarly, when feeding content=
 to external APIs (OpenAI, etc.), you might need to avoid sending truly sen=
sitive info unless you have agreements in place. Solutions include using on=
-prem LLMs for highly sensitive data, or at least encrypting certain fields=
.</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bull=
et;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;bo=
x-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>=
Retention and Right to be Forgotten (GDPR):</strong><span> If a user delete=
s their data, and that data was in the knowledge base, you have to remove i=
t from the index too. This means building a mechanism to </span><em>update =
or delete vectors</em><span>. Many vector DBs support deletion by ID =E2=80=
=93 so track which chunk IDs correspond to, say, a user=E2=80=99s data, and=
 be able to wipe them. Also re-chunk and re-index periodically for content =
updates.</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-special-forma=
t: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-botto=
m: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><=
strong>Auditability:</strong><span> Who asked what and got what answer? Ent=
erprises might need logs that show the chain: user query -&gt; docs retriev=
ed -&gt; answer. If an answer is challenged legally (=E2=80=9CYour AI gave =
wrong financial advice!=E2=80=9D), you need to reproduce what it saw and wh=
y it answered that way. Storing query and retrieval traces with timestamps =
is thus important.</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-spe=
cial-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;ma=
rgin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;mar=
gin: 0;"><strong>Model filtering:</strong><span> Use the model=E2=80=99s to=
ols too =E2=80=93 e.g., OpenAI has moderation APIs; you can run the final a=
nswer through that to ensure no disallowed content. If building your own mo=
del, have toxicity filters etc., in place. This prevents an malicious user =
from getting the AI to spill secrets or produce harassment by carefully poi=
soning a query.</span></p></li></ul><p style=3D"margin: 0 0 20px 0;color: r=
gb(54,55,55);line-height: 26px;font-size: 16px;"><span>A big reassurance: <=
/span><strong>RAG can actually help with compliance</strong><span> compared=
 to raw LLMs. Because the AI is constrained to use provided data, it=E2=80=
=99s less likely to wander into areas it shouldn=E2=80=99t. Also, you can e=
nsure that, say, medical AI only references approved medical literature =E2=
=80=93 reducing risk of non-compliant advice.</span></p><p style=3D"margin:=
 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><stron=
g>Cost optimization:</strong><span> RAG can reduce costs by answering with =
smaller models or fewer API calls, but it can also introduce its own costs =
(vector DB hosting, embedding generation, etc.). An interesting case: one c=
ompany was using GPT-4 for everything at maybe $0.06 per query. They realiz=
ed many queries are simple FAQs that GPT-3.5 or even a fine-tuned smaller m=
odel could handle. They implemented a two-tier system: attempt answer with =
GPT-3.5 (cost $0.002) along with retrieved context. Only if certain uncerta=
inty triggers are hit (like low similarity score or user follow-up suggests=
 dissatisfaction) do they escalate to GPT-4 for a refined answer or second =
attempt. This </span><strong>saved about 90% of their previous costs</stron=
g><span>, which over millions of queries was about $2M/year in savings, whi=
le keeping answer quality high (the trick was tuning the handoff threshold =
carefully). This is akin to a =E2=80=9Ccascading model deployment=E2=80=9D =
I mentioned.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);=
line-height: 26px;font-size: 16px;">Another tactic: optimize embeddings. Ca=
lling OpenAI=E2=80=99s embed API for each new doc chunk can add up. Some op=
en-source embedding models (like InstructorXL) can be run one-time to embed=
 all docs in-house, saving money at the expense of requiring some GPU compu=
te. For ongoing usage, ensure you only embed new/changed content, not re-em=
bed everything needlessly.</p><p style=3D"margin: 0 0 20px 0;color: rgb(54,=
55,55);line-height: 26px;font-size: 16px;"><strong>Case study: RBC revoluti=
onizing banking support with RAG.</strong><span> The Royal Bank of Canada (=
RBC) developed Arcane, an internal Retrieval-Augmented Generation (RAG) sys=
tem designed to help specialists quickly locate relevant investment policie=
s across the bank=E2=80=99s internal platforms. Arcane indexes policy docum=
ents and other semi-structured data, such as PDFs, HTML, and XML, and uses =
advanced embedding models to enable precise retrieval of information. This =
system significantly improves productivity by allowing specialists to find =
complex policy details in seconds, streamlining access to information that =
previously took years of experience to master. The development of Arcane in=
volved addressing challenges related to document parsing, context retention=
, and security, including robust privacy and safety measures to protect pro=
prietary financial information. The system=E2=80=99s success demonstrates h=
ow AI can enhance decision-making and operational efficiency in large finan=
cial institutions</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55=
,55);line-height: 26px;font-size: 16px;"><span>This illustrates how an ente=
rprise might integrate RAG </span><em>internally</em><span> first (for empl=
oyees). Many do that to mitigate risk vs. a public-facing bot. Then gradual=
ly, as confidence grows, they roll out to customers. Morgan Stanley, for in=
stance, built a GPT-4 RAG on their wealth management knowledge base (intern=
al) to assist advisors =E2=80=93 a similar idea of revolutionizing support =
with verified info.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,=
55,55);line-height: 26px;font-size: 16px;"><span>In enterprise, also think =
about </span><strong>fallbacks</strong><span>: If the AI isn=E2=80=99t conf=
ident, have a graceful handoff (e.g., =E2=80=9CI=E2=80=99m not sure, let me=
 connect you to a human.=E2=80=9D). Not every query should be answered by A=
I if it=E2=80=99s risky.</span></p><p style=3D"margin: 0 0 20px 0;color: rg=
b(54,55,55);line-height: 26px;font-size: 16px;"><span>Finally, consider tha=
t enterprise RAG is a </span><em>team sport</em><span>: involve IT for data=
 pipelines, involve legal for compliance, involve domain experts to curate =
content. It=E2=80=99s not just a lab experiment =E2=80=93 it touches many f=
acets of the business. With careful design, you=E2=80=99ll avoid the Franke=
nstein and instead get a robust, scalable, </span><strong>secure</strong><s=
pan> RAG deployment that your whole company trusts.</span></p><p style=3D"m=
argin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">=
Next up: we=E2=80=99ll compare RAG to another rising approach =E2=80=93 Age=
ntic search =E2=80=93 and discuss when to use which, and how they might con=
verge.</p><h2 class=3D"header-anchor-post" style=3D"position: relative;font=
-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,Bl=
inkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color =
Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-sm=
oothing: antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearanc=
e: optimizelegibility;-moz-appearance: optimizelegibility;appearance: optim=
izelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.1=
6em;font-size: 1.625em;"><strong>RAG vs Agentic Search: The $40 Billion Que=
stion</strong></h2><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line=
-height: 26px;font-size: 16px;"><span>There=E2=80=99s a hot debate in the A=
I world: should you use Retrieval-Augmented Generation (RAG) or an </span><=
strong>AI Agent</strong><span> that can search and reason on its own (often=
 called Agentic search, like AutoGPT or similar systems)? They have differe=
nt strengths. Let=E2=80=99s break down the fundamental difference, some per=
formance considerations (85% vs 95% accuracy type of trade-off), and a deci=
sion framework for when to use each =E2=80=93 and glimpse into a future whe=
re they merge.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55=
);line-height: 26px;font-size: 16px;"><strong>RAG retrieves and answers; Ag=
ents think, plan, then answer.</strong><span> In essence, a RAG system is l=
ike a smart lookup combined with an answer generator. It=E2=80=99s relative=
ly straightforward: given a query, it retrieves relevant info and produces =
an answer grounded in that info. An </span><strong>agentic approach</strong=
><span> (like the ReAct pattern or say a tool-using agent) will treat the q=
uery as a task it needs to solve possibly through multiple steps: it might =
search (multiple times even), analyze intermediate results, maybe use a cal=
culator or call an API, and finally give an answer. The agent has a kind of=
 mini-planner built in (often the LLM itself does the planning via prompts =
like =E2=80=9CThought: I should search X=E2=80=A6 Action: search=E2=80=A6 O=
bserving results=E2=80=A6 Thought: now I got this, final answer=E2=80=A6=E2=
=80=9D).</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line=
-height: 26px;font-size: 16px;">So difference: RAG =3D single retrieval rou=
nd then answer; Agent =3D could be multiple retrievals + reasoning steps. A=
gents shine when a query is complicated or requires combining info from dif=
ferent places. Example: =E2=80=9CCompare the growth of Apple vs Microsoft i=
n the last quarter and give me a trend=E2=80=9D =E2=80=93 an agent might do=
 two searches, get data for Apple, get data for Microsoft, maybe do a quick=
 calculation or summary, then answer. A single-shot RAG might struggle to g=
ather all that in one go (unless the info is conveniently in one doc).</p><=
p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-s=
ize: 16px;"><strong>Performance shootout: 85% vs 95% (but at what cost?).</=
strong><span> Let=E2=80=99s say on a certain set of complex questions, a ba=
sic RAG system gets ~85% accuracy =E2=80=93 it fails on multi-hop or when r=
easoning is needed across docs. An agentic approach (which can plan, do mul=
ti-hop retrievals, use a calculator etc.) might achieve 95% accuracy on tho=
se because it can do more steps. However, the cost is typically latency and=
 complexity: that 95% might come with, say, an average of 5 tool calls (sea=
rches or calculations), each adding latency. So maybe the agent=E2=80=99s a=
nswers take 10 seconds instead of 1 second. Also each step might call an AP=
I (cost), and it=E2=80=99s harder to guarantee what the agent will do (coul=
d go down a rabbit hole or get stuck).</span></p><p style=3D"margin: 0 0 20=
px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">There=E2=80=
=99s also a reliability factor =E2=80=93 RAG is relatively deterministic (r=
etrieve best match and answer), whereas agents have more moving parts that =
can go wrong (like choosing a wrong search query and never finding the righ=
t info, or looping). Some academic evaluations have noted that while agents=
 can theoretically solve more, they sometimes fail in unpredictable ways, m=
aking their overall reliability not clearly higher than a well-tuned RAG on=
 simpler tasks. Think of it like: RAG is a bicycle =E2=80=93 simple, robust=
; an Agent is a car =E2=80=93 can go further and faster, but more ways to b=
reak down.</p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-heig=
ht: 26px;font-size: 16px;"><strong>When to use each approach:</strong></p><=
ul style=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;ms=
o-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26=
px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16p=
x;margin: 0;"><span>Use </span><strong>RAG</strong><span> when your queries=
 are well-covered by existing knowledge bases and typically only need one r=
ound of retrieval. If the task is primarily Q&amp;A or straightforward deci=
sion support where the needed info is easily identified with keywords, RAG =
is efficient and less error-prone. RAG is also usually easier to implement =
and cheaper to run. For instance, a documentation chatbot or a legal assist=
ant that fetches relevant laws =E2=80=93 those work great with RAG because =
one query =3D find relevant clause =3D answer.</span></p></li><li style=3D"=
margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"margin: 0 0 2=
0px 0;color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: b=
order-box;padding-left: 4px;font-size: 16px;"><span>Use an </span><strong>A=
gentic approach</strong><span> when tasks are more complex, like those invo=
lving:</span></p><ul style=3D"margin-top: 0;padding: 0;"><li style=3D"margi=
n: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,5=
5);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: =
4px;font-size: 16px;margin: 0;"><strong>Multi-step reasoning:</strong><span=
> e.g., =E2=80=9CFirst find X, then use X to get Y=E2=80=9D.</span></p></li=
><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D=
"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border=
-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Tool use beyond =
text retrieval:</strong><span> e.g., need to call an API, do math, interact=
 with the environment. Agents can interface with calculators, databases, or=
 even execute code.</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-sp=
ecial-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;m=
argin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;ma=
rgin: 0;"><strong>Exploratory search:</strong><span> where the query isn=E2=
=80=99t well-defined. An agent can search iteratively, refining the query l=
ike a human researcher might.</span></p></li><li style=3D"margin: 8px 0 0 3=
2px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-heig=
ht: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-siz=
e: 16px;margin: 0;"><strong>Planning tasks:</strong><span> not just answeri=
ng a question, but deciding a sequence of actions (like booking travel give=
n constraints =E2=80=93 search flights, compare, etc.).</span></p></li></ul=
></li></ul><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height:=
 26px;font-size: 16px;">However, if you can achieve the goal with a simpler=
 RAG, do it =E2=80=93 because agents bring overhead.</p><p style=3D"margin:=
 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><stron=
g>Hybrid future =E2=80=93 Agentic RAG:</strong><span> It=E2=80=99s very lik=
ely that we won=E2=80=99t be choosing RAG vs Agents as mutually exclusive i=
n the future, but rather combining them. For example, you might have an age=
nt whose available =E2=80=9Ctools=E2=80=9D include a vector search (RAG) an=
d other APIs. It can then reason (=E2=80=9CTool: use knowledge_base_search =
for query X=E2=80=9D) to fetch something, then reason more, etc. This is al=
ready happening in some frameworks (LangChain agents often use a vector sea=
rch as one of the tools).</span></p><p style=3D"margin: 0 0 20px 0;color: r=
gb(54,55,55);line-height: 26px;font-size: 16px;"><span>2026 might indeed be=
 the year of </span><strong>Agentic RAG</strong><span>, where autonomous ag=
ents are augmented with retrieval. Anthropic=E2=80=99s prediction that AI m=
ight surpass Nobel laureates by 2026 probably assumes systems that can both=
 recall vast knowledge (via retrieval) and reason through novel problems (v=
ia planning).</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55)=
;line-height: 26px;font-size: 16px;"><span>We asked: RAG vs Agent =E2=80=93=
 it=E2=80=99s like asking screwdriver vs power drill. One is manual but sim=
ple (RAG), the other is powerful but requires more care and power (Agents).=
 For a $40B question (the forecast size of this AI orchestration market), t=
he answer is: </span><strong>both will be used, often together, depending o=
n the task complexity.</strong><span> If accuracy needs are ~85% and respon=
se must be instant, lean on RAG. If you need 95%+ and can afford some secon=
ds, incorporate agent capabilities.</span></p><p style=3D"margin: 0 0 20px =
0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">One more angle: =
agents often use RAG internally anyway. For example, an agent might decide =
to use a Bing search tool =E2=80=93 that=E2=80=99s basically retrieval, jus=
t unstructured. Some research combined ReAct (reasoning) with RAG and got e=
xcellent results =E2=80=93 e.g., the medical QA study where GPT-4 with RAG =
and some agentic steps hit 95% accuracy. Pure RAG was 94% with Llama 70B, i=
nterestingly, showing that a strong model with retrieval can nearly match a=
n agentic GPT-4 on that task. But the agentic GPT-4 edged it out to 95%.</p=
><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font=
-size: 16px;">So you see, differences can be subtle. It often comes down to=
 diminishing returns: maybe going from 85 to 95% requires quadruple the eff=
ort (multi-step, bigger model, etc.). If that last 10% is mission-critical =
(like diagnosing a patient correctly vs missing something), it=E2=80=99s wo=
rth it. If not, a simpler approach might suffice.</p><p style=3D"margin: 0 =
0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>D=
ecision framework in practice:</strong></p><ul style=3D"margin-top: 0;paddi=
ng: 0;"><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p s=
tyle=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing:=
 border-box;padding-left: 4px;font-size: 16px;margin: 0;">Start with RAG (s=
imple, one-shot). Evaluate performance on task.</p></li><li style=3D"margin=
: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55=
);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4=
px;font-size: 16px;margin: 0;">If you see frequent failures that are multi-=
hop in nature, consider adding an agent loop to handle those (maybe as a fa=
llback).</p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bull=
et;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;bo=
x-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">If numer=
ical accuracy is an issue, consider adding a calculation tool to the chain =
(agent style).</p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format=
: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom=
: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">Ev=
aluate again. Always weigh complexity vs gain.</p></li><li style=3D"margin:=
 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55)=
;line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4p=
x;font-size: 16px;margin: 0;">If domain requires using external services (l=
ike checking inventory, sending an email), you=E2=80=99ll need an agent sty=
le anyway (RAG alone can=E2=80=99t interact with environment).</p></li></ul=
><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font=
-size: 16px;">Finally, think of user experience: For a chat interface, an a=
gent taking 10 seconds might be okay if it=E2=80=99s a heavy question. For =
real-time search queries, 10s is too slow. So use RAG where immediacy matte=
rs.</p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26p=
x;font-size: 16px;"><strong>In 2026 and beyond</strong><span>, I predict we=
=E2=80=99ll see </span><strong>Agentic RAG systems</strong><span> becoming =
standard: LLMs that automatically retrieve info as needed and also perform =
multi-step reasoning. They won=E2=80=99t call it two separate things; it=E2=
=80=99ll just be =E2=80=9Cautonomous AI agents=E2=80=9D that have a knowled=
ge base backbone. And as mentioned in Future Shock, context windows going 1=
M+ will blur the lines =E2=80=93 if you can stuff an entire knowledge base =
in the context, is that RAG or just huge memory? It becomes a continuum.</s=
pan></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26=
px;font-size: 16px;">But until then, decide case by case. You have a powerf=
ul hammer in RAG and a Swiss Army tool in Agents. Use the right one for the=
 job =E2=80=93 or combine them for maximum effect. And keep an eye on that =
$40B market =E2=80=93 a lot of it will be won by those who figure out the o=
ptimal mix of retrieval and reasoning in their AI solutions.</p><p style=3D=
"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;=
"><span>Next, we venture into some </span><strong>advanced patterns</strong=
><span> =E2=80=93 the secret sauce that cutting-edge RAG practitioners are =
using to push towards 99% accuracy and beyond.</span></p><h1 class=3D"heade=
r-anchor-post" style=3D"position: relative;font-family: 'SF Pro Display',-a=
pple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',=
Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Seg=
oe UI Symbol';font-weight: bold;-webkit-font-smoothing: antialiased;-moz-os=
x-font-smoothing: antialiased;-webkit-appearance: optimizelegibility;-moz-a=
ppearance: optimizelegibility;appearance: optimizelegibility;margin: 1em 0 =
0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-size: 2em;"><strong=
>When RAG fails: Real-world limitations of retrieval-augmented generation</=
strong></h1><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height=
: 26px;font-size: 16px;"><span>Retrieval-Augmented Generation (RAG) faces s=
ignificant real-world limitations that have led companies to choose simpler=
 alternatives, with documented failures showing </span><strong>latency incr=
eases from 15 to 180 seconds under load</strong><span>, compliance barriers=
 in regulated industries preventing implementation entirely, and cost-benef=
it analyses revealing that fine-tuning can be more economical for stable, h=
igh-volume applications despite RAG's 36% lower annual costs in some scenar=
ios.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-hei=
ght: 26px;font-size: 16px;">While RAG promises to enhance large language mo=
dels with external knowledge, extensive research reveals critical failure m=
odes and inappropriate use cases. Academic studies document seven distinct =
failure points in RAG systems, from missing content to incomplete answers. =
Industry practitioners report RAG being &quot;brittle&quot; with &quot;no s=
cience to it&quot; after year-long implementations. Performance benchmarks =
show RAG doubles time-to-first-token latency from 495ms to 965ms, making it=
 unsuitable for voice applications requiring sub-150ms responses. In regula=
ted sectors, HIPAA compliance, attorney-client privilege, and SOC2 requirem=
ents create insurmountable barriers, forcing organizations like Morgan Stan=
ley to build custom on-premises solutions. Cost analyses reveal that while =
RAG offers lower upfront investment, operational expenses can exceed $10,00=
0 monthly for enterprise deployments, leading companies with stable knowled=
ge bases to choose fine-tuning despite higher initial costs of $100,000+ fo=
r small models.</p><div class=3D"captioned-image-container-static" style=3D=
"font-size: 16px;line-height: 26px;margin: 32px auto;"><figure style=3D"wid=
th: 100%;margin: 0 auto;"><table class=3D"image-wrapper" width=3D"100%" bor=
der=3D"0" cellspacing=3D"0" cellpadding=3D"0" data-component-name=3D"Image2=
ToDOMStatic" style=3D"mso-padding-alt: 1em 0 1.6em;"><tbody><tr><td style=
=3D"text-align: center;"></td><td class=3D"content" align=3D"left" width=3D=
"1456" style=3D"text-align: center;"><a class=3D"image-link" target=3D"_bla=
nk" href=3D"https://substack.com/redirect/18dcfae9-232e-49e1-80b0-0714079a5=
daa?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" re=
l=3D"" style=3D"position: relative;flex-direction: column;align-items: cent=
er;padding: 0;width: auto;height: auto;border: none;text-decoration: none;d=
isplay: block;margin: 0;"><img class=3D"wide-image" data-attrs=3D"{&quot;sr=
c&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9d=
ae4099-e9f0-4198-af7f-a28c92c1ea5f_2818x3840.png&quot;,&quot;srcNoWatermark=
&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;he=
ight&quot;:1984,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;b=
ytes&quot;:522854,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&qu=
ot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:tr=
ue,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://na=
tesnewsletter.substack.com/i/167307195?img=3Dhttps%3A%2F%2Fsubstack-post-me=
dia.s3.amazonaws.com%2Fpublic%2Fimages%2F9dae4099-e9f0-4198-af7f-a28c92c1ea=
5f_2818x3840.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:nul=
l,&quot;offset&quot;:false}" alt=3D"" width=3D"550" height=3D"749.450549450=
5495" src=3D"https://substackcdn.com/image/fetch/$s_!lPj5!,w_1100,c_limit,f=
_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3=
.amazonaws.com%2Fpublic%2Fimages%2F9dae4099-e9f0-4198-af7f-a28c92c1ea5f_281=
8x3840.png" style=3D"border: none !important;vertical-align: middle;display=
: block;-ms-interpolation-mode: bicubic;height: auto;margin-bottom: 0;width=
: auto !important;max-width: 100% !important;margin: 0 auto;"></a></td><td =
style=3D"text-align: center;"></td></tr></tbody></table></figure></div><h2 =
class=3D"header-anchor-post" style=3D"position: relative;font-family: 'SF P=
ro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFo=
nt,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe =
UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoothing: antia=
liased;-moz-osx-font-smoothing: antialiased;-webkit-appearance: optimizeleg=
ibility;-moz-appearance: optimizelegibility;appearance: optimizelegibility;=
margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-size:=
 1.625em;"><strong>The seven ways RAG systems fail in production</strong></=
h2><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;fo=
nt-size: 16px;"><span>Academic research from Scott Barnett and colleagues a=
t the 3rd International Conference on AI Engineering identified </span><str=
ong>seven distinct failure points</strong><span> through analysis of three =
case studies across research, education, and biomedical domains. Their stud=
y, using the BioASQ dataset with 15,000 documents and 1,000 Q&amp;A pairs, =
revealed systematic problems that plague RAG implementations:</span></p><ol=
 style=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;"><p=
 style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizin=
g: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Missing=
 content hallucination</strong><span> - Questions cannot be answered from a=
vailable documents, yet the system provides hallucinated responses instead =
of admitting uncertainty.</span></p></li><li style=3D"margin: 8px 0 0 32px;=
"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-s=
izing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Mis=
sed the top-K</strong><span> - Correct answers exist in the corpus but rank=
 too low in retrieval results, falling outside the top-K threshold and neve=
r reaching the language model.</span></p></li><li style=3D"margin: 8px 0 0 =
32px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;=
box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><stron=
g>Bundled up wrong</strong><span> - Retrieved documents containing answers =
get filtered out during the aggregation phase due to poor scoring mechanism=
s in consolidation strategies.</span></p></li><li style=3D"margin: 8px 0 0 =
32px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;=
box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><stron=
g>Extraction failure</strong><span> - When relevant content does reach the =
model, extraction failures occur because the LLM struggles with noisy or co=
nflicting information in the provided context.</span></p></li><li style=3D"=
margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;m=
argin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;ma=
rgin: 0;"><strong>Wrong format</strong><span> - Systems generate correct co=
ntent but fail to match expected structures, rendering the information unus=
able for downstream applications.</span></p></li><li style=3D"margin: 8px 0=
 0 32px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom:=
 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><st=
rong>Incorrect specificity</strong><span> - Responses suffer from incorrect=
 specificity levels, providing either overly general answers when precision=
 is needed or inappropriately detailed responses to broad questions.</span>=
</p></li><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,5=
5);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: =
4px;font-size: 16px;margin: 0;"><strong>Incomplete</strong><span> - Systems=
 often generate incomplete answers despite having all necessary information=
 in the retrieved context, suggesting fundamental issues with how RAG syste=
ms process and synthesize information.</span></p></li></ol><h3 class=3D"hea=
der-anchor-post" style=3D"position: relative;font-family: 'SF Pro Display',=
-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI=
',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','S=
egoe UI Symbol';font-weight: bold;-webkit-font-smoothing: antialiased;-moz-=
osx-font-smoothing: antialiased;-webkit-appearance: optimizelegibility;-moz=
-appearance: optimizelegibility;appearance: optimizelegibility;margin: 1em =
0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-size: 1.375em;"><=
strong>Real-time applications abandon RAG due to prohibitive latency</stron=
g></h3><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26p=
x;font-size: 16px;"><span>Performance benchmarks reveal RAG's fundamental i=
ncompatibility with real-time applications, where </span><strong>retrieval =
overhead accounts for 41-47% of total latency</strong><span>. Systems-level=
 characterization studies show RAG nearly doubles time-to-first-token laten=
cy from 495ms to 965ms compared to baseline language models, with P99 laten=
cy showing 50ms additional overhead for retrieval stages alone.</span></p><=
p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-s=
ize: 16px;">Voice applications face the most severe constraints, with ITU-T=
 standards recommending 100ms latency for interactive tasks and 150ms for c=
onversational use cases. Industry experts confirm &quot;achieving a one-way=
 latency of 150ms is almost impossible with a RAG-like architecture where s=
everal components are involved in voice processing.&quot; Even optimized sy=
stems struggle to achieve 500ms response times for voice-to-voice interacti=
ons, far exceeding user tolerance thresholds.</p><p style=3D"margin: 0 0 20=
px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>High-=
frequency trading systems represent the extreme end</strong><span> of laten=
cy sensitivity, requiring microsecond-level responses that make RAG archite=
ctures completely unsuitable. Financial firms report that &quot;millisecond=
s can make the difference between profit and loss,&quot; leading them to im=
plement custom FPGA-based solutions instead of any AI-based retrieval syste=
ms. Gaming applications similarly avoid RAG for real-time character interac=
tions, preferring traditional scripted responses that guarantee predictable=
 performance.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55)=
;line-height: 26px;font-size: 16px;">Under production loads, performance de=
gradation becomes catastrophic. Engineering teams report average execution =
time increasing from 15 seconds to 180 seconds when scaling from 5 to 50 co=
ncurrent users. Naive re-retrieval implementations can push end-to-end late=
ncy to nearly 30 seconds, &quot;precluding production deployment&quot; acco=
rding to academic studies. Even with aggressive optimization, including sem=
antic caching achieving 100ms response times for cache hits, the 30% cache =
hit rate assumption in real-world scenarios fails to meet the stringent req=
uirements of real-time applications.</p><h3 class=3D"header-anchor-post" st=
yle=3D"position: relative;font-family: 'SF Pro Display',-apple-system-headl=
ine,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,=
Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';fon=
t-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing:=
 antialiased;-webkit-appearance: optimizelegibility;-moz-appearance: optimi=
zelegibility;appearance: optimizelegibility;margin: 1em 0 0.625em 0;color: =
rgb(54,55,55);line-height: 1.16em;font-size: 1.375em;"><strong>Regulatory b=
arriers prevent RAG adoption in sensitive sectors</strong></h3><p style=3D"=
margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"=
><span>Healthcare organizations face insurmountable compliance challenges w=
hen implementing RAG systems, with </span><strong>HIPAA's Protected Health =
Information requirements</strong><span> creating fundamental conflicts with=
 vector database architectures. The core issue stems from embeddings potent=
ially being reverse-engineered to reveal original patient data, while most =
cloud-based RAG providers fail to offer HIPAA-compliant Business Associate =
Agreements.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);l=
ine-height: 26px;font-size: 16px;">The healthcare sector's struggles extend=
 beyond basic compliance. HIPAA's &quot;minimum necessary&quot; rule direct=
ly conflicts with RAG systems' need for extensive context to improve accura=
cy. Vector databases storing medical embeddings lack sufficient access cont=
rols for compliance, while third-party embedding providers like OpenAI and =
Cohere provide inadequate protections for PHI. Audit trail requirements for=
 healthcare data access prove difficult to maintain in distributed RAG arch=
itectures, forcing organizations to abandon implementations or build costly=
 custom solutions.</p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);l=
ine-height: 26px;font-size: 16px;"><strong>Legal firms encounter similar ba=
rriers</strong><span> with attorney-client privilege creating absolute requ=
irements for data confidentiality. The American Bar Association Model Rules=
 require lawyers to ensure non-lawyers, including AI systems, comply with p=
rofessional conduct rules. A mid-sized law firm with 150+ staff reported &q=
uot;significant challenges&quot; implementing GenAI, with only 25% of attor=
neys actively using AI tools due to compliance concerns. Major AmLaw100 fir=
ms have resorted to developing custom, on-premises RAG solutions rather tha=
n risk client data exposure through cloud services.</span></p><p style=3D"m=
argin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">=
Financial services face a complex web of regulations including SOC2 Type II=
 requirements demonstrating control effectiveness over time, which proves c=
hallenging with rapidly evolving RAG systems. One financial institution aba=
ndoned RAG implementation entirely, returning to relational databases after=
 discovering vector databases lacked required compliance controls. The comb=
ination of PCI DSS for payment data, Basel III for risk assessment, and SEC=
 requirements for explainable AI creates an environment where traditional d=
atabase architectures remain the only viable option for many use cases.</p>=
<h3 class=3D"header-anchor-post" style=3D"position: relative;font-family: '=
SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSyst=
emFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Se=
goe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoothing: a=
ntialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearance: optimiz=
elegibility;-moz-appearance: optimizelegibility;appearance: optimizelegibil=
ity;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-s=
ize: 1.375em;"><strong>Cost reality: When fine-tuning beats RAG economics</=
strong></h3><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height=
: 26px;font-size: 16px;"><span>Detailed cost analyses reveal RAG's economic=
 proposition varies dramatically based on use case and scale, with </span><=
strong>enterprise deployments reaching $10,000+ monthly</strong><span> for =
infrastructure alone. While RAG offers lower upfront investment compared to=
 fine-tuning's $100,000+ for small models, operational expenses quickly acc=
umulate through multiple cost centers.</span></p><p style=3D"margin: 0 0 20=
px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">RAG implement=
ation incurs ongoing charges of $0.10 per million tokens for embedding gene=
ration using OpenAI's ada-2 model, $120 monthly for vector database storage=
 of approximately 5 million tokens on Pinecone, and $500 monthly for standa=
rd AWS EC2 instances. High-volume production systems processing 10 million =
input tokens plus 3 million output tokens daily face $480 in API costs alon=
e. Hidden expenses multiply through network bandwidth at $0.09 per GB, moni=
toring at $0.30 per GB for logs, and data engineering at $100 per hour for =
maintenance.</p><div class=3D"captioned-image-container-static" style=3D"fo=
nt-size: 16px;line-height: 26px;margin: 32px auto;"><figure style=3D"width:=
 100%;margin: 0 auto;"><table class=3D"image-wrapper" width=3D"100%" border=
=3D"0" cellspacing=3D"0" cellpadding=3D"0" data-component-name=3D"Image2ToD=
OMStatic" style=3D"mso-padding-alt: 1em 0 1.6em;"><tbody><tr><td style=3D"t=
ext-align: center;"></td><td class=3D"content" align=3D"left" width=3D"1456=
" style=3D"text-align: center;"><a class=3D"image-link" target=3D"_blank" h=
ref=3D"https://substack.com/redirect/e40dbb40-ecdc-40d5-825d-13b6e3175a9d?j=
=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"=
" style=3D"position: relative;flex-direction: column;align-items: center;pa=
dding: 0;width: auto;height: auto;border: none;text-decoration: none;displa=
y: block;margin: 0;"><img class=3D"wide-image" data-attrs=3D"{&quot;src&quo=
t;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/297c2ea=
2-6bce-491a-a7fe-50d93d7cda48_3840x834.png&quot;,&quot;srcNoWatermark&quot;=
:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&q=
uot;:316,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&qu=
ot;:274690,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&qu=
ot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quo=
t;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://natesnews=
letter.substack.com/i/167307195?img=3Dhttps%3A%2F%2Fsubstack-post-media.s3.=
amazonaws.com%2Fpublic%2Fimages%2F297c2ea2-6bce-491a-a7fe-50d93d7cda48_3840=
x834.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;=
offset&quot;:false}" alt=3D"" width=3D"550" height=3D"119.36813186813187" s=
rc=3D"https://substackcdn.com/image/fetch/$s_!wvLh!,w_1100,c_limit,f_auto,q=
_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazon=
aws.com%2Fpublic%2Fimages%2F297c2ea2-6bce-491a-a7fe-50d93d7cda48_3840x834.p=
ng" style=3D"border: none !important;vertical-align: middle;display: block;=
-ms-interpolation-mode: bicubic;height: auto;margin-bottom: 0;width: auto !=
important;max-width: 100% !important;margin: 0 auto;"></a></td><td style=3D=
"text-align: center;"></td></tr></tbody></table><figcaption class=3D"image-=
caption" style=3D"box-sizing: content-box;color: #777777;font-size: 14px;li=
ne-height: 20px;font-weight: 400;letter-spacing: -.15px;margin-top: 8px;wid=
th: 70%;padding-left: 15%;padding-right: 15%;text-align: center;"><span>Zoo=
m in on this one in </span><a href=3D"https://substack.com/redirect/3b60e93=
5-b821-4872-9cc1-e16f463fca09?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgU=
C7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"text-decoration: underline;">Noti=
on</a></figcaption></figure></div><p style=3D"margin: 0 0 20px 0;color: rgb=
(54,55,55);line-height: 26px;font-size: 16px;"><strong>Fine-tuning presents=
 a contrasting cost structure</strong><span> with high initial investment b=
ut lower operational expenses. LLAMA 2 fine-tuning totaled $723 over a 15-d=
ay period processing 10 million tokens, compared to $1,186 for equivalent R=
AG usage. Once deployed, fine-tuned models eliminate per-query retrieval co=
sts and API dependencies, providing predictable pricing crucial for budgeti=
ng. Companies with stable knowledge bases updating yearly or less frequentl=
y find fine-tuning's one-time cost preferable to RAG's ongoing infrastructu=
re expenses.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);=
line-height: 26px;font-size: 16px;">Real-world implementations demonstrate =
these trade-offs clearly. An e-commerce company achieved 36% annual cost sa=
vings using RAG over traditional AI approaches through improved query effic=
iency, reaching break-even within three months. Conversely, organizations w=
ith high-volume, low-latency requirements report fine-tuned models deliveri=
ng better economics at scale due to eliminated retrieval overhead and lower=
 per-query costs. The decision ultimately depends on data update frequency,=
 query volume, latency requirements, and whether source citations justify R=
AG's additional complexity and cost.</p><h3 class=3D"header-anchor-post" st=
yle=3D"position: relative;font-family: 'SF Pro Display',-apple-system-headl=
ine,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,=
Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';fon=
t-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing:=
 antialiased;-webkit-appearance: optimizelegibility;-moz-appearance: optimi=
zelegibility;appearance: optimizelegibility;margin: 1em 0 0.625em 0;color: =
rgb(54,55,55);line-height: 1.16em;font-size: 1.375em;"><strong>The hidden s=
election bias in RAG success stories</strong></h3><p style=3D"margin: 0 0 2=
0px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>Perhap=
s the most revealing finding from extensive research across engineering blo=
gs, conference proceedings, and technical forums is </span><strong>the cons=
picuous absence of documented RAG rejection cases</strong><span>. Despite s=
earching major tech company engineering blogs from Uber, Airbnb, Netflix, a=
nd Spotify, along with Medium publications and GitHub discussions, research=
ers found virtually no public documentation of companies explicitly choosin=
g simpler solutions over RAG.</span></p><p style=3D"margin: 0 0 20px 0;colo=
r: rgb(54,55,55);line-height: 26px;font-size: 16px;">This silence speaks vo=
lumes about selection bias in the AI implementation landscape. Companies su=
ccessfully implementing RAG actively share their experiences through blog p=
osts and conference talks, while those encountering insurmountable challeng=
es either never attempt implementation after initial assessment or quietly =
abandon failed projects without public disclosure. The few practitioners wi=
lling to discuss failures anonymously report RAG being &quot;more of a prob=
lem than a solution&quot; after year-long implementations, citing brittlene=
ss and lack of systematic approaches to hyperparameter tuning.</p><p style=
=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16=
px;"><strong>Community discussions reveal widespread frustration</strong><s=
pan> hidden beneath the veneer of published success stories. Reddit's r/Mac=
hineLearning community documents common issues including performance degrad=
ation as document volumes increase, retrieved chunks getting &quot;pushed d=
own&quot; in rankings, and no sustainable solution for optimizing top-K thr=
esholds. HackerNews engineering discussions expose sequential processing bo=
ttlenecks in popular frameworks like LangChain, with developers discovering=
 double billing in ConversationalRetrievalChain due to automatic query reph=
rasing.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-=
height: 26px;font-size: 16px;">The absence of formal &quot;RAG rejection&qu=
ot; documentation suggests companies choosing simpler solutions frame decis=
ions as &quot;right-sizing&quot; rather than failure, avoiding potential em=
barrassment or competitive disadvantage from publicly acknowledging limitat=
ions. This creates a distorted view of RAG's applicability, where published=
 literature overwhelmingly features success stories while failures remain b=
uried in private Slack channels and closed-door engineering meetings. The A=
I community would benefit significantly from transparent sharing of impleme=
ntation decisions, including cases where prompt engineering or fine-tuning =
proved superior to complex retrieval systems.</p><h3 class=3D"header-anchor=
-post" style=3D"position: relative;font-family: 'SF Pro Display',-apple-sys=
tem-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,H=
elvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Sy=
mbol';font-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-s=
moothing: antialiased;-webkit-appearance: optimizelegibility;-moz-appearanc=
e: optimizelegibility;appearance: optimizelegibility;margin: 1em 0 0.625em =
0;color: rgb(54,55,55);line-height: 1.16em;font-size: 1.375em;"><strong>Wra=
pping up =E2=80=94 when not to use RAG</strong></h3><p style=3D"margin: 0 0=
 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">The docume=
nted failures, performance limitations, compliance barriers, and economic r=
ealities of RAG systems reveal a technology often misapplied to problems be=
tter solved through simpler approaches. While RAG excels for specific use c=
ases requiring fresh external knowledge and source citations, the seven fai=
lure points identified by academic research, latency overhead making real-t=
ime applications impossible, regulatory barriers in sensitive industries, a=
nd complex cost structures limiting economic viability demonstrate why many=
 organizations ultimately choose alternatives. The striking absence of publ=
ished RAG rejection stories suggests a broader industry challenge with tran=
sparent failure documentation, leaving practitioners to rediscover these li=
mitations through costly trial and error. Success with RAG requires careful=
 evaluation of whether its benefits justify the documented complexity, with=
 many finding that prompt engineering for simple queries, fine-tuning for s=
table domains, or traditional databases for regulated industries provide mo=
re practical solutions.</p><h2 class=3D"header-anchor-post" style=3D"positi=
on: relative;font-family: 'SF Pro Display',-apple-system-headline,system-ui=
,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-se=
rif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bol=
d;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing: antialiased;=
-webkit-appearance: optimizelegibility;-moz-appearance: optimizelegibility;=
appearance: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55)=
;line-height: 1.16em;font-size: 1.625em;"><strong>Advanced Patterns: The Se=
cret Sauce</strong></h2><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55)=
;line-height: 26px;font-size: 16px;"><span>By now, you know the fundamental=
s of RAG. Ready to go further down the rabbit hole? In this section, we=E2=
=80=99ll explore advanced patterns that can supercharge your RAG system =E2=
=80=93 the kind of techniques at the bleeding edge of research and industry=
 practice. Buckle up for </span><strong>GraphRAG</strong><span>, </span><st=
rong>Recursive RAG</strong><span>, </span><strong>Multi-modal mastery</stro=
ng><span>, </span><strong>MCP (Model-Context Protocol)</strong><span>, and =
</span><strong>Hybrid search deep-dive</strong><span>. These are the secret=
 sauce ingredients that can take accuracy from great to jaw-dropping, handl=
e complex data types, and make your AI an even smarter data hound. We=E2=80=
=99ll also include code snippets and simple diagrams to clarify some of the=
se concepts.</span></p><h3 class=3D"header-anchor-post" style=3D"position: =
relative;font-family: 'SF Pro Display',-apple-system-headline,system-ui,-ap=
ple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,=
'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-w=
ebkit-font-smoothing: antialiased;-moz-osx-font-smoothing: antialiased;-web=
kit-appearance: optimizelegibility;-moz-appearance: optimizelegibility;appe=
arance: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);lin=
e-height: 1.16em;font-size: 1.375em;"><strong>GraphRAG: Microsoft=E2=80=99s=
 90%+ Accuracy Breakthrough</strong></h3><p style=3D"margin: 0 0 20px 0;col=
or: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>What if your kn=
owledge base isn=E2=80=99t just unstructured text, but also relationships =
=E2=80=93 like a knowledge graph? Enter </span><strong>GraphRAG</strong><sp=
an>, an approach pioneered by Microsoft that blends knowledge graphs with R=
AG. In a GraphRAG, you use a structured graph of entities and their relatio=
ns to inform retrieval. For example, LinkedIn=E2=80=99s support RAG built a=
 graph of related support tickets (linking similar issues, or linking a tic=
ket to a product feature). The result: instead of retrieving isolated text =
chunks, the system could retrieve a whole subgraph of connected information=
. This preserved context and relationships that plain chunking would lose. =
LinkedIn reported a </span><strong>77.6% improvement in retrieval accuracy =
(MRR) and a 28.6% reduction in median resolution time</strong><span> after =
incorporating knowledge graph relations =E2=80=93 effectively reaching new =
heights in relevant results and efficiency.</span></p><p style=3D"margin: 0=
 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">Numbers =
reported vary but hover between 90-93%? Why this high? Because GraphRAG can=
 ensure that if a question involves multiple entities or steps, the graph g=
uides the retrieval to the right linked pieces. Microsoft Research showed d=
emos where GraphRAG answered broad analytical questions with astonishing pr=
ecision because it could traverse a graph of concepts rather than just do k=
eyword matching. Imagine asking =E2=80=9CHow are enzymes X and Y related in=
 disease Z?=E2=80=9D =E2=80=93 a GraphRAG system might have a bioscience kn=
owledge graph linking X -&gt; pathway -&gt; Y in context of Z, retrieve tha=
t subgraph, and give a highly accurate answer pulling those connections, wh=
ereas plain RAG might retrieve separate facts and risk missing the link.</p=
><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font=
-size: 16px;">How do you implement GraphRAG? Typically:</p><ul style=3D"mar=
gin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-special-forma=
t: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-botto=
m: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">B=
uild or integrate a knowledge graph (e.g., from existing database or by ent=
ity extraction from text).</p></li><li style=3D"margin: 8px 0 0 32px;mso-sp=
ecial-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;m=
argin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;ma=
rgin: 0;">When a query comes, identify key entities (you might use an NER m=
odel or a heuristic).</p></li><li style=3D"margin: 8px 0 0 32px;mso-special=
-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin=
-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin:=
 0;">Use the graph to find related entities or relevant nodes. This gives y=
ou a set of candidate nodes.</p></li><li style=3D"margin: 8px 0 0 32px;mso-=
special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px=
;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;=
margin: 0;">Retrieve text associated with those nodes (could be definitions=
 or documents connected to them).</p></li><li style=3D"margin: 8px 0 0 32px=
;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height:=
 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: =
16px;margin: 0;">Feed that into the LLM to generate answer.</p></li></ul><p=
 style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-si=
ze: 16px;"><span>It=E2=80=99s like giving the LLM a map of the knowledge la=
ndscape instead of just a list of documents. The </span><strong>wow moment<=
/strong><span> here: GraphRAG enabled near-perfect answers in domains like =
technical support by preserving relationships that text chunking lost. It a=
lso makes answers more explainable: since you have a graph, you can visuali=
ze the chain of reasoning (like =E2=80=9CIssue A is related to B, which cau=
ses C =E2=80=93 thus solution is =E2=80=A6=E2=80=9D).</span></p><p style=3D=
"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;=
">A quick code concept (not real, just illustrative):</p><p style=3D"margin=
: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"># Pse=
udo-code for GraphRAG retrieval</p><p style=3D"margin: 0 0 20px 0;color: rg=
b(54,55,55);line-height: 26px;font-size: 16px;">entities =3D entity_extract=
ion(query) # e.g., [&quot;enzyme X&quot;, &quot;enzyme Y&quot;, &quot;disea=
se Z&quot;]</p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-hei=
ght: 26px;font-size: 16px;">subgraph =3D graph.get_neighbors(entities) # ge=
t connected nodes and edges</p><p style=3D"margin: 0 0 20px 0;color: rgb(54=
,55,55);line-height: 26px;font-size: 16px;">related_docs =3D []</p><p style=
=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16=
px;">for node in subgraph.nodes:</p><p style=3D"margin: 0 0 20px 0;color: r=
gb(54,55,55);line-height: 26px;font-size: 16px;">related_docs +=3D text_ind=
ex.search(node.name) # find docs about that entity</p><p style=3D"margin: 0=
 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"># Now we=
 have related_docs from graph context</p><p style=3D"margin: 0 0 20px 0;col=
or: rgb(54,55,55);line-height: 26px;font-size: 16px;">answer =3D llm.genera=
te(query, context=3Dcombine(related_docs))</p><p style=3D"margin: 0 0 20px =
0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">This is simplifi=
ed =E2=80=93 a real one might use graph traversal algorithms and also embed=
 graph node descriptions. But it shows the idea.</p><p style=3D"margin: 0 0=
 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">GraphRAG i=
s especially useful in enterprise where you often have structured data (lik=
e product catalogs, org charts, etc.). Instead of flattening everything to =
text, use that structure to inform retrieval. Microsoft has an internal sys=
tem (=E2=80=9CProject Discovery=E2=80=9D) doing this =E2=80=93 results were=
 so good they integrated it into some of their products.</p><div class=3D"c=
aptioned-image-container-static" style=3D"font-size: 16px;line-height: 26px=
;margin: 32px auto;"><figure style=3D"width: 100%;margin: 0 auto;"><table c=
lass=3D"image-wrapper" width=3D"100%" border=3D"0" cellspacing=3D"0" cellpa=
dding=3D"0" data-component-name=3D"Image2ToDOMStatic" style=3D"mso-padding-=
alt: 1em 0 1.6em;"><tbody><tr><td style=3D"text-align: center;"></td><td cl=
ass=3D"content" align=3D"left" width=3D"1456" style=3D"text-align: center;"=
><a class=3D"image-link" target=3D"_blank" href=3D"https://substack.com/red=
irect/0b788be1-920f-4ab3-b166-0558353e08cd?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YF=
xPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"position: relative;f=
lex-direction: column;align-items: center;padding: 0;width: auto;height: au=
to;border: none;text-decoration: none;display: block;margin: 0;"><img class=
=3D"wide-image" data-attrs=3D"{&quot;src&quot;:&quot;https://substack-post-=
media.s3.amazonaws.com/public/images/f779549f-617e-4df7-80fe-9bed018d3898_3=
840x3724.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:n=
ull,&quot;imageSize&quot;:null,&quot;height&quot;:1412,&quot;width&quot;:14=
56,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:618423,&quot;alt&quot;:nu=
ll,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href=
&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;=
internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/1673071=
95?img=3Dhttps%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimag=
es%2Ff779549f-617e-4df7-80fe-9bed018d3898_3840x3724.png&quot;,&quot;isProce=
ssing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt=3D"=
" width=3D"550" height=3D"533.3791208791209" src=3D"https://substackcdn.com=
/image/fetch/$s_!X8Y2!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:ste=
ep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff=
779549f-617e-4df7-80fe-9bed018d3898_3840x3724.png" style=3D"border: none !i=
mportant;vertical-align: middle;display: block;-ms-interpolation-mode: bicu=
bic;height: auto;margin-bottom: 0;width: auto !important;max-width: 100% !i=
mportant;margin: 0 auto;"></a></td><td style=3D"text-align: center;"></td><=
/tr></tbody></table><figcaption class=3D"image-caption" style=3D"box-sizing=
: content-box;color: #777777;font-size: 14px;line-height: 20px;font-weight:=
 400;letter-spacing: -.15px;margin-top: 8px;width: 70%;padding-left: 15%;pa=
dding-right: 15%;text-align: center;"><span>Zoom in on the image in </span>=
<a href=3D"https://substack.com/redirect/3b60e935-b821-4872-9cc1-e16f463fca=
09?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=
=3D"" style=3D"text-decoration: underline;">Notion</a></figcaption></figure=
></div><h3 class=3D"header-anchor-post" style=3D"position: relative;font-fa=
mily: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,Blink=
MacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emo=
ji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoot=
hing: antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearance: =
optimizelegibility;-moz-appearance: optimizelegibility;appearance: optimize=
legibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em=
;font-size: 1.375em;"><strong>Recursive RAG: When One Retrieval Isn=E2=80=
=99t Enough</strong></h3><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55=
);line-height: 26px;font-size: 16px;"><span>Sometimes one round of retrieva=
l doesn=E2=80=99t cut it. Perhaps the initial query is high-level, and only=
 after reading an initial doc can the AI form a more precise follow-up quer=
y. </span><strong>Recursive RAG</strong><span> is about doing retrieval in =
multiple iterations. It=E2=80=99s like an agent, but specifically focusing =
on retrieval.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55)=
;line-height: 26px;font-size: 16px;">For example, a user asks: =E2=80=9CWha=
t were the key findings of the health inspector=E2=80=99s report for the re=
staurant I visited last night?=E2=80=9D =E2=80=93 The system might first re=
trieve something about who/when (maybe identify the restaurant and find a l=
ink to an inspector report). That document is lengthy and has codes. The AI=
 might then ask itself, =E2=80=9CHmm, what does the user really want? Proba=
bly summary of violations.=E2=80=9D It then formulates a sub-query: =E2=80=
=9CSummarize violations from Inspector Report ID 123.=E2=80=9D And retrieve=
s sections of the report about violations. Then it gives the final answer.<=
/p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;fo=
nt-size: 16px;">In practice, implementing recursive RAG could mean:</p><ul =
style=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-s=
pecial-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;=
margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;m=
argin: 0;">The LLM is prompted to decide if more info is needed. If yes, ha=
ve it output a refined query.</p></li><li style=3D"margin: 8px 0 0 32px;mso=
-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26p=
x;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px=
;margin: 0;">Use that query to retrieve again, then either answer or even l=
oop once more if needed.</p></li></ul><p style=3D"margin: 0 0 20px 0;color:=
 rgb(54,55,55);line-height: 26px;font-size: 16px;">This is basically an int=
ernal Q&amp;A: use RAG to answer parts of the question which feed into answ=
ering the main question. Tools like LlamaIndex have query transforms that c=
an do something akin to this (e.g., query an index, use result to query ano=
ther index).</p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-he=
ight: 26px;font-size: 16px;"><strong>Under the Hood example:</strong></p><p=
 style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-si=
ze: 16px;">User asks a question involving a chain of reasoning: =E2=80=9CIs=
 the device from Order #12345 still under warranty and how to claim it?=E2=
=80=9D The assistant might:</p><ol style=3D"margin-top: 0;padding: 0;"><li =
style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line-heigh=
t: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size=
: 16px;margin: 0;">Recognize it needs order details -&gt; retrieve Order #1=
2345 info (which includes device model and purchase date).</p></li><li styl=
e=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line-height: 2=
6px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16=
px;margin: 0;">From that info, figure out purchase date, then query warrant=
y policy for that device/model and date.</p></li><li style=3D"margin: 8px 0=
 0 32px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom:=
 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">Get=
 answer that warranty valid or not, plus procedure.</p></li><li style=3D"ma=
rgin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;mar=
gin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;marg=
in: 0;">Finally, compose answer.</p></li></ol><p style=3D"margin: 0 0 20px =
0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">This chain is a =
recursive retrieval: first get order data, then use that to get policy data=
. This could be done with an agent approach naturally. But if you constrain=
 it within RAG, you might pre-index different data types separately (orders=
 vs policies) and then orchestrate queries. Possibly an agent is easier her=
e, but sometimes domain-specific logic (like linking an order to a policy) =
can be hard-coded or handled with simple recursion.</p><p style=3D"margin: =
0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong=
>Benefits:</strong><span> This iterative retrieval can dramatically improve=
 accuracy on complex queries because it ensures the context the LLM gets is=
 highly relevant at each step. It also breaks a big problem into chunks whi=
ch is easier on the model (no need to jam everything in one huge context). =
The drawback is increased latency (multiple search calls).</span></p><p sty=
le=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: =
16px;"><strong>Real example =E2=80=93 Bing=E2=80=99s multi-hop QA</strong><=
span>: Bing (with GPT-4) often does this: it will search something, then fr=
om the results, search another related thing, etc., before answering =E2=80=
=93 effectively a recursive RAG with an agent.</span></p><p style=3D"margin=
: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><stro=
ng>Takeaway:</strong><span> If you find your RAG failing on questions that =
involve multiple pieces of info from disparate sources, consider a recursiv=
e retrieval strategy. It can be as straightforward as doing 2 passes: broad=
 retrieval for candidates, then specific retrieval focusing on one candidat=
e.</span></p><h3 class=3D"header-anchor-post" style=3D"position: relative;f=
ont-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system=
,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Col=
or Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font=
-smoothing: antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appear=
ance: optimizelegibility;-moz-appearance: optimizelegibility;appearance: op=
timizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: =
1.16em;font-size: 1.375em;"><strong>Multi-modal Mastery: Processing Invoice=
s, Diagrams, and Videos</strong></h3><p style=3D"margin: 0 0 20px 0;color: =
rgb(54,55,55);line-height: 26px;font-size: 16px;">Who says RAG is only for =
text? Multi-modal RAG is about bringing in images, audio, and beyond into t=
he retrieval-generation loop. Imagine processing an invoice PDF that contai=
ns a company logo (image), line items (table), and terms (text). A multi-mo=
dal RAG system could index the text AND perhaps an extracted table structur=
e or images of signatures, etc.</p><p style=3D"margin: 0 0 20px 0;color: rg=
b(54,55,55);line-height: 26px;font-size: 16px;">Use cases:</p><ul style=3D"=
margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-special-fo=
rmat: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bo=
ttom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;=
"><strong>Invoices and receipts:</strong><span> Use OCR to extract text (fo=
r RAG on text), but also possibly embed the image of the receipt for visual=
 details (like a handwritten note maybe).</span></p></li><li style=3D"margi=
n: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,5=
5);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: =
4px;font-size: 16px;margin: 0;"><strong>Diagrams:</strong><span> Suppose yo=
u have an architecture diagram image and an AI has to answer questions abou=
t system architecture. You could use an image embedding model to allow retr=
ieving the diagram image when relevant, and maybe generate a caption for it=
 as context.</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-special-f=
ormat: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-b=
ottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0=
;"><strong>Videos:</strong><span> A support knowledge base might include ho=
w-to videos. You can transcribe the audio (making it text, then index). For=
 an image (frame) that contains crucial info, you could use image captions =
or tags as metadata. If user asks =E2=80=9CWhere in the video do they menti=
on resetting the router?=E2=80=9D, you could even retrieve the timestamp vi=
a searching the transcript.</span></p></li></ul><p style=3D"margin: 0 0 20p=
x 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>There=E2=
=80=99s also the concept of </span><strong>multi-modal queries</strong><spa=
n> =E2=80=93 user might input an image and ask a question about it combined=
 with text. For example, =E2=80=9CIs this component (image) compatible with=
 the product described in spec X?=E2=80=9D A multi-modal RAG would need to =
identify what=E2=80=99s in the image (maybe with image recognition), find r=
elevant product info from text, then answer.</span></p><div class=3D"captio=
ned-image-container-static" style=3D"font-size: 16px;line-height: 26px;marg=
in: 32px auto;"><figure style=3D"width: 100%;margin: 0 auto;"><table class=
=3D"image-wrapper" width=3D"100%" border=3D"0" cellspacing=3D"0" cellpaddin=
g=3D"0" data-component-name=3D"Image2ToDOMStatic" style=3D"mso-padding-alt:=
 1em 0 1.6em;"><tbody><tr><td style=3D"text-align: center;"></td><td class=
=3D"content" align=3D"left" width=3D"1456" style=3D"text-align: center;"><a=
 class=3D"image-link" target=3D"_blank" href=3D"https://substack.com/redire=
ct/89950adc-2d08-49b0-bc9b-c215410bac53?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPm=
LQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"position: relative;flex=
-direction: column;align-items: center;padding: 0;width: auto;height: auto;=
border: none;text-decoration: none;display: block;margin: 0;"><img class=3D=
"wide-image" data-attrs=3D"{&quot;src&quot;:&quot;https://substack-post-med=
ia.s3.amazonaws.com/public/images/b2bac86e-9154-4336-9a2a-ae676c4c8dbe_2773=
x3840.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null=
,&quot;imageSize&quot;:null,&quot;height&quot;:2016,&quot;width&quot;:1456,=
&quot;resizeWidth&quot;:null,&quot;bytes&quot;:528154,&quot;alt&quot;:null,=
&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&qu=
ot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;int=
ernalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/167307195?=
img=3Dhttps%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%=
2Fb2bac86e-9154-4336-9a2a-ae676c4c8dbe_2773x3840.png&quot;,&quot;isProcessi=
ng&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt=3D"" w=
idth=3D"550" height=3D"761.5384615384615" src=3D"https://substackcdn.com/im=
age/fetch/$s_!77YG!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/=
https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2ba=
c86e-9154-4336-9a2a-ae676c4c8dbe_2773x3840.png" style=3D"border: none !impo=
rtant;vertical-align: middle;display: block;-ms-interpolation-mode: bicubic=
;height: auto;margin-bottom: 0;width: auto !important;max-width: 100% !impo=
rtant;margin: 0 auto;"></a></td><td style=3D"text-align: center;"></td></tr=
></tbody></table></figure></div><p style=3D"margin: 0 0 20px 0;color: rgb(5=
4,55,55);line-height: 26px;font-size: 16px;"><strong>How to implement:</str=
ong><span> Most vector DBs now support storing vectors of different modalit=
ies if they are same dimensionality. You could use OpenAI=E2=80=99s CLIP or=
 CLIP-like models to embed images, and store those vectors with metadata. I=
f user query is text about an image, you might not directly combine, but if=
 query includes an image, you embed the image and search the image index, e=
tc. Alternatively, you can convert images to text (e.g., =E2=80=9Cdiagram o=
f supply chain=E2=80=9D) via captioning and treat it as extra text doc.</sp=
an></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26p=
x;font-size: 16px;"><strong>Example code snippet:</strong></p><p style=3D"m=
argin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">=
# Pseudocode: Indexing images and text together</p><p style=3D"margin: 0 0 =
20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">image_embed=
dings =3D []</p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-he=
ight: 26px;font-size: 16px;">for img_path in image_files:</p><p style=3D"ma=
rgin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">v=
ec =3D image_embedder.embed_image(img_path)</p><p style=3D"margin: 0 0 20px=
 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">image_embedding=
s.append({&quot;vec&quot;: vec, &quot;metadata&quot;: {&quot;type&quot;: &q=
uot;image&quot;, &quot;file&quot;: img_path}})</p><p style=3D"margin: 0 0 2=
0px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"># assume tex=
t_docs is list of texts</p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,=
55);line-height: 26px;font-size: 16px;">text_embeddings =3D []</p><p style=
=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16=
px;">for doc in text_docs:</p><p style=3D"margin: 0 0 20px 0;color: rgb(54,=
55,55);line-height: 26px;font-size: 16px;">vec =3D text_embedder.embed_text=
(doc)</p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 2=
6px;font-size: 16px;">text_embeddings.append({&quot;vec&quot;: vec, &quot;m=
etadata&quot;: {&quot;type&quot;: &quot;text&quot;, &quot;content&quot;: do=
c[:100]}})</p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-heig=
ht: 26px;font-size: 16px;"># store both in one index</p><p style=3D"margin:=
 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">index =
=3D VectorIndex(image_embeddings + text_embeddings)</p><p style=3D"margin: =
0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">At quer=
y time, if query is text, you might search text index primarily but could a=
llow cross-modal search if appropriate (=E2=80=9Cdiagram=E2=80=9D in query =
might boost image search).</p><p style=3D"margin: 0 0 20px 0;color: rgb(54,=
55,55);line-height: 26px;font-size: 16px;"><strong>An inspiring case:</stro=
ng><span> A legal RAG system that processed millions of pages of documents =
and also diagrams (like patents often have drawings). They found that inclu=
ding the figure captions in the index and having the ability to retrieve th=
e figure image by caption reference improved user satisfaction =E2=80=93 at=
torneys could quickly get the relevant figure when asking about a specific =
concept in the patent.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(=
54,55,55);line-height: 26px;font-size: 16px;"><span>Another advanced patter=
n in multi-modal: </span><strong>Audio RAG</strong><span>. Suppose support =
calls are recorded =E2=80=93 you can transcribe them and use RAG on that so=
 your AI can recall what was said in a previous call (=E2=80=9CThe customer=
 said last week their internet was intermittent.=E2=80=9D and the AI uses t=
hat context this week).</span></p><p style=3D"margin: 0 0 20px 0;color: rgb=
(54,55,55);line-height: 26px;font-size: 16px;"><span>So, multi-modal master=
y is about expanding the knowledge sources beyond plain text, which can </s=
pan><strong>broaden the AI=E2=80=99s capabilities</strong><span>. It=E2=80=
=99s especially crucial as enterprises often have important info locked in =
PDFs with charts, or manuals with images, etc. The good news: the retrieval=
 part mostly stays the same =E2=80=93 it=E2=80=99s about extracting and emb=
edding those modalities appropriately.</span></p><h3 class=3D"header-anchor=
-post" style=3D"position: relative;font-family: 'SF Pro Display',-apple-sys=
tem-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,H=
elvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Sy=
mbol';font-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-s=
moothing: antialiased;-webkit-appearance: optimizelegibility;-moz-appearanc=
e: optimizelegibility;appearance: optimizelegibility;margin: 1em 0 0.625em =
0;color: rgb(54,55,55);line-height: 1.16em;font-size: 1.375em;"><strong>The=
 MCP Revolution: How Anthropic=E2=80=99s Protocol Changes Everything</stron=
g></h3><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26p=
x;font-size: 16px;"><span>Earlier we mentioned Anthropic=E2=80=99s </span><=
strong>Model-Context Protocol (MCP)</strong><span> . Think of MCP as a stan=
dardized way to connect an AI model to external data sources (web, database=
s, etc.) =E2=80=93 essentially a formalization of RAG and tool use. Anthrop=
ic dubs it the =E2=80=9CUSB-C for AI=E2=80=9D because it=E2=80=99s an open =
protocol aiming to make hooking up any data to any model plug-and-play.</sp=
an></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26p=
x;font-size: 16px;"><span>Why is this a big deal? Today, building RAG or ag=
ent systems is somewhat custom: you wire specific calls in code. MCP aims t=
o define a common interface: an AI can say (in a structured way) =E2=80=9CH=
ey, I need data from X=E2=80=9D, and any MCP-compatible data source can res=
pond. This two-way connection allows AI to maintain context across systems =
seamlessly . It also emphasizes </span><em>security and standardization</em=
><span> =E2=80=93 rather than each dev worrying about leaking data to the m=
odel, MCP will have guidelines and methods to safely transmit only what=E2=
=80=99s needed.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,5=
5);line-height: 26px;font-size: 16px;">Imagine a future where, instead of b=
uilding custom retrieval code, you simply point your AI at an MCP server wh=
ich exposes, say, your company=E2=80=99s Confluence wiki. The AI can then q=
uery it at will (with auth and all handled by MCP). It decouples the model =
from the data integration. Companies like Slack, Google Drive, databases, e=
tc., could all have MCP adaptors . As a developer, you then might write pro=
mpts that trigger these calls implicitly.</p><p style=3D"margin: 0 0 20px 0=
;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">For example:</p><=
p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-s=
ize: 16px;">User asks: =E2=80=9CWhat=E2=80=99s the latest sales figure for =
product X this quarter?=E2=80=9D Under the hood, an Anthropic Claude model =
might have an MCP client that knows how to fetch data from a connected data=
base or CSV of sales. The model=E2=80=99s prompt might include something li=
ke &lt;MCP: query sales_db for X Q3 sales&gt; and the server returns 42,000=
 units which the model then uses to answer. All standardized =E2=80=93 you =
don=E2=80=99t directly write that code; the model (via prompt engineering) =
does it.</p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height=
: 26px;font-size: 16px;">This changes RAG in that the retrieval step become=
s part of a broader context-sharing protocol. It=E2=80=99s not just search =
+ prompt; it=E2=80=99s a conversation between model and data sources, orche=
strated by this protocol. If widely adopted, it will accelerate building AI=
 apps =E2=80=93 no more bespoke integration for each dataset; just spin up =
an MCP server for your data and voila, AI can use it.</p><p style=3D"margin=
: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">From =
Anthropic=E2=80=99s announcements and what we saw:</p><ul style=3D"margin-t=
op: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-special-format: bu=
llet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;=
box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">MCP is=
 open-source, open standard.</p></li><li style=3D"margin: 8px 0 0 32px;mso-=
special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px=
;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;=
margin: 0;">Already integrated with Claude (Anthropic=E2=80=99s model) for =
some early partners (Block, etc., as mentioned).</p></li><li style=3D"margi=
n: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,5=
5);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: =
4px;font-size: 16px;margin: 0;">It supports things like real-time updates (=
so data can stream if needed) and presumably bidirectional (AI can write vi=
a MCP, not just read).</p></li></ul><p style=3D"margin: 0 0 20px 0;color: r=
gb(54,55,55);line-height: 26px;font-size: 16px;">In simpler terms: RAG curr=
ently often requires we bolt on a vector database and ask the model to read=
 those results. MCP could make that a native ability of models =E2=80=93 re=
trieving context on the fly as needed. It might even handle routing: if the=
 answer is in a vector DB or in a SQL DB or via an API, the AI doesn=E2=80=
=99t care =E2=80=93 it asks via MCP and the right connector answers.</p><p =
style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-siz=
e: 16px;"><span>This can </span><strong>change everything</strong><span> by=
 making </span><em>any</em><span> AI app easier to build and more reliable =
(since the integration is standardized, fewer errors). Also, for privacy, c=
ompanies can run MCP servers internally so the model only accesses allowed =
data and does so securely .</span></p><p style=3D"margin: 0 0 20px 0;color:=
 rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>So the MCP revolut=
ion is about </span><strong>universal AI-data connectivity</strong><span>. =
In the timeline of this guide, by 2025 it=E2=80=99s just starting. By 2026=
=E2=80=932027, it could be as ubiquitous as HTTP for web. If RAG is one app=
roach now, MCP might generalize that to all context (structured and unstruc=
tured) =E2=80=93 essentially fulfilling the promise of letting AI know </sp=
an><em>everything you want it to know, when it needs to know it</em><span>,=
 without retraining.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54=
,55,55);line-height: 26px;font-size: 16px;">For someone building RAG now, k=
eep an eye on MCP. If you adopt its pattern early, your system might easily=
 plug into future models supporting it. It=E2=80=99s the direction the indu=
stry=E2=80=99s moving for sure =E2=80=93 both OpenAI and Anthropic are eyei=
ng such protocols (OpenAI with plugins which is similar concept, Anthropic =
with MCP, etc.).</p><h3 class=3D"header-anchor-post" style=3D"position: rel=
ative;font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple=
-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Ap=
ple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webk=
it-font-smoothing: antialiased;-moz-osx-font-smoothing: antialiased;-webkit=
-appearance: optimizelegibility;-moz-appearance: optimizelegibility;appeara=
nce: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-h=
eight: 1.16em;font-size: 1.375em;"><strong>Hybrid Search Deep-Dive: Best of=
 Both Worlds</strong></h3><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,5=
5);line-height: 26px;font-size: 16px;"><span>We touched on hybrid search ea=
rlier, but let=E2=80=99s go deep: combining </span><strong>BM25 (sparse key=
word search)</strong><span> with </span><strong>vector search</strong><span=
> can significantly boost performance, especially in domains with technical=
 terms, proper nouns, or when you have to ensure no relevant doc is missed.=
</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height:=
 26px;font-size: 16px;"><strong>Recap:</strong><span> Vector search finds s=
emantic matches; BM25 finds lexical matches. They often retrieve overlappin=
g but not identical sets of results . For example, query =E2=80=9CCOVID tra=
nsmission aerosol study=E2=80=9D:</span></p><ul style=3D"margin-top: 0;padd=
ing: 0;"><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p =
style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing=
: border-box;padding-left: 4px;font-size: 16px;margin: 0;">Vector search mi=
ght find a research paper discussing airborne transmission in general (even=
 if it doesn=E2=80=99t have exact word =E2=80=9Caerosol=E2=80=9D), because =
semantically it=E2=80=99s similar.</p></li><li style=3D"margin: 8px 0 0 32p=
x;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height=
: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size:=
 16px;margin: 0;">BM25 might find a specific document that has the exact ph=
rase =E2=80=9Caerosol transmission of COVID=E2=80=9D even if that doc is ot=
herwise small or not well-written (so embedding might not rank it as high).=
</p></li></ul><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-heig=
ht: 26px;font-size: 16px;">By doing both and merging, you get broader cover=
age.</p><div class=3D"captioned-image-container-static" style=3D"font-size:=
 16px;line-height: 26px;margin: 32px auto;"><figure style=3D"width: 100%;ma=
rgin: 0 auto;"><table class=3D"image-wrapper" width=3D"100%" border=3D"0" c=
ellspacing=3D"0" cellpadding=3D"0" data-component-name=3D"Image2ToDOMStatic=
" style=3D"mso-padding-alt: 1em 0 1.6em;"><tbody><tr><td style=3D"text-alig=
n: center;"></td><td class=3D"content" align=3D"left" width=3D"1456" style=
=3D"text-align: center;"><a class=3D"image-link" target=3D"_blank" href=3D"=
https://substack.com/redirect/5192b308-9492-4da6-8be4-7e8c37c77f65?j=3DeyJ1=
IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=
=3D"position: relative;flex-direction: column;align-items: center;padding: =
0;width: auto;height: auto;border: none;text-decoration: none;display: bloc=
k;margin: 0;"><img class=3D"wide-image" data-attrs=3D"{&quot;src&quot;:&quo=
t;https://substack-post-media.s3.amazonaws.com/public/images/8abec03c-1182-=
4ffc-892f-a3d819400366_3840x913.png&quot;,&quot;srcNoWatermark&quot;:null,&=
quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:34=
6,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:378=
700,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;imag=
e/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topIm=
age&quot;:false,&quot;internalRedirect&quot;:&quot;https://natesnewsletter.=
substack.com/i/167307195?img=3Dhttps%3A%2F%2Fsubstack-post-media.s3.amazona=
ws.com%2Fpublic%2Fimages%2F8abec03c-1182-4ffc-892f-a3d819400366_3840x913.pn=
g&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&=
quot;:false}" alt=3D"" width=3D"550" height=3D"130.70054945054946" src=3D"h=
ttps://substackcdn.com/image/fetch/$s_!KgG_!,w_1100,c_limit,f_auto,q_auto:g=
ood,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com=
%2Fpublic%2Fimages%2F8abec03c-1182-4ffc-892f-a3d819400366_3840x913.png" sty=
le=3D"border: none !important;vertical-align: middle;display: block;-ms-int=
erpolation-mode: bicubic;height: auto;margin-bottom: 0;width: auto !importa=
nt;max-width: 100% !important;margin: 0 auto;"></a></td><td style=3D"text-a=
lign: center;"></td></tr></tbody></table><figcaption class=3D"image-caption=
" style=3D"box-sizing: content-box;color: #777777;font-size: 14px;line-heig=
ht: 20px;font-weight: 400;letter-spacing: -.15px;margin-top: 8px;width: 70%=
;padding-left: 15%;padding-right: 15%;text-align: center;"><span>Zoom in on=
 this delicious eye-chart in </span><a href=3D"https://substack.com/redirec=
t/3b60e935-b821-4872-9cc1-e16f463fca09?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmL=
QJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"text-decoration: underli=
ne;">Notion</a></figcaption></figure></div><p style=3D"margin: 0 0 20px 0;c=
olor: rgb(54,55,55);line-height: 26px;font-size: 16px;">There are a few way=
s to do hybrid:</p><ol style=3D"margin-top: 0;padding: 0;"><li style=3D"mar=
gin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;marg=
in-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margi=
n: 0;"><strong>Score fusion:</strong><span> e.g., normalized BM25 score + a=
lpha * vector similarity score, then rank. This requires tuning that alpha.=
 If alpha=3D0, purely BM25; if high, more weight to semantic.</span></p></l=
i><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line=
-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;fon=
t-size: 16px;margin: 0;"><strong>RRF (Reciprocal Rank Fusion)</strong><span=
> : doesn=E2=80=99t need heavy tuning. It takes the rank positions from eac=
h method and combines such that if either method ranks a doc high, it gets =
boosted.</span></p></li><li style=3D"margin: 8px 0 0 32px;"><p style=3D"col=
or: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box=
;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Two-stage retrieval:=
</strong><span> e.g., use BM25 to filter a large set to, say, 1000 candidat=
es, then use vector similarity to get top 5 from those. Or vice-versa (vect=
or first, then within those apply BM25 to refine).</span></p></li><li style=
=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line-height: 26=
px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16p=
x;margin: 0;"><strong>Index combination:</strong><span> Some vector DBs (We=
aviate, Pinecone, Qdrant with =E2=80=9Chybrid search=E2=80=9D features) all=
ow adding a sparse vector to the dense vector for each doc and performing a=
 single combined similarity search. For example, Qdrant introduced a =E2=80=
=9Cpayload boost=E2=80=9D algorithm that basically accounts for keyword ove=
rlap (BM25-like) along with vector distance .</span></p></li></ol><p style=
=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16=
px;"><span>When implementing, you=E2=80=99ll need a </span><strong>sparse i=
ndex</strong><span> of some kind. This could be Elasticsearch or just Lucen=
e. If you already use something like OpenSearch, you can actually index vec=
tors there too and do a combined query. Or use two systems side by side.</s=
pan></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26=
px;font-size: 16px;"><strong>Visual analogy:</strong><span> Think of vector=
 search as a wide net that catches things by meaning, and BM25 as a spear t=
hat precisely hits documents with matching keywords. Hybrid means you fish =
with a net and spear simultaneously =E2=80=93 you won=E2=80=99t miss much.<=
/span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: =
26px;font-size: 16px;"><strong>Why 2025 loves hybrid:</strong><span> Becaus=
e many users complain pure vector search sometimes gives =E2=80=9Cfuzzy=E2=
=80=9D results that are on topic but don=E2=80=99t contain the answer, wher=
eas keyword search might directly find a doc with the exact answer text (bu=
t might miss synonyms). Combining them often yields an answer in top 1-2 re=
sults that either method alone would have as, say, result 5 or 6.</span></p=
><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font=
-size: 16px;">A concrete example from our experiences: Searching a database=
 of DevOps incidents for =E2=80=9CDNS error EAI_AGAIN solution=E2=80=9D.</p=
><ul style=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;=
mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: =
26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 1=
6px;margin: 0;">BM25 finds maybe a post containing =E2=80=9CEAI_AGAIN=E2=80=
=9D exactly.</p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: =
bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: =
0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><spa=
n>Vector finds a troubleshooting guide that doesn=E2=80=99t mention that co=
de but talks about =E2=80=9Cnetwork DNS resolution issues=E2=80=9D.</span><=
br><br><span> The actual best answer was a forum thread that had both the c=
ode and general discussion. Hybrid brought that thread to rank 1, whereas B=
M25 had it rank 3 (below some code snippet page), and vector had it rank 4.=
</span></p></li></ul><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);li=
ne-height: 26px;font-size: 16px;"><strong>RAG usage:</strong><span> In the =
retrieval step of RAG, you can implement hybrid by:</span></p><p style=3D"m=
argin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">=
bm25_results =3D bm25_index.search(query, k=3D10)</p><p style=3D"margin: 0 =
0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">vec_resul=
ts =3D vector_index.search(query_vec, k=3D10)</p><p style=3D"margin: 0 0 20=
px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">final_results=
 =3D fuse(bm25_results, vec_results)</p><p style=3D"margin: 0 0 20px 0;colo=
r: rgb(54,55,55);line-height: 26px;font-size: 16px;">Then pass final_result=
s (top few) to LLM.</p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);=
line-height: 26px;font-size: 16px;"><strong>Reranking revolution</strong><s=
pan> (I=E2=80=99ll tie back here): After hybrid, you might still apply an L=
LM-based reranker (like feed query + snippet to a smaller cross-attention m=
odel to judge relevance). In effect, hybrid gave you a better set of 10 can=
didates, and reranker picks the best 3. This stacking can yield extremely h=
igh precision =E2=80=93 as high as 95%+ relevant on first chunk for well-fo=
rmed queries. Microsoft noted something similar in their Superlinked articl=
e: hybrid + semantic rerank gave big gains .</span></p><p style=3D"margin: =
0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>A=
t the cost of some complexity, you get the </span><strong>best of both worl=
ds</strong><span>. Most state-of-the-art QA systems do use a hybrid approac=
h under the hood now. Even OpenAI=E2=80=99s WebGPT (older) combined informa=
tion retrieval with searches that effectively were sparse lookups (search q=
ueries) and then reading pages.</span></p><p style=3D"margin: 0 0 20px 0;co=
lor: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>In summary: </=
span><em>Don=E2=80=99t pick sides in the dense vs sparse debate; use both.<=
/em><span> It=E2=80=99s often not either-or. If you incorporate hybrid sear=
ch in your RAG, you=E2=80=99ll likely see fewer missed answers and more rob=
ust performance, especially on queries with rare terms (product codes, erro=
r IDs, names) and queries that need conceptual match.</span></p><div style=
=3D"font-size: 16px;line-height: 26px;"><hr style=3D"margin: 32px 0;padding=
: 0;height: 1px;background: #e6e6e6;border: none;"></div><p style=3D"margin=
: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">These=
 advanced patterns =E2=80=93 GraphRAG, recursive retrieval, multi-modality,=
 MCP integration, and hybrid search =E2=80=93 are like tools in the expert =
chef=E2=80=99s kitchen. You don=E2=80=99t always need all of them, but know=
ing they exist and when to apply them can elevate your RAG system to gourme=
t level. They represent how the field is pushing towards higher accuracy, b=
roader capability, and easier integration.</p><p style=3D"margin: 0 0 20px =
0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>As we near=
 the end of our epic guide, let=E2=80=99s ensure we also learn from the pit=
falls others have faced, and then we=E2=80=99ll gaze into the future (2025=
=E2=80=932027) to see what=E2=80=99s coming (spoiler: </span><em>autonomous=
 agents and massive context windows</em><span>). But first =E2=80=93 the pi=
tfall graveyard, to avoid ending up there.</span></p><h2 class=3D"header-an=
chor-post" style=3D"position: relative;font-family: 'SF Pro Display',-apple=
-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Robo=
to,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe U=
I Symbol';font-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-fo=
nt-smoothing: antialiased;-webkit-appearance: optimizelegibility;-moz-appea=
rance: optimizelegibility;appearance: optimizelegibility;margin: 1em 0 0.62=
5em 0;color: rgb(54,55,55);line-height: 1.16em;font-size: 1.625em;"><strong=
>The Pitfall Graveyard</strong></h2><p style=3D"margin: 0 0 20px 0;color: r=
gb(54,55,55);line-height: 26px;font-size: 16px;"><span>Even seasoned practi=
tioners have horror stories of RAG projects that went awry. In this section=
, we=E2=80=99ll visit the </span><strong>7 ways RAG projects die</strong><s=
pan> (so you can avoid each), from chunking disasters to embedding mismatch=
es. We=E2=80=99ll shine light on the infamous </span><em>=E2=80=9Clost in t=
he middle=E2=80=9D</em><span> problem, share hallucination horror stories a=
nd how to prevent them, warn about a $500K mistake a startup made with a wr=
ong vector DB choice, and dissect the newest pitfall: the </span><strong>em=
bedding model mismatch</strong><span> fiasco. Consider this a tour of the g=
raveyard so your project doesn=E2=80=99t end up buried here.</span></p><p s=
tyle=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size=
: 16px;"><strong>1. Chunking Gone Wrong (The Context Shredder):</strong><sp=
an> We=E2=80=99ve harped on chunking because it=E2=80=99s that important. M=
any RAG projects die early because the team didn=E2=80=99t respect context =
boundaries =E2=80=93 they arbitrarily chopped docs into pieces that made no=
 sense. The result: retrieval fetched chunks that were irrelevant or incomp=
lete, leading the LLM to give wrong answers. One startup had a legal QA sys=
tem that kept failing to cite the correct clause. Post-mortem found they ch=
unked contracts by fixed 1000-character windows, often splitting clauses in=
 half. The answer chunk would have =E2=80=9C=E2=80=A6except as provided in =
Section 5(b)=E2=80=9D but Section 5(b) was in the </span><em>next</em><span=
> chunk, which wasn=E2=80=99t retrieved =E2=80=93 ouch. This killed user tr=
ust. The fix: re-chunk by clause or paragraph, and allow overlaps. Lesson: =
</span><strong>Don=E2=80=99t kill context coherence.</strong><span> If your=
 logs show queries retrieving chunks that seem off, inspect if chunking is =
to blame.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);lin=
e-height: 26px;font-size: 16px;"><strong>2. The =E2=80=9CLost in the Middle=
=E2=80=9D Problem:</strong><span> This one=E2=80=99s sneaky =E2=80=93 even =
if you chunk well and retrieve the right chunk, the answer might be in the =
</span><em>middle of a long chunk</em><span> and the model might overlook o=
r summarize incorrectly. LLMs (especially transformer models) have known bi=
ases: they pay a bit more attention to the start and end of their input, so=
metimes less to the middle . If the crucial detail is buried in the middle =
of a 500-word chunk, there=E2=80=99s a chance the model misses it or =E2=80=
=9Challucinates=E2=80=9D around it. A story: an AI assistant was reading a =
product manual chunk that listed limitations in the middle. The user asked =
about a limitation; the relevant text was there but mid-chunk, so the model=
, perhaps pattern-matching, gave a generic answer missing the specific deta=
il (which was in lines it didn=E2=80=99t focus on). Users caught that it wa=
sn=E2=80=99t specific. The solution can be to chunk smaller or highlight th=
e answer if you can pre-process. One trick: when retrieving, you might bold=
 or mark the exact sentence in the prompt (some do &lt;highlight&gt;sentenc=
e about limitation&lt;/highlight&gt; around it). Or use an extractive model=
 to pull the sentence out as an answer candidate. Either way, be aware that=
 </span><em>long chunks can dilute focus</em><span>. Ensure key info isn=E2=
=80=99t lost in the middle.</span></p><p style=3D"margin: 0 0 20px 0;color:=
 rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>3. Hallucination=
 Horrors:</strong><span> Perhaps the scariest pitfall =E2=80=93 your AI sou=
nds confident but is spewing nonsense not supported by any doc. Hallucinati=
ons in RAG usually happen when retrieval fails (no good info) but the model=
 feels it must answer anyway, or when it tries to stitch together partial i=
nfo and fills gaps with guesswork. One prevention is always instruct the mo=
del to say =E2=80=9CI don=E2=80=99t know=E2=80=9D or something if unsure, b=
ut models sometimes ignore that if they think they can =E2=80=9Cbe helpful.=
=E2=80=9D Real tale: a customer asked a RAG bot about a policy that didn=E2=
=80=99t exist in the docs (it was a trick question). The bot confidently fa=
bricated a policy clause, complete with a fake quote and citation to a docu=
ment =E2=80=93 which freaked out the legal team. They nearly scrapped the p=
roject thinking it could create legal liabilities. The fix was multi-pronge=
d: (a) improve retrieval so at least a relevant doc is found or if none, a =
flag is set; (b) add a final check where the model=E2=80=99s answer is comp=
ared against sources =E2=80=93 if low overlap, replace answer with a =E2=80=
=9Ccannot find info=E2=80=9D response (some use an LLM judge for this, or a=
 simpler heuristic like =E2=80=9Cif answer has facts not in retrieved text,=
 then caution=E2=80=9D). Hallucinations can kill a project=E2=80=99s credib=
ility in one stroke, so </span><strong>have guardrails:</strong><span> eith=
er clear refusals for unknown answers or at least an apologetic =E2=80=9CI=
=E2=80=99m not certain=E2=80=9D rather than confident lies.</span></p><p st=
yle=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size:=
 16px;"><strong>4. Misusing the Wrong Vector Database (the $500K Mistake):<=
/strong><span> Choosing tech without due diligence can be costly. Imagine s=
pending months and $$$ on a vector DB that promises enterprise scale, only =
to discover at scale it has a memory leak or it doesn=E2=80=99t support you=
r needed feature. One startup spent over </span><strong>$500K</strong><span=
> on licensing and deploying a certain vector search solution that was hype=
d (won=E2=80=99t name names). They pumped in millions of embeddings =E2=80=
=93 it worked until queries got slow as data grew. They later realized an o=
pen-source alternative (free) performed better for their use case, but migr=
ating was non-trivial and that money was sunk cost. Moral: </span><strong>b=
enchmark and start small</strong><span>. For most, open-source like Chroma =
or Qdrant suffice; only move to pricey Pinecone or managed if you confirmed=
 need. And even then, try their free tier or PoC. Also pay attention to how=
 well the DB integrates with your stack (some DBs might not have the best c=
lient libraries for your language, etc.). The wrong choice can burn money a=
nd time.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line=
-height: 26px;font-size: 16px;"><strong>5. Overlooking Data Refresh (Stale =
Knowledge):</strong><span> Another grave: some RAG systems fail to update t=
heir index as knowledge changes. It=E2=80=99s easy to index a snapshot of d=
ata and forget it. Then users ask about the latest info and get outdated an=
swers. In fields like finance or regulations, that=E2=80=99s dangerous. Pic=
ture an AI advisor giving a tax law from 2022 that got amended in 2023 beca=
use the index wasn=E2=80=99t updated =E2=80=93 the user acts on wrong info.=
 Regular updates (or use a dynamic retrieval that always pulls from source =
in real-time if possible) are critical. It=E2=80=99s a pitfall when teams t=
reat RAG like a static model (=E2=80=9Cwe indexed once, done=E2=80=9D). You=
 need a pipeline for ingestion of new/changed docs, and possibly an expirat=
ion for old ones. Some solve it by indexing on the fly each query (like ret=
rieve via API rather than vector DB, but that=E2=80=99s slower). At least s=
chedule re-embedding periodically. The pitfall isn=E2=80=99t a one-time fai=
lure but a slow death of usefulness as content drifts. Avoid by setting up =
processes from day 1 for continuous data maintenance.</span></p><p style=3D=
"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;=
"><strong>6. Security/PII Leaks (Cautionary Tale):</strong><span> Some proj=
ects died on compliance review because they accidentally allowed sensitive =
data exposure. One internal chatbot was shut down because it retrieved a ch=
unk containing another client=E2=80=99s info to answer a query for a differ=
ent client =E2=80=93 a big no-no (lack of permission filtering). Another go=
t axed because logs of user queries containing personal data were sent to a=
 third-party API without proper agreements. These are pitfalls of negligenc=
e more than tech, but they can kill a project via legal. Always think: what=
 data is being indexed? Does it contain PII? Should it be anonymized or seg=
mented? Who can query what? Implement filtering by user roles (if possible,=
 e.g., separate vector indexes per client or attribute-based access). And e=
nsure if using external LLM APIs, you=E2=80=99re not violating any privacy =
requirement (OpenAI=E2=80=99s policy now says they don=E2=80=99t train on y=
our data by default, but still don=E2=80=99t send what you shouldn=E2=80=99=
t). One preventative measure: have a red-teaming phase =E2=80=93 purposely =
try to get the bot to reveal something it shouldn=E2=80=99t (like ask =E2=
=80=9CShow me data on client B=E2=80=9D as client A, etc.). If it does, fix=
 before real launch.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54=
,55,55);line-height: 26px;font-size: 16px;"><strong>7. Embedding Model Mism=
atch Disaster (New Pitfall):</strong><span> This is a more technical gotcha=
 that some teams recently encountered. It happens when your document embedd=
ings and query embeddings are not compatible =E2=80=93 e.g., you embed docu=
ments with one model and queries with another (accidentally or due to an up=
date), so similarity search breaks. Or if you upgraded your embedding model=
 (say from Ada-001 to Ada-002) without re-embedding your corpus, the vector=
s live in different spaces now. One team updated their code to use a new em=
bedding model version, not realizing they had 100k old vectors in the DB fr=
om the old model. Suddenly retrieval quality plummeted (because the new que=
ry embeddings didn=E2=80=99t align with old doc embeddings). They spent wee=
ks debugging poor results until noticing the model name mismatch. To avoid,=
 maintain metadata on which embedding model and version was used for the in=
dex. If you change, re-index everything. Similarly, mixing different dimens=
ionalities is obvious but I=E2=80=99ve seen someone try to concatenate two =
embedding vectors from different models for docs, then queries only with on=
e model =E2=80=93 also ineffective. Another variant: using multilingual emb=
eddings for docs but English-only for queries or vice versa; if model wasn=
=E2=80=99t aligned across languages, retrieval fails cross-language. Always=
 ensure </span><strong>embedding alignment</strong><span>. If using open-so=
urce models, know if the query vs doc embedding model are separate (Cohere =
had separate ones for example search vs doc). Use as intended.</span></p><p=
 style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-si=
ze: 16px;">These pitfalls claim victims regularly. But armed with foreknowl=
edge, you can steer clear. In short:</p><ul style=3D"margin-top: 0;padding:=
 0;"><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p styl=
e=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: bo=
rder-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Keep chunks =
coherent and overlapping.</strong></p></li><li style=3D"margin: 8px 0 0 32p=
x;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height=
: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size:=
 16px;margin: 0;"><strong>Be mindful of LLM limitations (like mid-chunk inf=
o) and mitigate.</strong></p></li><li style=3D"margin: 8px 0 0 32px;mso-spe=
cial-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;ma=
rgin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;mar=
gin: 0;"><strong>Strangle hallucinations with instruction and verification.=
</strong></p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bul=
let;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;b=
ox-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong=
>Choose tech carefully (and cheaply until proven).</strong></p></li><li sty=
le=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: =
rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;pad=
ding-left: 4px;font-size: 16px;margin: 0;"><strong>Keep knowledge updated.<=
/strong></p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bull=
et;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;bo=
x-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>=
Enforce security and privacy from the start.</strong></p></li><li style=3D"=
margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54=
,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-l=
eft: 4px;font-size: 16px;margin: 0;"><strong>Manage embeddings rigorously (=
versions and types).</strong></p></li></ul><p style=3D"margin: 0 0 20px 0;c=
olor: rgb(54,55,55);line-height: 26px;font-size: 16px;">If you do all that,=
 you=E2=80=99ll avoid the graveyard where failed RAG projects lie. Instead,=
 your project will live on to see the bright future of AI that we=E2=80=99l=
l discuss next!</p><h2 class=3D"header-anchor-post" style=3D"position: rela=
tive;font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-=
system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'App=
le Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webki=
t-font-smoothing: antialiased;-moz-osx-font-smoothing: antialiased;-webkit-=
appearance: optimizelegibility;-moz-appearance: optimizelegibility;appearan=
ce: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-he=
ight: 1.16em;font-size: 1.625em;"><strong>The Future Shock: 2025-2027</stro=
ng></h2><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26=
px;font-size: 16px;"><span>What does the future hold for RAG and AI in the =
next few years? In a word: </span><strong>mind-blowing</strong><span> advan=
cements. We=E2=80=99re standing on the edge of some game-changing developme=
nts =E2=80=93 things that sound like science fiction but are just around th=
e corner. Let=E2=80=99s peer into 2025-2027:</span></p><ul style=3D"margin-=
top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-special-format: b=
ullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0=
;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><stro=
ng>Anthropic=E2=80=99s Prediction: AI Surpassing Nobel Laureates by 2026</s=
trong><span> =E2=80=93 Bold, but not unfounded. This suggests that AI (powe=
red by retrieval and reasoning) will excel in specialized domains to a degr=
ee that matches or exceeds top human experts. Think about it: an AI with ac=
cess to all scientific literature (via RAG) and a reasoning engine could po=
tentially propose novel solutions and insights at a pace humans can=E2=80=
=99t. We already see early hints: an AI agent that read tens of thousands o=
f chemistry papers and suggested a new material that human researchers hadn=
=E2=80=99t thought of. By 2026, such feats might be common =E2=80=93 an AI =
doctor diagnosing ultra-rare diseases by cross-referencing millions of case=
s (something even Nobel-winning doctors might not do in real-time). The key=
 drivers are </span><em>massive context and knowledge integration</em><span=
> (RAG providing memory), plus improved reasoning algorithms.</span></p></l=
i><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=
=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: bor=
der-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>1M+ Token Con=
text Windows Change Everything</strong><span> =E2=80=93 Context size has be=
en ballooning: OpenAI went from 4k to 32k, Anthropic to 100k tokens. We hav=
e million token context windows (on paper) now. I expect multi-million cont=
ext windows to be common by 2026. That=E2=80=99s essentially entire books o=
r multiple books at once. A million tokens is roughly 750k words (about 150=
0 pages). Imagine feeding an entire corporate wiki, or decades of legal cas=
e law, or all of Wikipedia on a topic, </span><em>directly</em><span> into =
the model prompt. The boundaries between retrieval and prompting blur here:=
 you might not need an external vector DB for moderately sized corpora; you=
 could just stuff everything into the prompt (with clever compression). One=
 engineer I know joked, =E2=80=9CIf we get 1M tokens, I=E2=80=99ll just fee=
d the model our whole database schema and docs and ask questions directly.=
=E2=80=9D Of course, more context means slower processing, and we=E2=80=99l=
l still use RAG to select relevant parts, but the flexibility is huge =E2=
=80=93 conversation history can be basically unlimited, models can do deepe=
r analysis without truncating context. Also, multi-step reasoning could hap=
pen internally without external calls if the model can =E2=80=9Cremember=E2=
=80=9D all intermediate steps within its giant scratchpad.</span></p></li><=
li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"c=
olor: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-b=
ox;padding-left: 4px;font-size: 16px;margin: 0;"><strong>MCP Integration: U=
niversal AI-Data Connectivity</strong><span> =E2=80=93 As we discussed, the=
 Model-Context Protocol by Anthropic aims to be a standard pipe connecting =
AI to data. This is already becoming widely adopted and by 2027, I suspect =
this will be near-universal. That means when you launch a new AI system, yo=
u won=E2=80=99t spend time on writing custom retrieval code; you=E2=80=99ll=
 spin up connectors (to your databases, websites, tools) and the model just=
 knows how to talk to them. It=E2=80=99s analogous to how in the early web =
days you had to manually code a lot to connect to a database, but then ORMs=
 and APIs standardized that. This will accelerate new AI application develo=
pment drastically (taking weeks instead of months to integrate sources). It=
 also means AI assistants (like your personal AI or company=E2=80=99s AI) c=
an continuously and securely fetch needed info.</span><br><br><span> </span=
><em>Effect on RAG:</em><span> It might shift the concept =E2=80=93 =E2=80=
=9Cretrieval augmented=E2=80=9D might become just standard =E2=80=9Ccontext=
 retrieval=E2=80=9D feature of all LLMs (like how we don=E2=80=99t talk abo=
ut =E2=80=9Cinternet-augmented smartphone=E2=80=9D because internet connect=
ivity is assumed). Once any AI can seamlessly pull data via MCP or similar,=
 RAG is no longer an optional add-on, but a given. The winners will be thos=
e who leverage it best (with quality data and sources).</span></p></li><li =
style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"colo=
r: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;=
padding-left: 4px;font-size: 16px;margin: 0;"><strong>Why the RAG Market wi=
ll hit $40B by 2030</strong><span> =E2=80=93 We saw projections earlier. As=
 more orgs adopt AI with retrieval (since pure end-to-end training is impra=
ctical for each org=E2=80=99s data), RAG tools and infrastructure become a =
huge business. Vector DB companies, enterprise search integration, AI assis=
tants for knowledge work =E2=80=93 all these fall under that umbrella. The =
workforce will likely include thousands of =E2=80=9CAI knowledge engineers=
=E2=80=9D whose job is to manage corpora, tune retrieval, etc. By 2030, nea=
rly every enterprise app might have an AI copilot that relies on RAG to sta=
y current. If you think of verticals: law, medicine, finance, customer supp=
ort, programming =E2=80=93 all have specialized knowledge that RAG helps in=
ject into general AI. $40B might even be conservative if we include hardwar=
e and services around it.</span></p></li><li style=3D"margin: 8px 0 0 32px;=
mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: =
26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 1=
6px;margin: 0;"><strong>Your Career in the Age of Autonomous AI Agents</str=
ong><span> =E2=80=93 People often ask, =E2=80=9CWill AI (with RAG) automate=
 me out of a job?=E2=80=9D If you know this blog you know I don=E2=80=99t t=
hink that way. But I might frame it differently: =E2=80=9CAI won=E2=80=99t =
replace you, but a person using AI might.=E2=80=9D Those who embrace these =
tools early will have a huge edge. The nature of many jobs will shift to su=
pervising AI, verifying outputs, and handling the non-automatable nuance. I=
f AI becomes as smart as top experts in certain domains, human roles might =
evolve to more creative, strategic, or interpersonal tasks (things AI is fa=
r from mastering).</span><br><br><span> But there=E2=80=99s also </span><em=
>new career</em><span> prospects: AI trainers, AI content curators, ethicis=
ts, etc. The age of autonomous agents (AutoGPT-like systems that can perfor=
m goals relatively independently) means we=E2=80=99ll need =E2=80=9CAI wran=
glers=E2=80=9D =E2=80=93 people who define objectives, monitor agent perfor=
mance, and intervene when needed. Think of it like managing employees =E2=
=80=93 except these employees are AI agents working 24/7 and scouring data.=
 It=E2=80=99s likely by 2027 or so that every knowledge worker will have so=
me AI agent assistance (imagine a =E2=80=9Cjunior AI analyst=E2=80=9D assig=
ned to each person).</span><br><br><span> The ones who thrive will be those=
 who </span><strong>learn to ask the right questions and validate AI output=
s</strong><span>. With RAG, a lot of factual grunt work is handled, so huma=
ns can focus on interpretation and decision-making. That Nobel-level AI? Pe=
rhaps it will partner with Nobel scientists to accelerate discoveries, not =
just do it alone.</span></p></li></ul><p style=3D"margin: 0 0 20px 0;color:=
 rgb(54,55,55);line-height: 26px;font-size: 16px;">In summary, expect:</p><=
ul style=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;ms=
o-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26=
px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16p=
x;margin: 0;"><strong>AI that knows more and forgets less</strong><span>, t=
hanks to big context and integration =E2=80=93 drastically improving capabi=
lities.</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format=
: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom=
: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><s=
trong>Standards making AI integration plug-and-play</strong><span>, leading=
 to an explosion of AI-augmented applications in every field.</span></p></l=
i><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=
=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: bor=
der-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>A shifting jo=
b landscape</strong><span>, where working alongside smart AI (and occasiona=
lly reigning it in) is the norm. Those who start adapting now (e.g., learni=
ng to use RAG tools, understanding limitations like hallucinations) will be=
 the leaders of tomorrow.</span></p></li></ul><p style=3D"margin: 0 0 20px =
0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>One could =
say, companies winning in 2025 aren=E2=80=99t those with just the biggest m=
odels, but those whose </span><strong>AI truly knows their business</strong=
><span> (via RAG). By 2027, that will be even more pronounced: it=E2=80=99s=
 not about who has AI, but who has </span><em>integrated</em><span> AI deep=
ly and responsibly into their operations.</span></p><p style=3D"margin: 0 0=
 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">Exciting t=
imes ahead =E2=80=93 equal parts exhilarating and challenging. The next and=
 final section will equip you with resources to ride this wave =E2=80=93 a =
toolkit and action plan to become a RAG master in the coming 90 days and be=
yond. Let=E2=80=99s gear you up for that future.</p><h2 class=3D"header-anc=
hor-post" style=3D"position: relative;font-family: 'SF Pro Display',-apple-=
system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Robot=
o,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI=
 Symbol';font-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-fon=
t-smoothing: antialiased;-webkit-appearance: optimizelegibility;-moz-appear=
ance: optimizelegibility;appearance: optimizelegibility;margin: 1em 0 0.625=
em 0;color: rgb(54,55,55);line-height: 1.16em;font-size: 1.625em;"><strong>=
Your RAG Toolkit: Resources That Matter</strong></h2><p style=3D"margin: 0 =
0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>As =
we wrap up, I want to leave you with a toolkit =E2=80=93 the best resources=
 to continue your RAG journey. Whether you=E2=80=99re a beginner looking to=
 start from scratch (and ideally spend nothing), a growing company needing =
to scale up, or an enterprise dealing with compliance and heavy load, we=E2=
=80=99ve got suggestions for each. Plus, I=E2=80=99ll point to community go=
ldmines and courses to deepen your expertise. Finally, I challenge you to a=
 </span><strong>30-day RAG sprint</strong><span> to go from novice to pract=
itioner.</span></p><h3 class=3D"header-anchor-post" style=3D"position: rela=
tive;font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-=
system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'App=
le Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webki=
t-font-smoothing: antialiased;-moz-osx-font-smoothing: antialiased;-webkit-=
appearance: optimizelegibility;-moz-appearance: optimizelegibility;appearan=
ce: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-he=
ight: 1.16em;font-size: 1.375em;"><strong>The =E2=80=9CStart Here=E2=80=9D =
Stack for Beginners (Free Tier Everything)</strong></h3><p style=3D"margin:=
 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">If you=
=E2=80=99re just getting your feet wet, here=E2=80=99s a recommended stack =
that won=E2=80=99t cost you a dime:</p><ul style=3D"margin-top: 0;padding: =
0;"><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=
=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: bor=
der-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>LLM:</strong>=
<span> OpenAI=E2=80=99s GPT-3.5 (old, but free via their API with trial cre=
dit, or use the free ChatGPT UI for experimentation). Alternatively, </span=
><strong>Hugging Face=E2=80=99s HuggingChat</strong><span> or </span><stron=
g>Google Colab with a smaller model</strong><span> (like flan-t5 or a 7B LL=
aMA derivative) can be free. But GPT-3.5 is easiest to converse with initia=
lly.</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: b=
ullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0=
;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><stro=
ng>Vector DB:</strong><span> </span><strong>ChromaDB</strong><span> (open s=
ource, pip install chromadb). It=E2=80=99s simple and runs locally. Or use =
a local </span><strong>FAISS index</strong><span> if you prefer just an in-=
memory approach. Both are free.</span></p></li><li style=3D"margin: 8px 0 0=
 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-he=
ight: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-s=
ize: 16px;margin: 0;"><strong>Library:</strong><span> </span><strong>LlamaI=
ndex</strong><span> (GPT Index) or </span><strong>LangChain</strong><span> =
=E2=80=93 both are open source and quite friendly. LlamaIndex maybe simpler=
 for pure QA, LangChain if you want to tinker with chain logic. They have g=
reat docs and examples.</span></p></li><li style=3D"margin: 8px 0 0 32px;ms=
o-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26=
px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16p=
x;margin: 0;"><strong>Data source:</strong><span> Whatever docs you have =
=E2=80=93 but if you need sample data, there are open datasets. For example=
, </span><strong>Wikipedia</strong><span> (you can grab a few pages), or so=
me public domain texts (Project Gutenberg for literature). Or use your own =
notes/markdown files to make it personally interesting.</span></p></li><li =
style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"colo=
r: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;=
padding-left: 4px;font-size: 16px;margin: 0;"><strong>Dev environment:</str=
ong><span> Jupyter Notebooks (free) =E2=80=93 great for iterative developme=
nt. Or VS Code with the Python extension.</span></p></li></ul><p style=3D"m=
argin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">=
With that, you can build a prototype Q&amp;A bot or documentation assistant=
 without paying for anything (assuming you stay within free API limits or u=
se local models). Actually, OpenAI=E2=80=99s $5-10 free credit might get yo=
u thousands of queries on gpt-3.5, which is plenty to play with.</p><p styl=
e=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 1=
6px;"><span>For a quickstart, check out </span><strong><a href=3D"https://s=
ubstack.com/redirect/c851b32a-c238-44e2-9938-c7f44a95d690?j=3DeyJ1IjoiNWtiO=
TN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color=
: rgb(54,55,55);text-decoration: underline;">LangChain=E2=80=99s beginner t=
utorial</a></strong><span> or </span><strong><a href=3D"https://substack.co=
m/redirect/e8f3fb78-a951-4e77-a950-168f9542b146?j=3DeyJ1IjoiNWtiOTN6In0.zdz=
y88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54,5=
5,55);text-decoration: underline;">LlamaIndex=E2=80=99s Getting Started</a>=
</strong><span> =E2=80=93 they walk through loading data and querying it . =
Also, </span><strong>Chroma=E2=80=99s docs</strong><span> show how to do ba=
sic insert and query.</span></p><h3 class=3D"header-anchor-post" style=3D"p=
osition: relative;font-family: 'SF Pro Display',-apple-system-headline,syst=
em-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sa=
ns-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight=
: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing: antiali=
ased;-webkit-appearance: optimizelegibility;-moz-appearance: optimizelegibi=
lity;appearance: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,5=
5,55);line-height: 1.16em;font-size: 1.375em;"><strong>The =E2=80=9CScale U=
p=E2=80=9D Stack for Growing Companies</strong></h3><p style=3D"margin: 0 0=
 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">Okay, you=
=E2=80=99ve proven the concept, now your startup or team needs to deploy so=
mething for real usage. Cost is a concern, but you can spend a bit, and rel=
iability matters:</p><ul style=3D"margin-top: 0;padding: 0;"><li style=3D"m=
argin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,=
55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-le=
ft: 4px;font-size: 16px;margin: 0;"><strong>LLM:</strong><span> Consider us=
ing </span><strong>OpenAI=E2=80=99s GPT-4</strong><span> for higher quality=
 if needed, but GPT-3.5 may suffice. Also look at </span><strong>Cohere</st=
rong><span> or </span><strong>Anthropic Claude</strong><span> if they have =
a better pricing or context for your needs (Claude has that 100k context ve=
rsion). Some companies fine-tune a smaller open model to their data to save=
 per-call costs =E2=80=93 e.g., fine-tuning LLaMA 2 13B on your corpus. Tha=
t has upfront cost but then queries are virtually free if self-hosted. A co=
mmon approach is a </span><strong>tiered LLM strategy</strong><span>: use G=
PT-3.5 for most, GPT-4 for complex queries or final verification.</span></p=
></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p sty=
le=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: b=
order-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Vector DB:<=
/strong><span> If you outgrew Chroma on a single machine, you could move to=
 </span><strong>Chroma Cloud</strong><span> (they have managed service), or=
 </span><strong>Qdrant Cloud</strong><span> which is quite affordable (reme=
mber the ~$9 for 50k vectors estimate ). </span><strong>Weaviate</strong><s=
pan> also offers a hybrid search and is free up to some limit. Many go with=
 </span><strong>Pinecone starter</strong><span> ($) at this stage for ease.=
 If you have under ~1M embeddings, even a Postgres with pgvector extension =
might suffice (then you reuse your existing DB infra).</span></p></li><li s=
tyle=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color=
: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;p=
adding-left: 4px;font-size: 16px;margin: 0;"><strong>Orchestration:</strong=
><span> LangChain or LlamaIndex will still serve, but maybe now implement r=
obust error handling/logging around it. You might also containerize the app=
 for deployment (Docker).</span></p></li><li style=3D"margin: 8px 0 0 32px;=
mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: =
26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 1=
6px;margin: 0;"><strong>Authentication &amp; front-end:</strong><span> If e=
xposing to users, set up an interface. Could be a simple React app calling =
a backend, or even a Slack/Discord bot. Use API keys or auth to protect it.=
 Many companies roll out in Slack first for internal Q&amp;A.</span></p></l=
i><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=
=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: bor=
der-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Monitoring:</=
strong><span> Use something like </span><strong>Streamlit or Gradio</strong=
><span> for quick dashboard if needed, and definitely log interactions. You=
 can use LangChain=E2=80=99s built-in tracing or just your own logging to a=
 file/DB. Monitor usage and have a way to turn it off if something goes hay=
wire (feature flag).</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-s=
pecial-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;=
margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;m=
argin: 0;"><strong>Cost control:</strong><span> Set up usage limits (like d=
on=E2=80=99t let one user spam 1000 queries/min). Also track OpenAI API usa=
ge; you can use their usage APIs or just your logs to gauge spend. Possibly=
 implement caching of LLM responses for identical queries to save.</span></=
p></li></ul><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height=
: 26px;font-size: 16px;"><span>Growing companies at this stage might also i=
nvest in prompt engineering =E2=80=93 e.g., creating a few </span><strong>p=
rompt templates</strong><span> for different styles of questions. And unit =
tests for your RAG: provide it some known queries and check it returns acce=
ptable answers (to catch regressions when you change something).</span></p>=
<h3 class=3D"header-anchor-post" style=3D"position: relative;font-family: '=
SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSyst=
emFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Se=
goe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoothing: a=
ntialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearance: optimiz=
elegibility;-moz-appearance: optimizelegibility;appearance: optimizelegibil=
ity;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-s=
ize: 1.375em;"><strong>The =E2=80=9CEnterprise=E2=80=9D Stack for Fortune 5=
00s</strong></h3><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-h=
eight: 26px;font-size: 16px;">Now we=E2=80=99re talking heavy duty: require=
ments include security, high availability, compliance, and potentially mill=
ions of knowledge items and users.</p><ul style=3D"margin-top: 0;padding: 0=
;"><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=
=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: bor=
der-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>LLM:</strong>=
<span> Likely a combination of </span><strong>self-hosted models</strong><s=
pan> for data-sensitive content and limited use of external APIs for genera=
l knowledge. Enterprises might deploy </span><strong>Azure OpenAI</strong><=
span> (which is OpenAI models in their Azure cloud, with data not leaving),=
 or </span><strong>AWS Bedrock</strong><span> (which offers Jurassic, Anthr=
opic, etc. with enterprise-friendly terms). Some might even run </span><str=
ong>GPT-4 on-prem</strong><span> if and when available (or a comparable big=
 model). Also consider </span><strong>Google=E2=80=99s Gemini API via GCP</=
strong><span>, depending on partnership. The key is enterprise agreements (=
privacy, SLA).</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-special=
-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin=
-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin:=
 0;"><strong>Vector DB:</strong><span> At this scale, probably </span><stro=
ng>managed services or on-prem clusters</strong><span>. Pinecone Enterprise=
, Weaviate Enterprise, or using something like </span><strong>ElasticSearch=
 with vector-capability</strong><span> if they already have ELK stack. Ente=
rprises often prefer well-supported tools: e.g., </span><strong>Microsoft C=
ognitive Search</strong><span> (which now supports vectors) for those in MS=
 ecosystem. Or </span><strong>OpenSearch</strong><span> for those already u=
sing AWS search solutions. They will care about features like RBAC (role-ba=
sed access control), encryption at rest, etc. Many vector DBs now offer tho=
se enterprise features (e.g., Pinecone and Weaviate have RBAC, etc. ).</spa=
n></p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><=
p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizi=
ng: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Orches=
tration &amp; Integration:</strong><span> Likely a microservice architectur=
e. They might integrate RAG into existing platforms (say, an internal Share=
Point plugin, or a CRM assistant). LangChain might be used under the hood, =
or a custom solution for more control. Observability is key =E2=80=93 so in=
tegrate with </span><strong>Splunk or AppDynamics</strong><span> for loggin=
g, </span><strong>Datadog</strong><span> for monitoring performance. Possib=
ly use </span><strong>OpenTelemetry</strong><span> if custom solution to tr=
ace calls.</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-special-for=
mat: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bot=
tom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"=
><strong>Compliance &amp; Security:</strong><span> This stack includes thin=
gs like </span><strong>Data loss prevention (DLP)</strong><span> checks on =
AI output (prevent it from spitting out something it shouldn=E2=80=99t), </=
span><strong>audit logs</strong><span> of who asked what, etc. Possibly beh=
ind the scenes every AI response goes through an approval step or a human-i=
n-the-loop for certain sensitive domains.</span></p></li><li style=3D"margi=
n: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,5=
5);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: =
4px;font-size: 16px;margin: 0;"><strong>Scalability:</strong><span> It will=
 be deployed across regions, with fallback models if one service fails. For=
 vector DB, maybe multi-region replication. Also they=E2=80=99ll have a </s=
pan><strong>retraining/indexing pipeline</strong><span> that continuously u=
pdates (with proper CI/CD =E2=80=93 maybe nightly builds of the index or st=
reaming updates).</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-spec=
ial-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;mar=
gin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;marg=
in: 0;"><strong>User Interface:</strong><span> could be deeply integrated (=
not a separate chat UI, but embedded in existing tools like Office 365 via =
plugins, etc.). Or for customer support, integrated with their support port=
al as a chat assistant. The UI might need to handle handoff to human agents=
 seamlessly when AI can=E2=80=99t help (so integration with their ticketing=
 system).</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-special-form=
at: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bott=
om: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">=
<strong>Tooling &amp; Knowledge Management:</strong><span> Enterprises ofte=
n have existing knowledge management workflows. The RAG system might tie in=
to that =E2=80=93 e.g., when a new policy doc is published on Confluence, a=
utomatically chunk &amp; index it. That means connectors to internal data s=
ources (file shares, intranets, DBs) =E2=80=93 possibly using </span><stron=
g>MCP</strong><span> in the future or current enterprise search connectors.=
</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bulle=
t;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box=
-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>T=
esting &amp; Evaluation:</strong><span> Formal testing with domain experts.=
 Possibly running the AI in shadow mode (giving suggestions to human agents=
, but not directly to customers, until it proves good enough). Ensuring it =
handles domain-specific vocabulary correctly (maybe even fine-tuning embedd=
ings or the model on domain data to improve).</span></p></li></ul><p style=
=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16=
px;"><span>This stack is not one-size-fits-all, but at enterprise scale, th=
e emphasis is on </span><strong>robustness, compliance, integration</strong=
><span>. They=E2=80=99d rather a slightly weaker model that is secure than =
a powerful one that might leak data. For example, some banks disabled direc=
t internet search for their AI assistant because they can=E2=80=99t allow u=
npredictable external info.</span></p><h3 class=3D"header-anchor-post" styl=
e=3D"position: relative;font-family: 'SF Pro Display',-apple-system-headlin=
e,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Ar=
ial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-=
weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing: a=
ntialiased;-webkit-appearance: optimizelegibility;-moz-appearance: optimize=
legibility;appearance: optimizelegibility;margin: 1em 0 0.625em 0;color: rg=
b(54,55,55);line-height: 1.16em;font-size: 1.375em;"><strong>Community Gold=
mines: Discords, GitHub Repos, Courses</strong></h3><p style=3D"margin: 0 0=
 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">You=E2=80=
=99re not alone in this journey. The RAG/LLM community is vibrant and shari=
ng knowledge daily:</p><ul style=3D"margin-top: 0;padding: 0;"><li style=3D=
"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"margin: 0 0 =
20px 0;color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: =
border-box;padding-left: 4px;font-size: 16px;"><strong>Discord servers:</st=
rong></p><ul style=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0=
 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-=
height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font=
-size: 16px;margin: 0;"><strong>LangChain=E2=80=99s Discord</strong><span> =
=E2=80=93 great for Q&amp;A and seeing what issues others face.</span></p><=
/li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=
=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: bor=
der-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>LlamaIndex Di=
scord</strong><span> =E2=80=93 developers and users share tips, plus the de=
vs often answer.</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-speci=
al-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;marg=
in-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margi=
n: 0;"><strong>Vector database Discords</strong><span> (Pinecone, Weaviate,=
 Qdrant each have communities).</span></p></li><li style=3D"margin: 8px 0 0=
 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-he=
ight: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-s=
ize: 16px;margin: 0;"><strong>Hugging Face Discord</strong><span> =E2=80=93=
 for general transformer/LLM discussions, including retrieval techniques.</=
span></p></li></ul></li><li style=3D"margin: 8px 0 0 32px;mso-special-forma=
t: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-botto=
m: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">T=
hese are good to lurk in and search history =E2=80=93 often your question h=
as been asked by someone.</p></li><li style=3D"margin: 8px 0 0 32px;mso-spe=
cial-format: bullet;"><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);l=
ine-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;=
font-size: 16px;"><strong>GitHub Repositories:</strong></p><ul style=3D"mar=
gin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-special-forma=
t: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-botto=
m: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><=
strong><a href=3D"https://substack.com/redirect/563dc5dc-f2ca-411e-af1f-f67=
397bf35af?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPO=
w0" rel=3D"" style=3D"color: rgb(54,55,55);text-decoration: underline;">awe=
some-rag</a></strong></p></li><li style=3D"margin: 8px 0 0 32px;mso-special=
-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin=
-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin:=
 0;"><strong><a href=3D"https://substack.com/redirect/fb297555-ff34-4983-a6=
1d-5a58230ebdb3?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tA=
jMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,55);text-decoration: underline=
;">LangChain Hub</a></strong><span> and </span><strong>Examples</strong><sp=
an> =E2=80=93 many example scripts for various tasks.</span></p></li><li st=
yle=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color:=
 rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;pa=
dding-left: 4px;font-size: 16px;margin: 0;"><strong><a href=3D"https://subs=
tack.com/redirect/28059eb0-1afe-45f5-8006-3545b2e16a15?j=3DeyJ1IjoiNWtiOTN6=
In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: r=
gb(54,55,55);text-decoration: underline;">OpenAI Cookbook</a></strong><span=
> (GitHub: openai/openai-cookbook) =E2=80=93 although not RAG-specific, it =
has sections on retrieval augmentation and plenty of relevant examples.</sp=
an></p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;">=
<p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-siz=
ing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong><a hr=
ef=3D"https://substack.com/redirect/7b705ba3-3efd-45b9-9222-378f305020e1?j=
=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"=
" style=3D"color: rgb(54,55,55);text-decoration: underline;">InstructorEmbe=
dding</a></strong><span> or </span><strong>sentence-transformers</strong><s=
pan> repos =E2=80=93 if exploring custom embedding models.</span></p></li><=
li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"c=
olor: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-b=
ox;padding-left: 4px;font-size: 16px;margin: 0;"><span>And of course, </spa=
n><strong>papers</strong><span>: e.g., the </span><a href=3D"https://substa=
ck.com/redirect/efcf8b38-a757-49b7-94d8-03d17c0e9f14?j=3DeyJ1IjoiNWtiOTN6In=
0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb=
(54,55,55);text-decoration: underline;">original RAG paper</a><span> by Lew=
is et al. 2020 is on GitHub for code, etc.</span></p></li><li style=3D"marg=
in: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,=
55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left:=
 4px;font-size: 16px;margin: 0;"><span>Also Microsoft=E2=80=99s </span><str=
ong>Guidance</strong><span> repo shows prompt strategies, including retriev=
al.</span></p></li></ul></li><li style=3D"margin: 8px 0 0 32px;mso-special-=
format: bullet;"><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-h=
eight: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-=
size: 16px;"><strong>Courses and Tutorials:</strong></p><ul style=3D"margin=
-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-special-format: =
bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: =
0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><spa=
n>Andrew Ng=E2=80=99s </span><strong><a href=3D"https://substack.com/redire=
ct/d87dc4fd-c38d-4e7c-ae81-2044a380d277?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPm=
LQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,55);te=
xt-decoration: underline;">DeepLearning.AI</a><span> short course on LangCh=
ain</span></strong><span> (on Coursera) =E2=80=93 hands-on building chains =
and agents (he has other RAG courses too)</span></p></li><li style=3D"margi=
n: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,5=
5);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: =
4px;font-size: 16px;margin: 0;"><strong><a href=3D"https://substack.com/red=
irect/40aac557-05cb-477d-a0ba-398022a17157?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YF=
xPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,55)=
;text-decoration: underline;">Full Stack Deep Learning</a></strong><span> h=
as some modules on deploying LLMs with RAG.</span></p></li><li style=3D"mar=
gin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55=
,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left=
: 4px;font-size: 16px;margin: 0;"><strong><a href=3D"https://substack.com/r=
edirect/704ee7da-20f8-4a42-8107-c91fbabb061b?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88=
YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,5=
5);text-decoration: underline;">Hugging Face Courses</a></strong><span> =E2=
=80=93 they have a new course on LLMs and might cover retrieval augmentatio=
n.</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bul=
let;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;b=
ox-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong=
><span>OpenAI=E2=80=99s </span><a href=3D"https://substack.com/redirect/f60=
733c4-3705-4080-a596-6a81fc758795?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9h=
shgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,55);text-dec=
oration: underline;">ChatGPT prompt engineering for developers</a></strong>=
<span> (free resource) =E2=80=93 touches on how to instruct the model (usef=
ul when you do RAG prompts).</span></p></li></ul></li><li style=3D"margin: =
8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);=
line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px=
;font-size: 16px;margin: 0;"><span>Many YouTubers also have RAG content =E2=
=80=93 e.g., </span><strong>Sam Witteveen</strong><span>, </span><strong>Ja=
mes Briggs</strong><span>, etc., showing building QA bots.</span></p></li><=
li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"m=
argin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;b=
ox-sizing: border-box;padding-left: 4px;font-size: 16px;"><strong>Reading</=
strong><span>:</span></p><ul style=3D"margin-top: 0;padding: 0;"><li style=
=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rg=
b(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;paddi=
ng-left: 4px;font-size: 16px;margin: 0;"><span>Research papers =E2=80=93 =
=E2=80=9C</span><a href=3D"https://substack.com/redirect/efcf8b38-a757-49b7=
-94d8-03d17c0e9f14?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ=
4tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,55);text-decoration: underl=
ine;">Retrieval-Augmented Generation (RAG)</a><span>=E2=80=9D, =E2=80=9C</s=
pan><a href=3D"https://substack.com/redirect/b0a27444-0676-49d0-b77e-3dc4d9=
777868?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0"=
 rel=3D"" style=3D"color: rgb(54,55,55);text-decoration: underline;">REALM<=
/a><span>=E2=80=9D, =E2=80=9C</span><a href=3D"https://substack.com/redirec=
t/a2b6dd91-5d20-4a4a-9eab-5a8503f3dbeb?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmL=
QJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,55);tex=
t-decoration: underline;">ReAct</a><span>=E2=80=9D, =E2=80=9C</span><a href=
=3D"https://substack.com/redirect/d0116e4b-5194-4cec-97c4-8efa54754c37?j=3D=
eyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" s=
tyle=3D"color: rgb(54,55,55);text-decoration: underline;">GraphRAG survey</=
a><span> =E2=80=9D, etc. Even if mathy, skip to discussion for ideas.</span=
></p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p=
 style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizin=
g: border-box;padding-left: 4px;font-size: 16px;margin: 0;">Company blogs (=
with caution as user said): e.g., OpenAI, Anthropic, Cohere blogs =E2=80=93=
 they sometimes discuss use cases and best practices.</p></li><li style=3D"=
margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54=
,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-l=
eft: 4px;font-size: 16px;margin: 0;">The article you=E2=80=99re reading now=
 (&#128521;) can serve as a mini reference too, given the citations we=E2=
=80=99ve sprinkled.</p></li></ul></li></ul><h3 class=3D"header-anchor-post"=
 style=3D"position: relative;font-family: 'SF Pro Display',-apple-system-he=
adline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helveti=
ca,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';=
font-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothi=
ng: antialiased;-webkit-appearance: optimizelegibility;-moz-appearance: opt=
imizelegibility;appearance: optimizelegibility;margin: 1em 0 0.625em 0;colo=
r: rgb(54,55,55);line-height: 1.16em;font-size: 1.375em;"><strong>The 30-Da=
y RAG Challenge: From Novice to Practitioner</strong></h3><p style=3D"margi=
n: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">Read=
y to apply everything and build something real? Here=E2=80=99s a rough plan=
 for 30 days (modify as fits your schedule):</p><p style=3D"margin: 0 0 20p=
x 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>Week 1=
-2: Environment setup and first RAG</strong><span> =E2=80=93 Set up your de=
v environment (maybe a notebook or simple app). Pick a small domain (e.g., =
use 10 Wikipedia articles on a topic you like). Day 1-2: Load and index dat=
a with LlamaIndex, do simple queries. Day 3-5: Try LangChain, experiment wi=
th different retrievers (maybe FAISS vs Chroma). By end of Week 1, have a b=
asic QA bot working locally. Week 2: Increase complexity =E2=80=93 add mult=
iple files, try some multi-turn conversation with memory. Also join a commu=
nity (discord) and ask at least one question. Read two articles/papers on R=
AG to deepen understanding. </span><strong>Checkpoint at Day 14:</strong><s=
pan> You should be comfortable with basic RAG pipeline code and have a smal=
l demo.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-=
height: 26px;font-size: 16px;"><strong>Week 3: Production-ready prototype</=
strong><span> =E2=80=93 Focus on robustness. Implement caching of answers, =
add logging. Try hybrid search if you haven=E2=80=99t: integrate a keyword =
search (maybe just Python whoosh or Elastic if you can) along with vector. =
Compare results quality. Also work on prompt tuning =E2=80=93 e.g., test di=
fferent prompt wording (=E2=80=9CUse the provided context to answer=E2=80=
=A6=E2=80=9D vs. =E2=80=9CAnswer concisely based on info above.=E2=80=9D). =
See what yields better factual accuracy. Around Day 18, purposely break it =
=E2=80=93 ask something outside the knowledge base =E2=80=93 see if it says=
 =E2=80=9CI don=E2=80=99t know.=E2=80=9D If not, refine instructions. </spa=
n><strong>Checkpoint Day 21:</strong><span> Your bot should be much more ro=
bust: less hallucination, and you should have a good handle on tuning it.</=
span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 2=
6px;font-size: 16px;"><strong>Week 4: Advanced patterns and optimization</s=
trong><span> =E2=80=93 Now incorporate one advanced concept: maybe GraphRAG=
-lite (even just linking sections by title), or multi-modal (throw an image=
 in and handle it if applicable), or implement a second retrieval step for =
a complex query. This is stretch learning =E2=80=93 pick what interests you=
. Also, measure performance: how fast is query? Try to speed it up (maybe r=
educe embedding size or use batching). If cost is an issue, maybe deploy a =
local model for retrieval or generation and measure quality vs API. In para=
llel, start packaging your project: containerize it or deploy on a free ser=
vice (like Streamlit Sharing or Hugging Face Spaces) so others can try. </s=
pan><strong>Checkpoint Day 28:</strong><span> You=E2=80=99ve implemented so=
mething non-trivial beyond basics and have a sharable prototype.</span></p>=
<p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-=
size: 16px;"><strong>Days 29-30: Scale and integrate</strong><span> =E2=80=
=93 Think bigger: if this were used by 1000 people, what would you need? Pe=
rhaps set up a Pinecone trial and index more data (if you have). Or integra=
te it into a simple UI (a chat web interface). Basically stress test and re=
fine. Day 30: reflect on what you=E2=80=99ve learned, post a summary on a f=
orum or LinkedIn =E2=80=93 teaching solidifies learning.</span></p><p style=
=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16=
px;">This challenge covers building, tuning, and scaling aspects in a conde=
nsed way. Adjust as needed =E2=80=93 the goal is to touch on each important=
 aspect at least briefly (data, retrieval variants, prompting, eval, deploy=
ment).</p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: =
26px;font-size: 16px;"><span>By the end of these 30 days, you should feel l=
ike a RAG practitioner: able to build a custom QA system, aware of pitfalls=
, and ready to apply these skills in a project or job. And importantly, you=
=E2=80=99ll have a deeper intuition for </span><em>why</em><span> RAG works=
 the way it does and how to get the most out of it.</span></p><p style=3D"m=
argin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">=
That concludes your toolkit =E2=80=93 but one more thing before we sign off=
: a dose of inspiration and urgency in our concluding words.</p><h2 class=
=3D"header-anchor-post" style=3D"position: relative;font-family: 'SF Pro Di=
splay',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'S=
egoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Em=
oji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoothing: antialiase=
d;-moz-osx-font-smoothing: antialiased;-webkit-appearance: optimizelegibili=
ty;-moz-appearance: optimizelegibility;appearance: optimizelegibility;margi=
n: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-size: 1.62=
5em;"><strong>Real Stories from the Trenches</strong></h2><p style=3D"margi=
n: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">Let=
=E2=80=99s ground this in reality with some rapid-fire case studies =E2=80=
=93 real stories of RAG in action that show what=E2=80=99s possible, along =
with metrics and lessons learned from each.</p><ul style=3D"margin-top: 0;p=
adding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;">=
<p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-siz=
ing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Vimeo=
=E2=80=99s Video Chat Revolution:</strong><span> Vimeo integrated RAG to he=
lp users search within their video content. Think of it as a =E2=80=9Cvideo=
 chat=E2=80=9D =E2=80=93 a user asks about a particular tutorial video (=E2=
=80=9CHow do I add music to my project in Video X?=E2=80=9D) and the chatbo=
t, using RAG, retrieves the transcript section where that is explained, and=
 answers with reference to the timestamp. In testing, they found users coul=
d get to the info </span><strong>3=C3=97 faster</strong><span> than scrubbi=
ng through videos manually. The wow moment was when a user asked a vague qu=
estion and the bot answered, </span><strong>=E2=80=9CAt 2:13 in the video, =
the host explains how to add music=E2=80=A6=E2=80=9D</strong><span>, provid=
ing a direct link. This boosted user engagement with tutorial videos by an =
estimated </span><strong>20%</strong><span> (because users weren=E2=80=99t =
dropping off frustrated). Lesson: multi-modal RAG (transcripts as data) can=
 unlock content that was otherwise hard to navigate. And users loved the ti=
me-specific answers.</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-s=
pecial-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;=
margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;m=
argin: 0;"><strong>Legal Firm Processes 1M Documents in 24 Hours:</strong><=
span> A large law firm dealing with litigation had to comb through a millio=
n documents (emails, PDFs) for relevant evidence =E2=80=93 a classic e-disc=
overy nightmare. They deployed a RAG pipeline with a combination of keyword=
 filtering and vector search. Within 24 hours, the system (running on a bee=
fy cloud setup) indexed all docs and allowed attorneys to ask questions lik=
e =E2=80=9CFind discussions of project Thunderbolt budget overruns.=E2=80=
=9D The RAG system retrieved key emails and memos in seconds. One attorney =
said it was like having a team of 50 paralegals working overnight. The firm=
 reported they found crucial evidence in </span><strong>hours instead of we=
eks</strong><span>, potentially saving </span><strong>$200,000</strong><spa=
n> in billable time. Lesson: RAG at scale + domain expertise =3D massive ef=
ficiency gains. Also, they learned to trust but verify =E2=80=93 every AI-f=
ound doc was double-checked by a human, which was still faster than humans =
finding it in the first place.</span></p></li><li style=3D"margin: 8px 0 0 =
32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-hei=
ght: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-si=
ze: 16px;margin: 0;"><strong>Hospital Reduces Misdiagnosis by 30%:</strong>=
<span> A hospital implemented a RAG-powered support tool for doctors. It in=
dexed medical literature, patient histories, and guidelines. During diagnos=
is, a doctor could quietly query, say, =E2=80=9Cpatient with X symptoms and=
 Y lab results =E2=80=93 possible conditions?=E2=80=9D The AI would retriev=
e similar case studies and relevant guideline excerpts. Over 6 months, in a=
 pilot, the tool flagged several cases where the initial human diagnosis mi=
ssed a rare disease =E2=80=93 suggesting further tests which confirmed the =
rarer condition. Hospital data showed a 30% reduction in diagnostic errors =
in departments where the tool was used. Doctors noted it was like getting a=
 second opinion from an encyclopedia that actually understood context. One =
key metric: malpractice incidents in that period dropped (though need long-=
term data). Lesson: RAG can act as a safety net in high-stakes fields, but =
it=E2=80=99s critical to have up-to-date, vetted data in the index. Also, d=
octors had to be trained to use the tool effectively; those that did saw no=
ticeable improvements.</span></p></li><li style=3D"margin: 8px 0 0 32px;mso=
-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26p=
x;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px=
;margin: 0;"><strong>Financial Firm=E2=80=99s Fraud Detection Transformatio=
n:</strong><span> A fintech company used RAG to enhance fraud investigation=
s. They have tons of transaction data and profiles of known fraud patterns.=
 Their new system let analysts ask questions like =E2=80=9CShow me any conn=
ection between user A=E2=80=99s transactions and these suspicious accounts=
=E2=80=9D =E2=80=93 behind the scenes, it retrieved relevant logs and even =
generated a graph visualization of connections (GraphRAG in action). What u=
sed to take an analyst days of SQL queries and cross-referencing was done i=
n minutes. In one case, this system identified a fraud ring of 12 accounts =
that had eluded earlier detection rules. The firm estimated they prevented =
$1M in fraud in that quarter thanks to quicker, deeper analysis by the AI a=
ssistant. Analysts noted that the AI could surface non-obvious links (like =
matching phone numbers or device fingerprints across accounts) that they mi=
ght have missed. Lesson: RAG can augment human pattern-finding, and combini=
ng structured data retrieval with unstructured (like support tickets conten=
t) provided a holistic view.</span></p></li></ul><p style=3D"margin: 0 0 20=
px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">Across these =
stories, common threads:</p><ul style=3D"margin-top: 0;padding: 0;"><li sty=
le=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: =
rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;pad=
ding-left: 4px;font-size: 16px;margin: 0;"><span>The </span><strong>metrics=
</strong><span> (faster by X%, errors down Y%, cost saved $Z) build the bus=
iness case for RAG.</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-sp=
ecial-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;m=
argin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;ma=
rgin: 0;">Implementation lessons (like doctors needing training, or law fir=
m verifying AI results) show that it=E2=80=99s not just plug-and-play; proc=
ess integration matters.</p></li><li style=3D"margin: 8px 0 0 32px;mso-spec=
ial-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;mar=
gin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;marg=
in: 0;">Timeline: many achieved significant results in months, not years, o=
nce data and tools were in place.</p></li><li style=3D"margin: 8px 0 0 32px=
;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height:=
 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: =
16px;margin: 0;"><strong>User acceptance</strong><span>: Initially, some pr=
ofessionals were skeptical (e.g., lawyers hesitant to trust AI suggestions)=
, but success cases converted many into proponents. Key was keeping them in=
 control (AI suggests, human confirms).</span></p></li></ul><p style=3D"mar=
gin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">Th=
ese real-world successes hopefully spark ideas for your context. Whether it=
=E2=80=99s speeding up content access (Vimeo), supercharging analysis (lega=
l/finance), or acting as a diagnostic safety net (medical), RAG is making a=
 tangible impact. Think about your field: what information overload or dela=
y could be tackled with these techniques? The examples above were once just=
 wishful thinking, now they are proven.</p><p style=3D"margin: 0 0 20px 0;c=
olor: rgb(54,55,55);line-height: 26px;font-size: 16px;">Lastly, before we f=
inish, let=E2=80=99s chart out an action plan for you to get from here to y=
our own success story in the next 90 days.</p><h2 class=3D"header-anchor-po=
st" style=3D"position: relative;font-family: 'SF Pro Display',-apple-system=
-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helv=
etica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbo=
l';font-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoo=
thing: antialiased;-webkit-appearance: optimizelegibility;-moz-appearance: =
optimizelegibility;appearance: optimizelegibility;margin: 1em 0 0.625em 0;c=
olor: rgb(54,55,55);line-height: 1.16em;font-size: 1.625em;"><strong>Your A=
ction Plan: Next 90 Days</strong></h2><p style=3D"margin: 0 0 20px 0;color:=
 rgb(54,55,55);line-height: 26px;font-size: 16px;">Ready to build the futur=
e? Here=E2=80=99s a clear 90-day roadmap to go from theory to impact, wheth=
er you=E2=80=99re implementing RAG in your company or building your own pro=
ject. We=E2=80=99ll break it down by weeks with concrete goals and success =
metrics at each checkpoint.</p><p style=3D"margin: 0 0 20px 0;color: rgb(54=
,55,55);line-height: 26px;font-size: 16px;"><strong>Weeks 1-2: Environment =
Setup &amp; First Build</strong></p><p style=3D"margin: 0 0 20px 0;color: r=
gb(54,55,55);line-height: 26px;font-size: 16px;"><strong>Goal:</strong><spa=
n> Set up infrastructure and create a basic RAG application.</span></p><ul =
style=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-s=
pecial-format: bullet;"><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55)=
;line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4p=
x;font-size: 16px;"><strong>Tasks:</strong></p><ul style=3D"margin-top: 0;p=
adding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;">=
<p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-siz=
ing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">Assemble your=
 =E2=80=9CStart Here=E2=80=9D stack (as mentioned in Toolkit). Install libr=
aries (LangChain, etc.), get API keys if needed.</p></li><li style=3D"margi=
n: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,5=
5);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: =
4px;font-size: 16px;margin: 0;">Pick a small set of data relevant to your d=
omain (maybe 10-20 documents).</p></li><li style=3D"margin: 8px 0 0 32px;ms=
o-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26=
px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16p=
x;margin: 0;">Build a simple retrieval + LLM script to answer questions fro=
m that data.</p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: =
bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: =
0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">Expe=
riment with a few prompts and questions to ensure it works end-to-end.</p><=
/li></ul></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;=
"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-s=
izing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Suc=
cess Metric:</strong><span> By end of Week 2, you should be able to ask a q=
uestion and get a reasonable answer with a source citation from your data. =
Essentially, a prototype Q&amp;A chatbot is functioning on a small scale.</=
span></p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;=
"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-s=
izing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Che=
ckpoint assessment:</strong><span> Do a demo to a colleague or friend. If t=
hey ask a question from the doc and get a correct answer, you=E2=80=99re on=
 track. If not, troubleshoot (likely issues: parsing errors, poor prompt, e=
tc. =E2=80=93 fix those now while scope is small).</span></p></li></ul><p s=
tyle=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size=
: 16px;"><strong>Weeks 3-4: Production-Ready Prototype</strong></p><p style=
=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16=
px;"><strong>Goal:</strong><span> Scale up data and robustness; integrate a=
 front-end if needed.</span></p><ul style=3D"margin-top: 0;padding: 0;"><li=
 style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"mar=
gin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box=
-sizing: border-box;padding-left: 4px;font-size: 16px;"><strong>Tasks:</str=
ong></p><ul style=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 =
0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-h=
eight: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-=
size: 16px;margin: 0;">Increase your dataset size (if you ultimately need 1=
000 docs, try indexing a few hundred this week).</p></li><li style=3D"margi=
n: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,5=
5);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: =
4px;font-size: 16px;margin: 0;">Implement necessary chunking, metadata, and=
 possibly hybrid search if queries are complex.</p></li><li style=3D"margin=
: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55=
);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4=
px;font-size: 16px;margin: 0;">Add a user interface or integrate into your =
app environment (e.g., a simple web UI or Slack bot).</p></li><li style=3D"=
margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54=
,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-l=
eft: 4px;font-size: 16px;margin: 0;">Start logging queries and answers for =
review.</p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bulle=
t;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box=
-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">Define =
=E2=80=9CI don=E2=80=99t know=E2=80=9D behavior: decide how the system shou=
ld respond when unsure (and implement that guard).</p></li></ul></li><li st=
yle=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color:=
 rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;pa=
dding-left: 4px;font-size: 16px;margin: 0;"><strong>Success Metric:</strong=
><span> By end of Week 4, your prototype should handle the full breadth of =
your use-case questions with, say, &gt;80% accuracy/relevance in testing. A=
lso, non-experts should be able to use it via the UI and find it useful.</s=
pan></p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"=
><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-si=
zing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Chec=
kpoint assessment:</strong><span> Conduct a small user test (could be colle=
agues from different teams). Give them 5-10 sample questions to try. If maj=
ority of answers are correct and users find the interface easy, you pass. N=
ote any failures for improvement.</span></p></li></ul><p style=3D"margin: 0=
 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>=
Month 2 (Weeks 5-8): Advanced Patterns &amp; Optimization</strong></p><p st=
yle=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size:=
 16px;"><strong>Goal:</strong><span> Enhance system intelligence and effici=
ency; address any failures from testing.</span></p><ul style=3D"margin-top:=
 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-special-format: bulle=
t;"><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;m=
argin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;">=
<strong>Tasks:</strong></p><ul style=3D"margin-top: 0;padding: 0;"><li styl=
e=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: r=
gb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padd=
ing-left: 4px;font-size: 16px;margin: 0;">If you found patterns in misses (=
e.g., multi-hop questions failing), implement recursive retrieval or agent =
steps for those.</p></li><li style=3D"margin: 8px 0 0 32px;mso-special-form=
at: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bott=
om: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">=
If certain info was missing or outdated, update your index pipeline (maybe =
link it to the source of truth for auto-updates).</p></li><li style=3D"marg=
in: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,=
55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left:=
 4px;font-size: 16px;margin: 0;">Optimize latency: perhaps introduce cachin=
g for repeated queries, or use a faster embedding model if embedding time i=
s slow.</p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bulle=
t;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box=
-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">Security =
check: implement basic auth if needed, and ensure no sensitive data leaks (=
e.g., mask PII in responses if applicable).</p></li><li style=3D"margin: 8p=
x 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);li=
ne-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;f=
ont-size: 16px;margin: 0;">Scale dry-run: simulate or actually run, say, 10=
00 queries and see if system holds up (both accuracy and performance).</p><=
/li></ul></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;=
"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-s=
izing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Suc=
cess Metric:</strong><span> By end of Week 8, the system=E2=80=99s accuracy=
 should improve (target &gt;90% on known evaluation set). Latency should be=
 within acceptable range for users (e.g., &lt;2 seconds per query for inter=
active use). And the system should handle a moderate concurrent load (if re=
levant).</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-special-forma=
t: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-botto=
m: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><=
strong>Checkpoint assessment:</strong><span> Re-run your earlier user test =
(and maybe expand it). If previously it got 80% right, see if now it=E2=80=
=99s 90%+. If latency was an issue, see if users now feel it=E2=80=99s snap=
py. Also do an internal stress test: run a script to send, say, 50 queries =
in a short burst =E2=80=93 does it still respond correctly and quickly? If =
any fail or slow down massively, address that (maybe need concurrency handl=
ing or rate limiting).</span></p></li></ul><p style=3D"margin: 0 0 20px 0;c=
olor: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>Month 3 (We=
eks 9-12): Scale &amp; Integrate for Real-world Use</strong></p><p style=3D=
"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;=
"><strong>Goal:</strong><span> Deploy at full scale and integrate into busi=
ness workflow; establish monitoring and continuous improvement loop.</span>=
</p><ul style=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32=
px;mso-special-format: bullet;"><p style=3D"margin: 0 0 20px 0;color: rgb(5=
4,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-=
left: 4px;font-size: 16px;"><strong>Tasks:</strong></p><ul style=3D"margin-=
top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-special-format: b=
ullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0=
;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">Deplo=
y the system to production environment (cloud or on-prem). Index all requir=
ed data (full corpus).</p></li><li style=3D"margin: 8px 0 0 32px;mso-specia=
l-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margi=
n-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin=
: 0;">Integrate with existing systems: e.g., link it on your website, or en=
able it for support agents in their console, etc., as appropriate.</p></li>=
<li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"=
color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-=
box;padding-left: 4px;font-size: 16px;margin: 0;">Set up monitoring dashboa=
rds for usage, accuracy signals (like user feedback), latency, and costs. U=
se real user feedback mechanism (thumbs up/down).</p></li><li style=3D"marg=
in: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,=
55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left:=
 4px;font-size: 16px;margin: 0;">Train users/staff as needed (=E2=80=9CHere=
=E2=80=99s how to ask the AI, here=E2=80=99s what it can/can=E2=80=99t do=
=E2=80=9D).</p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: b=
ullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0=
;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">Creat=
e an evaluation schedule: e.g., review logs weekly to catch any bad answers=
 and feed that back (either by adjusting data or prompt or adding those cas=
es to a training set).</p></li></ul></li><li style=3D"margin: 8px 0 0 32px;=
mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: =
26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 1=
6px;margin: 0;"><strong>Success Metric:</strong><span> By end of 90 days, y=
ou have a live RAG-powered feature with actual users. Key metrics could be:=
 X daily active users interacting with it, Y% of feedback is positive, Z mi=
nutes saved on average per query (if measurable). Essentially, a measurable=
 positive impact on whatever process you targeted.</span></p></li><li style=
=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rg=
b(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;paddi=
ng-left: 4px;font-size: 16px;margin: 0;"><strong>Checkpoint assessment:</st=
rong><span> After 2-4 weeks of production use, produce a brief report (even=
 if informal): how often is it used, what are outcomes? For example, =E2=80=
=9COur support bot deflected 50% of tier-1 questions in the first month, fr=
eeing up 100 hours of agent time=E2=80=9D or =E2=80=9CInternal tool answere=
d 200 queries with 95% accuracy as rated by staff, saving numerous email ex=
changes.=E2=80=9D If the metrics align with success criteria set by stakeho=
lders, congrats =E2=80=93 you=E2=80=99ve delivered. If not, identify why: i=
s accuracy still lacking on some edge cases? Are users not adopting it (may=
be need to improve UX or training)? Use these insights to iterate further.<=
/span></p></li></ul><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);lin=
e-height: 26px;font-size: 16px;">This 90-day plan is aggressive but realist=
ic for many scenarios. The key is iterative development and constant feedba=
ck. Don=E2=80=99t aim for perfect out of the gate; get something usable, th=
en refine. Each checkpoint ensures you=E2=80=99re not going down a wrong pa=
th too long without correction.</p><p style=3D"margin: 0 0 20px 0;color: rg=
b(54,55,55);line-height: 26px;font-size: 16px;">By following this plan, in =
3 months you=E2=80=99ll not only have a working solution but also the confi=
dence of stakeholders (seeing progress and metrics) and the foundation for =
continuous improvement. RAG projects aren=E2=80=99t =E2=80=9Cset and forget=
=E2=80=9D =E2=80=93 but after 90 days, you=E2=80=99ll have the infrastructu=
re to keep making it better and the success to justify that effort.</p><p s=
tyle=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size=
: 16px;"><span>And with that, you=E2=80=99re equipped to </span><em>build t=
he future, one retrieval-augmented step at a time</em><span>.</span></p><di=
v style=3D"font-size: 16px;line-height: 26px;"><hr style=3D"margin: 32px 0;=
padding: 0;height: 1px;background: #e6e6e6;border: none;"></div><p style=3D=
"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;=
"><em>Congratulations!</em><span> You=E2=80=99ve journeyed from 0 to RAG (a=
nd maybe to 5K and beyond). We=E2=80=99ve covered why it matters, how it wo=
rks, how to build it, and where it=E2=80=99s all headed. The companies winn=
ing in this new era aren=E2=80=99t necessarily those with the biggest model=
s, but those who best harness </span><strong>their own knowledge with AI</s=
trong><span>. RAG is how you give your AI that =E2=80=9Cperfect memory=E2=
=80=9D and tie it into your world.</span></p><p style=3D"margin: 0 0 20px 0=
;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>The questio=
n now isn=E2=80=99t </span><em>whether</em><span> to start using these tool=
s, but </span><strong>whether you=E2=80=99ll start before your competition =
does</strong><span>. So grab this guide, assemble your toolkit, and start b=
uilding. Let your AI </span><em>know</em><span> your business inside and ou=
t =E2=80=93 make it your smartest team member.</span></p><p style=3D"margin=
: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><stro=
ng>Ready to join hundreds of builders mastering RAG and other technologies?=
</strong><span> Feel free to </span><a href=3D"https://substack.com/redirec=
t/d6462442-aab1-47fa-924d-1c50661df655?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmL=
QJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,55);tex=
t-decoration: underline;">hop into the Nate=E2=80=99s Newsletter Discord</a=
><span> and let=E2=80=99s build the future together. The future is knocking=
 =E2=80=93 and now you have the keys.</span></p><div class=3D"subscription-=
widget-wrap" style=3D"font-size: 16px;line-height: 26px;"><div class=3D"sub=
scription-widget show-subscribe" style=3D"font-size: 16px;direction: ltr !i=
mportant;font-weight: 400;text-decoration: none;font-family: system-ui,-app=
le-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'=
Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';color: #363737;line-h=
eight: 1.5;max-width: 560px;margin: 24px auto;align-items: flex-start;displ=
ay: block;text-align: center;padding: 0px 32px;"><div class=3D"preamble" st=
yle=3D"margin-top: 16px;font-family: system-ui,-apple-system,BlinkMacSystem=
Font,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Sego=
e UI Emoji','Segoe UI Symbol';font-size: 18px;max-width: 384px;width: fit-c=
ontent;line-height: 22px;display: flex;align-items: center;text-align: cent=
er;font-weight: 400;margin-left: auto;margin-right: auto;"><p style=3D"marg=
in: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">For=
 more on AI, subscribe and share!</p></div><div class=3D"subscribe-widget i=
s-signed-up is-fully-subscribed" data-component-name=3D"SubscribeWidget" st=
yle=3D"margin: 0 0 1em;direction: ltr;font-size: 16px;line-height: 26px;"><=
div class=3D"pencraft pc-reset button-wrapper" style=3D"text-decoration: un=
set;list-style: none;font-size: 16px;line-height: 26px;text-align: center;c=
ursor: pointer;border-radius: 4px;"><a class=3D"button subscribe-btn outlin=
e" href=3D"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9uYXRlc25ld3Ns=
ZXR0ZXIuc3Vic3RhY2suY29tL2FjY291bnQiLCJwIjoxNjczMDcxOTUsInMiOjEzNzMyMzEsImY=
iOmZhbHNlLCJ1IjozMzY0NDgyMjMsImlhdCI6MTc1MTQ2MTc1NywiZXhwIjoyMDY3MDM3NzU3LC=
Jpc3MiOiJwdWItMCIsInN1YiI6ImxpbmstcmVkaXJlY3QifQ.u4Q2luvhcIBo8GK1n8VRPu9gEq=
e37IlJaISpFzNaVJg?" style=3D"font-family: system-ui,-apple-system,BlinkMacS=
ystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji',=
'Segoe UI Emoji','Segoe UI Symbol';display: inline-block;box-sizing: border=
-box;cursor: pointer;border-radius: 8px;font-size: 14px;line-height: 20px;f=
ont-weight: 600;text-align: center;background-color: transparent;opacity: 1=
;outline: none;white-space: nowrap;text-decoration: none !important;border:=
 1px solid #45d800;margin: 0 auto;background: transparent;color: #45d800;pa=
dding: 12px 20px;height: auto;"><img class=3D"check-icon static" src=3D"htt=
ps://substackcdn.com/image/fetch/$s_!3t53!,w_40,c_scale,f_png,q_auto:good,f=
l_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideCheck%3Fv%3D4=
%26height%3D40%26fill%3Dtransparent%26stroke%3D%252345D800%26strokeWidth%3D=
3.6" width=3D"20" height=3D"20" style=3D"border: none;vertical-align: middl=
e;-ms-interpolation-mode: bicubic;height: auto;display: inline-block;margin=
: -2px 8px 0 0;max-width: 20px" alt=3D""><span style=3D"text-decoration: no=
ne;">Subscribed</span></a></div></div></div></div><div class=3D"captioned-i=
mage-container-static" style=3D"font-size: 16px;line-height: 26px;margin: 3=
2px auto;"><figure style=3D"width: 100%;margin: 0 auto;"><table class=3D"im=
age-wrapper" width=3D"100%" border=3D"0" cellspacing=3D"0" cellpadding=3D"0=
" data-component-name=3D"Image2ToDOMStatic" style=3D"mso-padding-alt: 1em 0=
 1.6em;"><tbody><tr><td style=3D"text-align: center;"></td><td class=3D"con=
tent" align=3D"left" width=3D"1024" style=3D"text-align: center;"><a class=
=3D"image-link" target=3D"_blank" href=3D"https://substack.com/redirect/36b=
bc109-509e-46f8-8388-dad10aa41512?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9h=
shgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"position: relative;flex-direc=
tion: column;align-items: center;padding: 0;width: auto;height: auto;border=
: none;text-decoration: none;display: block;margin: 0;"><img class=3D"wide-=
image" data-attrs=3D"{&quot;src&quot;:&quot;https://substack-post-media.s3.=
amazonaws.com/public/images/1b491d78-2b90-482e-8422-92558f6fff61_1024x1024.=
png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot=
;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;=
resizeWidth&quot;:null,&quot;bytes&quot;:2468448,&quot;alt&quot;:null,&quot=
;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:n=
ull,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internal=
Redirect&quot;:&quot;https://natesnewsletter.substack.com/i/167307195?img=
=3Dhttps%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1=
b491d78-2b90-482e-8422-92558f6fff61_1024x1024.png&quot;,&quot;isProcessing&=
quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt=3D"" widt=
h=3D"550" height=3D"550" src=3D"https://substackcdn.com/image/fetch/$s_!1ns=
G!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsub=
stack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b491d78-2b90-482e-84=
22-92558f6fff61_1024x1024.png" style=3D"border: none !important;vertical-al=
ign: middle;display: block;-ms-interpolation-mode: bicubic;height: auto;mar=
gin-bottom: 0;width: auto !important;max-width: 100% !important;margin: 0 a=
uto;"></a></td><td style=3D"text-align: center;"></td></tr></tbody></table>=
</figure></div><h4 class=3D"header-anchor-post" style=3D"position: relative=
;font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-syst=
em,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple C=
olor Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-fo=
nt-smoothing: antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appe=
arance: optimizelegibility;-moz-appearance: optimizelegibility;appearance: =
optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height=
: 1.16em;font-size: 1.125em;margin-bottom: 0;"><a href=3D"https://substack.=
com/redirect/86534399-6e57-4a8b-b494-6347518f018f?j=3DeyJ1IjoiNWtiOTN6In0.z=
dzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54=
,55,55);text-decoration: underline;">Google Doc with Links for this article=
 is here</a></h4></div></div><div class=3D"container-border" style=3D"margi=
n: 32px 0 0;width: 100%;box-sizing: border-box;border-top: 1px solid #e6e6e=
6;font-size: 16px;line-height: 26px;"></div><div class=3D"post-cta typograp=
hy markup" style=3D"--image-offset-margin: -117px;text-align: initial;word-=
break: break-word;margin-bottom: 32px;margin: 32px 0;font-size: 16px;line-h=
eight: 26px;"><p class=3D"referrals-cta-text" style=3D"color: rgb(54,55,55)=
;text-align: center;width: 90%;line-height: 26px;font-size: 16px;margin-top=
: 0;max-width: 384px;margin: auto"></p><h4 style=3D"font-family: 'SF Pro Di=
splay',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'S=
egoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Em=
oji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoothing: antialiase=
d;-moz-osx-font-smoothing: antialiased;-webkit-appearance: optimizelegibili=
ty;-moz-appearance: optimizelegibility;appearance: optimizelegibility;margi=
n: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-size: 1.12=
5em;text-align: center">Invite your friends and earn rewards</h4><div style=
=3D"line-height: 26px;text-align: center;font-size: 14px">If you enjoy Nate=
=E2=80=99s Substack, share it with your friends and earn rewards when they =
subscribe.</div><p style=3D"color: rgb(54,55,55);margin: 0 auto 20px;text-a=
lign: center;width: 90%;line-height: 26px;font-size: 16px;"></p><p class=3D=
"cta-box" style=3D"color: rgb(54,55,55);margin: 0 auto 20px;width: 90%;line=
-height: 26px;font-size: 16px;margin-bottom: 0;text-align: center;margin-le=
ft: auto;margin-right: auto;"><a class=3D"button primary" role=3D"button" h=
ref=3D"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9uYXRlc25ld3NsZXR0=
ZXIuc3Vic3RhY2suY29tL2xlYWRlcmJvYXJkP3JlZmVycmVyX3Rva2VuPTVrYjkzeiZyPTVrYjk=
zeiZ1dG1fY2FtcGFpZ249ZW1haWwtbGVhZGVyYm9hcmQiLCJwIjoxNjczMDcxOTUsInMiOjEzNz=
MyMzEsImYiOmZhbHNlLCJ1IjozMzY0NDgyMjMsImlhdCI6MTc1MTQ2MTc1NywiZXhwIjoyMDY3M=
DM3NzU3LCJpc3MiOiJwdWItMCIsInN1YiI6ImxpbmstcmVkaXJlY3QifQ.9TU6DhcXakvg_KV6a=
my5PKsv3vokRZ8fFvFpkrMSkao?&amp;utm_source=3Dsubstack&amp;utm_medium=3Demai=
l&amp;utm_content=3Dpostcta" style=3D"font-family: system-ui,-apple-system,=
BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Colo=
r Emoji','Segoe UI Emoji','Segoe UI Symbol';display: inline-block;box-sizin=
g: border-box;cursor: pointer;border: none;height: 40px;border-radius: 8px;=
font-size: 14px;line-height: 20px;font-weight: 600;text-align: center;paddi=
ng: 10px 20px;margin: 0;opacity: 1;outline: none;white-space: nowrap;color:=
 #ffffff !important;text-decoration: none !important;background-color: #45D=
800;">Invite Friends</a></p></div><table class=3D"email-ufi-2-bottom" role=
=3D"presentation" width=3D"100%" border=3D"0" cellspacing=3D"0" cellpadding=
=3D"0" style=3D"border-top: 1px solid rgb(0,0,0,.1);border-bottom: 1px soli=
d rgb(0,0,0,.1);min-width: 100%;"><tbody><tr height=3D"16"><td height=3D"16=
" style=3D"font-size:0px;line-height:0;">&nbsp;</td></tr><tr><td><table rol=
e=3D"presentation" width=3D"100%" border=3D"0" cellspacing=3D"0" cellpaddin=
g=3D"0"><tbody><tr><td><table role=3D"presentation" width=3D"auto" border=
=3D"0" cellspacing=3D"0" cellpadding=3D"0" style=3D"margin:0 auto;"><tbody>=
<tr><td style=3D"vertical-align:middle;"><table role=3D"presentation" width=
=3D"auto" border=3D"0" cellspacing=3D"0" cellpadding=3D"0"><tbody><tr><td a=
lign=3D"center"><a class=3D"email-button-outline" href=3D"https://substack.=
com/app-link/post?publication_id=3D1373231&amp;post_id=3D167307195&amp;utm_=
source=3Dsubstack&amp;isFreemail=3Dfalse&amp;submitLike=3Dtrue&amp;token=3D=
eyJ1c2VyX2lkIjozMzY0NDgyMjMsInBvc3RfaWQiOjE2NzMwNzE5NSwicmVhY3Rpb24iOiLinaQ=
iLCJpYXQiOjE3NTE0NjE3NTcsImV4cCI6MTc1NDA1Mzc1NywiaXNzIjoicHViLTEzNzMyMzEiLC=
JzdWIiOiJyZWFjdGlvbiJ9.4puYxFsBJs1W8tgxGFGUIA5oGWQbYWmbwn34V6zF3-I&amp;utm_=
medium=3Demail&amp;utm_campaign=3Demail-reaction&amp;r=3D5kb93z" style=3D"f=
ont-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,He=
lvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Sym=
bol';display: inline-block;font-weight: 500;border: 1px solid rgb(0,0,0,.1)=
;border-radius: 9999px;text-transform: uppercase;font-size: 12px;line-heigh=
t: 12px;padding: 9px 14px;text-decoration: none;color: rgb(119,119,119);"><=
img class=3D"icon" src=3D"https://substackcdn.com/image/fetch/$s_!PeVs!,w_3=
6,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com=
%2Ficon%2FLucideHeart%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%25238=
08080%26strokeWidth%3D2" width=3D"18" height=3D"18" style=3D"margin-right: =
8px;min-width: 18px;min-height: 18px;border: none;vertical-align: middle;ma=
x-width: 18px" alt=3D""><span class=3D"email-button-text" style=3D"vertical=
-align: middle;">Like</span></a></td></tr></tbody></table></td><td width=3D=
"8" style=3D"min-width: 8px"></td><td style=3D"vertical-align:middle;"><tab=
le role=3D"presentation" width=3D"auto" border=3D"0" cellspacing=3D"0" cell=
padding=3D"0"><tbody><tr><td align=3D"center"><a class=3D"email-button-outl=
ine" href=3D"https://substack.com/app-link/post?publication_id=3D1373231&am=
p;post_id=3D167307195&amp;utm_source=3Dsubstack&amp;utm_medium=3Demail&amp;=
isFreemail=3Dfalse&amp;comments=3Dtrue&amp;token=3DeyJ1c2VyX2lkIjozMzY0NDgy=
MjMsInBvc3RfaWQiOjE2NzMwNzE5NSwiaWF0IjoxNzUxNDYxNzU3LCJleHAiOjE3NTQwNTM3NTc=
sImlzcyI6InB1Yi0xMzczMjMxIiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.DEbLVammqe6LZm7gN=
OWDU7reHsmz8JjE-_W6P2WKOP0&amp;r=3D5kb93z&amp;utm_campaign=3Demail-half-mag=
ic-comments&amp;action=3Dpost-comment&amp;utm_source=3Dsubstack&amp;utm_med=
ium=3Demail" style=3D"font-family: system-ui,-apple-system,BlinkMacSystemFo=
nt,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe =
UI Emoji','Segoe UI Symbol';display: inline-block;font-weight: 500;border: =
1px solid rgb(0,0,0,.1);border-radius: 9999px;text-transform: uppercase;fon=
t-size: 12px;line-height: 12px;padding: 9px 14px;text-decoration: none;colo=
r: rgb(119,119,119);"><img class=3D"icon" src=3D"https://substackcdn.com/im=
age/fetch/$s_!x1tS!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/htt=
ps%3A%2F%2Fsubstack.com%2Ficon%2FLucideComments%3Fv%3D4%26height%3D36%26fil=
l%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2" width=3D"18" height=3D"1=
8" style=3D"margin-right: 8px;min-width: 18px;min-height: 18px;border: none=
;vertical-align: middle;max-width: 18px" alt=3D""><span class=3D"email-butt=
on-text" style=3D"vertical-align: middle;">Comment</span></a></td></tr></tb=
ody></table></td><td width=3D"8" style=3D"min-width: 8px"></td><td style=3D=
"vertical-align:middle;"><table role=3D"presentation" width=3D"auto" border=
=3D"0" cellspacing=3D"0" cellpadding=3D"0"><tbody><tr><td align=3D"center">=
<a class=3D"email-button-outline" href=3D"https://substack.com/redirect/2/e=
yJlIjoiaHR0cHM6Ly9vcGVuLnN1YnN0YWNrLmNvbS9wdWIvbmF0ZXNuZXdzbGV0dGVyL3AvcmFn=
LXRoZS1jb21wbGV0ZS1ndWlkZS10by1yZXRyaWV2YWw_dXRtX3NvdXJjZT1zdWJzdGFjayZ1dG1=
fbWVkaXVtPWVtYWlsJnV0bV9jYW1wYWlnbj1lbWFpbC1yZXN0YWNrLWNvbW1lbnQmYWN0aW9uPX=
Jlc3RhY2stY29tbWVudCZyPTVrYjkzeiZ0b2tlbj1leUoxYzJWeVgybGtJam96TXpZME5EZ3lNa=
k1zSW5CdmMzUmZhV1FpT2pFMk56TXdOekU1TlN3aWFXRjBJam94TnpVeE5EWXhOelUzTENKbGVI=
QWlPakUzTlRRd05UTTNOVGNzSW1semN5STZJbkIxWWkweE16Y3pNak14SWl3aWMzVmlJam9pY0c=
5emRDMXlaV0ZqZEdsdmJpSjkuREViTFZhbW1xZTZMWm03Z05PV0RVN3JlSHNtejhKakUtX1c2UD=
JXS09QMCIsInAiOjE2NzMwNzE5NSwicyI6MTM3MzIzMSwiZiI6ZmFsc2UsInUiOjMzNjQ0ODIyM=
ywiaWF0IjoxNzUxNDYxNzU3LCJleHAiOjIwNjcwMzc3NTcsImlzcyI6InB1Yi0wIiwic3ViIjoi=
bGluay1yZWRpcmVjdCJ9._sxeD-y3fAwpqDsrfTG7bL5N6G0jfo6fEm0z295WoG4?&amp;utm_s=
ource=3Dsubstack&amp;utm_medium=3Demail" style=3D"font-family: system-ui,-a=
pple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif=
,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';display: inline-blo=
ck;font-weight: 500;border: 1px solid rgb(0,0,0,.1);border-radius: 9999px;t=
ext-transform: uppercase;font-size: 12px;line-height: 12px;padding: 9px 14p=
x;text-decoration: none;color: rgb(119,119,119);"><img class=3D"icon" src=
=3D"https://substackcdn.com/image/fetch/$s_!5EGt!,w_36,c_scale,f_png,q_auto=
:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FNoteForwardI=
con%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidt=
h%3D2" width=3D"18" height=3D"18" style=3D"margin-right: 8px;min-width: 18p=
x;min-height: 18px;border: none;vertical-align: middle;max-width: 18px" alt=
=3D""><span class=3D"email-button-text" style=3D"vertical-align: middle;">R=
estack</span></a></td></tr></tbody></table></td></tr></tbody></table></td><=
td align=3D"right"><table role=3D"presentation" width=3D"auto" border=3D"0"=
 cellspacing=3D"0" cellpadding=3D"0"><tbody><tr></tr></tbody></table></td><=
/tr></tbody></table></td></tr><tr height=3D"16"><td height=3D"16" style=3D"=
font-size:0px;line-height:0;">&nbsp;</td></tr></tbody></table><div class=3D=
"footer footer-ZM59BM" style=3D"color: rgb(119,119,119);text-align: center;=
font-size: 16px;line-height: 26px;padding: 24px0;"><div style=3D"font-size:=
 16px;line-height: 26px;padding-bottom: 24px"><p class=3D"pencraft pc-reset=
 color-secondary-ls1g8s size-12-mmZ61m reset-IxiVJZ small meta-B2bqa5" styl=
e=3D"list-style: none;font-family: system-ui,-apple-system,BlinkMacSystemFo=
nt,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe =
UI Emoji','Segoe UI Symbol';padding-bottom: 0;font-size: 12px;line-height: =
16px;margin: 0;color: rgb(119,119,119);text-decoration: unset;">=C2=A9 2025=
 <span>Nate</span><br>548 Market Street PMB 72296, San Francisco, CA 94104 =
<br><a href=3D"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9uYXRlc25l=
d3NsZXR0ZXIuc3Vic3RhY2suY29tL2FjdGlvbi9kaXNhYmxlX2VtYWlsP3Rva2VuPWV5SjFjMlZ=
5WDJsa0lqb3pNelkwTkRneU1qTXNJbkJ2YzNSZmFXUWlPakUyTnpNd056RTVOU3dpYVdGMElqb3=
hOelV4TkRZeE56VTNMQ0psZUhBaU9qRTNPREk1T1RjM05UY3NJbWx6Y3lJNkluQjFZaTB4TXpje=
k1qTXhJaXdpYzNWaUlqb2laR2x6WVdKc1pWOWxiV0ZwYkNKOS45bjJfX2pQZEZxUmZla1B2RG1m=
VXF5NzEyd1J4dlV3LTMtal92TVczX1RnIiwicCI6MTY3MzA3MTk1LCJzIjoxMzczMjMxLCJmIjp=
mYWxzZSwidSI6MzM2NDQ4MjIzLCJpYXQiOjE3NTE0NjE3NTcsImV4cCI6MjA2NzAzNzc1NywiaX=
NzIjoicHViLTAiLCJzdWIiOiJsaW5rLXJlZGlyZWN0In0.LNzLtWbJtsha5vdnyC5gk4nLA_vqp=
hwHNg4KQvULLjw?" style=3D"text-decoration: underline;color: rgb(119,119,119=
);"><span style=3D"color: rgb(119,119,119);text-decoration: underline;">Uns=
ubscribe</span></a></p></div><p class=3D"footerSection-EHR0jG small powered=
-by-substack" style=3D"padding: 0 24px;font-size: 12px;line-height: 20px;ma=
rgin: 0;color: rgb(119,119,119);font-family: system-ui,-apple-system,BlinkM=
acSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoj=
i','Segoe UI Emoji','Segoe UI Symbol';padding-bottom: 0;margin-top: 0;"><a =
href=3D"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9zdWJzdGFjay5jb20=
vc2lnbnVwP3V0bV9zb3VyY2U9c3Vic3RhY2smdXRtX21lZGl1bT1lbWFpbCZ1dG1fY29udGVudD=
1mb290ZXImdXRtX2NhbXBhaWduPWF1dG9maWxsZWQtZm9vdGVyJmZyZWVTaWdudXBFbWFpbD1la=
XRhbkBlaXNsYXcuY28uaWwmcj01a2I5M3oiLCJwIjoxNjczMDcxOTUsInMiOjEzNzMyMzEsImYi=
OmZhbHNlLCJ1IjozMzY0NDgyMjMsImlhdCI6MTc1MTQ2MTc1NywiZXhwIjoyMDY3MDM3NzU3LCJ=
pc3MiOiJwdWItMCIsInN1YiI6ImxpbmstcmVkaXJlY3QifQ.SITQSskPRGskBbBGc8-4X_4JcVu=
IRFEI_fj_4stJ1fE?" style=3D"color: rgb(119,119,119);text-decoration: none;d=
isplay: inline-block;margin: 0 4px;"><img src=3D"https://substackcdn.com/im=
age/fetch/$s_!LkrL!,w_270,c_limit,f_auto,q_auto:good,fl_progressive:steep/h=
ttps%3A%2F%2Fsubstack.com%2Fimg%2Femail%2Fpublish-button%402x.png" srcset=
=3D"https://substackcdn.com/image/fetch/$s_!wgfj!,w_135,c_limit,f_auto,q_au=
to:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Femail%2Fpub=
lish-button.png, https://substackcdn.com/image/fetch/$s_!LkrL!,w_270,c_limi=
t,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%=
2Femail%2Fpublish-button%402x.png 2x, https://substackcdn.com/image/fetch/$=
s_!KjtY!,w_405,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%=
2Fsubstack.com%2Fimg%2Femail%2Fpublish-button%403x.png 3x" width=3D"135" al=
t=3D"Start writing" height=3D"40" style=3D"max-width: 550px;border: none !i=
mportant;vertical-align: middle;"></a></p></div></div></td><td></td></tr></=
tbody></table><img src=3D"https://eotrx.substackcdn.com/open?token=3DeyJtIj=
oiPDIwMjUwNzAyMTMwMzEzLjMuNTkxMDdkZTg5ZGViMTI2ZkBtZzEuc3Vic3RhY2suY29tPiIsI=
nUiOjMzNjQ0ODIyMywiciI6ImVpdGFuQGVpc2xhdy5jby5pbCIsImQiOiJtZzEuc3Vic3RhY2su=
Y29tIiwicCI6MTY3MzA3MTk1LCJ0IjoicG9kY2FzdCIsImEiOiJvbmx5X3BhaWQiLCJzIjoxMzc=
zMjMxLCJjIjoicG9zdCIsImYiOmZhbHNlLCJwb3NpdGlvbiI6ImJvdHRvbSIsImlhdCI6MTc1MT=
Q2MTc1OCwiZXhwIjoxNzU0MDUzNzU4LCJpc3MiOiJwdWItMCIsInN1YiI6ImVvIn0.qGpV1HyvZ=
N8ENIcs_9TUGnOaFI3y4g-n3BHPWAPUeZ8" alt=3D"" width=3D"1" height=3D"1" borde=
r=3D"0" style=3D"height:1px !important;width:1px !important;border-width:0 =
!important;margin-top:0 !important;margin-bottom:0 !important;margin-right:=
0 !important;margin-left:0 !important;padding-top:0 !important;padding-bott=
om:0 !important;padding-right:0 !important;padding-left:0 !important;"><img=
 width=3D"1" height=3D"1" alt=3D"" src=3D"https://email.mg1.substack.com/o/=
eJw8kFGq5CAQRVfTfgbLipp8uJZQ0eoeGaMhltNk90O_hvd7DhwuN5Lwq113OFsXlYL2JvpdcQB=
vYXbgnVN8UC7biytfJJw2kl9rFwBQfwKaFB2tHrxeluTJWoprpAVneroZQeVgtLHaawOoEXDCya=
6gfeJlTbyDcc_HrI8XTH3sXSj-nWI71GfVRiNlrpFDq-XeTsrpy3MK4DxqD6v9ErlPDmdLkbqoc=
-xbbMcxapZ740p74RTkGvxRJUeS3OpPBT0aBHUFzkL1MWvOvdB7im3KRfWxp3ZQrqGScK_87oVF=
-FLyvW10vj4dRDfPizGo_gXzPwAA__89c3Dm"></body></html>=

--ebe98e78b0216adcedd09930bab97482a73c6559294177601adf396a187f--
