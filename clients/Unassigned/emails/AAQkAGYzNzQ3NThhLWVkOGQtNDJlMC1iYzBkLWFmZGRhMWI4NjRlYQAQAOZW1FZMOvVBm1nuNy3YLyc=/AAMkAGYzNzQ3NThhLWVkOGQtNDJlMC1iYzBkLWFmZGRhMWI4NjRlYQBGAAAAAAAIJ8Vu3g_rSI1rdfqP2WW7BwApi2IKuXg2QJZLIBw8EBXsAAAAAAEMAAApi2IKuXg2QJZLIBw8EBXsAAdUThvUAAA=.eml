Received: from PR3PR01MB6858.eurprd01.prod.exchangelabs.com
 (2603:10a6:102:5d::21) by AS8PR01MB10319.eurprd01.prod.exchangelabs.com with
 HTTPS; Mon, 7 Jul 2025 13:04:13 +0000
Received: from DU2PR04CA0241.eurprd04.prod.outlook.com (2603:10a6:10:28e::6)
 by PR3PR01MB6858.eurprd01.prod.exchangelabs.com (2603:10a6:102:5d::21) with
 Microsoft SMTP Server (version=TLS1_2,
 cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id 15.20.8901.26; Mon, 7 Jul
 2025 13:04:01 +0000
Received: from DB3PEPF00008859.eurprd02.prod.outlook.com
 (2603:10a6:10:28e:cafe::57) by DU2PR04CA0241.outlook.office365.com
 (2603:10a6:10:28e::6) with Microsoft SMTP Server (version=TLS1_3,
 cipher=TLS_AES_256_GCM_SHA384) id 15.20.8901.26 via Frontend Transport; Mon,
 7 Jul 2025 13:04:01 +0000
Authentication-Results: spf=pass (sender IP is 161.38.202.234)
 smtp.mailfrom=mg1.substack.com; dkim=pass (signature was verified)
 header.d=mg1.substack.com;dmarc=pass action=none
 header.from=substack.com;compauth=pass reason=100
Received-SPF: Pass (protection.outlook.com: domain of mg1.substack.com
 designates 161.38.202.234 as permitted sender)
 receiver=protection.outlook.com; client-ip=161.38.202.234;
 helo=mg-202-234.substack.com; pr=C
Received: from mg-202-234.substack.com (161.38.202.234) by
 DB3PEPF00008859.mail.protection.outlook.com (10.167.242.4) with Microsoft
 SMTP Server (version=TLS1_2, cipher=TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384) id
 15.20.8901.15 via Frontend Transport; Mon, 7 Jul 2025 13:04:00 +0000
DKIM-Signature: a=rsa-sha256; v=1; c=relaxed/relaxed; d=mg1.substack.com; q=dns/txt; s=mailo; t=1751893439; x=1751900639;
 h=List-Unsubscribe-Post: List-Unsubscribe: List-Post: List-Id: List-Archive: List-Owner: Reply-To: In-Reply-To: References: sender: sender: Date: Message-Id: To: To: From: From: Subject: Subject: Content-Type: Mime-Version;
 bh=rgfANHF3Zbn7Ko3TcIxcjJ6CfFX5JmdG/KgO5IbBTfs=;
 b=HbaONxfEGNOAQkn35dzE7ntDpgeG7Q+UrTakt8GlU0c6y2PdPPp0zxEpbuWFSekWgJ4kTL5G89r/h0cvimCSdvSeDzIhWIAYrsa5ZaIUPzMYpEstjw5I0zsA1/JUJZnWYxrPsT+vaaQnpTB9RpzwfvXoHDJE5Wrhsq+K7PoaHpE=
X-Mailgun-Sid: WyI3NTNmNCIsImVpdGFuQGVpc2xhdy5jby5pbCIsIjA3MmM3YiJd
Received: by 1abd7fd4f1fb with HTTP id 686bc5bfda78c78aefc469d9; Mon, 07 Jul 2025
 13:03:57 GMT
X-Mailgun-Sending-Ip: 161.38.202.234
X-Mailgun-Batch-Id: 686bc5be7fadfa8ab32bb8a9
Content-Type: multipart/alternative;
 boundary="d4a764290111796f177397fbcc44f39587ce16a92695f81b46e9ced34d81"
Subject: =?UTF-8?q?Context_Windows_Are_a_Lie:_The_Myth_Blocking_AGI=E2=80=94And_Ho?=
 =?UTF-8?q?w_to_Fix_It?=
From: =?utf-8?b?TmF0ZSBmcm9tIE5hdGXigJlzIFN1YnN0YWNr?=
 <natesnewsletter@substack.com>
To: eitan@eislaw.co.il
X-Mailgun-Tag: post
X-Mailgun-Track-Clicks: false
Message-Id: <20250707130206.3.cc5c0f0be212c8d8@mg1.substack.com>
Date: Mon, 7 Jul 2025 13:02:06 +0000
Feedback-ID: post-167683527:cat-post:pub-1373231:substack
sender: =?utf-8?b?TmF0ZSBmcm9tIE5hdGXigJlzIFN1YnN0YWNr?=
 <natesnewsletter@substack.com>
References: <post-167683527@substack.com>
In-Reply-To: <post-167683527@substack.com>
Reply-To: =?utf-8?b?TmF0ZSBmcm9tIE5hdGXigJlzIFN1YnN0YWNr?=
 <reply+2ru1fr&5kb93z&&3e3394af44a41c1fcd6c5d6c5b8ff8c36a8332f8ae399ae103c37f9209488f08@mg1.substack.com>
List-Owner: <mailto:natesnewsletter@substack.com>
List-URL: <https://natesnewsletter.substack.com/>
List-Archive: <https://natesnewsletter.substack.com/archive>
List-Id: <natesnewsletter.substack.com>
List-Post: <https://natesnewsletter.substack.com/p/context-windows-are-a-lie-the-myth>
List-Unsubscribe: <https://natesnewsletter.substack.com/action/disable_email/disable?token=eyJ1c2VyX2lkIjozMzY0NDgyMjMsInBvc3RfaWQiOjE2NzY4MzUyNywiaWF0IjoxNzUxODkzNDM3LCJleHAiOjE3ODM0Mjk0MzcsImlzcyI6InB1Yi0xMzczMjMxIiwic3ViIjoiZGlzYWJsZV9lbWFpbCJ9.Cy3sfDrEFt70ucEy0bggiIJebv66p9gnKdyIL8T-8L8&all_sections=true>
List-Unsubscribe-Post: List-Unsubscribe=One-Click
X-Mailgun-Variables: {"category": "post", "email_generated_at": "1751893437869", "post_audience":
 "only_paid", "post_id": "167683527", "post_type": "podcast",
 "pub_community_enabled": "true", "publication_id": "1373231", "subdomain":
 "natesnewsletter", "user_id": "336448223"}
Return-Path: bounce+bdbcee.072c7b-eitan=eislaw.co.il@mg1.substack.com
X-MS-Exchange-Organization-ExpirationStartTime: 07 Jul 2025 13:04:01.4244
 (UTC)
X-MS-Exchange-Organization-ExpirationStartTimeReason: OriginalSubmit
X-MS-Exchange-Organization-ExpirationInterval: 1:00:00:00.0000000
X-MS-Exchange-Organization-ExpirationIntervalReason: OriginalSubmit
X-MS-Exchange-Organization-Network-Message-Id: b10dbf36-1b70-43ce-3c3a-08ddbd56be50
X-EOPAttributedMessage: 0
X-EOPTenantAttributedMessage: 384c4129-e818-4ea7-8f8b-189d997170d1:0
X-MS-Exchange-Organization-MessageDirectionality: Incoming
X-MS-PublicTrafficType: Email
X-MS-TrafficTypeDiagnostic: DB3PEPF00008859:EE_|PR3PR01MB6858:EE_|AS8PR01MB10319:EE_
X-MS-Exchange-Organization-AuthSource: DB3PEPF00008859.eurprd02.prod.outlook.com
X-MS-Exchange-Organization-AuthAs: Anonymous
X-MS-Office365-Filtering-Correlation-Id: b10dbf36-1b70-43ce-3c3a-08ddbd56be50
X-MS-Exchange-Organization-SCL: 1
X-Microsoft-Antispam: BCL:3;ARA:13230040|69100299015|12012899012|2092899012|4022899009|13003099007|8096899003;
X-Forefront-Antispam-Report: CIP:161.38.202.234;CTRY:US;LANG:en;SCL:1;SRV:;IPV:NLI;SFV:NSPM;H:mg-202-234.substack.com;PTR:mg-202-234.substack.com;CAT:NONE;SFS:(13230040)(69100299015)(12012899012)(2092899012)(4022899009)(13003099007)(8096899003);DIR:INB;
X-MS-Exchange-CrossTenant-OriginalArrivalTime: 07 Jul 2025 13:04:00.8843
 (UTC)
X-MS-Exchange-CrossTenant-Network-Message-Id: b10dbf36-1b70-43ce-3c3a-08ddbd56be50
X-MS-Exchange-CrossTenant-Id: 384c4129-e818-4ea7-8f8b-189d997170d1
X-MS-Exchange-CrossTenant-AuthSource: DB3PEPF00008859.eurprd02.prod.outlook.com
X-MS-Exchange-CrossTenant-AuthAs: Anonymous
X-MS-Exchange-CrossTenant-FromEntityHeader: Internet
X-MS-Exchange-Transport-CrossTenantHeadersStamped: PR3PR01MB6858
X-MS-Exchange-Transport-EndToEndLatency: 00:00:12.9172671
X-MS-Exchange-Processed-By-BccFoldering: 15.20.8901.023
X-Microsoft-Antispam-Mailbox-Delivery:
	ucf:0;jmr:0;auth:0;dest:I;ENG:(910005)(944506478)(944626604)(920097)(930097)(140003);
X-Microsoft-Antispam-Message-Info:
	=?utf-8?B?N2ROZ3J6YzBxM2IwekVwOFZyK3VuNGJmcFVFUit1WEdjY1ZrVHFsQUlweVFk?=
 =?utf-8?B?Tk1Dcmk2TE1sVUR6ZC9zb1dQTGVNUXhDYm5oZ25QODhVRGdoaTY3ckJjMWty?=
 =?utf-8?B?dm9pV3QyNW95L3JjVGJFKzRYdDBZU3pFQmh6Zzl2aXF2VlJ5WlFGdVpiS3Bn?=
 =?utf-8?B?M0ZUdUlBMXlmNVNjNldLcm5jeWNrcW85aW5hdHBQbTNBb0N5aVZrUXZUZldL?=
 =?utf-8?B?d3cwNnVWbGZqMzZROEtvUGNYcU54ckR1SmF2bGVyRnhlT3ptaFduU3JDdE9m?=
 =?utf-8?B?Y0NEayt3RjZBQkJzOHJYa09iOFNYd2QrKzNuT2tBcU41SHZvd3pPa0JuUXlQ?=
 =?utf-8?B?SjJWc3FxVEJud2x6Sk9KTnV3TFFiT0loU1dPR0swZk05aXl1cnE3WVJYR1BE?=
 =?utf-8?B?bGFiZHFjQkJvTWZqTDR5SFZiTFEybCtCS0ZuK1hKcStXbHlWdHhXNDJzaGoz?=
 =?utf-8?B?WDYwb0Y3dVMzbk1mSkpPZ2VPNUViM1F5eDBIS3dJZTBHby9NanpSSVRHSmg0?=
 =?utf-8?B?dFA1ZXFjR2tlWjBpVkpUaFViWEhlUFIvMTNKeCtXNkE5WWVCeGtSRGpmaGk1?=
 =?utf-8?B?YkptSFJJVGJ2REhPdDFyT0RxTFhnODZHV1dTTWNlbFROWGtNK3lBTnEwSHFV?=
 =?utf-8?B?ODN5TVpwMWlFaUZKY2I0TDd4d3dYR05iMXdPNWI0OVRNYkJFTEEzU0w3WUkz?=
 =?utf-8?B?bE4vZWZFUFFwaU91M0dpVWZwT01hdVBpQ0NqU2loN1QvSVhVYllSdGdiVGJh?=
 =?utf-8?B?Mm1vZS94QWZMM0JicVpIVkxWU3FMWlVnc1FGeFJvU0dLZVp3OXBRMTdwZXJN?=
 =?utf-8?B?eG5jQXdUUWtqWEFKVnhXVENET1F6MVh2bnEyVVJqOHJEQWFQaWw5UllkK0RT?=
 =?utf-8?B?RllKZ1Y2dzZUWjVtdklETnJ4YTg1NXpvNjExK3NGeGVkQ2dxYnpVSHRyY3pu?=
 =?utf-8?B?NFh6aWlva1YvSTBxakgxcFhoZUluQm5DaytxaFk1VFVwREl5YjNiYlA5Nk5a?=
 =?utf-8?B?YjFTbGtka0RrM2hGcVZWNGZDTGp6cmp1emp3NTdHSTdzTzJacUsxdWRWbVIz?=
 =?utf-8?B?Y1BKMlZEbzBFSmUyM3hwMDVpL3RJYzNUR241T1NqYy9maytpOXphV2VWUHNK?=
 =?utf-8?B?UHppZkRIUWQ4SDRzK1dZakFVRVpwSXFCTy9OOGd5d1ZYUkdRL05oVktXckha?=
 =?utf-8?B?VXZwZzY0eHk5NGlOSGsxNkgyd2lJS0VvcHZRUzlOMUFjbVlPUTJ5aXhYcUFK?=
 =?utf-8?B?aGJFWmlVbTgvcU1FVnc1T0oxWWo1dTFvS28vRHd6TGwwSHFrc004MmVFTmRn?=
 =?utf-8?B?UVlIdS8xUHJMTnJEYXVURzJMVmYrOVdYL1UvSjFuT1hmdjJyRHJmc1k3ejVV?=
 =?utf-8?B?L2ZUNjB2dDBrSndHd0Z3QzMxc2R4UGRXTHlBdkZ0R2IyVTIwSXU5ajdqQjVs?=
 =?utf-8?B?UWNMKzdBSUxrOTN5b1NiMGRDa3hRM0tiNFhZZTVZeXpueElKQkZvL2IyRXk5?=
 =?utf-8?B?bWxCUnN0anI5SzlRdXgvRm1xM2FwZnJnamVleEVweVZNSDBIZ0lSbTFLNExh?=
 =?utf-8?B?UWVNZGd3VTgxVW1SL2svOWRxaGVXWXB1Vm9sMFlUZFNUWUg2K0F5emYvbjYy?=
 =?utf-8?B?WnNiZzZ0R09iTStodStxekhQTzlFeC9PRC9qZW5yWFVBMEpQUVRCNWFLKy8v?=
 =?utf-8?B?WlRvYXNvbG1CekVFVFo0aFVXckVKbGxxUjBJZ2Q0d2dQRTVHSXk3S1pGRy9a?=
 =?utf-8?B?S1JPK3NQYmNjUjdacjFBTmtSclhtTHR2N0NDVTVKazc4VlhSWE16RGVrQXhj?=
 =?utf-8?B?MjRJTHhxbmZUQWhEZTN3VEFsSUZXS21mcjhzUE5tSTZWeXFzaithRy9sVVlx?=
 =?utf-8?B?dFYxV0pLVmtyVFRDNHgzSFMvdnF5cFdURmFVZUJxcnNtWHVGTXQ1K1cyeVZZ?=
 =?utf-8?B?TFp2c3FnbklLZWVITnF5Szc5SXFSQkxpd2lsdFNXNWRveEcxQitvdjU0RzVv?=
 =?utf-8?B?amJ2aEtUSmdKZWptYlQvSCt4VXRQRkxKRTBrR1dQa3Q0TTFEbkpudFBzRURy?=
 =?utf-8?B?ZU1TanF1dVN4a3E1Z3pRSjd2TDAwUXlzZzJkUGk1OTlLd0pwZEdDd1pDMzBY?=
 =?utf-8?B?THZPZkM1WGd5NEpSb3gvdEozcXJjUUVQYnpKem5WV0hPMFVBVThzTFQycHBj?=
 =?utf-8?B?NUs4eDZOWWo5N2FpNHRJSEhPbG1hc0xPalg0cTlwOXBkWEJpWW5Bb2ZpdlBm?=
 =?utf-8?B?d1lYOTFqZTBzb3NnTlMxOTcwTFdmVkUyTEEzM3pEcmwrU3lHMmdxNUFZelc4?=
 =?utf-8?B?bElDRCtSQzBUVlFiY0Jud0JDNHJHU0JncGlBeVY3K2hyWXhDWVJOR3pZYWpW?=
 =?utf-8?B?bWN0akY5U1JLZFhJdW5KeS8vV0hWak9jQjdXWktCZGpPVzlUZGVlUDZxMmZr?=
 =?utf-8?B?Nm9DeEZJRGtRUWU0Q0JsWmhyczVuZXNHZXpzVUt2OWhadnp0bXFYY0x2cUZ2?=
 =?utf-8?B?VTdmZEpDRnVFb0pENWJxTkM1UTFNVGZzUTRRK1ZIWWpCb2l2bnVxV1JxR0s1?=
 =?utf-8?B?WEUxQWs5dmhsY0JFN3NJREZIMm9BK1VjR0cvNkhrd3hhd1UrMHJkRytoMXdn?=
 =?utf-8?B?MjhUODBlUW1taFpOSEJ5S3RBQ1pLYWp2Sml0cFpiYlhuRzJmOVFTUnI4QjZ3?=
 =?utf-8?B?VXZEZDZXRDE3THQrSG5HU0VnUlFzQ3NtRlRVaU9wSnhRcFFNWGRXbmdSVzVR?=
 =?utf-8?Q?Zk=3D?=
MIME-Version: 1.0

--d4a764290111796f177397fbcc44f39587ce16a92695f81b46e9ced34d81
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable

View this post on the web at https://natesnewsletter.substack.com/p/context=
-windows-are-a-lie-the-myth

I tend to get pretty turned off by AI hype, and my favorite example of AI h=
ype these days is AI context windows. So being me, I wrote a piece about wh=
at the heck is actually going on under the surface, why it works that way a=
rchitecturally, and how to fix it. Oh, and I threw some AGI reflections in =
there just for fun!
Why bother? I get a LOT of messages that basically boil down to =E2=80=9CI =
was promised a great context window that held all this stuff, and I put all=
 this stuff in it=E2=80=94and this supposedly smart model just crapped out =
on me.=E2=80=9D People think the problem is them, or the prompt. When it=E2=
=80=99s often mismanagement of the context window coupled with believing ve=
ndor hype. We=E2=80=99ll address both here.
Imagine this: You=E2=80=99ve finally convinced your CTO to approve a hefty =
investment in an AI solution that promises to understand, retain, and proce=
ss vast amounts of information effortlessly=E2=80=94=E2=80=9C1M tokens of c=
ontext!=E2=80=9D the vendor boasted. Everyone was excited, thinking you=E2=
=80=99ve just purchased the digital equivalent of a photographic memory. Th=
en, reality strikes. Important details from page 40 (or 400) of your critic=
al legal document vanish as if they=E2=80=99d never existed. The financial =
analysis that hinged on precise figures scattered throughout an extensive q=
uarterly report? Completely botched. Your AI is hallucinating, forgetting i=
nstructions, and leaving you wondering if you=E2=80=99ve made an expensive =
mistake.
You=E2=80=99re not alone. Welcome to the Context Window Trap.
It=E2=80=99s 2025, and we=E2=80=99ve arrived at a pivotal moment in AI wher=
e promises and practicalities are diverging rapidly. As you read this, coun=
tless teams worldwide are discovering the unsettling truth about long-conte=
xt AI models: they don=E2=80=99t quite work as advertised. This isn=E2=80=
=99t just an annoyance=E2=80=94it=E2=80=99s a fundamental barrier threateni=
ng to stall progress across industries from healthcare to finance, from leg=
al to software development.=20
What was sold was perfect memory, what we got was a fairly lossy semantic m=
eaning pattern matcher with big holes.
But why does this matter so urgently right now?
Because the problem isn=E2=80=99t just about models forgetting. It=E2=80=99=
s about what that forgetting reveals about the nature of AI itself. When yo=
ur =E2=80=9Ccutting-edge=E2=80=9D model with a 1M-token capacity behaves li=
ke it has just a fraction of that, it=E2=80=99s not simply a technical glit=
ch. It=E2=80=99s a window into the very essence of how these models functio=
n=E2=80=94and more importantly, how they don=E2=80=99t.
In this essential guide, we=E2=80=99re going to explore the uncomfortable r=
eality behind the =E2=80=9Clost in the middle=E2=80=9D phenomenon. You=E2=
=80=99ll understand exactly why models struggle to maintain coherence over =
extensive contexts, how their underlying architecture=E2=80=94built on prob=
abilistic attention mechanisms=E2=80=94sets them up for failure, and why sc=
aling up the context size isn=E2=80=99t the silver bullet vendors claim it =
to be.
But this guide is more than a wake-up call; it=E2=80=99s a practical bluepr=
int. Because despite it all, I definitely still see powerful AI systems eve=
ry day. AI is worth building=E2=80=94but as this newsletter emphasizes=E2=
=80=94you have to pay attention to the details to get the value you are loo=
king for out of AI! It is not a magic wand.=20
Anyway, inside I stuffed all the secret sauce for beating the context windo=
w problem. Yes, that means it=E2=80=99s one of my classic longer posts, but=
 you are human, and you can use smart strategies to get what you want out o=
f a full guide. I toyed with cutting it, and ultimately I figure you deserv=
e a full guide to one of the biggest problems in AI right now. It=E2=80=99s=
 too critical to get this right if you want to build systems that work.
What we're really talking about here is the difference between using AI and=
 architecting with AI. Any fool can yeet 100K tokens into a prompt. It take=
s understanding to know when to chunk, when to retrieve, when to summarize,=
 and when to tell the model exactly where to look. That's not a limitation =
=E2=80=93 that's craftsmanship.
The strategies in here are used by actual production teams and genuinely wo=
rk: intelligent chunking methods, smart retrieval systems, strategic summar=
ization chains, and more. We=E2=80=99ll walk through tailored industry-spec=
ific playbooks to tackle your exact use-cases, whether you=E2=80=99re navig=
ating intricate contracts in legal, dissecting dense financial reports, syn=
thesizing extensive healthcare records, or analyzing vast software codebase=
s.
We=E2=80=99ll also critically assess the cost implications of these oversiz=
ed contexts=E2=80=94real numbers and real scenarios=E2=80=94to ensure your =
CFO won=E2=80=99t have a heart attack when the bill arrives.
Ultimately, this is a call to action: The smartest teams aren=E2=80=99t wai=
ting for a magical =E2=80=9Cfix=E2=80=9D promised by future AI versions. Th=
ey=E2=80=99re proactively architecting around these fundamental limitations=
, adopting methods that don=E2=80=99t just sidestep the problem but activel=
y leverage AI=E2=80=99s strengths. Whether you=E2=80=99re a believer in the=
 potential for AGI or a skeptic seeing these systems as glorified pattern-m=
atchers, the strategies outlined here will significantly elevate your appro=
ach.
The Context Window Trap is real, it=E2=80=99s significant, and addressing i=
t isn=E2=80=99t optional=E2=80=94it=E2=80=99s critical. Let=E2=80=99s dive =
in and start building with the context windows we have=E2=80=94not the Cind=
erella context windows we=E2=80=99re promised lol
Subscribers get all these pieces!
TLDR: The Context Window Trap Matters More Than You Think
The revelation isn't that models forget. It's what the forgetting reveals.
When a 200K-token model performs like a 20K model, we're not looking at a b=
ug=E2=80=94we're looking at the fundamental nature of these systems. The "l=
ost in the middle" problem isn't an implementation detail. It's a window in=
to how these models actually work, and why the path to AGI might be nothing=
 like what we're building.
Consider what's really happening: When you feed a model 100K tokens, it doe=
sn't "read" them like you would. It performs 10 billion attention operation=
s, each asking "how related are these two tokens?" But attention is probabi=
listic, not deterministic. The model isn't maintaining a coherent mental mo=
del=E2=80=94it's playing a massive game of "what word comes next?" with wei=
ghted dice.
This is why position matters so much. The beginning tokens set the dice. Th=
e ending tokens are freshest when the model starts generating. Everything i=
n between? It's there, technically, but only in the way that page 1,847 of =
the phone book is "there" when you're looking for a number. The model can t=
heoretically access it, but won't.
The deeper problem: We've built systems that look like they understand but =
actually perform sophisticated pattern matching over sliding windows. When =
Thomson Reuters found that feeding more than 10 documents hurt performance,=
 they discovered something profound: these models don't build understanding=
=E2=80=94they sample from possibility space. More context doesn't mean deep=
er comprehension; it means more noise in the sampling process.
This explains why RAG works so well (I wrote a guide to RAG here [ https://=
substack.com/redirect/596f23bb-a80d-427d-9b7f-738e6538b7ce?j=3DeyJ1IjoiNWti=
OTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0 ]). A tiny 1.1B model w=
ith good retrieval beating GPT-4 Turbo [ https://substack.com/redirect/aec5=
0b08-7b89-401c-b5d6-06286d1af05a?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hs=
hgUC7BDnPG6RxQ4tAjMRMPOw0 ] isn't a quirk=E2=80=94it's revealing that intel=
ligence might be more about knowing where to look than holding everything i=
n your head. Human experts don't memorize entire libraries; they know exact=
ly which book, which chapter, which paragraph contains what they need. Soun=
d familiar? Oh and btw this does not get fixed with o3 Pro=E2=80=94I=E2=80=
=99ve caught o3 Pro getting confused by long documents too. This is a persi=
stent AI problem, and OpenAI and every other model maker have struggled wit=
h it. There are no good answers, partly because physics (we=E2=80=99ll get =
to that).
The AGI implications are stark: If we can't get a model to reliably track i=
nformation across a mere 100K tokens=E2=80=94about a book's worth=E2=80=94h=
ow exactly do we expect it to maintain coherent understanding across a life=
time of experience? The current architecture fundamentally cannot do what w=
e're asking. It's not a scale problem. It's like trying to build a skyscrap=
er with materials that can't support more than three floors=E2=80=94at some=
 point, you need different materials, not just more of them.
Here's the uncomfortable truth: The entire bet on LLMs achieving AGI is ess=
entially a bet that humans are lossy compression functions too=E2=80=94so m=
aybe it's close enough. We're betting that human intelligence is just very =
sophisticated pattern matching with strategic forgetting. That consciousnes=
s is compression. That understanding is actually just very good bluffing ba=
sed on statistical patterns.
And you know what? The context window problem suggests this bet might be...=
 wrong.
The economic reality reinforces this: The O(n=C2=B2) attention complexity i=
sn't just a computational annoyance=E2=80=94it's a fundamental barrier. Eve=
ry doubling of context quadruples compute. The universe doesn't have enough=
 energy to run true "full attention" at the scales AGI would require. Even =
if Moore's Law continued forever, we'd hit thermodynamic limits before achi=
eving human-scale coherent context.
So what's really going on? We've built brilliant compression algorithms tha=
t can mimic understanding within narrow windows. When you chat with GPT-4o,=
 you're not talking to an entity that "knows" things=E2=80=94you're interac=
ting with a system that can reconstruct plausible knowledge from compressed=
 patterns. The fact that it falls apart at scale reveals the trick. As I=E2=
=80=99ve written elsewhere, the AI is a vibe [ https://substack.com/redirec=
t/2b2a8ff6-15b4-4bf1-b164-00a76570a16a?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmL=
QJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0 ].
But humans don't fall apart this way. Yes, we forget. Yes, we compress. But=
 we maintain coherent models of reality across time. We can read a book and=
 understand how chapter 20 relates to chapter 1, even if we can't recite ei=
ther verbatim. We build persistent mental structures, not just statistical =
patterns. The difference between human "lossy" cognition and LLM "lossy" pr=
ocessing might be the difference between true intelligence and its simulacr=
um.
And there=E2=80=99s a difference between pre-trained weights and memory. I =
can get o3 to do decent literary analysis because it=E2=80=99s pre-trained =
on a lot of literature. If I throw an actual book in the context window the=
 success at reading carefully is markedly worse. Humans don=E2=80=99t have =
the fundamental gap between what we=E2=80=99ve read before and what we=E2=
=80=99re reading now. If anything, what we=E2=80=99re reading now is better=
 remembered.=20
Anyway, this is why the solutions work: When we chunk documents, build retr=
ieval systems, and carefully manage context, we're not working around a lim=
itation=E2=80=94we're building prosthetics for what these models fundamenta=
lly lack. Yes, even humans don't hold entire books in working memory. But w=
e do something LLMs don't: we build mental models that persist and evolve. =
We know that "the contract stuff is in section 5" not because we're doing a=
ttention operations, but because we've built a structured understanding.
The vendors can't admit this because it undermines the entire narrative. If=
 context windows reveal that LLMs are lossy in a fundamentally different wa=
y than humans=E2=80=94if they're compressing semantically without comprehen=
ding=E2=80=94then there=E2=80=99s a chance we're not "just a few breakthrou=
ghs" from AGI. We might just be playing poker with a deck that's missing a =
half dozen critical cards and pretending we just need to shuffle better.
What this means for builders: Let=E2=80=99s stop fighting the architecture =
and start embracing it. The companies winning aren't those waiting for 1M c=
ontext to "finally work"=E2=80=94they're those building systems that mirror=
 how intelligence actually operates:
Aggressive abstraction and summarization (like human memory=E2=80=94but ext=
ernally managed)
Pointer-based retrieval (compensating for lack of mental models)
Focused attention windows (the only thing these models do well)
External memory stores (replacing what should be internal coherence)
The philosophical punch: We've accidentally proven something profound. Eith=
er:
Intelligence isn't about holding all information simultaneously=E2=80=94it'=
s about knowing what to forget and when to remember it, OR
LLMs are lossy in a fundamentally different way than humans, and we're buil=
ding elaborate workarounds for their inability to form coherent mental mode=
ls
The context window problem forces us to pick a side. If you believe humans =
are just lossy functions too, then these workarounds are steps toward AGI. =
If you believe human intelligence involves something more=E2=80=94persisten=
t models, true understanding, conscious experience=E2=80=94then we're build=
ing increasingly sophisticated parrots.
The practical result: Every dollar you spend trying to force models to use =
massive context is a dollar spent fighting physics and philosophy. Every do=
llar spent on smart retrieval and abstraction is either:
A dollar spent aligning with how intelligence actually works, OR
A dollar spent building scaffolding around systems that have fundamental li=
mitations that are not getting fixed
The market is betting on option one. The context window problem suggests we=
=E2=80=99re probably not taking the possibility of option two seriously eno=
ugh. I don=E2=80=99t know which I go for right now: probably 40% of my mone=
y is on =E2=80=9Cthis is how intelligence actually works and we=E2=80=99re =
too proud to admit we=E2=80=99re this bad at memory as humans=E2=80=9D and =
60% is on =E2=80=9Cwell we have fundamental limitations with LLMs here that=
 mean we need new breakthroughs for AGI.=E2=80=9D
Regardless, the vendors will keep promising larger context windows. The res=
earch papers will keep showing U-shaped attention curves. But now you know =
what's really happening: we've built systems that compress and reconstruct =
meaning, not systems that understand it. Whether that's "close enough" for =
AGI depends on whether you think you're a lossy function too.
The most successful AI systems of the next decade won't be those with the l=
ongest context windows. They'll be those that gave up on the fantasy of per=
fect memory and either:
Embraced the reality of intelligent forgetting (if you're a believer), OR
Built the best prosthetics for systems that can't truly think (if you're a =
skeptic)
Either way, the strategy is the same: architect around the limitation. The =
philosophy behind why it works is what's up for debate, but that doesn=E2=
=80=99t stop you building. It shouldn=E2=80=99t! Even if the LLMs are =E2=
=80=9Chuman dumb" with context windows and this is a limitation that preven=
ts AGI, that gap won=E2=80=99t prevent AI from absolutely transforming ever=
y industry I can think of in the next couple decades. It=E2=80=99s already =
smart enough for that.=20
Plan accordingly. And maybe hedge your bets.=20
The Context Window Trap: Why Your 200K Token LLM Only Works Like It Has 20K=
 (And What To Do About It)
Every week, I get the same DMs: =E2=80=9CWhy does my model forget things I =
told it 50K tokens ago?=E2=80=9D =E2=80=9CClaude says it has 200K context b=
ut chokes on my 30-page document.=E2=80=9D =E2=80=9CWe paid for o3 Pro and =
it still can=E2=80=99t handle our use case.=E2=80=9D If you=E2=80=99ve been=
 promised 100K+ token context windows and are now watching your model hallu=
cinate, forget instructions, or slow to a crawl, you=E2=80=99re not alone. =
There=E2=80=99s a gap between marketing promises and production reality =E2=
=80=93 and it=E2=80=99s time to talk about it frankly.
This guide cuts through the vendor hype and explains why long context windo=
ws often fail in practice, and how to work around these limitations. We=E2=
=80=99ll start by exposing the =E2=80=9Cbig lie=E2=80=9D of advertised vs e=
ffective context. Then we=E2=80=99ll dive into the technical truth (in simp=
le terms) of why this happens =E2=80=93 covering attention mechanics, scali=
ng costs, and hidden infrastructure limits. From there, we=E2=80=99ll quant=
ify the hidden costs that will shock your CFO (token dollars, latency, hard=
ware). But we won=E2=80=99t stop at complaints: you=E2=80=99ll get battle-t=
ested strategies (summary chains, smart chunking, retrieval hybrid methods,=
 context budgeting, prompt tricks, dynamic compression) with real examples =
for how to actually handle long inputs. We=E2=80=99ll explore playbooks for=
 specific industries (legal, finance, healthcare, software) so you can see =
what works in scenarios like contract analysis or code reviews. We=E2=80=99=
ll highlight the tools and frameworks that genuinely help manage context an=
d when to rely on alternatives like vector databases. Finally, we=E2=80=99l=
l separate what=E2=80=99s coming next from what=E2=80=99s just hype, and pr=
ovide a practical 30-day plan (with a =E2=80=9CContext Strategy Canvas=E2=
=80=9D) to implement these solutions in your own projects.
Tone check: Expect a direct, no-nonsense voice that calls out the BS. We=E2=
=80=99ll acknowledge the frustration (we=E2=80=99ve all been burned by exag=
gerated claims) but stay solution-focused. By the end, you=E2=80=99ll think=
 =E2=80=9CFinally, someone told me the truth about context windows =E2=80=
=93 and what to actually DO about it.=E2=80=9D
Let=E2=80=99s get started.
The Big Lie Nobody Talks About
Advertised vs. Effective Context: Vendors love to boast about massive conte=
xt windows =E2=80=93 100K, 200K, even a million tokens. But in practice, ef=
fective context is often a fraction of that. Yes, models like GPT-4.1 claim=
 a 1,000,000-token context (enough for War and Peace in one go), and Claude=
 advertises 200K. Yet developers find that beyond a certain point, the mode=
l behaves as if earlier content =E2=80=9Cisn=E2=80=99t really there.=E2=80=
=9D As one Google forum user put it [ https://substack.com/redirect/e3db059=
a-6661-43c5-ae58-51f7a0a179fa?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgU=
C7BDnPG6RxQ4tAjMRMPOw0 ], the 1M token context is basically =E2=80=9Ccreati=
ve embellishment=E2=80=9D =E2=80=93 the model can only truly focus on about=
 128K tokens at a time, =E2=80=9Csame as all the other models.=E2=80=9D In =
fact, not long ago this user observed Gemini 2.0 Pro (a Google model) =E2=
=80=9Cwas failing immediately, even at 500K=E2=80=9D tokens until an update=
d version was released. In other words, that shiny million-token window oft=
en shrinks to a few tens of thousands of usable tokens once you actually te=
st it. No this problem hasn=E2=80=99t been fundamentally solved with Gemini=
 2.5 Pro, or any model out there.
Real Examples of Failures: The internet is rife with examples of long-conte=
xt models dropping the ball. A particularly vivid case: an experimenter pla=
ced a specific sentence in the middle of a 2,000-token text (well within an=
y large context) =E2=80=93 =E2=80=9CAstrofield creates a normal understandi=
ng of non-celestial phenomena.=E2=80=9D They then asked various models to f=
ind that sentence. About 2/3 of the models failed, including some that supp=
osedly support 100K+ contexts. Think about that: even with just 2K tokens (=
tiny relative to advertised 128K or 200K limits), many models couldn=E2=80=
=99t recall a simple sentence hidden in the middle. This =E2=80=9Clost in t=
he middle=E2=80=9D phenomenon has been documented in research: performance =
is highest when relevant info is at the beginning or end of the input, and =
it significantly degrades for information buried in the middle. Even explic=
itly long-context models (like GPT-4=E2=80=99s extended versions or Claude =
100K) show this behavior. They often nail questions if the answer is in the=
 first or last few pages, but completely miss it if it=E2=80=99s on page 50=
 of 100. Users report models =E2=80=9Cforgetting=E2=80=9D instructions give=
n halfway through a prompt, or a chatbot failing to remember a critical det=
ail from earlier conversation turns =E2=80=93 despite plenty of context win=
dow left.
What does this mean in concrete terms? It means that your =E2=80=9C200K tok=
en=E2=80=9D model might effectively have only ~20K of reliable memory in pr=
actice. One academic study found that many open-source LLMs only use less t=
han half of their claimed context length effectively. For example, a versio=
n of Llama 70B trained with a 128K window actually had an effective context=
 of around 64K in tests. And 64K is being generous =E2=80=93 that=E2=80=99s=
 under ideal conditions. In messy real-world tasks, the effective span can =
shrink further. Thomson Reuters (who build legal AI tools) recently warned =
not to be fooled by advertised context sizes: =E2=80=9COften, the more text=
 included, the higher the risk of missing important details.=E2=80=9D In th=
eir internal benchmarks, they=E2=80=99ve seen that longer inputs can hurt =
=E2=80=93 the model starts overlooking key points when flooded with too muc=
h text. They observed a U-shaped performance curve: feeding more and more d=
ocuments initially helps (up to a point), then performance drops off as con=
text grows. In fact, beyond ~10 documents worth of text, these models often=
 start to ignore the middle docs entirely.
Hallucinations and Degraded Performance: Along with forgotten info, long co=
ntexts trigger other failures. Models may begin to hallucinate =E2=80=93 in=
serting details that were never in the prompt =E2=80=93 because they lose t=
rack of what=E2=80=99s ground truth versus their own guess. We=E2=80=99ve s=
een GPT-4 128K produce =E2=80=9Cgarbage results=E2=80=9D after a long wait =
when pushed to its limit (users have reported response times of 30+ minutes=
 followed by incoherent output). Another common symptom is incoherent chain=
ing: by the time the model generates output about an earlier part of the pr=
ompt, it may have =E2=80=9Cforgotten=E2=80=9D context from much later that =
should have informed it. For instance, you give a 30-page document to summa=
rize, and the summary omits whole sections or contradicts itself because th=
e model essentially ran out of attention half-way through. Empirical tests =
by Stanford and others confirm a =E2=80=9Csignificant decline in performanc=
e=E2=80=9D when the relevant info is in the middle of a long input. They no=
ted that models with 32K or 100K windows all showed this issue =E2=80=93 a =
U-curve where performance was high if answer was at start or end, but tanke=
d for middle-position info. This aligns with the human primacy/recency effe=
ct, but for LLMs it=E2=80=99s an architectural limitation: they=E2=80=99re =
just not effectively using the full context even though they accept it.
Concrete Numbers: Just how bad is the drop-off? Early tests of GPT-4.1 (wit=
h the 1M token window) show it does okay up to some hundreds of thousands, =
but accuracy plunges to ~50% at the full 1M tokens, compared to ~84% on an =
8K prompt. And that=E2=80=99s a new model supposedly trained for long range=
=2E Another long-context benchmark b=
y Databricks found that GPT-4=E2=80=99s q=
uality in a retrieval task started to degrade after ~64K tokens, even thoug=
h it could take 128K. A 405B-parameter Llama prototype dropped off after ~3=
2K. In other words, half or quarter of the advertised max is where the mode=
l starts slipping. Anecdotally, developers working with Google=E2=80=99s Ge=
mini 2.5 (1M context) have found it reliable only up to ~100K in complex sc=
enarios, and highly variable beyond that. One dev on the AI StackExchange b=
luntly titled a post =E2=80=9CThe 1M context window lie=E2=80=9D, after wat=
ching the model make =E2=80=9Cabsolutely stupid stuff=E2=80=9D edits when a=
sked to handle code at ~200K tokens =E2=80=93 he concluded it really only h=
ad a ~100K effective window in practice.
=E2=80=9CLost in the Middle=E2=80=9D Phenomenon: We have to give this probl=
em a name because you=E2=80=99ll see it over and over: lost in the middle. =
The model pays a lot of attention to the earliest part of the input (where =
often the instructions or system prompts are) and to the very end (often th=
e user=E2=80=99s latest question). But the farther something is from those =
ends, the more it becomes =E2=80=9Cbackground noise=E2=80=9D that the model=
 might neglect. It=E2=80=99s not strictly linear decay =E2=80=93 it can be =
pretty sharp drop-off beyond certain positions. In one study, GPT-3.5=E2=80=
=99s performance on a task actually got worse when given more context beyon=
d a sweet spot, to the point that providing extra documents hurt accuracy c=
ompared to giving it nothing at all. Imagine that =E2=80=93 your fancy LLM =
might do worse with the full 100-page dossier than it would if you gave it =
no dossier! This is why many of you have seen bizarre omissions: e.g., you =
prompt with a long prepended company background then ask for analysis, and =
the model=E2=80=99s answer ignores a crucial detail that was buried around =
token #15,000 in that background. It=E2=80=99s not malicious; it literally =
didn=E2=80=99t register that detail deeply.
The Big Lie: So, the big lie nobody talks about is that context length !=3D=
 useful context. The marketing says =E2=80=9Cjust throw everything in, our =
model can handle 100K tokens no problem,=E2=80=9D but the truth is that you=
 often still need to summarize, chunk, and prioritize because the model won=
=E2=80=99t genuinely leverage all that information. In high-stakes settings=
, assuming an LLM will consistently remember something on page 50 of its in=
put is a recipe for disaster. As Thomson Reuters=E2=80=99 CTO put it, =E2=
=80=9Cwhen you look at the advertised context window for leading models tod=
ay, don=E2=80=99t be fooled into thinking this is a solved problem. In comp=
lex, reasoning-heavy real-world tasks, the effective context window shrinks=
=2E=E2=80=9D Even they, with early access=20=
to GPT-4.1 and Claude Opus, found t=
hey still had to chunk documents to avoid missed info.
In summary, the =E2=80=9Clong context=E2=80=9D promise has been oversold. Y=
ou feed a novel-sized prompt and get a response that feels like the model o=
nly read the first and last chapters. Now let=E2=80=99s see why this happen=
s.
Why Long Context Windows Fail (The Technical Truth =E2=80=93 Simply Explain=
ed)
Why can=E2=80=99t these giant models just use all the tokens we give them? =
The answer lies in how attention mechanisms work and how models are trained=
 =E2=80=93 but we=E2=80=99ll keep it high-level and business-friendly.
Attention and Its Limits: Large Language Models use a mechanism called =E2=
=80=9Cself-attention=E2=80=9D to weigh different parts of the input text wh=
en generating each word of output. In essence, the model scans the entire i=
nput context to decide what=E2=80=99s relevant at each step. The trouble is=
, attention is an O(n=C2=B2) operation =E2=80=93 meaning the computational =
work (and memory) grows quadratically with the number of tokens n. A 100K t=
oken context isn=E2=80=99t just 10=C3=97 harder than a 10K context; it coul=
d be 100=C3=97 or more in cost. Vendors know this, and they employ engineer=
ing tricks to reduce the pain (like optimized kernels, or truncating low-re=
levance tokens, or using sparse attention patterns). But fundamentally, ask=
ing a model to truly consider 200,000 tokens all at once is a massive ask. =
The model might do =E2=80=9Csome=E2=80=9D attention across that range but n=
ot uniformly =E2=80=93 often, it ends up effectively focusing on a subset o=
f the context to save compute. Think of it like a student with a stack of 2=
0 textbooks open: they say they=E2=80=99ve read them all, but really they s=
kimmed and only fully read a couple chapters.
Beginning and End Bias (=E2=80=9CRecency=E2=80=9D effects): Most transforme=
r models also have a built-in bias to prioritize earlier and later tokens. =
Partly this is due to how they=E2=80=99re trained =E2=80=93 many training e=
xamples are shorter than the max length, and models learn that the start of=
 input often contains instructions or key info, and the end often contains =
the question or summary. Also, some positional encoding schemes (like the r=
otary embeddings used by many LLMs) effectively undervalue very distant pos=
itions because of how they extrapolate beyond trained lengths. Research has=
 identified a =E2=80=9Cleft-skewed position frequency=E2=80=9D issue: durin=
g training, models see far more examples of tokens at position 1-1000 than =
at position 50,000, so they=E2=80=99re under-practiced at using those later=
 positions. In plain terms: if a model mostly saw 2K-length texts in traini=
ng, giving it a 100K text is forcing it into unfamiliar territory. One stud=
y bluntly concluded: =E2=80=9Ceffective context lengths of open-source LLMs=
 often fall short, typically not exceeding half of their training lengths.=
=E2=80=9D. They traced it to under-training of long positions =E2=80=93 the=
 model=E2=80=99s =E2=80=9Cattention focus=E2=80=9D is heavily skewed to the=
 first few thousand tokens. So when you stuff a huge input, it=E2=80=99s li=
ke a person trying to recall page 200 of a book they skimmed =E2=80=93 the =
fidelity just isn=E2=80=99t there.
Quadratic Scaling and Latency: The quadratic complexity doesn=E2=80=99t jus=
t affect cost; it affects performance and memory. As context grows, inferen=
ce slows down dramatically. Even if a provider doesn=E2=80=99t charge you m=
ore for a long prompt (though many do charge by token, which we=E2=80=99ll =
cover next), you=E2=80=99ll wait much longer for a response. For example, s=
ome users of Azure GPT-4 noticed that using near the max tokens made initia=
l response latency very slow, and throughput dropped =E2=80=93 the model wo=
uld sometimes output a big chunk after a long pause (a sign it was struggli=
ng internally). One dev observed that =E2=80=9Cincreasing the context to 32=
K is actually a regression in most cases=E2=80=9D due to added latency, fin=
ding 8K to be a sweet spot for speed vs info. If 32K is already a latency p=
roblem, imagine 200K. These models require so much computation to do full a=
ttention that either inference slows to a crawl, or the provider quietly us=
es approximations (like not attending fully to everything, which ties back =
to lost-middle issues). It=E2=80=99s telling that one LLM service decided n=
ot to use 32K for general use because it made responses slower without clea=
r benefit in most queries.
Infrastructure Constraints (Vendor Secrets): There are also hidden infrastr=
ucture limitations vendors don=E2=80=99t shout about. Running a 100K or 1M =
token model is memory intensive. The KV cache (which stores intermediate ke=
ys/values for attention) grows linearly with context length. This means if =
you self-host a model, doubling the context can double memory use. On a GPU=
, if the KV cache can=E2=80=99t fit, the system starts swapping to CPU, cau=
sing extreme slowdowns (token generation speed can plummet from e.g. 50 tok=
ens/sec to 2 tokens/sec when spilling to CPU ). One guide bluntly stated: =
=E2=80=9CYou can run big models or long prompts on a consumer GPU =E2=80=93=
 but rarely both.=E2=80=9D. For instance, a 70B parameter model might need =
two 80GB GPUs just to run at 16K context in float32 ; to push to 100K+ cont=
ext typically requires 8-bit or 4-bit compression and still huge memory. Cl=
oud providers manage this behind the scenes with partitioned inference acro=
ss multiple GPUs or specialized hardware, but there=E2=80=99s a cost. Somet=
imes to offer a 100K window, providers deploy a larger-but-slower model or =
use model parallelism, which might be why some =E2=80=9Cturbo=E2=80=9D long=
-context models actually respond slower than their short-context counterpar=
ts.
Also, not all model architectures scale equally. There=E2=80=99s rumor (and=
 some evidence) that some API providers don=E2=80=99t actually use one sing=
le model for the entire 100K input. They might have a retrieval or summariz=
ation step internally for very long inputs. For example. These rumors have =
swirled around Gemini and Llama in particular, with one strategy speculatin=
g that a =E2=80=9CScout=E2=80=9D model finds relevant bits in the million-t=
oken input, and a =E2=80=9CMaverick=E2=80=9D model (smaller context but sma=
rter) then processes those. Yes I=E2=80=99m aware these are Llama names. Th=
e irony is not lost on me.=20
If the rumors are true, that means when you send 1M tokens, the system isn=
=E2=80=99t truly feeding all that to a single model =E2=80=93 it=E2=80=99s =
doing a two-stage process. This isn=E2=80=99t necessarily bad (it=E2=80=99s=
 actually a sensible approach), but it underscores that the model isn=E2=80=
=99t doing what you think =E2=80=93 it=E2=80=99s not linearly reading token=
 1 to 1,000,000 and retaining all that in a single pass. The vendor marketi=
ng won=E2=80=99t detail these trade-offs (hence why this is firmly a rumor,=
 not a fact), but as builders we need to be aware that under the hood, long=
 context might be implemented with tricks that have implications for accura=
cy.
=E2=80=9CMiddle=E2=80=9D Gets Overlooked by Design: One more technical poin=
t: if an LLM has to drop some attention for efficiency, where will it drop =
it? Likely in the middle. Some recent methods (like LongContextReorder in L=
angChain) explicitly reorder retrieved documents so that the most relevant =
are placed at the edges of the prompt (beginning or end) to exploit this bi=
as. This hack exists precisely because models tend to focus on the edges. I=
f you suspect the middle of your prompt is being ignored, you=E2=80=99re pr=
obably right. Until architectures evolve (there are research efforts like r=
ecurrent memory transformers, but they=E2=80=99re not mainstream in 2025 pr=
oduction yet), this issue will persist.
In summary, long contexts fail because: (1) computationally it=E2=80=99s ha=
rd to attend to everything (O(n=C2=B2) bottleneck), (2) models weren=E2=80=
=99t sufficiently trained on ultra-long sequences (so they extrapolate poor=
ly beyond a certain length), (3) practical implementations quietly favor th=
e start and end of the input, and (4) resource constraints force either slo=
wer performance or heuristics that drop information. In non-academic terms:=
 the model=E2=80=99s attention is a mile wide and an inch deep when you giv=
e it a huge prompt. It simply can=E2=80=99t maintain uniform focus across 2=
00K tokens with today=E2=80=99s technology and training.
Now that we know why this happens, let=E2=80=99s talk money =E2=80=93 becau=
se long context isn=E2=80=99t just a technical headache, it can hit your wa=
llet hard.
The Surprising Cost of Context
Many teams have been lured by =E2=80=9Cfeed it everything=E2=80=9D promises=
 only to get a nasty surprise when the bill comes or the system grinds unde=
r load. Long contexts carry hidden costs in both dollars and performance th=
at you must budget for.
Token Costs =E2=80=93 Pay by the Prompt: Most API providers charge per toke=
n of input and output. A token is roughly ~0.75 words in English. So a 100K=
 token input is like a 75,000-word dump =E2=80=93 about a 300-page book! Op=
enAI=E2=80=99s pricing for GPT-4.1 (as of mid-2025) is around $2.00 per mil=
lion input tokens and $8.00 per million output tokens. That might sound che=
ap until you do the math. If you max out a 1M context in one go, that=E2=80=
=99s $2 just to feed it in. If the answer is, say, 50K tokens (50k is ~40k =
words, perhaps a very detailed report), that=E2=80=99s another $0.40. So $2=
=2E40 for one query at best. If your ap=
plication makes hundreds or thousands=20=
of such calls, the costs balloon. And note: older models were even pricier =
per token; OpenAI significantly reduced costs in 2025. Some other providers=
 charge more. Claude 2=E2=80=99s 100K context, for example (back in 2024) w=
as priced such that a full window could cost in the ballpark of $5-$10 per =
prompt once you include output. If you=E2=80=99re doing interactive chat wi=
th long history, those tokens accumulate. The CFO might not realize that ea=
ch user conversation could be eating up tens of cents or dollars if you kee=
p stuffing the full history or documents each turn.
Compute Time and Throughput: Even if token costs drop, compute time costs (=
and opportunity cost of throughput) are significant. A long context request=
 ties up the model for more time. That means fewer requests per second hand=
led, and possibly needing more replicas or paying for higher-rate API tiers=
 to handle the same user load. If one 100K request takes as long as 10 shor=
t requests, you=E2=80=99ve effectively multiplied your infrastructure cost =
by 10 for that use case. This is why some teams find that throughput collap=
ses when they try to scale out long-context queries =E2=80=93 you hit rate =
limits or your instance saturates. Cloud providers often have rate limits t=
hat scale with token count too (e.g., you might only get a few 100K-token r=
equests per minute on an API key, versus many more smaller requests). All o=
f this can translate to needing to purchase higher tiers or more instances =
=E2=80=93 directly hitting the budget.
Latency =3D User Experience Cost: Don=E2=80=99t underestimate the latency p=
enalty. We touched on it =E2=80=93 long contexts are slower. If a user has =
to wait 30 seconds for a response because you sent a giant prompt, that=E2=
=80=99s a cost in user satisfaction (and maybe a cost in terms of having to=
 implement more complex async handling, etc.). In some domains, you simply =
cannot afford that wait. Picture a customer support chatbot that needs to s=
can a knowledge base =E2=80=93 if it tries to stuff all articles into the p=
rompt and responds in 1 minute, the user already bounced. So teams might in=
vest in workarounds or additional tech (like caching) to mitigate latency, =
which is another indirect cost.
Memory and Hardware Requirements: If you=E2=80=99re running models yourself=
 (or even if you=E2=80=99re not, the vendor=E2=80=99s costs here get passed=
 to you indirectly), long context means huge memory usage. The KV cache (wh=
ich stores info per token per layer) is massive at high lengths. A rough he=
uristic: For a model with H hidden size and L layers, KV memory is proporti=
onal to L =C3=97 H =C3=97 n (n =3D number of tokens). For GPT-4 sized model=
s, this is in the gigabytes at n=3D100K. One analysis showed that a 14B par=
ameter model in 12GB VRAM could only handle ~4K context; a 2-3B model was n=
eeded to handle 100K+ in that memory. So either you go with smaller models =
(losing quality) or bigger hardware. High-end A100 GPUs with 80GB memory mi=
ght handle ~16-32K context for a 70B model in float16, but 100K might requi=
re two or more GPUs or 8-bit quantization with performance hits. If you nee=
d to run this in real time, you might require model parallelism, which intr=
oduces complexity (multiple GPUs coordinating on one request). That=E2=80=
=99s fine for big companies, but for many teams it=E2=80=99s a headache and=
 a cost (e.g., cloud GPU instances are expensive =E2=80=93 an 8=C3=97A100 s=
etup can run thousands of dollars a month). Even in API usage, you pay for =
the provider=E2=80=99s hardware usage. It=E2=80=99s no coincidence the mill=
ion-token GPT-4.1 costs a decent chunk per call =E2=80=93 running that mons=
ter isn=E2=80=99t cheap on their side either.
Example =E2=80=93 Cost Reality Check: Let=E2=80=99s do a quick cost reality=
 check scenario: Suppose you have an application that processes 50-page doc=
uments. You consider using GPT-4 Turbo 128K context vs a retrieval approach=
=2E If you go brute force, each documen=
t is ~50 pages ~ 12,500 tokens. You in=
clude maybe some instructions and queries, totaling 13K input, and you get =
a 1000-token output. That=E2=80=99s ~14K tokens per request. At $2 per mill=
ion input and $8 per million output, that=E2=80=99s about $0.028 per reques=
t. 1000 such requests (maybe 1000 documents processed) is $28. Not terrible=
=2E But what if you thought =E2=80=9Che=
y, we have 128K, let=E2=80=99s batch 1=
0 documents together and ask the model to find relevant info among them=E2=
=80=9D? Now your prompt is 130K tokens (10=C3=97 bigger). Output maybe slig=
htly larger, say 2K tokens. That single request costs ~$0.26. To cover the =
same 1000 documents (100 requests now, each with 10 docs), it=E2=80=99s $26=
 =E2=80=93 sounds similar, but wait: will the model even handle 10 docs wel=
l? We saw after ~10 docs performance drops. Perhaps you then decide to incl=
ude even more context =E2=80=9Cto be safe=E2=80=9D =E2=80=93 maybe you thro=
w a whole database of info (100s of pages) at it. You could easily hit the =
full 128K and $0.50 per call. And if you try GPT-4.1 with near 1M tokens in=
put (like an entire corpus), you=E2=80=99re burning $2+ each time. Multiply=
 by many calls or users and it adds up quickly. In contrast, a well-designe=
d retrieval (vector DB) approach might feed only, say, 2K tokens of relevan=
t text per query (cost $0.004) with negligible difference in result quality=
 =E2=80=93 as long as the retrieval finds the right info.
The sticker shock really comes when teams test at small scale (a few calls)=
 and see maybe a $10 bill, and then deploy to production and the monthly in=
voice is thousands of dollars due to heavy token usage. One finance company=
 discovered that letting a GPT-4 32K model ingest full financial reports fo=
r Q&A was costing them 5=C3=97 more than a refined approach that summarized=
 sections first. Another startup found that turning on the 128K context opt=
ion for their chatbot made their API costs triple, with only marginal gains=
 in accuracy (because the model wasn=E2=80=99t actually using most of that =
extra info effectively!).
TL;DR for the CFO: Long context =3D more tokens per request =3D more money =
per request. It also often =3D slower responses =3D potentially fewer reque=
sts handled (or more servers needed) =3D more cost. And if we use more comp=
lex infrastructure (like bigger GPUs or memory), that=E2=80=99s yet more co=
st.
We=E2=80=99ll discuss solutions soon (like retrieval, chunking, etc.) that =
often provide 80-90% of the value of long context at a fraction of the cost=
=2E But before that, let=E2=80=99s enumerate these hidden costs clearly:
Token Fees: More tokens =3D higher API charges. A 100K prompt can cost 10-2=
0=C3=97 a 10K prompt. Multiply by usage volumes.
Compute & Memory: Quadratic scaling means potentially 100=C3=97 the computa=
tion at 10=C3=97 length. You pay in either cloud credits or hardware.
Latency: User wait times can increase 5x-10x for long inputs, which can hur=
t conversion or require engineering workarounds (loading spinners, etc.).
Throughput & Rate Limits: Need more parallel calls or hit vendor rate caps =
sooner, possibly requiring higher pricing tiers or workarounds.
Engineering Complexity: Not directly dollars, but trying to utilize long co=
ntexts might involve new tools (e.g. special vector indexes for partial ret=
rieval, or monitoring to prevent runaway token usage) =E2=80=93 effectively=
 engineering time =3D money.
To make this concrete: imagine telling your CFO =E2=80=9CWe want to enable =
100K context summarization of client documents.=E2=80=9D Without the truth,=
 they might think =E2=80=9Cwe already pay for GPT-4, it=E2=80=99s fine.=E2=
=80=9D But the truth is =E2=80=9Cthis could make each summary cost $1 inste=
ad of $0.05, and we=E2=80=99ll need to invest in optimizing or the users wi=
ll wait too long.=E2=80=9D That=E2=80=99s a different conversation. It=E2=
=80=99s better to have that up front with realistic numbers.
We=E2=80=99ll provide a =E2=80=9CCost Reality Check=E2=80=9D framework late=
r to help you calculate these trade-offs for your use case. Now, onto the g=
ood news: how you can still achieve your goals without falling into the con=
text trap. The answer is combining strategies =E2=80=93 essentially doing w=
hat the model isn=E2=80=99t doing for you automatically. Think of it as man=
aging context proactively.
Battle-Tested Strategies That Actually Work
Enough about problems =E2=80=93 let=E2=80=99s talk solutions. No single sil=
ver bullet exists (sorry, you can=E2=80=99t just flip a switch to make a 20=
0K model magically remember the 100K mark content). But a toolkit of strate=
gies can cover 99% of real-world needs. The key idea: treat long context as=
 a tool, not a crutch. Use the big window when it truly helps, but augment =
it with clever workflows so you=E2=80=99re not relying on it blindly. Here =
are six battle-tested approaches, each with step-by-step guidance, examples=
, and when to use them. You=E2=80=99ll likely combine several in practice.
1. The =E2=80=9CSummary Chain=E2=80=9D Pattern
When to use: You have a large body of text (say a long report, book, or tra=
nscript) and you need an overall summary or to answer high-level questions =
about it. The text doesn=E2=80=99t fit (or shouldn=E2=80=99t be dumped) in =
one prompt. Summary Chain shines when you need to distill information from =
a long source without losing key details. It=E2=80=99s great for things lik=
e summarizing a long earnings call transcript, legal brief, or technical pa=
per. Also useful to preserve information over many chat turns (compress old=
er dialogue).
What it is: A multi-step prompting workflow that incrementally compresses c=
ontent into a summary (or other intermediate form) that the model can handl=
e. Instead of feeding 100 pages at once, you break the text into smaller ch=
unks, process each, then combine results. This is sometimes called =E2=80=
=9Chierarchical summarization=E2=80=9D or a map-reduce chain.
Step-by-step implementation (Basic):
Chunk the Text: Split the document into reasonable chunks =E2=80=93 e.g., 5=
 pages or 1000 tokens each (whatever the model can easily handle with some =
room for output). Ensure chunks end at logical boundaries (don=E2=80=99t cu=
t mid-sentence if possible).
Summarize Each Chunk (Map Phase): For each chunk, prompt the model: =E2=80=
=9CSummarize the following text in X sentences, focusing on key facts=E2=80=
=A6=E2=80=9D (you can tailor X based on how much compression you need). Thi=
s yields a summary for each piece. Use the same model for this or even a sm=
aller/cheaper model if you trust it for summarization.
Combine Summaries (Reduce Phase): Now take the chunk summaries and either c=
oncatenate them and summarize again (if there are many), or do it iterative=
ly: e.g., summarize 5 summaries into a higher-level summary, etc. Essential=
ly you are doing a tree reduction =E2=80=93 summaries of summaries until yo=
u have one final concise summary. Another approach is the Refine Chain: sta=
rt with summary of chunk1, then feed summary + chunk2 summary to the model =
with a prompt =E2=80=9CRefine this summary by incorporating the following i=
nformation=E2=80=A6=E2=80=9D, and iterate chunk by chunk.
Final Answer: The end result is a manageable-length summary that captures t=
he whole document. You can present that to the user, or if the goal was Q&A=
, you can now feed the final summary plus the question into the model to ge=
t an answer.
Enhanced Implementation: If you have infrastructure, you can parallelize th=
e Map phase (summarize chunks concurrently) to save time. You can also add =
an intermediate step where you ask the model to extract key facts or data p=
oints from each chunk (instead of freeform summary) and later aggregate tho=
se. For example, in a legal document, you might extract a list of =E2=80=9C=
obligations=E2=80=9D from each section, then merge them. Or use a structure=
d format (JSON) for summaries so they are easier to merge.
Real Example: A company needed to summarize a 200-page financial report for=
 analysts. Directly prompting GPT-4 with 200 pages was impossible, and even=
 32K context wasn=E2=80=99t enough. They implemented a summary chain: split=
 into ~40 sections by report headings, summarized each (with GPT-3.5), then=
 summarized those summaries (with GPT-4) into a 2-page briefing. The chain =
of summaries preserved all major points (revenues, risks, outlook) and it c=
ost under $5 of API calls, compared to an estimate of $50+ if they had trie=
d to brute-force feed raw text into GPT-4 (even if it were able).
Another example: Summarizing a long Slack chat history for adding new teamm=
ates quickly. Rather than giving the model 100 screens of chat (which it mi=
ght forget mid-way), they chunked the chat by week, summarized each week, t=
hen summarized the summaries into a =E2=80=9Clast month recap=E2=80=9D. Thi=
s hierarchical approach maintained context that was lost when they tried a =
one-shot long summary.
Quick Try (5-minute experiment): Take a long Wikipedia article (or any text=
 ~5000 words). Try to get a one-shot summary from your LLM in one go (you m=
ight need GPT-4 32K or just use GPT-3.5 and see it fail due to length). The=
n implement a quick manual summary chain: split the article into 5 parts, a=
sk the model to summarize each part in a paragraph. Then give it those 5 su=
mmaries and ask for an overall summary. Compare the results. You=E2=80=99ll=
 likely see the chain summary is more coherent and hits all major points, w=
hereas a one-shot (if it even worked) might miss something or just truncate=
=2E This small test demonstrates how br=
eaking down the task yields better ret=
ention.
Why it works: You=E2=80=99re sidestepping the model=E2=80=99s weakness with=
 long inputs by never giving it more than it can handle at once. By doing t=
he heavy lifting of remembering via intermediate summaries, you reduce the =
burden on the final prompt. It also reduces cost =E2=80=93 you may use chea=
per models for chunk summaries and only use the powerful model for the fina=
l step. Summary chaining does require some careful prompt design (to ensure=
 each summary contains what you need for the next stage), but many librarie=
s (LangChain, etc.) have utilities for this pattern out of the box.
2. Strategic Chunking Workflows
When to use: Any time you have to process large texts or datasets and the t=
ask can be localized to pieces of that input. This includes use cases like:=
 searching a large document for answers, analyzing sections of code across =
a codebase, or applying the same question to multiple records (like =E2=80=
=9Csummarize each chapter of this book=E2=80=9D). Strategic chunking is abo=
ut how you break up content and iterate, to avoid overwhelming the model. U=
se this when the problem doesn=E2=80=99t require combining everything at on=
ce, or when you can structure the approach in a divide-and-conquer way.
What it is: Chunking simply means splitting input into smaller parts (chunk=
s) and processing them individually. But the =E2=80=9Cstrategic=E2=80=9D pa=
rt implies doing it intelligently: choosing chunk sizes, overlaps, and flow=
s that preserve context and minimize error. Key techniques include sliding =
windows, overlapping chunks, and smart chunk size selection.
Step-by-step implementation (Basic):
Define Chunk Size: Decide how big each chunk should be. The ideal chunk is =
as large as possible while still being manageable, to reduce how many total=
 chunks you have (fewer chunks =3D fewer calls and less chance of missing c=
ontext across a boundary). For straightforward tasks, chunks around 2K-4K t=
okens often work well (for a model with 8K or 16K window). If you have a 10=
0K model, you might chunk at 20K or 50K. The chunk size also depends on the=
 content structure =E2=80=93 e.g., for code, you might chunk by file or fun=
ction; for a book, by chapter.
Overlap (if needed): If context continuity matters (like reading comprehens=
ion across chunk boundaries), use overlapping chunks. For example, take 200=
0-token chunks but overlap by 200 tokens (so chunk1 is tokens 1-2000, chunk=
2 is 1801-3800, etc.). This ensures anything that falls at a boundary is se=
en in at least one chunk entirely. Overlap helps avoid losing a sentence th=
at gets cut in half between chunks.
Process Each Chunk: For each chunk, perform the needed operation with the L=
LM. This could be summarizing (as above), Q&A (=E2=80=9CDoes this chunk con=
tain X?=E2=80=9D), classification, etc. Because each chunk is within contex=
t size, the model handles it well. You might parallelize these calls if usi=
ng an API.
Aggregate Results: Combine the results from chunks. The aggregation depends=
 on the task:
If you asked Q&A on each chunk, you might merge the answers or choose the b=
est answer (or have the model combine them if needed).
If you are searching for which chunk has the answer, you identify the chunk=
 where it was found.
If summarizing each chunk (like each section), you might then do another pa=
ss to join those (which becomes a summary chain approach).
For code analysis, maybe each chunk yields an analysis of that file, and yo=
u present or further process those analyses.
Verify or Refine: Sometimes after getting per-chunk outputs, you might do a=
 follow-up. E.g., if two chunks had partial info to a question, you may pro=
mpt the model with both relevant outputs to synthesize a final answer.
Enhanced Implementation: Use a dispatcher function that automatically handl=
es chunk creation and result combination. Many frameworks allow you to spec=
ify chunk size and overlap for text, and they=E2=80=99ll split for you (e.g=
=2E, LangChain=E2=80=99s TextSplitter). Fo=
r irregular data (like JSON logs, o=
r code), you might chunk by logical sections (like one log entry per chunk)=
=2E If you need cross-chunk reasoning,=20=
you can incorporate a routing step: fi=
rst identify which chunks are relevant, then only send those to the model. =
This crosses into retrieval (next strategy) but underscores that chunking c=
an be paired with search.
A powerful pattern is Sliding Window + Re-ask: slide through the text in ov=
erlapping windows and have the model attempt the task on each. If the model=
 answers confidently in window 3, you stop. If not, or if answers differ ac=
ross windows, you have logic to reconcile that (maybe ask the model to cons=
olidate answers).
Real Example: Document QA: A team built a feature to answer questions from =
a large policy document (~80 pages). Instead of giving the whole doc to the=
 model, they chunked it by section and used a simple heuristic: ask each ch=
unk =E2=80=9CDo you contain information relevant to [the question]? If yes,=
 provide the relevant text; if no, say NONE.=E2=80=9D Then they concatenate=
d all outputs that were not =E2=80=9CNONE=E2=80=9D (i.e., the relevant snip=
pets the model identified) and fed that to a final prompt to answer the use=
r=E2=80=99s question. Essentially, the model itself did a first pass skim o=
n each chunk and found candidate info, and the second pass answered with th=
ose. This worked better than vector search in this case because the questio=
n was nuanced (the model=E2=80=99s understanding was helpful to decide rele=
vance), and it was far cheaper and more reliable than feeding all 80 pages =
at once. It was also fast because each chunk Q&A was done in parallel.
Another example: Large codebase analysis. A company wanted an LLM to find p=
otential bugs across a 100k-line codebase. They chunked the code by file (e=
ach file is a chunk), and asked the model to analyze each file for certain =
code smells or bug patterns. They collected the potential issues per file, =
then did a second pass where they provided relevant snippets from multiple =
files if an issue spanned files (like inconsistent function usage) for the =
model to consolidate. By chunking per file, they kept each analysis focused=
 and within context. The result was a report of issues across the codebase.=
 If they had tried to load the entire codebase into a context=E2=80=A6 well=
, they just couldn=E2=80=99t, it=E2=80=99s too large. Even if they tried wi=
th a 1M token model, it would have cost a fortune and likely missed things.
Quick Try: Take a long text (maybe a 20-page PDF, you can convert to text).=
 Without reading it, ask an LLM a detail question that would require scanni=
ng the whole thing (e.g., =E2=80=9CAccording to this document, what is the =
deadline for submissions?=E2=80=9D). If you try to stuff the whole text, it=
 might not be possible, or if possible, see if it answers correctly. Now im=
plement a simple chunk approach: break the text into, say, 4 chunks, each ~=
5 pages. Ask the model the same question for each chunk separately (=E2=80=
=9Cbased on this chunk, if it contains the answer: [question]. If not, say =
=E2=80=98Not in this part=E2=80=99.). You might see one chunk says =E2=80=
=9CDeadline for submissions is June 30, 2025=E2=80=9D and others say =E2=80=
=9CNot in this part=E2=80=9D. Now you know which chunk had the info. You ca=
n then just use that chunk to answer the question fully. This simulates how=
 chunking + targeted reading can outperform blind full-context usage, espec=
ially if only a small part of the text had the answer.
Why it works: Strategic chunking works because it respects the model=E2=80=
=99s attention limits. By not overloading it and by intelligently covering =
the input (with overlaps to catch boundary info), you ensure the model has =
what it needs in each invocation. It also scales: you can distribute work o=
ver chunks in parallel, and you only pay proportional to content length (no=
t quadratic). Essentially, you=E2=80=99re manually doing what a perfect lon=
g-context model should do =E2=80=93 read bit by bit and combine =E2=80=93 b=
ut we control the process to avoid missed spots. Done right, the model=E2=
=80=99s accuracy stays high because it=E2=80=99s never lost in a sea of tex=
t.
One caveat: If the task truly requires synthesizing disparate pieces from a=
cross the entire text, chunking alone isn=E2=80=99t enough =E2=80=93 you th=
en need to add a subsequent step (like we did with combining answers or lik=
e a summary of summaries). But chunking sets the stage for those advanced s=
teps by breaking the problem down.
3. Hybrid Retrieval-Enhanced Context (RAG++)
When to use: When you have a large knowledge base or corpus and users ask q=
uestions that pertain to only a small part of it. Classic examples: a chatb=
ot over your company docs or manuals, customer support knowledge base query=
, Q&A over a set of PDFs, etc. If you find yourself tempted to stuff a ton =
of reference text into the prompt =E2=80=9Cjust in case,=E2=80=9D retrieval=
 is the smarter approach. Also use this when the same corpus is queried rep=
eatedly =E2=80=93 retrieval lets you reuse an index rather than feeding the=
 same data every time.
What it is: Retrieval-Augmented Generation (RAG) [ https://substack.com/red=
irect/596f23bb-a80d-427d-9b7f-738e6538b7ce?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YF=
xPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0 ] is the practice of using a search or =
database step to fetch relevant pieces of text, and inserting only those in=
to the LLM prompt. The LLM then bases its answer on that retrieved context =
(plus its own knowledge). Hybrid retrieval+context means even if you have a=
 long-context model, you still use retrieval to shrink the needed context a=
nd improve accuracy. Essentially, the LLM=E2=80=99s context window is treat=
ed as a precious budget =E2=80=93 you fill it with only the most relevant s=
nippets from your documents, rather than raw dumping. This mitigates the lo=
st-in-middle issue by making sure the truly important info is likely at the=
 top of the prompt (since you choose what to include).
Step-by-step implementation:
Index Your Documents: Use an embedding-based vector store or a keyword sear=
ch index on your corpus. For each document or section, store an embedding (=
semantic vector) or inverted index for text.
User Query to Search: When a question comes in, first transform the questio=
n into a search query. For vector DBs, embed the question and do a similari=
ty search. For keyword, do a keyword match. Retrieve the top N relevant chu=
nks/documents. N is typically small, like 3-5, to avoid too much text.
Construct the Prompt with Retrieved Context: Prepare a prompt that includes=
 the question and the retrieved texts as context. A typical format: =E2=80=
=9CHere are relevant excerpts from our knowledge base: =E2=80=A6=E2=80=A6 U=
sing only this information, answer the question:.=E2=80=9D Optionally, cite=
 sources if needed.
Generate Answer: The LLM sees only those few relevant pieces, which might t=
otal, say, 1000 tokens, well within limits. It then produces an answer grou=
nded in that context.
(Optional) Feedback Loop: If the answer seems off or the model says it lack=
s info, you might attempt another round (maybe expanding the search or retr=
ieving more). But ideally, a good retrieval set yields a good answer in one=
 go.
Enhanced Implementation: For hybrid context, sometimes you might combine re=
trieval with a long context. For example, if you have a model with 100K win=
dow and you=E2=80=99re analyzing a collection of documents, you might retri=
eve 50 relevant pages (which might be, say, 25K tokens) and feed those in. =
That way you=E2=80=99re not using the full 100K blindly, but you=E2=80=99re=
 also taking advantage of more context than a smaller model could. Another =
improvement is re-ranking the retrieved results to ensure the most importan=
t ones go first (if you have more than can fit). The LongContextReorder tri=
ck we saw is an example =E2=80=93 it ensures the most relevant docs are at =
the prompt extremes , which can help the model pick them up despite any mid=
dle-loss issue.
You could also use multiple retrieval passes: e.g., first retrieve a releva=
nt section, then within that section do a finer search for the exact answer=
=2E But that can usually be simplified b=
y the LLM reading the section itself.
Real Example: Enterprise Q&A: A company had a million documents of internal=
 knowledge. They enabled GPT-4.1 with a 1M window on it, but initial trials=
 failed =E2=80=93 the model often rambled or hallucinated because they were=
 feeding too much and hoping it finds the needle in haystack. They switched=
 to a RAG approach: when a user asks a question, use ElasticSearch to find =
the top 5 documents. Then they feed those (maybe 5-10K tokens total) into G=
PT-4 and get an answer that is nearly always correct and with sources cited=
=2E The cost per query dropped by >9=
0% because they weren=E2=80=99t pumping h=
undreds of thousands of tokens each time, and the answers were more accurat=
e since irrelevant info was gone. Users also got faster responses (a few se=
conds instead of 30+).
Interestingly, they found that even if the final model (GPT-4) could have a=
ccepted all docs, it performed better with retrieval. That aligns with rese=
arch: =E2=80=9CMost model performance decreases after a certain context siz=
e=E2=80=A6 Modern LLMs can improve RAG with long context, but longer contex=
t is not always optimal=E2=80=9D. It=E2=80=99s often better to retrieve say=
 20 relevant documents than to feed 100 and rely on the model to sift throu=
gh.
Another example: ChatGPT plugins / tools. When you ask ChatGPT something ab=
out recent info, it might use the Browsing tool or a vector database behind=
 the scenes. That=E2=80=99s RAG in action =E2=80=93 instead of relying on i=
ts entire training or context, it looks up the answer and then provides it.=
 Many applications (like Bing Chat, etc.) do this hybrid approach: search f=
irst, then feed results into the model. If you consider search as producing=
 a dynamic context, it=E2=80=99s essentially extending context but in a tar=
geted way. Businesses can replicate this pattern with their own data.
Quick Try: If you have a folder of text/PDF files, try using an open source=
 tool or simple script: use a vector database like FAISS or Pinecone (some =
have free tiers) to index them. Then ask a few questions and retrieve top m=
atches, feed those into the model. Compare the answer vs if you tried to st=
uff entire documents. Even simpler: Use Bing or Google to search a question=
 about a topic, copy the relevant snippet results into ChatGPT and see the =
answer quality. You=E2=80=99ll notice that providing just the needed info y=
ields a very direct and factual answer, whereas if you gave the model an en=
tire Wikipedia article, it might get lost or give a very general summary ra=
ther than the specific detail you want.
Why it works: RAG plays to each component=E2=80=99s strengths =E2=80=93 sea=
rch (or embeddings) excels at pinpointing relevant chunks, and the LLM exce=
ls at reasoning/generating with a moderate amount of text. It avoids drowni=
ng the model in irrelevant info. The model doesn=E2=80=99t have to be a lib=
rarian; you already handed it the right pages. Also, since the retrieved pi=
eces are usually smaller and focused, the model is less likely to drift off=
 or hallucinate =E2=80=93 it has concrete text to ground its answer. It=E2=
=80=99s like open-book exam vs closed-book: in a closed-book scenario with =
a giant book memorized (long context with everything), the model might misr=
emember; in an open-book scenario with the index (retrieval) to the exact p=
age, the model can quote or rely on the actual text. This reduces the load =
on the model=E2=80=99s =E2=80=9Cmemory=E2=80=9D component and mitigates the=
 lost-in-middle problem because the truly relevant info is likely at the fr=
ont of the prompt (you usually paste it in fresh, often preceded by a syste=
m message telling the model to use it).
One caution: Ensure your retrieval results are actually relevant. If your s=
earch pulls in some unrelated chunk (false positive), the model might get c=
onfused or use that incorrectly. So invest in good embeddings or search str=
ategies (there are whole guides on vector DB tuning). Also, limit how many =
documents you stuff in =E2=80=93 more is not merrier beyond a point. Empiri=
cally, after ~5-10 documents, performance drops , so better to retrieve the=
 top few and perhaps iterate if needed rather than load 50 excerpts at once=
=2E
4. The =E2=80=9CContext Budget=E2=80=9D Framework
When to use: In complex applications, especially agent-like systems or mult=
i-turn conversations, where many different types of information compete for=
 context space =E2=80=93 e.g., system instructions, user dialogue history, =
retrieved knowledge, intermediate calculations, etc. Use a context budget w=
hen you have to juggle these components and want to ensure the model sees t=
he most important pieces. It=E2=80=99s a proactive strategy to avoid contex=
t bloat and forgetting.
What it is: Treat your model=E2=80=99s context window like a budget to be a=
llocated among categories: instructions, relevant facts, recent conversatio=
n, etc.. You then establish rules or dynamic policies for what gets include=
d and what gets trimmed when the budget is tight. Think of it as a sliding =
window memory with prioritization. This approach is about balancing context=
 components so that the LLM doesn=E2=80=99t lose key details due to an over=
flowing prompt.
Step-by-step implementation:
Identify Context Components: List what elements go into your prompts. Commo=
n components:
System Prompt (instructions on style/behavior)
User=E2=80=99s last query
Conversation history (previous Q&A pairs)
Retrieved knowledge/docs
Tool outputs or intermediate results (if an agent)
Possibly a brief summary of the conversation or user profile (for personali=
zation)
Each of these takes up tokens.
Allocate Priority/Budget: Decide which components are highest priority to a=
lways include, and which can be truncated or summarized when needed. For ex=
ample:
System instructions: high priority (always include core rules, maybe ~500 t=
okens budget).
Last user query: high (must include in full).
Conversation history: medium =E2=80=93 allocate maybe 1000 tokens to it; if=
 history is longer, include recent turns and summarize or drop older ones.
Retrieved docs: high, but only include top 3 results, etc., with maybe up t=
o 2000 tokens.
Tool outputs: medium, maybe include last tool result if needed.
This is your context budget plan =E2=80=93 e.g., total 8000 tokens, where e=
ach category has a soft limit.
Dynamic Trimming/Compression: Implement logic that if the conversation gets=
 too long or retrieval returns too much text, you trim less important parts=
=2E For example, remove or shorten olde=
r chat turns (replace them with a summ=
ary). This can be automated: after each turn, check token length; if > thre=
shold, use an LLM to summarize the oldest interactions and replace them wit=
h a note (this is what ChatGPT does with long chats =E2=80=93 it will stop =
referencing very old turns or compress them).
Consistent Formatting: Use structure in your prompt to separate components,=
 so the model knows which is which (system vs user vs context text). This r=
educes confusion and helps the model follow the important parts.
Testing and Tuning: Simulate scenarios where the context budget would overf=
low and see how your system handles it. For instance, have a 50-turn conver=
sation and see if earlier points are truly gone or properly summarized. Adj=
ust the summarization prompt if the summary loses critical info. Essentiall=
y, tune how aggressively you compress vs drop.
Enhanced Implementation: A sophisticated approach is to maintain an explici=
t memory object. For example, store a running summary of the conversation s=
eparately and always include that in the prompt instead of full history (up=
dating it each turn). Some use vector databases to store conversation chunk=
s and retrieve relevant past dialogue dynamically (though that can be overk=
ill). Another advanced tactic is priority tagging: mark certain messages or=
 facts as =E2=80=9Ccritical=E2=80=9D so they are never omitted from context=
=2E For example, if a user provid=
ed crucial info early on (=E2=80=9CI=E2=80=
=99m allergic to shellfish=E2=80=9D in a medical chat), you might pin that =
in the system prompt in future turns (=E2=80=9CRemember: User is allergic t=
o shellfish.=E2=80=9D).
If building an agent with tool usage, design a prompt template where the wo=
rking memory (scratchpad) has limited lines =E2=80=93 if it=E2=80=99s too l=
ong, summarize earlier reasoning steps.
Some frameworks have this baked in. For instance, LangChain has various Con=
versationMemory classes that implement buffer (last N messages) or summariz=
e+buffer strategies. You can configure those.
Real Example: AI Customer Support Agent: This agent had to follow a set of =
company policies (system prompt ~1000 tokens), carry on a possibly lengthy =
conversation with a customer, and also pull in knowledge base articles as n=
eeded. Without control, prompts got huge and the agent started dropping ear=
lier context (like forgetting what the customer asked initially) around tur=
n 15. They implemented a context budget: always include system policies, al=
ways include the last user message and agent reply. For conversation histor=
y, they only kept the last 5 exchanges verbatim, and older ones were summar=
ized into a shorter =E2=80=9CSession recap: =E2=80=A6=E2=80=9D that stayed =
in the prompt. For knowledge base, they limited to top 2 articles (instead =
of sometimes 5 or 6 that the search might return). They also stored the cus=
tomer=E2=80=99s key info (name, account issue) in a short summary that pers=
isted. Result: the agent stopped losing track of the conversation and didn=
=E2=80=99t exceed model limits. Importantly, by summarizing older turns, th=
ey reduced token usage significantly such that even a 30-turn dialog stayed=
 under 8K tokens (whereas before it blew past 16K and had to drop context a=
rbitrarily).
The Context Budget Canvas (which we=E2=80=99ll present at the end) essentia=
lly formalizes this: you draw boxes for each context component and assign t=
hem token budgets and rules.
Another example: Slack/GitHub Bot: A bot that followed lengthy threads had =
to remember what=E2=80=99s been resolved. They gave it a habit of writing i=
ts own =E2=80=9CNotes so far=E2=80=9D after every few messages, which were =
then used in lieu of raw history. This kept it from repeating earlier answe=
rs and ensured it didn=E2=80=99t ask the same questions twice (a form of sh=
ort-term memory management).
Quick Try: If you=E2=80=99ve used ChatGPT in a very long conversation, you =
might have noticed it sometimes forgets or you get the sense it=E2=80=99s l=
osing info. A quick demo: Start a new chat with GPT-4, tell it a list of 10=
 specific facts about you. Then have a 15-turn conversation on various topi=
cs. At turn 16, ask it to recall those 10 facts. Chances are it will recall=
 only some, or with errors =E2=80=93 because its effective context shrunk a=
nd it dropped earlier info. Now try a context budget approach manually: aft=
er the 10 facts, ask it to summarize those facts in 2 sentences and keep th=
at summary. Throughout the conversation, occasionally remind it with the su=
mmary. You=E2=80=99ll see it=E2=80=99s more consistent. This is essentially=
 what you=E2=80=99d automate: always persisting the critical info summary i=
n the prompt.
Why it works: Without a budget, the model=E2=80=99s context gets filled ad-=
hoc and the least recent or lower priority info may be truncated or just me=
ntally ignored by the model. By budgeting, you ensure the most important pi=
eces are always present and prominent. You=E2=80=99re also aligning with ho=
w the model=E2=80=99s attention tends to work =E2=80=93 since we know it fo=
cuses on recent tokens, we explicitly keep important things recent (by repe=
ating or summarizing them at the end of the prompt as needed). By doing sum=
marization of older content, we compress low-value tokens into high-value t=
okens, freeing space. Essentially, context budgeting acknowledges that not =
everything can fit, so it makes conscious trade-offs about what stays at fu=
ll detail, what gets abridged, and what gets dropped entirely. The result i=
s a system that feels like it has a longer memory and better focus than the=
 raw model would.
As a plus, this can dramatically cut costs. You=E2=80=99re not sending the =
entire chat history every time, just a summary. If each message is ~50 toke=
ns and you have 20 turns, that=E2=80=99s 1000 tokens of history, but a summ=
ary might only be 100 tokens =E2=80=93 10=C3=97 reduction every prompt. Mul=
tiply by thousands of conversations and you saved serious money.
5. Position-Aware Prompting (Edge Optimization)
When to use: When you must feed a lot of information and you know some part=
s are crucial. Also when using retrieval or multi-doc prompts, where you wa=
nt to ensure the most relevant info isn=E2=80=99t lost in the crowd. Essent=
ially, anytime you=E2=80=99re constructing a prompt with multiple parts, yo=
u should be position-aware =E2=80=93 leveraging the fact that beginnings an=
d ends of the prompt carry more weight (to the model) than the middle. This=
 strategy helps mitigate the =E2=80=9Clost in the middle=E2=80=9D problem b=
y reordering or emphasizing content.
What it is: Position-aware prompting means you intentionally place importan=
t context at the start or end of your prompt, and/or repeat key points in t=
hose positions, so the model is more likely to attend to them. It can also =
involve injecting reminders or TL;DRs at intervals if the prompt is super l=
ong. It=E2=80=99s about not treating the prompt as a neutral sequence =E2=
=80=93 order matters, and we exploit that.
Step-by-step implementation:
Determine Priority Info: Identify the absolutely critical pieces of content=
/instructions the model must not ignore. For example: the main question/tas=
k, any must-follow directive (=E2=80=9Cdon=E2=80=99t reveal internal data=
=E2=80=9D), or a key fact that the answer depends on.
Place Key Instructions at Start: The system or initial prompt should contai=
n the fundamental instructions. Models heavily weight the beginning of the =
prompt for directives , so don=E2=80=99t bury the actual task at token 1500=
0. For instance, instead of listing 10 pages of reference then saying =E2=
=80=9CNow answer question X=E2=80=9D, consider stating: =E2=80=9CQuestion: =
X. We have provided relevant information below. Use it to answer.=E2=80=9D =
at the top, so the model knows from the get-go what it=E2=80=99s looking fo=
r.
Order Retrieved Docs by Relevance: If you retrieved chunks, don=E2=80=99t j=
ust list them in arbitrary order. Put the most relevant one first, second m=
ost last, and the rest in the middle. This =E2=80=9Csandwiching=E2=80=9D wa=
s suggested by research to surface relevant info to the model=E2=80=99s att=
ention. So if chunk A is highest similarity and chunk B is second highest, =
put A at top of context, B at bottom of context, and others in between.
Reiterate Critical Facts at End: If there=E2=80=99s something the model abs=
olutely should use, you might repeat or summarize it in the final user prom=
pt part. E.g., =E2=80=9CAs a reminder, the user=E2=80=99s core requirement =
is to increase conversion by 5%.=E2=80=9D This goes just before the model s=
tarts generating its answer. The model=E2=80=99s output head often pays str=
ong attention to the last few tokens of the prompt (since it=E2=80=99s fres=
h in =E2=80=9Cmemory=E2=80=9D), so a well-placed reminder can steer the ans=
wer.
Use Section Headings or Delimiters: Clearly label parts of the context to a=
void confusion. For example: =E2=80=9CInfo 1: [text] Info 2: [text] =E2=80=
=A6 Question: [text]=E2=80=9D. This helps ensure the model knows which part=
 is the user question vs context. It also allows you to reference these sec=
tions in the prompt: =E2=80=9CAnswer based on Info 1 and Info 2 above.=E2=
=80=9D By naming them, you=E2=80=99ve anchored the references.
Check Model Focus via Testing: You can test whether the model is picking up=
 the important bits by asking it to explain its answer or by inserting know=
n dummy info. For instance, plant a clearly irrelevant but flashy sentence =
in the middle of context and see if the model ignores it. If it doesn=E2=80=
=99t, you might need to better structure prompts or limit length.
Enhanced Implementation: In multi-turn interactions, if the user provides n=
ew crucial info mid-way (like =E2=80=9CActually, our budget is $0=E2=80=9D)=
, you may want to bump that info up in the prompt in future turns (like inc=
lude it in system message: =E2=80=9CUser=E2=80=99s budget =3D $0=E2=80=9D).=
 This ensures it=E2=80=99s not lost down in the middle of a long history.
Another advanced trick: mid-context summaries. If you provide a very long c=
ontext (say a 50K document) to a model that can handle it, consider splitti=
ng it into sections and for each section, prepend a one-sentence summary. T=
hat way even if the model skims, those summaries (which appear regularly) m=
ight catch its attention. Essentially, you scaffold the context with =E2=80=
=9Cheads=E2=80=9D that outline what=E2=80=99s coming. It=E2=80=99s like an =
executive summary for each portion. Models do pick up on those if phrased c=
learly (=E2=80=9CSection Summary: This part describes the legal definitions=
=2E=E2=80=9D).
If using open-source models or fiddling with architecture, one could also a=
djust positional encoding biases (but that=E2=80=99s out of scope for most =
users). Our focus is on prompt-level tactics.
Real Example: Multi-document analysis: Here=E2=80=99s an example that=E2=80=
=99s been coming up from Claude 2 through Opus 4. When I was getting more u=
sed to LLMs, I sometimes just pasted all the papers I wanted Claude to work=
 on one after the other and asked Claude for common themes. The output was =
superficial and missed a key theme that was actually only discussed in one =
paper buried in the middle of the prompt. The fix I found: add a bullet lis=
t at the top of the prompt: =E2=80=9CPaper Titles and Main Topics: 1) Paper=
 A =E2=80=93 about theme X, 2) Paper B =E2=80=93 about theme Y, =E2=80=A6=
=E2=80=9D. Then put the full text of each paper below. Then finish the prom=
pt with: =E2=80=9CIn summary, focus on themes X and Y.=E2=80=9D The result =
is a much more on-point analysis that does mention the previously missed th=
eme. By giving the model a map of the context up front and a reminder at th=
e end, I can successfully guide its attention within a very large prompt. I=
 still use this with cutting edge models today.
Another example: Lost info in code assistance. A developer noticed that whe=
n she pasted a long code file and then added =E2=80=9C// TODO: fix bug here=
=E2=80=9D at the bottom, the LLM would sometimes forget context from the to=
p of the file relevant to the bug. She changed the approach: she wrote a br=
ief description of the bug at the top (=E2=80=9CBug: the output is wrong wh=
en X happens=E2=80=9D) before the code, then pasted code, then after the co=
de again said =E2=80=9CRefer to the above code to fix the bug where output =
is wrong when X.=E2=80=9D This sandwiching made the model focus on that spe=
cific issue throughout its reading of the code, leading to correct fixes. I=
t=E2=80=99s essentially reminding at both ends what to look for in the midd=
le.
Quick Try: If you have an LLM that tends to ignore something, try reorderin=
g. A fun mini-test: give the model a prompt like =E2=80=9CText1: [some cont=
ent with the answer] Text2: [some irrelevant fluff] Question: [ask about th=
e answer].=E2=80=9D See if it answers correctly. Now swap Text1 and Text2 (=
so the fluff comes first, answer second) and see if it gets it right or str=
uggles. Many models perform better when relevant info is earlier. That demo=
nstrates positioning matters. Now try labeling: =E2=80=9CRelevant Info: [an=
swer content]. Irrelevant Info: [fluff]. Question: =E2=80=A6=E2=80=9D and s=
ee =E2=80=93 likely it will ignore the irrelevant section properly. That sh=
ows how labeling and positioning help steer attention.
Why it works: We know from =E2=80=9CLost in the Middle=E2=80=9D research th=
at position has a huge impact on recall. By controlling positions, we circu=
mvent the model=E2=80=99s tendency to downweight the middle. Essentially, w=
e hack the attention. The first tokens condition the model heavily (that=E2=
=80=99s how prompting works), and the last tokens are fresh in its short-te=
rm working memory when generating. By planting important things there, we b=
oost their chances of influencing the output. Also, instructing the model w=
here to look (=E2=80=9CInfo 1 above pertains to X=E2=80=9D) can overcome so=
me passivity =E2=80=93 instead of hoping it notices, we explicitly say =E2=
=80=9Cuse Info 1 and Info 3, ignore the rest if not relevant.=E2=80=9D Mode=
ls respond well to such explicit guidance.
One caution: If overdone, you might introduce bias or redundancy. Don=E2=80=
=99t repeat large chunks verbatim at the top and bottom, that wastes tokens=
=2E Summarize or highlight. And avoid c=
onfusing the model by conflicting info=
 (don=E2=80=99t say at top =E2=80=9CThe sky is blue=E2=80=9D and at bottom =
=E2=80=9CThe sky is green=E2=80=9D even if as a test =E2=80=93 it might ave=
rage them or get uncertain). The goal is emphasis, not contradiction.
6. Dynamic Context Compression (On-the-fly Simplification)
When to use: When dealing with data that has a lot of low-value or repetiti=
ve content, or when you need to iteratively shorten the context as a proces=
s goes on (like an agent reasoning through steps, gradually compressing wha=
t=E2=80=99s been done so far to fit more steps). Also applicable if the inp=
ut has boilerplate or noise that can be reduced without losing meaning (lik=
e logs, verbose transcripts, etc.). Use dynamic compression if you find tha=
t simply excluding data isn=E2=80=99t an option but it can be abbreviated.
What it is: Dynamically compressing context means using either algorithmic =
or model-driven methods to shorten the content while preserving the needed =
info, during the runtime of your application. This could be as simple as st=
ripping stopwords or as advanced as asking a smaller LLM to =E2=80=9Csummar=
ize this paragraph in one sentence=E2=80=9D before feeding it to the main L=
LM. It=E2=80=99s akin to zipping your context. The difference from the =E2=
=80=9Csummary chain=E2=80=9D is that dynamic compression often happens cont=
inuously or as part of an agent loop, not just a one-time summarization of =
a static document.
Step-by-step implementation:
Identify Compressible Parts: Determine what parts of your context can be sa=
fely compressed without losing fidelity for the task. E.g., if you have a J=
SON or structured data, maybe you can drop fields or reduce precision. If y=
ou have a list of records, maybe certain columns can be summarized (like av=
erage values instead of listing all).
Choose Compression Method: There are a few types:
Lossy compression (summarization): Use an LLM or rule-based method to summa=
rize text. e.g., compress a verbose meeting transcript into bullet points. =
Or instruct: =E2=80=9CRewrite this content in a more concise form.=E2=80=9D
Non-lossy compression (reformatting): Remove unnecessary tokens. For instan=
ce, remove extra whitespace, comments in code (if not needed), verbose form=
ality in text. Some texts can be rephrased shorter without losing info (e.g=
=2E, =E2=80=9Cdue to the fact tha=
t=E2=80=9D -> =E2=80=9Cbecause=E2=80=9D).
Structured compression: If the data is structured, you can compress by aggr=
egating. E.g., if 100 log lines have the same pattern, replace them with =
=E2=80=9C(repeated 100 times: [pattern])=E2=80=9D.
Model-driven compression pipelines: Some research approaches train models s=
pecifically to compress (like smaller models that maintain meaning). An exa=
mple is the =E2=80=9Cminifier=E2=80=9D approach that tries to drop words bu=
t keep meaning.
Integrate into Workflow: Decide when to compress. Maybe at the end of each =
conversation turn, compress the history beyond N turns (similar to context =
budget but focusing on making it shorter not just dropping). Or before send=
ing retrieved documents to the final LLM, run a compression model on them (=
either a smaller LLM or even a rule-based extractor).
Quality Check: Have a step to verify that compressed content still contains=
 key info. This might involve testing or even prompting the model to double=
-check something like =E2=80=9CIs anything critical missing from the summar=
y above? If so, what?=E2=80=9D. Often, iteratively compressing can degrade =
quality, so maybe compress gradually and stop when hitting a certain size.
Fallback: If the compressed version yields a weird or wrong answer, you mig=
ht need to fall back to using the full context (perhaps at the cost of a se=
cond call). For important tasks, it can be worth checking =E2=80=93 e.g., r=
un the model with compressed context, get answer A. Then maybe ask the mode=
l with original context (if possible) on a sample to ensure no major discre=
pancy. If differences, improve the compression strategy.
Enhanced Implementation: Use specialized libraries or models for compressio=
n. For example, OpenAI=E2=80=99s tiktoken or similar can count tokens =E2=
=80=93 you could build an automated system: =E2=80=9CIf > X tokens, feed ch=
unks into GPT-3.5 with a prompt =E2=80=98summarize this chunk preserving al=
l numbers and names.=E2=80=99 Then combine those.=E2=80=9D Or use extractiv=
e compression: have the model identify the most relevant sentences rather t=
han generating a summary (less chance of adding errors). This is like an ex=
treme form of retrieval where you retrieve within a single document by sele=
cting sentences.
Another concept is Knowledge Distillation: if you fine-tune a smaller model=
 on generating equivalent answers using less context, effectively the small=
er model learns to implicitly compress the context. But that=E2=80=99s heav=
y-lift R&D and likely unnecessary if simpler compressions suffice.
Real Example: Chat log compression: A support chatbot dealing with long dia=
logues started to exceed context and sometimes forgot older issues the user=
 mentioned. They implemented dynamic compression: the bot had an internal a=
ction it could take =E2=80=9CSUMMARIZE_HISTORY=E2=80=9D which, when trigger=
ed (say after 10 turns), would produce a concise summary of the chat so far=
=2E It would then replace the full h=
istory with =E2=80=9C[History Summary]:=20=
=E2=80=A6=E2=80=9D plus the last couple exchanges. This is dynamic because =
if the chat kept going, it could summarize again (a summary of summaries). =
They essentially replicated how a human might take notes during a long conv=
ersation. The result was the bot stayed aware of past points even 50 turns =
in, and the context length remained bounded. This is similar to context bud=
geting, but specifically focusing on using summarization as the tool to shr=
ink content.
Another example: Log file analysis. A team was using an LLM to analyze very=
 large log files for anomalies. Rather than feed raw logs, they wrote a pre=
-processor that would:
Remove all lines that matched a known =E2=80=9Cnormal=E2=80=9D pattern.
Cluster similar lines and just show one example with a count.
Trim each log line=E2=80=99s timestamp and extraneous debug info if not nee=
ded.
This cut down the log content by ~80% tokens without losing the unusual eve=
nts.
Then they gave the compressed log to the LLM, which could then focus on the=
 anomalies. If they hadn=E2=80=99t done that, the model would waste effort =
on repetitive normal entries and likely miss the needle in the haystack.
Quick Try: Take a verbose piece of text, like an overly wordy blog article =
or legalese paragraph. First, feed it to the model and ask a question that =
requires detail from it. Now compress the text manually or using the model =
(=E2=80=9CSummarize this in 3 sentences.=E2=80=9D). Feed the summary and as=
k the same question. Does it answer correctly? If yes, you managed to compr=
ess without losing the needed info for that question. If no, it means the s=
ummary left out something important. Adjust the summary prompt: maybe =E2=
=80=9Csummarize, but make sure to include the specific figures and names.=
=E2=80=9D Try again. Through this, you learn what kind of compression retai=
ns the critical data. This is essentially how you=E2=80=99d refine a dynami=
c compression approach =E2=80=93 ensure the compressed representation is su=
fficient for the queries you expect.
Why it works: Dynamic compression acknowledges that a lot of real-world dat=
a is not information-dense. There are redundancies, boilerplate, and fluff.=
 By removing those, you deliver a higher signal-to-token ratio to the model=
=2E The model then has less noise to wa=
de through and can focus on the core c=
ontent. This improves not just speed and cost (fewer tokens) but often accu=
racy, because the model isn=E2=80=99t distracted or wasting attention on ir=
relevant tokens. It=E2=80=99s similar to how you might highlight a document=
 for someone before they read it =E2=80=93 here we =E2=80=9Chighlight=E2=80=
=9D by condensing.
It also effectively extends what you can handle. If you compress a text by =
50%, you=E2=80=99ve effectively doubled the reach of your context window fo=
r that task. Some experiments have shown that carefully compressed long inp=
uts allow smaller models to get results as good as bigger models with raw i=
nput, because the smaller context is used more effectively.
However, be cautious: any lossy compression can omit a subtle detail. Alway=
s align compression with your needs. If exact recall of numbers or specific=
 quotes is required (like legal or financial contexts), consider extractive=
 approaches (keeping those specific parts verbatim) combined with summarizi=
ng the rest, so you don=E2=80=99t accidentally lose them.
Now we=E2=80=99ve covered six key strategies. In practice, you will mix and=
 match these. For example, a legal analysis workflow might use Retrieval (t=
o get relevant clauses) + Chunking (to process each clause) + Summary chain=
 (to summarize findings) + Position-aware prompting (to highlight crucial c=
ontract terms up front). A coding assistant might use Context budget (to ke=
ep important prior code context) + Dynamic compression (to shorten long cod=
e comments or diffs) + Retrieval (to fetch relevant documentation).
The overarching theme is: don=E2=80=99t rely on a single long prompt to do =
all the work. Structure it. Use multiple calls if needed. By orchestrating =
these patterns, you essentially build an LLM solution that feels much smart=
er and more reliable, even if under the hood it=E2=80=99s the same base mod=
el.
We=E2=80=99ll next see how these strategies play out in specific industry s=
cenarios =E2=80=93 because each domain has its quirks and best practices.
Industry-Specific Playbooks
Different industries have different =E2=80=9Clong context=E2=80=9D challeng=
es. Let=E2=80=99s delve into a few and outline how to apply the above strat=
egies in tailored ways, along with examples.
Legal: Long Contracts and Case Files
Scenario: You need to analyze or answer questions about lengthy legal docum=
ents =E2=80=93 e.g., a 100-page contract, a deposition transcript, or a bun=
dle of related case files. Mistakes or missed clauses can be costly, so acc=
uracy is paramount. The text is formal, with sections, definitions, and pos=
sibly a need to cross-reference (=E2=80=9Csee Section 5.2(b)=E2=80=9D).
Challenges: Legal documents often exceed 50K tokens. You can=E2=80=99t just=
 dump them wholesale. They have internal structure (sections, clauses). Als=
o, nuance is key =E2=80=93 skipping a =E2=80=9Cnot=E2=80=9D or misreading a=
 definition can invert meaning. So summarization must be careful not to dis=
tort. Also, some queries require comparing parts of the document (e.g., doe=
s clause X conflict with clause Y?). The model might need to see both in co=
ntext to answer properly.
Strategies to Use: Strategic Chunking + Retrieval, Summary Chain, and Conte=
xt Budgeting. Also possibly position-aware highlighting of definitions.
Playbook:
Split by Sections: Leverage the document=E2=80=99s structure. Contracts usu=
ally have numbered sections. Process each section as a chunk. Use a Map-Red=
uce summary chain if you want an overall summary: summarize each section, t=
hen summarize those summaries into a contract brief. This ensures no sectio=
n=E2=80=99s key points are missed.
Definitions: Contracts have a definitions section. It=E2=80=99s crucial bec=
ause terms like =E2=80=9CAffiliate=E2=80=9D or =E2=80=9CEffective Date=E2=
=80=9D have precise meanings. A trick: extract all definitions into a small=
 list at the top of your prompt as reference. E.g., =E2=80=9CDefinitions: =
=E2=80=98Effective Date=E2=80=99 means June 1, 2025; =E2=80=98Territory=E2=
=80=99 means North America=E2=80=A6=E2=80=9D. By position-aware prompting, =
the model will use these definitions throughout analysis.
QA via Retrieval: For Q&A, use a hierarchical retrieval approach: First, fi=
nd which sections likely contain the answer (you might keyword search withi=
n the contract =E2=80=93 e.g., question asks about indemnification, find wh=
ere =E2=80=9Cindemnify=E2=80=9D appears). Then feed just those sections to =
the model with the question. If multiple documents (like multiple contracts=
 or a law + a case), do retrieval across docs.
Comparisons: If task is comparing two contracts (e.g., differences in claus=
es), a good strategy is to summarize each contract=E2=80=99s key clauses fi=
rst, then have the model compare summaries, citing specific clauses. Altern=
atively, you can align them section by section (if similar structure) and h=
ave the model analyze section pairs.
Long Context Model with Chunking: Thomson Reuters=E2=80=99 experience showe=
d that even with GPT-4 1M context, they still chunk documents for complex t=
asks. For example, in their CoCounsel product, they discovered feeding the =
full long document can be less effective than chunking and then using the m=
odel on each chunk. So, if you have GPT-4.1 1M, you could technically stuff=
 a giant contract, but you might get a better result splitting it into e.g.=
 5 parts and asking 5 questions then combining. Their internal testing foun=
d multi-hop reasoning across a whole contract works more reliably by not re=
lying on the model=E2=80=99s raw long context alone.
Use Summaries for Memory: In a workflow like due diligence, you may analyze=
 dozens of contracts. Summarize each with a consistent template (e.g., part=
ies, term, liabilities, termination conditions). Those summaries are much s=
horter and can be compared or further processed easily. Possibly have the L=
LM generate a table of key points across contracts from those summaries.
Verification & Sources: Have the model quote the clause or section number a=
s evidence for its answers (you can instruct it: =E2=80=9CCite the section =
where this is stated.=E2=80=9D). This keeps it grounded. Some LLMs will fab=
ricate if unsure, so when it cites something, you can cross-check that clau=
se in the text. It=E2=80=99s an extra step but important in legal.
Example: Contract QA: Question: =E2=80=9CWhat are the termination rights in=
 this agreement, and under what conditions can each party terminate?=E2=80=
=9D Approach: vector search the contract text for =E2=80=9Cterminate, termi=
nation=E2=80=9D. Likely hits: a section named Termination, maybe others (li=
ke term of agreement). Extract those sections (maybe 2 sections). Prompt mo=
del: =E2=80=9CContext: Section 8: Term and Termination =E2=80=93 [text]; Se=
ction 12.4: Termination for Cause =E2=80=93 [text] Question: Summarize the =
termination rights of each party in this contract, including conditions.=E2=
=80=9D The model then outputs: =E2=80=9CThe agreement can be terminated by =
either party with 30 days=E2=80=99 notice if the other party breaches a mat=
erial term (Section 12.4). Additionally, Client may terminate for convenien=
ce with 60 days=E2=80=99 notice (Section 8). The rights are=E2=80=A6=E2=80=
=9D etc., ideally referencing those sections. Because we fed it only the re=
levant parts, it wasn=E2=80=99t distracted by unrelated content and could f=
ocus. If we had given the entire 50-page contract, the answer might have be=
en incomplete or buried something, especially if termination was in the mid=
dle somewhere.
Pitfall to Avoid: Don=E2=80=99t over-summarize critical legal language. If =
a question is about a precise condition (e.g., what triggers force majeure)=
, a summary might oversimplify and miss nuance like =E2=80=9Cincluding but =
not limited to=E2=80=A6=E2=80=9D. In those cases, you might want the model =
to quote or at least closely paraphrase the original language from that cla=
use. It might be better to retrieve the clause and ask: =E2=80=9CAccording =
to the clause, under what circumstances does force majeure apply?=E2=80=9D =
So, compress context up to a point, but for actual legal interpretation, so=
metimes the raw words matter. Balance is key.
Financial: Analyzing Reports and Data
Scenario: You have lengthy financial documents =E2=80=93 annual reports (10=
-Ks), earnings call transcripts, market research reports =E2=80=93 and you =
need insights or answers. These often include tables, numbers, and are upda=
ted regularly. Or you might feed a model lots of transactional data or logs=
 to identify trends.
Challenges: Financial texts mix narrative (CEO=E2=80=99s letter) and data (=
financial statements). They can be hundreds of pages. Also, precision with =
numbers is crucial; models can mess up numbers or do math wrong. There=E2=
=80=99s often repetition (quarterly reports have similar sections each year=
). You may want a summary but also exact figures for certain metrics.
Strategies: Summary Chain for narrative, Retrieval for specific data, Tools=
 for calculations, Context Budget for multi-quarter analysis. Possibly use =
external tools for number crunching (like an in-prompt calculator).
Playbook:
Separate Narrative vs Data: If analyzing a 10-K, consider treating the MD&A=
 (Management Discussion & Analysis) narrative differently from the financia=
l statements tables. Summarize the narrative sections (they=E2=80=99re verb=
ose) but perhaps extract the key figures from tables without summarizing th=
em (you don=E2=80=99t want to lose accuracy by summarizing numbers).
Tabular Data Handling: If context allows, you can include some tables. But =
models often struggle reading big tables reliably. An alternative is to pre=
-process tables: e.g., parse the table with code and then present the model=
 with a summary like =E2=80=9CRevenue in 2022 was $X (up Y% from 2021=E2=80=
=99s $Z).=E2=80=9D This is dynamic compression for numbers. Or use the new =
function calling capabilities: have the model output a JSON of key numbers =
then verify or calculate. If the model has a tool plugin (like a Python int=
erpreter), leverage it to do precise math rather than mental math.
Use Comparisons Over Time: For multi-period analysis, chunk by year or quar=
ter. E.g., if analyzing Q1 reports for the last 4 years, you might summariz=
e each Q1, then ask the model to identify trends from those summaries. This=
 ensures it actually sees all four periods (because you feed summary of eac=
h, instead of raw data where it might miss middle ones).
Context Budget for Conversations: If you have an interactive financial anal=
ysis chat, maintain a =E2=80=9Cfacts so far=E2=80=9D list. If the user and =
model have identified certain important numbers or insights, keep those in =
a running summary that persists in the prompt (so the model doesn=E2=80=99t=
 have to re-read tables to remember the revenue was $5M).
Ask Specific Questions vs Open-Ended: Long financial documents can overwhel=
m an open =E2=80=9Canalyze this=E2=80=9D prompt. Instead, break the task: A=
sk, =E2=80=9CList the top 5 risk factors mentioned.=E2=80=9D or =E2=80=9CWh=
at were the main drivers of growth this year, according to the report?=E2=
=80=9D One can follow a sequence: first ask model to list sections and thei=
r gist (it can usually skim headings), then dive deeper into each section a=
s needed (using retrieval for that section=E2=80=99s text). This is basical=
ly summary chain applied section by section.
Use Retrieval for Reference Data: If a user asks =E2=80=9CHow did ACME Corp=
=E2=80=99s 2024 R&D expense compare to 2023?=E2=80=9D, you could retrieve t=
he parts of the report that mention R&D expense or find those figures (perh=
aps via a small search in the document text for =E2=80=9CResearch and devel=
opment=E2=80=9D). Provide the snippet or value to the model. If the model s=
hould calculate the growth rate, ensure it has both numbers clearly and con=
sider instructing it to do math carefully (or just calculate externally and=
 feed it the result to be safe).
Monitoring Model Math: Be aware the model might make arithmetic errors espe=
cially with big numbers. One approach: after the model answers with numeric=
 analysis, you could have a verification step. For instance, ask the model:=
 =E2=80=9CDouble-check the calculations above.=E2=80=9D Sometimes it will c=
atch its own error on a second pass. Or use programmatic function calling t=
o actually compute differences. The =E2=80=9CTools That Help=E2=80=9D secti=
on below will mention monitoring solutions =E2=80=93 in financial context, =
one might have a small script to validate that the percentages it states ma=
tch the numbers provided.
Example: Earnings Call Transcript: ~30 pages of conversation. The goal: sum=
marize main points and extract any forward-looking statements or sentiments=
=2E Approach:
Use summary chain: break transcript by speaker turn blocks (e.g., CEO speec=
h, CFO speech, Q&A part).
Summarize each part.
Then prompt: =E2=80=9CSummarize the key points from the earnings call: [CEO=
 summary] [CFO summary] [Q&A summary].=E2=80=9D The final output: bullet po=
ints of results, guidance, market reaction, etc.
If interested in sentiment, maybe instruct model: =E2=80=9CIdentify any con=
cerns or optimistic notes the management expressed.=E2=80=9D
This yields a digest a human could have taken hours to assemble.
If there=E2=80=99s a specific question like =E2=80=9CWhat revenue guidance =
was given for next quarter?=E2=80=9D, just retrieve that portion of transcr=
ipt (find where CFO says =E2=80=9Cguidance=E2=80=9D) and have the model quo=
te it or paraphrase with high accuracy.
Pitfall to Avoid: Hallucinating numbers. Do not trust an LLM to pull a numb=
er out of thin air. It should come from provided text. To mitigate: always =
provide the model the actual text or data that contains the number. If it=
=E2=80=99s summarizing and needs a number, either include that in the conte=
xt or verify afterwards. Also, extremely domain-specific finance concepts (=
like complex accounting terms) might confuse a general model; consider prov=
iding definitions or using a specialized model if available.
Healthcare: Patient Records and Clinical Notes
Scenario: You want an LLM to help summarize a patient=E2=80=99s medical his=
tory from electronic health records (EHR), or answer questions based on a p=
atient=E2=80=99s records (which might include doctor=E2=80=99s notes, lab r=
esults, medication lists, etc.). These records can be very lengthy over a c=
hronically ill patient=E2=80=99s history.
Challenges: Privacy and accuracy are critical. Hallucination is dangerous h=
ere (you don=E2=80=99t want the model to =E2=80=9Cmake up=E2=80=9D a diagno=
sis). Medical terminology and shorthand in notes can be complex. Also, the =
information might be spread over many notes and timepoints. It=E2=80=99s si=
milar to legal in that detail matters (e.g., medication dosage, allergy inf=
o). Also, EHR data might not be nicely formatted =E2=80=93 lots of abbrevia=
tions, maybe scanned OCR text, etc.
Strategies: Chunking by record type/date, Context Budget focusing on proble=
m list, Retrieval for relevant notes, Summarization for older history. Also=
 maybe use specialized medical LLM or at least provide context for terms.
Playbook:
Organize by Source: Split the patient record into logical groups: e.g., His=
tory & Physical notes, Progress notes by date, Lab results, Medication list=
, Imaging reports. Each is a chunk category. This helps because the model m=
ight better handle one category at a time (and you can prompt accordingly).
Summarize Chronologically: Often you need a narrative of the patient journe=
y. You can apply a refine chain across chronological notes: start with the =
earliest note summary, then refine it with new info from each subsequent no=
te (=E2=80=9CUpdate the summary given this new note=E2=80=9D). This yields =
a progressive patient summary that ideally includes all major events. This =
is a dynamic summarization approach and helps compress dozens of notes into=
 one story.
Highlight Key Data: Use position-aware technique to ensure critical info li=
ke Allergies, Current Medications, Major Diagnoses are up front. For instan=
ce, if present, always prepend: =E2=80=9CAllergies: X; Current Meds: Y; Pro=
blems: Z.=E2=80=9D This context at the top ensures the model never suggests=
 a medication the patient is allergic to, for example (because the allergy =
is right there in prompt).
Retrieval for Q&A: If a clinician asks =E2=80=9CHas the patient ever had an=
 MRI of the spine and what were the results?=E2=80=9D, you can search the r=
ecords for =E2=80=9CMRI spine=E2=80=9D and find that imaging report. Feed j=
ust that to answer. Or if asked =E2=80=9CList all medications the patient h=
as been on for diabetes,=E2=80=9D retrieve medication lists and relevant no=
tes about diabetes management.
Context Budget =E2=80=93 Focus on Recent Info: In healthcare, recent events=
 are usually more relevant than a decade-old note, except maybe past surgic=
al history. Use a strategy to always include the latest notes fully, and ol=
der ones only in summary form. E.g., =E2=80=9CLast 3 office visits full tex=
t, older visits summarized in 1 paragraph.=E2=80=9D This ensures current da=
ta (like last week=E2=80=99s lab results) aren=E2=80=99t missed because the=
 model got lost in a 2010 note about a resolved issue.
Verification / No Hallucination Prompting: Use the prompt to discourage gue=
ssing: e.g. =E2=80=9CIf the records do not explicitly mention something, do=
 not assume it.=E2=80=9D This might reduce hallucination. Also, use a tool =
or knowledge base for medical facts if needed rather than rely on model=E2=
=80=99s possibly outdated knowledge (if question is about treatment guideli=
nes, either provide that info or instruct it to base answers only on patien=
t data).
Use of Medical NER (entity recognition): Pre-process notes to extract struc=
tured data: patient=E2=80=99s conditions, meds, etc. Then provide that as a=
 structured summary in prompt (like a problem list). This is like compressi=
ng and organizing the info. Some open tools or libraries can do this or one=
 can prompt an LLM to list the problems mentioned in notes, then feed that =
list back in as a reference.
Example: Patient Summary: Input: 50 pages of varied notes. Task: produce a =
summary for a referral letter. Approach:
Divide into sections: Past Medical History, Meds, Allergies (these are ofte=
n available in the chart separately), then chronological notes.
For notes: do a map-reduce: summarize each year=E2=80=99s notes into =E2=80=
=9CYear X summary=E2=80=9D (e.g., 2020: =E2=80=9CPatient had two hospitaliz=
ations for COPD exacerbation; started on home oxygen.=E2=80=9D).
Then combine those yearly summaries into an overall summary.
Double-check any critical values (like ensure latest lab values are mention=
ed).
The final summary to doctor includes a separate list: =E2=80=9CActive issue=
s: COPD, Type 2 Diabetes, Hypertension; Surgeries: Appendectomy (2005)=E2=
=80=A6=E2=80=9D followed by narrative timeline. This ensures completeness.
The model did not have to see every single note word-for-word at once, but =
via summarization it =E2=80=9Csaw=E2=80=9D them in aggregate.
If a question arises like =E2=80=9CWhen was the first diagnosis of diabetes=
 and what was the initial treatment?=E2=80=9D, one could either parse the s=
ummary (if it=E2=80=99s in there) or specifically search the notes for =E2=
=80=9Cdiabetes=E2=80=9D and find earliest mention and the plan.
Pitfall to Avoid: Mixing patients=E2=80=99 data. If by any chance multiple =
patient data are together, ensure separation (shouldn=E2=80=99t happen in a=
 single prompt scenario, but just in case). Also, privacy: if using an API,=
 one must be careful with PHI (though that=E2=80=99s a whole other topic be=
yond context handling). On a technical level, avoid hallucinated medical ad=
vice =E2=80=93 ideally the system would only present what=E2=80=99s in reco=
rds and not outside medical judgment (unless that=E2=80=99s desired and the=
n perhaps a specialized model or known references should be used). Always h=
ave a human clinician review outputs; use LLM as assistant, not final decis=
ion maker.
Software: Large Codebases and Repositories
Scenario: Using LLMs for code =E2=80=93 e.g., analyzing a large codebase fo=
r bugs or generating summaries of code functionality, or assisting with cod=
e reviews across thousands of lines. You might have files that far exceed c=
ontext (maybe generated code or large configs), or multiple files that need=
 to be considered together (like definition in one, usage in another).
Challenges: Code requires exactness. LLMs can do well in small context but =
on huge code they may forget important lines or functions. Also, repetition=
 and boilerplate are common in code (e.g., many similar functions, or long =
repetitive data structures) which waste context. The user might want a spec=
ific part fixed, but provided the whole file and the model might miss the r=
elevant part. Also, code often has structured hierarchy (projects, modules)=
=2E
Strategies: Chunk by file/function, Retrieval by symbol, Position-aware for=
 focus, Dynamic compression via abstraction. Possibly chain-of-thought prom=
pting for step-by-step debugging.
Playbook:
Chunk by File or Module: Treat each file as a chunk. Use file-level context=
 to answer questions or generate summaries per file. If an analysis needs m=
ultiple files (like find where a function is called across files), consider=
 building an index of function definitions and references (like a simple sy=
mbol index), then retrieve relevant snippets.
Use Code-Aware Tools: Many LLM coding assistants use a technique: they firs=
t retrieve relevant code snippets via static analysis or regex (like find a=
ll references to a function name) and only feed those into the model. You c=
an do similar: if user asks about function foo(), search the codebase for f=
oo( and pull those contexts.
Position hints: If you feed a large code file in (say 500 lines) and you wa=
nt the model to focus on a particular section (maybe where a bug likely is)=
, you can insert a comment like // NOTE: Potential issue here in the code b=
efore giving it to the model. This is a form of position-aware prompting in=
 code. The model will likely pay attention there.
Summarize/Abstract Repeated Code: If a file contains a lot of repetitive co=
de (like 100 similar entries), you can compress it: =E2=80=9C// =E2=80=A6 5=
0 similar lines omitted =E2=80=A6=E2=80=9D or =E2=80=9Cfunction definitions=
 for getters are similar and omitted for brevity.=E2=80=9D The model doesn=
=E2=80=99t need to see all if they are formulaic. It can trust that omitted=
 content is just more of the same. This manual compression of code can save=
 a ton of tokens.
Iterative Debugging: If trying to debug with the model, don=E2=80=99t feed =
entire project at once. Instead:
Ask it to analyze one part (maybe the error log or a specific function).
Then if it suspects the issue is elsewhere, retrieve that file and feed it.
The agent approach: the model might say =E2=80=9CIt might be an issue in Fu=
nction Y in file2.py.=E2=80=9D Then you fetch file2.py and provide it to co=
nfirm. This is similar to how one would debug manually =E2=80=93 focus wher=
e clues lead.
Context Budget in Chat Code Review: If using an LLM in a code review chat, =
keep the code diff or relevant chunk and drop older diffs as they become ir=
relevant. Summarize what decisions have been made (=E2=80=9CWe decided to u=
se approach X to fix Y=E2=80=9D) and keep that as context rather than the e=
ntire prior code discussion.
Leverage Embeddings for Code: Represent each function or class by an embedd=
ing of its docstring or signature. This can allow semantic search (=E2=80=
=9Cembedding search for Vector class multiply method=E2=80=9D). This is mor=
e advanced, but retrieval can be semantic (e.g., user asks =E2=80=9Cwhere i=
n code do we parse JSON?=E2=80=9D =E2=80=93 embed that question, find neare=
st code blocks embedding, likely lands in a JSON parser file).
Two-Model Approach: This is interesting and was hinted at in that forum: us=
e one model or pass to scan a lot of code (maybe smaller context model or o=
ne specialized in code search) to find candidate locations, then use the bi=
gger reasoning model to analyze those. E.g., Model A: =E2=80=9CFind all fun=
ctions named X or similar in the code =E2=80=93 output their paths.=E2=80=
=9D Then Model B: =E2=80=9CGiven these code snippets from those locations, =
do Z.=E2=80=9D
Example: Large Codebase Q&A: Question: =E2=80=9CWhere is the user=E2=80=99s=
 password hashed in this project?=E2=80=9D Instead of giving the whole proj=
ect, you:
Search code for =E2=80=9Chash(=E2=80=9C or =E2=80=9Cbcrypt=E2=80=9D etc. Su=
ppose it finds auth.py line 150 and utils/security.py line 20.
You feed the model: =E2=80=9CIn auth.py at line 150: =E2=80=A6 (code) =E2=
=80=A6, in utils/security.py at line 20: =E2=80=A6 (code) =E2=80=A6 The que=
stion: where is the password hashed and how?=E2=80=9D
The model then sees exactly the relevant code and can answer: =E2=80=9CThe =
password is hashed in utils/security.py in the function hash_password(passw=
ord) which uses bcrypt. In auth.py line 150, that function is called before=
 saving the user.=E2=80=9D It might even quote some snippet. This is a prec=
ise and cost-effective answer.
Another: Code Summary: Summarize a 1000-line file. You could chunk it by fu=
nction: have the model summarize each function (maybe by prompt =E2=80=9CSu=
mmarize what this function does=E2=80=9D), then combine. Or first, strip ou=
t comments and trivial code, compress the repetitive sections as described,=
 maybe you end up with a 400-line gist that=E2=80=99s still semantically re=
presentative. Then feed that for summary. This might be semi-automated with=
 scripts.
Pitfall to Avoid: The model guessing something about code it didn=E2=80=99t=
 see. If you skip too much, it might assume. So when compressing code, ensu=
re you mark clearly when you=E2=80=99ve omitted something and maybe describ=
e it. E.g., =E2=80=9C// =E2=80=A6 similar logic repeated =E2=80=A6=E2=80=9D=
=2E Also, watch out for the model out=
putting hallucinated code that isn=E2=80=
=99t in the base =E2=80=93 stick to tasks like summarization or Q&A rather =
than generating completely new large code in these contexts unless you have=
 tests. For bug fixes, always test the suggested fix; LLM might fix one thi=
ng but break another if it lacked full context.
We have now looked at specific playbooks for different domains. While the t=
ools are similar (chunk, retrieve, summarize), the way you apply them varie=
s. Legal needed careful retention of exact language for citing clauses. Fin=
ance needed careful handling of numbers. Healthcare needs summarization wit=
h caution on facts. Code needs targeted searching for relevant parts and po=
ssibly ignoring repetitive boilerplate.
Next, let=E2=80=99s go over some tools and frameworks that can help impleme=
nt these strategies without reinventing the wheel.
The Tools That Actually Help
Managing context can be tough to implement from scratch, but luckily there=
=E2=80=99s an evolving ecosystem of tools, libraries, and platform features=
 to make it easier. Here we focus on those that actually help in production=
 (as opposed to just theoretical demos).
Context Management Libraries & Frameworks
LangChain (and LangChain Hub): LangChain became popular for chaining LLM ca=
lls and has utilities for many patterns we discussed. For context:
It provides TextSplitter classes to chunk text in various ways (by chars, t=
okens, sentences, etc.) =E2=80=93 useful for Strategic Chunking.
It has DocumentLoaders and VectorStore integrations to do RAG easily =E2=80=
=93 you can ingest your docs and query them with a few lines.
The Memory classes implement conversation memory: e.g., ConversationBufferM=
emory (keeps full history), ConversationSummaryMemory (summarizes old messa=
ges) =E2=80=93 implementing Context Budget logic out-of-the-box.
There=E2=80=99s a LongContextReorder transformer (we saw earlier) in LangCh=
ain=E2=80=99s community utils which will reorder retrieved docs to avoid lo=
st-in-middle.
Using it: If you=E2=80=99re building a Python app, LangChain can save time =
orchestrating calls. Just be mindful of performance and test that it=E2=80=
=99s doing what you expect (some early versions were less optimized).
LlamaIndex [ https://substack.com/redirect/7abc1ba2-fbc0-403f-97c8-c4085178=
8b58?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0 ] =
(aka GPT Index): Another library focused originally on documents. It builds=
 indices (vectors, keyword, etc.) and has query engines that automatically =
break queries into retrieval + synthesis steps.
It has capabilities like TreeIndex which essentially does summarization in =
a tree (great for summary chain).
LlamaIndex is nice for large documents =E2=80=93 it can recursively summari=
ze big texts by building an index.
It also can do structured retrieval (pull specific fields from JSON, etc.).
If you have lots of data sources, it can unify them (SQL, documents, APIs).
It may be a good fit if your use case is document question answering or bui=
lding a chatbot over knowledge.
OpenAI Function Calling [ https://substack.com/redirect/665c508e-f2f9-44cc-=
b2db-21e3acfaf315?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4=
tAjMRMPOw0 ] / Tools: OpenAI=E2=80=99s API now supports function calling wh=
ere you can define tools (like search, calculators) that the model can use.=
 This is useful for context:
You can implement a search tool that when the model queries it, you do a ve=
ctor DB lookup and return relevant text. The model then sees that text. Thi=
s essentially is in-chat retrieval without you handling the prompting logic=
=2E
You can have a summarize function that triggers a known sequence (though of=
ten you might handle summarization outside the model).
E.g., have a function get_document(section_id) that returns a section of a =
doc. The model can decide to call it if it needs more info.
This moves some orchestration inside the model=E2=80=99s reasoning, which i=
s powerful but requires careful setup so it doesn=E2=80=99t hallucinate too=
l usage.
If done right, you get dynamic retrieval and even iterative reading (the mo=
del can decide =E2=80=9CI need section 2=E2=80=A6 now section 3=E2=80=A6=E2=
=80=9D).
Many open-source frameworks (LangChain, etc.) also support this or similar =
agent capabilities.
Zep Memory [ https://substack.com/redirect/6c5ba8cf-7f4a-481b-8d88-421da858=
fb61?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0 ]:=
 Zep is an open-source LLM memory store. It=E2=80=99s designed to plug into=
 chatbots to store conversation history with embeddings, and serve summarie=
s or relevant pieces back.
It automatically updates a summary of the conversation as it grows, and can=
 fetch historically relevant messages (like if the user asks something rela=
ted to what was said 50 turns ago, it might bring that back).
This directly helps context budget and retrieval in dialogues. Instead of y=
ou coding memory logic, Zep handles it.
Useful if you want a persistent long-term memory that survives beyond one s=
ession (it can store vector embeddings of all conversation chunks).
Pinecone [ https://substack.com/redirect/0c009758-68c2-4855-8243-7d0e03d945=
c8?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0 ] / =
Weaviate [ https://substack.com/redirect/a4084310-5982-41c6-af86-49e486f08f=
f3?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0 ] / =
Chroma [ https://substack.com/redirect/f8a88eea-984b-4c1c-bef0-02f85ae30742=
?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0 ] (Vec=
tor DBs): These are vector databases that store embeddings and let you quer=
y by similarity. They are almost a must-have for implementing RAG at scale.
They handle large volumes, indexing, and fast search =E2=80=93 so for tens =
of thousands of docs, you can retrieve in milliseconds the top relevant bit=
s.
They often have metadata filters (e.g., only search docs of type X or date =
range Y).
Many of them have demos or templates for pairing with LLMs.
If you use OpenAI or Azure, you also have the option of their built-in vect=
or stores (e.g., Cognitive Search with embeddings, or Pinecone hosted).
Using effectively: ensure you embed queries and docs with the same model an=
d a good dimensionality. Monitor the similarity scores and experiment with =
chunk sizes for indexing (too large chunk might reduce precision, too small=
 might lose context).
Weaviate and others even have modules for specific data types (images, etc.=
) but for text, basic ones suffice.
Prompt Templates and Libraries: Tools like Promptify [ https://substack.com=
/redirect/d411a9ac-31f3-4e8e-84aa-83eef316f902?j=3DeyJ1IjoiNWtiOTN6In0.zdzy=
88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0 ], Guidance (by Microsoft), etc., a=
llow more programmatic control over prompts.
Guidance can let you write a prompt with control flow (like loops, conditio=
nals) which can be useful to implement things like =E2=80=9Cfor each chunk,=
 do X=E2=80=9D within one prompt if using an LLM that way.
These are more advanced and sometimes it=E2=80=99s easier to just script lo=
gic in Python than to manage a single complex prompt, but they=E2=80=99re w=
orth knowing.
Zilliz [ https://substack.com/redirect/351f4f88-9a00-4d77-be44-99f3453dfdbb=
?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0 ] / Mi=
lvus [ https://substack.com/redirect/32d10319-0c8a-412f-8a6e-0f513b7a0328?j=
=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0 ]: These=
 are vector DB options (Milvus is open source, Zilliz is cloud) similar to =
Pinecone etc. They=E2=80=99re quite scalable if you need enterprise scale.
Hugging Face Transformer [ https://substack.com/redirect/c8003d2f-fd10-46bb=
-b80f-188b6b2553bf?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ=
4tAjMRMPOw0 ]s / Text Generation Inference: If you=E2=80=99re using open-so=
urce models, the HF ecosystem has utilities like transformers pipeline, and=
 text-generation-inference server which handle large context (with some opt=
imizations like streaming, etc.). You might need to adjust settings like ma=
x_length (for context) carefully.
Monitoring and Evaluation Tools for Context
Arize Phoenix [ https://substack.com/redirect/b10d3f53-0636-4a30-ae66-c4d9a=
4e77ab7?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0=
 ]: Mentioned earlier, Arize=E2=80=99s Phoenix is an LLM observability tool=
=2E It can trace and evaluate model performance.
It allows you to see where in the input the model is focusing (via attentio=
n heatmaps) or to run the lost-in-middle evaluation by perturbing input.
You could use it to test your system: feed a known input with some salient =
info in the middle and see if the model picks it up. Phoenix would help ide=
ntify such failure modes.
Also, Arize has error analysis; e.g., you can log your LLM=E2=80=99s respon=
ses and use their tools to spot if longer inputs correlate with more errors=
 =E2=80=93 a way to quantify effective context.
LangSmith (by LangChain): A newer tool to trace LLM interactions. It logs e=
ach step of a chain or agent. This is useful to debug how your context stra=
tegies are working in multi-step flows (e.g., did the agent actually call t=
he search tool with a good query? Did the summarizer step produce something=
 too lossy?).
It=E2=80=99s more devtool than monitoring for production, but helps refine =
your prompt chains.
OpenAI Eval Harness: OpenAI has an evals framework (and others in community=
) where you can script evaluations for your prompt flows. For example, you =
can create test cases to ensure that when info is at various positions, the=
 model still finds it. Running those regularly can catch regressions if you=
 change prompts or update models.
This is more for offline testing but is crucial for building trust that you=
r context management works.
Custom Logging & Alerts: Don=E2=80=99t underestimate rolling your own simpl=
e monitors: e.g., log the length of each prompt and response, and perhaps t=
he portion of context that was actually used (if model ends up quoting from=
 context or not). If you see responses like =E2=80=9CI cannot find X=E2=80=
=9D but X was indeed in the context (just buried), that=E2=80=99s a flag yo=
ur method failed =E2=80=93 you could set up alerts or triggers for that kin=
d of scenario in logs.
Another trick: plant a hidden marker in each chunk of context (like a uniqu=
e token) and see if the model ever mentions or uses it. If some marker neve=
r appears or affects output across many runs, maybe that chunk is consisten=
tly ignored (could be fine if irrelevant, or sign of lost-mid).
Alternatives and When to Use Them
We=E2=80=99ve focused on using long contexts effectively, but sometimes the=
 best solution is to not use long context at all for certain parts:
Vector DB + Smaller Model vs Giant Context Model: As one experiment cited, =
a 1.1B model with RAG outperformed GPT-4 Turbo on fact search. If your use =
case is straightforward QA on a static knowledge base, consider using a mod=
erate LLM (maybe open-source) with a good retrieval system, instead of payi=
ng for super long context on GPT-4. The cost can be lower and performance s=
urprisingly good since the retrieval does heavy lifting.
Traditional Search/Database: For data like logs, code search, FAQs =E2=80=
=93 sometimes a good old keyword search or SQL query can get the needed inf=
o faster than embedding search or feeding everything to an LLM. Don=E2=80=
=99t force the model to read a big table to find a max value =E2=80=93 you =
can query that from a database and just show the model the result. Use the =
right tool for the job and use LLM for what it=E2=80=99s best at (language =
understanding, summarizing, reasoning).
Fine-tuning or Smaller Specialized Models: If you repeatedly need to do a s=
pecific compression (like always summarizing legal docs in a format), a sma=
ll fine-tuned model might do that in one step with less context (because it=
=E2=80=99s learned to pick out important parts). Fine-tuning extends =E2=80=
=9Ceffective=E2=80=9D context by teaching the model to ignore irrelevant mi=
ddle parts.
Structured approaches (graphs, ontologies): In something like medical or ev=
en legal, you could store info in a structured form (like a knowledge graph=
 of patient conditions, or contract obligation matrix). Then queries can be=
 answered by traversing that and using LLM just to phrase the answer. That =
reduces reliance on long text prompts.
The theme is: Integrate LLMs with the rest of your stack. Don=E2=80=99t tre=
at them as magical oracles that must ingest everything raw. Use databases, =
search engines, caching. There are emerging libraries that help with this o=
rchestration:
LlamaIndex (again) can integrate with SQL: e.g., if data is better fetched =
via SQL query, you can allow the LLM to do that rather than scanning a text=
 dump. This is available through tools like the SQLDatabaseAgent in LangCha=
in or similar in LlamaIndex, where the model can decide to pull data from a=
 DB.
Memory Augmented Chains: like the Reflexion idea (model writes important in=
fo to its =E2=80=9Cnotes=E2=80=9D and reads later). Some frameworks impleme=
nt prototypes of this. Could be relevant if you want the model to create an=
d refer to an external scratchpad file or notes.
Technical Notes (Callouts):
Tokenization differences: Remember that models have different tokenization.=
 100K tokens for GPT-4 might be ~80K words, but for a different model 100K =
tokens could be more or fewer words. When planning chunk sizes, always meas=
ure with the tokenizer of your target model (OpenAI=E2=80=99s tiktoken can =
mimic GPT-4 tokenization).
Batching calls: If chunking leads to many parallel calls (say splitting a d=
oc into 20 chunks and summarizing each), watch your API rate limits. You mi=
ght need to batch or throttle. Some libraries can batch prompts for you to =
maximize throughput.
Model version drift: A prompt that works on GPT-4-0314 might behave differe=
ntly on GPT-4-0613. So if you rely on position or certain formatting, retes=
t when models update. E.g., some lost-in-middle quirks might improve over t=
ime (OpenAI claims GPT-4.1 was specifically trained to handle full 1M bette=
r , albeit still with drop at extreme). So adapt your strategies as models =
get better, but never assume the hype is fully realized without testing.
Open-source models context: Many open ones (Llama 2 etc.) have context limi=
ts like 4K or 16K. There are projects extending them (e.g., 100K Llama via =
finetuning positional embeddings). If you use those, verify the effective c=
ontext. Some of these extended models might still effectively only do much =
less. Always validate with a lost-in-middle style test for any model you ch=
oose.
Using these tools and approaches can significantly reduce the engineering b=
urden. Instead of writing your own chunk-splitting logic or vector search, =
you plug one of these in. Just be sure to evaluate them on your data =E2=80=
=93 sometimes default settings aren=E2=80=99t optimal, and you=E2=80=99ll t=
weak chunk size or prompt templates.
What=E2=80=99s Coming Next (And What=E2=80=99s Just Hype)
It=E2=80=99s 2025; we=E2=80=99ve seen context windows explode this past yea=
r. What=E2=80=99s next in the pipeline, and which claims should you view sk=
eptically?
Emerging Solutions:
Truly Longer Context Models: Google=E2=80=99s Gemini 2.5 Pro (1M) and talk =
of 2M context in the next version. OpenAI=E2=80=99s GPT-4.1 series at 1M. T=
hese indicate a push towards million+ token contexts being mainstream. We m=
ight see OpenAI GPT-5 or Anthropic=E2=80=99s Claude-5 boasting even more (m=
aybe 10M? Who knows). DeepMind=E2=80=99s work hints at multi-million contex=
t (they did a 2M context demo).
Architectural Changes: Researchers are tackling the fundamental limits. For=
 example:
Efficient Attention (FlashAttention, etc.) =E2=80=93 these don=E2=80=99t ch=
ange quality per se but allow models to handle longer inputs faster. This c=
an reduce cost a bit. By 2025, many models employ FlashAttention under the =
hood for speed, but they still have the =E2=80=9Clost middle=E2=80=9D issue=
 because that=E2=80=99s a conceptual issue, not just speed.
Segmented or Hierarchical Models: E.g., models that first summarize chunks =
internally and then combine (basically building summary chain into the arch=
itecture). These could use something like Transformer-XL or Retentive Netwo=
rks where there=E2=80=99s a notion of state or recurrence, so they don=E2=
=80=99t attend fully to far past tokens but have a kind of memory. If those=
 take off, effective context might improve. There=E2=80=99s research like R=
ecurrent GPT that aim to let models process streams indefinitely by compres=
sing state. Not production-ready yet, but keep an eye.
Retrieval-Augmented Generation (RAG) built-in: Expect models that come with=
 built-in retrieval mechanisms. E.g., DeepMind=E2=80=99s retro (from 2022) =
used a database for factual info. OpenAI might integrate a vector search in=
 future models so that you give it an API to fetch relevant text instead of=
 expanding context. This blurs context window concept, but effectively it=
=E2=80=99s infinite context via retrieval. If that happens, it might solve =
many issues as the model learns to pull only what it needs.
Better Positional Embeddings: The rotary embeddings scaling trick took us t=
his far, but new research like that  [ https://substack.com/redirect/d26da6=
26-fa79-4f2d-9c13-b8c6a0dc7248?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshg=
UC7BDnPG6RxQ4tAjMRMPOw0 ]SHIFT (StRING) [ https://substack.com/redirect/d26=
da626-fa79-4f2d-9c13-b8c6a0dc7248?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9h=
shgUC7BDnPG6RxQ4tAjMRMPOw0 ] paper [ https://substack.com/redirect/d26da626=
-fa79-4f2d-9c13-b8c6a0dc7248?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC=
7BDnPG6RxQ4tAjMRMPOw0 ] suggests we can train models to use long positions =
more uniformly. Perhaps new models (like Llama 4 or others) will incorporat=
e such techniques to have more uniform attention across the window. That co=
uld mitigate lost-in-middle somewhat, making effective context closer to ac=
tual context.
Memory and Tool Use: There=E2=80=99s a lot of work on making LLMs more agen=
tic =E2=80=93 e.g., automatically storing info in a =E2=80=9Cmemory=E2=80=
=9D (external) when context is full, and retrieving it later as needed. Pro=
jects like BabyAGI, AutoGPT etc., are not really new, but they signal inter=
est in LLMs that can self-manage context by writing notes. We might see pro=
ducts that implement robust long-term memory via vector DB behind the scene=
s, giving illusion of huge context.
Open-Source Developments: If history repeats, open models will catch up on =
context length. E.g., maybe Llama 5 will target 500K of effective working m=
emory. Also models like Mistral or others might push long context as a diff=
erentiator (like Anthropic did). There=E2=80=99s also techniques to fine-tu=
ne or patch open models to extend context without full retraining (some suc=
cess with Llama 2 extended to 32K by fine-tuning position encodings). Expec=
t the community to keep pushing those boundaries, though as we know, length=
 alone doesn=E2=80=99t equal effectiveness.
Expanded Multimodal Context: Context is already not just text. Models accep=
t images, audio, etc., along with text. That=E2=80=99s already started but =
it needs to expand to include more video. Tools that do multimodal compress=
ion will be part of the toolset (e.g., summarizing a video transcript to fe=
ed into a text model).
Hardware Advances: There are hardware innovations (like more GPU memory, fa=
ster interconnects) that could make large context less slow or costly, thus=
 more feasible to always use. But they don=E2=80=99t fix the core forgettin=
g issue =E2=80=93 they just remove some performance penalty. Still, if infe=
rence cost drops, some might choose brute force usage of context (and maybe=
 just over-sample context to hide lost info =E2=80=93 like repeat important=
 parts multiple times in input, which some have tried).
Realistic Timeline: Over the next 1-2 years, I suspect we=E2=80=99ll get ma=
rginal improvements in effective long context but not a miraculous fix. May=
be GPT-5 will be somewhat better at long contexts through more training or =
architecture tweaks, but not perfect memory. It=E2=80=99s likely the vendor=
 marketing will continue to outpace reality, as we saw with Gemini boasting=
 1M but devs finding it effectively ~128K. So, remain healthily skeptical.
What=E2=80=99s likely to remain a problem:
Lost-in-the-middle is not fully solved by just scaling context. Without fun=
damentally new architectures, the U-shaped performance issue will persist. =
Vendors might not talk about it, but independent evals (like that Stanford =
paper ) will keep showing it.
Cost will still scale with length. Even if per-token cost drops, feeding 1M=
 tokens will always cost more than 1K. So the CFO=E2=80=99s shock might les=
sen per call, but high volume usage of mega contexts will still need budget=
 considerations. If anything, as context capacity grows, people will try to=
 stuff more, possibly raising costs again in aggregate.
Model hallucination and quality dips on long inputs: More context means mor=
e chances to include conflicting or irrelevant info that confuses the model=
=2E That fundamental challenge =E2=80=
=93 discerning what=E2=80=99s relevant a=
mong a huge context =E2=80=93 is not solved by bigger memory alone; it=E2=
=80=99s a reasoning challenge. The model might still latch onto a random de=
tail and base answer on it erroneously.
Human factors: Long contexts can lull you into not curating inputs. If some=
one thinks =E2=80=9CI have 500K tokens, I=E2=80=99ll just toss in everythin=
g=E2=80=9D, the answers might degrade as the model drowns, and it=E2=80=99s=
 harder for a human to even verify since the prompt is huge. So, the practi=
ce of intelligently providing context will remain important (garbage in, ga=
rbage out still applies).
Worth Watching (Emerging solutions):
Retrieval-augmented models: As mentioned, any news of models that integrate=
 search or memory in architecture is promising.
Continual learning or adaptive models: There=E2=80=99s an idea of models th=
at can update their weights or knowledge incrementally (instead of static t=
raining). If that happens, you might not need long prompts =E2=80=93 the mo=
del could =E2=80=9Clearn=E2=80=9D your data. But short term, most will use =
retrieval as the form of memory.
Tool ecosystems: With function calling, the trend might be to break tasks i=
nto sub-tasks. For example, rather than one giant prompt, the model of 2026=
 might routinely do a sequence: search -> read -> calculate -> answer. So y=
ou=E2=80=99ll orchestrate less manually as the model does it internally or =
via an agent framework. Keep an eye on frameworks like OpenAI=E2=80=99s =E2=
=80=9CAutonomous Agents=E2=80=9D if they release official ones, or on Micro=
soft=E2=80=99s Jarvis (which chains model with tools).
Quality metrics: We might see better ways to measure if a model effectively=
 used the context. For instance, automated checks or model self-diagnostics=
 (=E2=80=9CI might have missed something above=E2=80=9D). Some research is =
on calibrating LLM confidence with respect to provided context. If that wor=
ks, a model might say, =E2=80=9CI=E2=80=99m not certain I saw info about X =
in the above; maybe provide more details.=E2=80=9D That could be useful in =
production to know if more retrieval is needed.
Hype to be cautious about:
=E2=80=9CWe have 1M context, so you don=E2=80=99t need RAG anymore.=E2=80=
=9D Don=E2=80=99t buy it, as we thoroughly explored. Databricks=E2=80=99 st=
udy even titled about whether long context will subsume RAG found that most=
 models still dipped beyond certain lengths. It concluded long context comp=
lements retrieval, doesn=E2=80=99t replace it.
=E2=80=9CLoad your entire data warehouse into the prompt.=E2=80=9D Some ven=
dors might pitch that their model can consume tons of data directly, making=
 databases obsolete. In reality, for any structured large data, a combinati=
on of database + LLM is far superior. So hype that says =E2=80=9CLLM will j=
ust replace your SQL analytics by reading everything in one go=E2=80=9D =E2=
=80=93 extremely impractical and error-prone.
=E2=80=9COur model never forgets.=E2=80=9D If you hear that, ask for proof.=
 It=E2=80=99s a red flag as of 2025. At best, some smaller domain-specific =
models fine-tuned on long sequences might do better (like a code model fine=
-tuned to use a whole code file could perhaps utilize more of it than a gen=
eral model). But =E2=80=9Cnever forgets=E2=80=9D is not realistic given how=
 attention works.
=E2=80=9CJust increase the tokens.=E2=80=9D Some might think if lost-mid at=
 100K, maybe at 1M it just pushes the problem further out. But effective ut=
ilization % might even drop as window grows, if training doesn=E2=80=99t ke=
ep up. Without targeted training, giving models bigger windows could mostly=
 benefit at the extremes (like being able to do one prompt instead of multi=
ple) but not improve mid usage.
Investment Recommendations:
Invest in RAG infrastructure: It=E2=80=99s likely to remain valuable if pro=
perly deployed and designed. Build or buy a good vector database, set up pi=
pelines to keep it updated with your latest data. This will serve you regar=
dless of context window expansions.
Invest in evaluation and monitoring: as discussed, knowing how your LLM app=
s handle context now will let you gauge when a new model is actually an imp=
rovement or not. If GPT-5 comes and claims some breakthrough, you=E2=80=99l=
l have baseline tests to verify those claims on your workload.
Keep a portion of budget for experimenting with new context features, but d=
on=E2=80=99t commit fully until proven. For example, maybe try a project on=
 GPT-4.1 1M to see how it works, but don=E2=80=99t refactor everything to r=
ely on it until it=E2=80=99s stable and you have mitigations for its weakne=
sses.
Training and fine-tuning: Consider fine-tuning smaller models on your domai=
n data to encapsulate more context. This is an investment in reducing depen=
dency on huge contexts. For instance, fine-tune a model to summarize your r=
eports in one shot =E2=80=93 then you don=E2=80=99t need to feed entire rep=
orts at inference.
Hybrid approaches: invest in team skills or tech that combine symbolic/algo=
rithmic methods with LLMs (like knowledge graphs, search, logic rules). Tho=
se will help in domains where raw LLM performance might stall. It=E2=80=99s=
 clear that pure end-to-end LLM reasoning on long text has limits; hybrid s=
ystems often win in benchmarks (e.g., retrieval + LLM was stronger than jus=
t feeding text on QA tasks).
Keep an eye on open-source: The gap between open and closed is still large =
for some capabilities, but open models are catching up fast in context and =
I expect the new OpenAI open source model to be a step in this direction. I=
nvesting time in an open solution might pay off in cost savings long run, e=
specially if you can fine-tune it to your needs and not worry about sending=
 sensitive data to a third party. But factor in the engineering effort and =
maybe the model quality gap.
Bottom Line: Long contexts will get longer, but you=E2=80=99ll likely alway=
s need strategies to handle them smartly. The truth is, the silver bullet o=
f =E2=80=9Cjust throw it all in=E2=80=9D is not appearing magically. So the=
 multi-strategy approach we=E2=80=99ve outlined will remain relevant. Over =
time, some manual steps may be automated by improved models or agents, but =
understanding these principles ensures you can adapt as the tooling evolves=
=2E
Wrapping it up
So here we are. You've just spent considerable time learning what the vendo=
rs didn't want you to know: that their shiny 1M context windows are, functi=
onally speaking, about as reliable as a chocolate teapot. You've seen the e=
vidence, understood the technical reasons, and most importantly, learned wh=
at to actually do about it.
Let's be honest about what just happened. You probably started reading this=
 because something wasn't working. Maybe you watched your model forget cruc=
ial information from page 30 of that contract. Maybe you got the monthly AP=
I bill and had to explain to finance why each customer query costs more tha=
n a coffee at Starbucks. Or maybe you're just tired of your "state-of-the-a=
rt" AI assistant performing worse than an intern who actually read the docu=
ments.
The real insight here isn't that long context is broken =E2=80=93 you proba=
bly suspected that already. The real insight is that this isn't going to ma=
gically fix itself. GPT-5 won't solve it. Claude-whatever won't solve it. G=
emini-infinity won't solve it. Because it's not fundamentally a model capab=
ility problem =E2=80=93 it's an attention mechanics problem, a training dat=
a problem, and honestly, a bit of a physics problem (that O(n=C2=B2) comple=
xity isn't going anywhere).
But here's the thing: you don't need it to be fixed.
Throughout this guide, we've seen over and over that the smart, structured =
approaches outperform the brute force "dump everything" method. That legal =
team using retrieval to find specific clauses? They're getting better resul=
ts than the team trying to feed entire contracts. That financial analyst wh=
o chunks reports by section? More accurate than the one maxing out GPT-4's =
context. The healthcare system that maintains a careful context budget? It =
actually remembers patient allergies, unlike the one drowning in medical hi=
story.
I=E2=80=99ll say it one more time: What we're really talking about here is =
the difference between using AI and architecting with AI. Any fool can yeet=
 100K tokens into a prompt. It takes understanding to know when to chunk, w=
hen to retrieve, when to summarize, and when to tell the model exactly wher=
e to look. That's not a limitation =E2=80=93 that's craftsmanship.
The vendors will keep playing their game. Next month, someone else will ann=
ounce a 10M token model (Llama=E2=80=99s claim was always a bit sketchy). T=
he month after, 100M. Some hapless author will keep publishing those U-shap=
ed graphs showing performance tanking in the middle, buried on page 47 of p=
apers no one reads. And the model makes will keep charging you for every to=
ken while the model quietly ignores most of them. Don=E2=80=99t be fooled.=
=20
Build for the context window we have, not the context window you wish exist=
ed. And then debate with me in the comments about where you=E2=80=99re lean=
ing on AGI now that you=E2=80=99ve digested all this context window context=
 :)
For more on AI, subscribe and share!
A Google Doc with all the links for this article [ https://substack.com/red=
irect/103e730c-47e4-42b7-b153-75d15e10ef55?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YF=
xPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0 ]

Unsubscribe https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9uYXRlc25ld3N=
sZXR0ZXIuc3Vic3RhY2suY29tL2FjdGlvbi9kaXNhYmxlX2VtYWlsP3Rva2VuPWV5SjFjMlZ5WD=
Jsa0lqb3pNelkwTkRneU1qTXNJbkJ2YzNSZmFXUWlPakUyTnpZNE16VXlOeXdpYVdGMElqb3hOe=
lV4T0Rrek5ETTNMQ0psZUhBaU9qRTNPRE0wTWprME16Y3NJbWx6Y3lJNkluQjFZaTB4TXpjek1q=
TXhJaXdpYzNWaUlqb2laR2x6WVdKc1pWOWxiV0ZwYkNKOS5DeTNzZkRyRUZ0NzB1Y0V5MGJnZ2l=
JSmVidjY2cDlnbktkeUlMOFQtOEw4IiwicCI6MTY3NjgzNTI3LCJzIjoxMzczMjMxLCJmIjpmYW=
xzZSwidSI6MzM2NDQ4MjIzLCJpYXQiOjE3NTE4OTM0MzcsImV4cCI6MjA2NzQ2OTQzNywiaXNzI=
joicHViLTAiLCJzdWIiOiJsaW5rLXJlZGlyZWN0In0.TvFiqXnlGsazm03-mEncGKrEvZZV5Wig=
9LLldnjxUPs?

--d4a764290111796f177397fbcc44f39587ce16a92695f81b46e9ced34d81
Content-Type: text/html; charset="utf-8"
Content-Transfer-Encoding: quoted-printable

<html style=3D"scrollbar-width: thin;scrollbar-color: rgb(219,219,219)rgb(2=
55,255,255);"><head>
<meta http-equiv=3D"Content-Type" content=3D"text/html; charset=3Dutf-8"><t=
itle>Context Windows Are a Lie: The Myth Blocking AGI=E2=80=94And How to Fi=
x It</title><style>
@media all and (-ms-high-contrast: none), (-ms-high-contrast: active) {
  .typography .markup table.image-wrapper img,
  .typography.editor .markup table.image-wrapper img,
  .typography .markup table.kindle-wrapper img,
  .typography.editor .markup table.kindle-wrapper img {
    max-width: 550px;
  }
}
@media (max-width: 1024px) {
  .typography,
  .typography.editor {
    /* Disable offset on mobile/tablet */
  }
  .typography .captioned-image-container figure:has(> a.image2-align-left),
  .typography.editor .captioned-image-container figure:has(> a.image2-align=
-left),
  .typography .captioned-image-container figure:has(> a.image2-align-right)=
,
  .typography.editor .captioned-image-container figure:has(> a.image2-align=
-right) {
    float: none;
    margin: 1em auto;
    max-width: 100%;
    width: auto;
    padding: 0;
  }
  .typography .captioned-image-container figure:has(> a.image2-align-left.t=
hefp),
  .typography.editor .captioned-image-container figure:has(> a.image2-align=
-left.thefp),
  .typography .captioned-image-container figure:has(> a.image2-align-right.=
thefp),
  .typography.editor .captioned-image-container figure:has(> a.image2-align=
-right.thefp) {
    margin: 1em auto;
  }
  .typography .captioned-image-container figure:has(> a.image2-offset-left)=
,
  .typography.editor .captioned-image-container figure:has(> a.image2-offse=
t-left),
  .typography .captioned-image-container figure:has(> a.image2-offset-right=
),
  .typography.editor .captioned-image-container figure:has(> a.image2-offse=
t-right) {
    margin: 1em auto;
  }
  .typography .captioned-image-container figure:has(> a.image2-align-left) =
.image2-inset,
  .typography.editor .captioned-image-container figure:has(> a.image2-align=
-left) .image2-inset,
  .typography .captioned-image-container figure:has(> a.image2-align-right)=
 .image2-inset,
  .typography.editor .captioned-image-container figure:has(> a.image2-align=
-right) .image2-inset {
    display: block;
    justify-content: initial;
  }
}
@media (max-width: 768px) {
  .typography .markup div.sponsorship-campaign-embed,
  .typography.editor .markup div.sponsorship-campaign-embed {
    margin-top: 24px;
    margin-bottom: 24px;
  }
  .typography .markup div.sponsorship-campaign-embed:first-child,
  .typography.editor .markup div.sponsorship-campaign-embed:first-child {
    margin-top: 0px;
  }
}
@media screen and (max-width: 650px) {
  .typography .markup div.youtube-overlay,
  .typography.editor .markup div.youtube-overlay,
  .typography .markup div.vimeo-overlay,
  .typography.editor .markup div.vimeo-overlay {
    display: none !important;
  }
}
@media screen and (max-width: 370px) {
  .typography .markup div.tiktok-wrap,
  .typography.editor .markup div.tiktok-wrap {
    width: calc(95vw - 32px);
    height: calc((95vw - 32px - 2px) / 0.485714);
  }
}
@media screen and (max-width: 650px) {
  .typography .markup div.embedded-publication-wrap .embedded-publication.s=
how-subscribe,
  .typography.editor .markup div.embedded-publication-wrap .embedded-public=
ation.show-subscribe {
    padding: 24px;
  }
}
@media screen and (max-width: 650px) {
  .typography .markup div.subscription-widget-wrap .subscription-widget.sho=
w-subscribe,
  .typography.editor .markup div.subscription-widget-wrap .subscription-wid=
get.show-subscribe,
  .typography .markup div.subscription-widget-wrap-editor .subscription-wid=
get.show-subscribe,
  .typography.editor .markup div.subscription-widget-wrap-editor .subscript=
ion-widget.show-subscribe,
  .typography .markup div.captioned-button-wrap .subscription-widget.show-s=
ubscribe,
  .typography.editor .markup div.captioned-button-wrap .subscription-widget=
.show-subscribe {
    padding: 0px 24px;
  }
}
@media screen and (max-width: 650px) {
  .typography .markup div.subscription-widget-wrap .subscription-widget.sho=
w-subscribe .subscription-widget-subscribe .button,
  .typography.editor .markup div.subscription-widget-wrap .subscription-wid=
get.show-subscribe .subscription-widget-subscribe .button,
  .typography .markup div.subscription-widget-wrap-editor .subscription-wid=
get.show-subscribe .subscription-widget-subscribe .button,
  .typography.editor .markup div.subscription-widget-wrap-editor .subscript=
ion-widget.show-subscribe .subscription-widget-subscribe .button,
  .typography .markup div.captioned-button-wrap .subscription-widget.show-s=
ubscribe .subscription-widget-subscribe .button,
  .typography.editor .markup div.captioned-button-wrap .subscription-widget=
.show-subscribe .subscription-widget-subscribe .button {
    padding: 10px 12px;
    min-width: 110px;
  }
}
@media (max-width: 650px) {
  .typography .markup .tweet,
  .typography.editor .markup .tweet {
    padding: 12px;
  }
}
@media (max-width: 650px) {
  .typography .markup .tweet .tweet-text,
  .typography.editor .markup .tweet .tweet-text {
    font-size: 14px;
    line-height: 20px;
  }
}
@media (max-width: 650px) {
  .typography .markup .tweet .tweet-photos-container.two,
  .typography.editor .markup .tweet .tweet-photos-container.two,
  .typography .markup .tweet .tweet-photos-container.three,
  .typography.editor .markup .tweet .tweet-photos-container.three,
  .typography .markup .tweet .tweet-photos-container.four,
  .typography.editor .markup .tweet .tweet-photos-container.four {
    height: 200px;
  }
}
@media (max-width: 650px) {
  .typography .markup .tweet a.expanded-link .expanded-link-img,
  .typography.editor .markup .tweet a.expanded-link .expanded-link-img {
    max-height: 180px;
  }
}
@media (max-width: 650px) {
  .typography .markup .tweet a.expanded-link .expanded-link-description,
  .typography.editor .markup .tweet a.expanded-link .expanded-link-descript=
ion {
    display: none;
  }
}
@media screen and (max-width: 650px) {
  .typography .markup .apple-podcast-container,
  .typography.editor .markup .apple-podcast-container {
    width: unset;
  }
}
@media (max-width: 420px) {
  .typography .markup .install-substack-app-embed img.install-substack-app-=
embed-img,
  .typography.editor .markup .install-substack-app-embed img.install-substa=
ck-app-embed-img {
    margin: 0 auto 16px auto;
  }
}
@media (max-width: 420px) {
  .typography .markup .install-substack-app-embed .install-substack-app-emb=
ed-text,
  .typography.editor .markup .install-substack-app-embed .install-substack-=
app-embed-text {
    margin: 0 0 12px 0;
    max-width: 100%;
    width: auto;
    text-align: center;
  }
}
@media (max-width: 420px) {
  .typography .markup .install-substack-app-embed .install-substack-app-emb=
ed-link,
  .typography.editor .markup .install-substack-app-embed .install-substack-=
app-embed-link {
    display: flex;
    justify-content: center;
  }
}
@media screen and (min-width: 481px) {
  .share-button-container {
    height: 38px;
  }
}
@media screen and (min-width: 481px) {
  .share-button-container a.comment {
    height: 38px;
    line-height: 38px;
    padding-right: 10px;
  }
}
@media screen and (max-width: 480px) {
  .share-button-container .separator {
    display: block;
    margin: 0;
    height: 8px;
    border-left: none;
  }
}
@media screen and (max-width: 480px) {
  .share-button-container a.share.first img {
    padding-left: 0;
  }
}
@media screen and (min-width: 481px) {
  .share-button-container a.mobile {
    display: none !important;
  }
}
@media screen and (min-width: 541px) {
  .settings-add-pub-modal-wrapper .container .add-recommending-pub-modal-co=
ntainer {
    padding: 36px;
    height: 680px;
  }
}
@media screen and (min-width: 541px) {
  .settings-add-pub-modal-wrapper .container .add-recommending-pub-modal-co=
ntainer .footer {
    position: absolute;
    bottom: 36px;
    margin: 0px;
  }
}
@media screen and (max-width: 650px) {
  .header-anchor-parent {
    display: none;
  }
}
@media screen and (max-width: 768px) {
  .post {
    padding: 16px 0 0 0;
  }
}
@media screen and (max-width: 650px) {
  .post .post-header .post-label {
    margin-top: 8px;
  }
}
@media screen and (max-width: 650px) {
  .post .post-header .meta-author-wrap.alternative-meta .meta-right-column =
.post-meta {
    margin-top: 6px;
  }
}
@media screen and (max-width: 650px) {
  .post .footer-facepile-container {
    height: 64px;
    padding: 0 16px;
    display: flex;
    align-items: center;
    justify-content: flex-start;
    width: 100%;
  }
}
@media screen and (max-width: 650px) {
  .post .post-footer.use-separators {
    justify-content: center;
  }
}
@media screen and (max-width: 650px) {
  .post .post-footer.next-prev {
    height: 64px;
    justify-content: space-between;
    box-sizing: border-box;
  }
}
@media screen and (max-width: 650px) {
  .post-contributor-footer .post-contributor-bio-table {
    display: block;
  }
  .post-contributor-footer .post-contributor-bio-table-row {
    display: flex;
    flex-direction: row;
  }
  .post-contributor-footer .post-contributor-bio-userhead-cell,
  .post-contributor-footer .post-contributor-bio-body-cell {
    display: block;
  }
  .post-contributor-footer .post-contributor-bio-body-cell {
    flex-grow: 1;
  }
  .post-contributor-footer .post-contributor-bio-body-table {
    display: block;
  }
  .post-contributor-footer .post-contributor-bio-body-table-row {
    display: block;
  }
  .post-contributor-footer .post-contributor-bio-copy-cell,
  .post-contributor-footer .post-contributor-bio-controls-cell {
    display: block;
  }
  .post-contributor-footer .post-contributor-bio-copy-cell {
    margin: 0 0 16px 0;
  }
  .post-contributor-footer .post-contributor-bio-controls-cell {
    width: auto;
  }
  .post-contributor-footer .post-contributor-bio-controls {
    margin: auto;
  }
  .post-contributor-footer .post-contributor-bio-controls .button.primary {
    width: 100%;
  }
  .post-contributor-footer .post-contributor-bio-text {
    font-size: 14px;
  }
}
@media screen and (min-width: 768px) {
  .post-silhouette {
    padding: 32px 0;
  }
}
@media screen and (max-width: 650px) {
  .post-silhouette .post-silhouette-title {
    margin-top: 10.44225025px;
    height: 120px;
  }
}
@media screen and (max-width: 650px) {
  .post-silhouette .post-silhouette-meta {
    width: 75%;
  }
}
@media screen and (max-width: 650px) {
  .post-silhouette .post-silhouette-meta.with-byline-image {
    margin: 20px 0;
  }
}
@media screen and (max-width: 650px) {
  .use-theme-bg .post-meta.alternative-meta .post-meta-item,
  .post-meta.alternative-meta .post-meta-item {
    padding-right: 16px;
  }
}
@media screen and (max-width: 370px) {
  .use-theme-bg .post-meta.alternative-meta .post-meta-item,
  .post-meta.alternative-meta .post-meta-item {
    font-size: 14px;
  }
}
@media screen and (max-width: 650px) {
  .use-theme-bg .post-meta.alternative-meta .post-meta-item.guest-author-pu=
blication,
  .post-meta.alternative-meta .post-meta-item.guest-author-publication {
    display: none;
  }
}
@media screen and (max-width: 370px) {
  .post-meta .post-meta-item .post-meta-button {
    height: 36px !important;
    /* important to override in-line height style on emails */
  }
  .post-meta .post-meta-item .post-meta-button .meta-button-label {
    display: none;
  }
  .post-meta .post-meta-item .post-meta-button > svg {
    margin-right: 0;
  }
}
@media screen and (max-width: 370px) {
  .post-meta .post-meta-item {
    font-size: 12px;
  }
}
@media screen and (max-width: 650px) {
  .post .floating-subscribe-button {
    bottom: 20px;
    right: 20px;
  }
}
@media all and (-ms-high-contrast: none), (-ms-high-contrast: active) {
  body .markup table.image-wrapper img,
  body .markup table.kindle-wrapper img {
    max-width: 550px;
  }
}
@media (max-width: 1024px) {
  body {
    /* Disable offset on mobile/tablet */
  }
  body .captioned-image-container figure:has(> a.image2-align-left),
  body .captioned-image-container figure:has(> a.image2-align-right) {
    float: none;
    margin: 1em auto;
    max-width: 100%;
    width: auto;
    padding: 0;
  }
  body .captioned-image-container figure:has(> a.image2-align-left.thefp),
  body .captioned-image-container figure:has(> a.image2-align-right.thefp) =
{
    margin: 1em auto;
  }
  body .captioned-image-container figure:has(> a.image2-offset-left),
  body .captioned-image-container figure:has(> a.image2-offset-right) {
    margin: 1em auto;
  }
  body .captioned-image-container figure:has(> a.image2-align-left) .image2=
-inset,
  body .captioned-image-container figure:has(> a.image2-align-right) .image=
2-inset {
    display: block;
    justify-content: initial;
  }
}
@media (max-width: 768px) {
  body .markup div.sponsorship-campaign-embed {
    margin-top: 24px;
    margin-bottom: 24px;
  }
  body .markup div.sponsorship-campaign-embed:first-child {
    margin-top: 0px;
  }
}
@media screen and (max-width: 650px) {
  body .markup div.youtube-overlay,
  body .markup div.vimeo-overlay {
    display: none !important;
  }
}
@media screen and (max-width: 370px) {
  body .markup div.tiktok-wrap {
    width: calc(95vw - 32px);
    height: calc((95vw - 32px - 2px) / 0.485714);
  }
}
@media screen and (max-width: 650px) {
  body .markup div.embedded-publication-wrap .embedded-publication.show-sub=
scribe {
    padding: 24px;
  }
}
@media screen and (max-width: 650px) {
  body .markup div.subscription-widget-wrap .subscription-widget.show-subsc=
ribe,
  body .markup div.subscription-widget-wrap-editor .subscription-widget.sho=
w-subscribe,
  body .markup div.captioned-button-wrap .subscription-widget.show-subscrib=
e {
    padding: 0px 24px;
  }
}
@media screen and (max-width: 650px) {
  body .markup div.subscription-widget-wrap .subscription-widget.show-subsc=
ribe .subscription-widget-subscribe .button,
  body .markup div.subscription-widget-wrap-editor .subscription-widget.sho=
w-subscribe .subscription-widget-subscribe .button,
  body .markup div.captioned-button-wrap .subscription-widget.show-subscrib=
e .subscription-widget-subscribe .button {
    padding: 10px 12px;
    min-width: 110px;
  }
}
@media (max-width: 650px) {
  body .markup .tweet {
    padding: 12px;
  }
}
@media (max-width: 650px) {
  body .markup .tweet .tweet-text {
    font-size: 14px;
    line-height: 20px;
  }
}
@media (max-width: 650px) {
  body .markup .tweet .tweet-photos-container.two,
  body .markup .tweet .tweet-photos-container.three,
  body .markup .tweet .tweet-photos-container.four {
    height: 200px;
  }
}
@media (max-width: 650px) {
  body .markup .tweet a.expanded-link .expanded-link-img {
    max-height: 180px;
  }
}
@media (max-width: 650px) {
  body .markup .tweet a.expanded-link .expanded-link-description {
    display: none;
  }
}
@media screen and (max-width: 650px) {
  body .markup .apple-podcast-container {
    width: unset;
  }
}
@media (max-width: 420px) {
  body .markup .install-substack-app-embed img.install-substack-app-embed-i=
mg {
    margin: 0 auto 16px auto;
  }
}
@media (max-width: 420px) {
  body .markup .install-substack-app-embed .install-substack-app-embed-text=
 {
    margin: 0 0 12px 0;
    max-width: 100%;
    width: auto;
    text-align: center;
  }
}
@media (max-width: 420px) {
  body .markup .install-substack-app-embed .install-substack-app-embed-link=
 {
    display: flex;
    justify-content: center;
  }
}
@media screen and (min-width: 500px) {
  body .header a.logo {
    width: 42px;
    height: 42px;
    border-radius: 12px;
  }
}
@media screen and (max-width: 420px) {
  body .subscription-receipt table:first-of-type .subscription-amount .subs=
cription-discount {
    width: 72px !important;
  }
}
@media screen and (min-width: 481px) {
  body .share-button-container {
    height: auto;
  }
}
@media screen and (max-width: 480px) {
  body .share-button-container .separator {
    display: block !important;
    margin: 0 !important;
    height: 8px !important;
    border-left: none !important;
  }
}
@media screen and (max-width: 650px) {
  .digest .item .post-meta-item.audience {
    display: none;
  }
}
@media screen and (min-width: 500px) {
  .digest-publication .logo img {
    width: 42px;
    height: 42px;
    border-radius: 8px;
  }
}
@media screen and (max-width: 650px) {
  .comments-page .container .comment-list .collapsed-reply {
    margin-left: calc(10 + 32px - 24px);
  }
}
@media screen and (max-width: 650px) {
  .comment > .comment-list {
    padding-left: 24px;
  }
}
@media screen and (max-width: 650px) {
  .finish-magic-login-modal .modal-content .container {
    padding: 24px 0;
  }
}
@media (max-width: 650px) {
  .reader2-text-b3 {
    line-height: 24px;
  }
}
@media screen and (max-width: 650px) {
  .reader2-text-h4 {
    line-height: 24px;
  }
}
@media screen and (min-width: 541px) {
  .user-profile-modal {
    padding-left: 12px;
    padding-right: 12px;
  }
}
@media screen and (max-width: 650px) {
  .subscribe-widget form.form .sideBySideWrap button.rightButton {
    padding: 10px 12px;
  }
}
@media screen and (min-width: 541px) {
  .pub-icon:hover .logo-hover,
  .feed-item-icon:hover .logo-hover {
    display: block;
  }
}
@media screen and (max-width: 650px) {
  .post-ufi.single-full-width-button .post-ufi-button-wrapper {
    width: 100%;
    padding: 16px;
  }
  .post-ufi.single-full-width-button .post-ufi-button-wrapper:empty {
    display: none;
  }
  .post-ufi.single-full-width-button .post-ufi-button {
    width: 100%;
    justify-content: center;
  }
}
@media screen and (max-width: 768px) {
  .file-embed-wrapper {
    padding: 0;
  }
}
@media screen and (max-width: 768px) {
  .file-embed-wrapper-editor {
    padding: 0;
  }
}
@media screen and (max-width: 768px) {
  .file-embed-wrapper-editor:active {
    padding: 0;
  }
}
@media only screen and (max-width: 650px) {
  .file-embed-button.wide,
  .file-embed-error-button.wide {
    display: none;
  }
}
@media only screen and (min-width: 630px) {
  .file-embed-button.narrow,
  .file-embed-error-button.narrow {
    display: none;
  }
}
@media screen and (min-width: 541px) {
  .audio-player-wrapper .audio-player {
    min-width: 500px;
  }
}
@media screen and (max-width: 650px) {
  .audio-player-wrapper .audio-player .audio-player-progress {
    border-left-width: 16px;
    border-right-width: 16px;
  }
}
@media screen and (max-width: 650px) {
  .audio-player-wrapper .audio-player .audio-player-progress .audio-player-=
progress-bar .audio-player-progress-bar-popup {
    top: -54px;
  }
}
@media screen and (max-width: 650px) {
  .audio-player-wrapper-fancy .audio-player .audio-player-progress {
    border-left-width: 16px;
    border-right-width: 16px;
  }
}
@media screen and (max-width: 650px) {
  .audio-player-wrapper-fancy .audio-player .audio-player-progress .audio-p=
layer-progress-bar .audio-player-progress-bar-popup {
    top: -54px;
  }
}
@media (min-width: 250px) {
  .audio-player-wrapper-fancy .audio-player {
    padding: 32px;
  }
  .audio-player-wrapper-fancy .audio-player .btn-group {
    display: flex;
  }
  .audio-player-wrapper-fancy .audio-player .btn-group .button:last-of-type=
 {
    display: block;
  }
}
@media (min-width: 300px) {
  .audio-player-wrapper-fancy .audio-player .btn-group {
    display: block;
  }
  .audio-player-wrapper-fancy .audio-player .btn-group .button:first-of-typ=
e {
    display: block;
  }
}
@media (min-width: 350px) {
  .audio-player-wrapper-fancy .audio-player .audio-player-substack-logo {
    display: block;
  }
  .audio-player-wrapper-fancy .audio-player .audio-player-title {
    margin-top: 16px;
  }
  .audio-player-wrapper-fancy .audio-player .audio-player-hero-image-contai=
ner {
    padding-top: 15%;
    width: 15%;
    display: block;
  }
  .audio-player-wrapper-fancy .audio-player .btn-group .button:first-of-typ=
e {
    display: block;
  }
  .audio-player-wrapper-fancy .audio-player .audio-player-substack-logo {
    display: block;
  }
}
@media (min-width: 350px) {
  .audio-player-wrapper-fancy .audio-player .audio-player-hero-image-contai=
ner {
    padding-top: 25%;
    width: 25%;
    display: block;
  }
  .audio-player-wrapper-fancy .audio-player .btn-group {
    display: flex;
  }
  .audio-player-wrapper-fancy .audio-player .btn-group .button:first-of-typ=
e {
    display: block;
  }
}
@media (min-width: 400px) {
  .audio-player-wrapper-fancy .audio-player .audio-player-hero-image-contai=
ner {
    padding-top: 40%;
    width: 40%;
  }
}
@media (max-width: 400px) {
  .audio-player-wrapper-fancy .audio-player .btn-group {
    margin-top: 12px;
  }
  .audio-player-wrapper-fancy .audio-player .btn-group .button {
    font-size: 13px;
    padding: 6px 12px;
    height: auto;
    margin-top: 10px;
  }
}
@media (min-width: 600px) {
  .audio-player-wrapper-fancy .audio-player .audio-player-hero-image-contai=
ner {
    padding-top: 55%;
    width: 55%;
  }
}
@media (max-width: 650px) {
  .poll-editor-modal {
    min-width: calc(100% - 20px);
  }
}
@media (max-width: 750px) {
  .poll-embed .poll-anchor-target .poll-anchor-copy-button {
    left: 8px;
    top: 45px;
  }
}
@media screen and (min-width: 541px) {
  .poll-embed .poll-wrapper.poll-web .poll-dialog .modal-table .modal-row .=
modal-content > .container {
    width: 552px;
    padding: 26px 24px;
  }
}
@media screen and (max-width: 650px) {
  .poll-embed .poll-wrapper.poll-web .poll-dialog .modal-table .modal-row .=
modal-content > .container {
    padding: 40px 0;
  }
}
@media screen and (max-width: 650px) {
  .poll-embed .poll-wrapper.poll-web .poll-dialog .modal-row .modal-cell .m=
odal-exit-btn {
    margin-right: -20px;
  }
}</style></head><body class=3D"email-body" style=3D"font-kerning: auto;--im=
age-offset-margin: -117px;"><img src=3D"https://eotrx.substackcdn.com/open?=
token=3DeyJtIjoiPDIwMjUwNzA3MTMwMjA2LjMuY2M1YzBmMGJlMjEyYzhkOEBtZzEuc3Vic3R=
hY2suY29tPiIsInUiOjMzNjQ0ODIyMywiciI6ImVpdGFuQGVpc2xhdy5jby5pbCIsImQiOiJtZz=
Euc3Vic3RhY2suY29tIiwicCI6MTY3NjgzNTI3LCJ0IjoicG9kY2FzdCIsImEiOiJvbmx5X3Bha=
WQiLCJzIjoxMzczMjMxLCJjIjoicG9zdCIsImYiOmZhbHNlLCJwb3NpdGlvbiI6InRvcCIsImlh=
dCI6MTc1MTg5MzQzNywiZXhwIjoxNzU0NDg1NDM3LCJpc3MiOiJwdWItMCIsInN1YiI6ImVvIn0=
.1EjnvQ3hld5_7L69o8dXSyU4OldiOfgbdlzF5aIifYw" alt=3D"" width=3D"1" height=
=3D"1" border=3D"0" style=3D"height:1px !important;width:1px !important;bor=
der-width:0 !important;margin-top:0 !important;margin-bottom:0 !important;m=
argin-right:0 !important;margin-left:0 !important;padding-top:0 !important;=
padding-bottom:0 !important;padding-right:0 !important;padding-left:0 !impo=
rtant;"><div class=3D"preview" style=3D"display:none;font-size:1px;color:#3=
33333;line-height:1px;max-height:0px;max-width:0px;opacity:0;overflow:hidde=
n;">Watch now (15 mins) | Get hard numbers + six proven playbooks to rescue=
 forgotten prompts, slash token spend, and beat 1M-token hype=E2=80=94today=
.</div><div class=3D"preview" style=3D"display:none;font-size:1px;color:#33=
3333;line-height:1px;max-height:0px;max-width:0px;opacity:0;overflow:hidden=
;">=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp=
; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=AD=CD=8F &nbsp; =E2=80=87 =C2=
=AD</div><table class=3D"email-body-container" role=3D"presentation" width=
=3D"100%" border=3D"0" cellspacing=3D"0" cellpadding=3D"0"><tbody><tr><td><=
/td><td class=3D"content" width=3D"550"></td><td></td></tr><tr><td></td><td=
 class=3D"content" width=3D"550" align=3D"left"><div style=3D"font-size: 16=
px;line-height: 26px;max-width: 550px;width: 100%;margin: 0 auto;overflow-w=
rap: break-word;"><table role=3D"presentation" width=3D"100%" border=3D"0" =
cellspacing=3D"0" cellpadding=3D"0"><tbody><tr><td align=3D"right" style=3D=
"height:20px;"><table role=3D"presentation" width=3D"auto" border=3D"0" cel=
lspacing=3D"0" cellpadding=3D"0"><tbody><tr><td style=3D"vertical-align:mid=
dle;"><span class=3D"pencraft pc-reset reset-IxiVJZ tw-font-body tw-text-ss=
m tw-text-substack-secondary" style=3D"font-family: SF Pro Text, -apple-sys=
tem, system-ui, BlinkMacSystemFont, Inter, Segoe UI, Roboto, Helvetica, Ari=
al, sans-serif, Apple Color Emoji, Segoe UI Emoji, Segoe UI Symbol;font-siz=
e: 13px;color: unset;list-style: none;text-decoration: unset;margin: 0;"><a=
 class=3D"tw-text-substack-secondary tw-underline" href=3D"https://substack=
.com/redirect/2/eyJlIjoiaHR0cHM6Ly9uYXRlc25ld3NsZXR0ZXIuc3Vic3RhY2suY29tL3A=
vY29udGV4dC13aW5kb3dzLWFyZS1hLWxpZS10aGUtbXl0aD91dG1fY2FtcGFpZ249ZW1haWwtcG=
9zdCZyPTVrYjkzeiZ0b2tlbj1leUoxYzJWeVgybGtJam96TXpZME5EZ3lNak1zSW5CdmMzUmZhV=
1FpT2pFMk56WTRNelV5Tnl3aWFXRjBJam94TnpVeE9Ea3pORE0zTENKbGVIQWlPakUzTlRRME9E=
VTBNemNzSW1semN5STZJbkIxWWkweE16Y3pNak14SWl3aWMzVmlJam9pY0c5emRDMXlaV0ZqZEd=
sdmJpSjkuMlQtTkR5ZGo2dS1nMFpiaDlmUEcxaGZYb3pieTExU2xZWkVuOWhuN0FobyIsInAiOj=
E2NzY4MzUyNywicyI6MTM3MzIzMSwiZiI6ZmFsc2UsInUiOjMzNjQ0ODIyMywiaWF0IjoxNzUxO=
DkzNDM3LCJleHAiOjIwNjc0Njk0MzcsImlzcyI6InB1Yi0wIiwic3ViIjoibGluay1yZWRpcmVj=
dCJ9.SktEU2sfzz3seyiNMbHgJ1pHwoqk1tg0jE-nMV9wFHs?" style=3D"color: rgb(119,=
119,119);-webkit-text-decoration-line: underline;text-decoration-line: unde=
rline;">View in browser</a></span></td></tr></tbody></table></td></tr></tbo=
dy></table><div class=3D"typography" style=3D"--image-offset-margin: -117px=
;font-size: 16px;line-height: 26px;"><div class=3D"preamble" style=3D"font-=
size: 16px;line-height: 26px;margin-top: 16px;"><div class=3D"body markup" =
dir=3D"auto" style=3D"text-align: initial;font-size: 16px;line-height: 26px=
;width: 100%;word-break: break-word;margin-bottom: 16px;"><p style=3D"margi=
n: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;margin=
-top: 0;margin-bottom: 0;">Look at you getting killer career perspective an=
d the full AI picture. Give yourself a pat on the back for diving in on AI =
and go get a coffee =E2=98=95</p></div><hr style=3D"padding: 0;height: 1px;=
background: #e6e6e6;border: none;margin: 16px 0 0;"></div></div><table role=
=3D"presentation" width=3D"auto" border=3D"0" cellspacing=3D"0" cellpadding=
=3D"0" style=3D"width:100%;"><tbody><tr><td style=3D"vertical-align:middle;=
width:100%;"><a href=3D"https://substack.com/app-link/post?publication_id=
=3D1373231&amp;post_id=3D167683527&amp;utm_source=3Dpodcast-email&amp;play_=
audio=3Dtrue&amp;r=3D5kb93z&amp;utm_campaign=3Demail-play-on-substack&amp;t=
oken=3DeyJ1c2VyX2lkIjozMzY0NDgyMjMsInBvc3RfaWQiOjE2NzY4MzUyNywiaWF0IjoxNzUx=
ODkzNDM3LCJleHAiOjE3NTQ0ODU0MzcsImlzcyI6InB1Yi0xMzczMjMxIiwic3ViIjoicG9zdC1=
yZWFjdGlvbiJ9.2T-NDydj6u-g0Zbh9fPG1hfXozby11SlYZEn9hn7Aho&amp;utm_content=
=3Dwatch_now_gif"><img src=3D"https://substackcdn.com/image/fetch/$s_!FoyF!=
,w_1138,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubst=
ack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88f21925-a1d6-47a4-b224=
-4f9ae9d6b65a_320x180.gif" alt=3D"Riverside Jul 7 2025 Nate Jones Studio.mp=
4" style=3D"max-width: 550px;border: none;vertical-align: middle;width: 100=
%;border-radius: 12px"></a></td></tr></tbody></table><table role=3D"present=
ation" width=3D"auto" border=3D"0" cellspacing=3D"0" cellpadding=3D"0" styl=
e=3D"width:100%;"><tbody><tr><td><div style=3D"font-size: 16px;line-height:=
 26px;height: 16px">&nbsp;</div></td></tr><tr height=3D"8"><td></td></tr><t=
r><td><table role=3D"presentation" width=3D"auto" border=3D"0" cellspacing=
=3D"0" cellpadding=3D"0" style=3D"width:100%;"><tbody><tr><td style=3D"vert=
ical-align:middle;width:100%;"><table class=3D"fullWidth-mgXGs7" role=3D"pr=
esentation" width=3D"auto" border=3D"0" cellspacing=3D"0" cellpadding=3D"0"=
 style=3D"box-sizing: border-box;width: 100%;min-height: 40px;padding-left:=
 0;padding-right: 0;"><tbody><tr><td class=3D"fullWidth-mgXGs7 emailButtonT=
d-o2ymya priority_primary-vWRHI0" align=3D"center" style=3D"box-sizing: bor=
der-box;width: 100%;min-height: 40px;padding-left: 0;padding-right: 0;borde=
r-radius: 8px;background-color: #45D800;"><a class=3D"fullWidth-mgXGs7 emai=
lButtonA-Ktpg7h" href=3D"https://substack.com/app-link/post?publication_id=
=3D1373231&amp;post_id=3D167683527&amp;utm_source=3Dpodcast-email&amp;play_=
audio=3Dtrue&amp;r=3D5kb93z&amp;utm_campaign=3Demail-play-on-substack&amp;t=
oken=3DeyJ1c2VyX2lkIjozMzY0NDgyMjMsInBvc3RfaWQiOjE2NzY4MzUyNywiaWF0IjoxNzUx=
ODkzNDM3LCJleHAiOjE3NTQ0ODU0MzcsImlzcyI6InB1Yi0xMzczMjMxIiwic3ViIjoicG9zdC1=
yZWFjdGlvbiJ9.2T-NDydj6u-g0Zbh9fPG1hfXozby11SlYZEn9hn7Aho&amp;utm_content=
=3Dwatch_now_button" style=3D"box-sizing: border-box;width: 100%;min-height=
: 40px;padding-left: 0;padding-right: 0;font-family: system-ui,-apple-syste=
m,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Co=
lor Emoji','Segoe UI Emoji','Segoe UI Symbol';font-size: 14px;font-weight: =
600;letter-spacing: -.15px;border-radius: 8px;padding: 12px 24px;line-heigh=
t: 1;text-decoration: none;display: inline-block;color: #ffffff;border: non=
e;"><img src=3D"https://substackcdn.com/image/fetch/$s_!JR-4!,w_26.04651162=
7906977,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubsta=
ck.com%2Ficon%2FPlayIcon%3Fv%3D4%26height%3D32%26fill%3D%2523ffffff%26strok=
e%3D%2523ffffff%26strokeWidth%3D2" width=3D"13.023255813953488" height=3D"1=
6" style=3D"border: none;vertical-align: middle;max-width: 13.0232558139534=
88px" alt=3D""><span style=3D"margin-left:8px;vertical-align:middle;">Watch=
 now</span></a></td></tr></tbody></table></td></tr></tbody></table></td></t=
r><tr height=3D"8"><td></td></tr><tr><td><div style=3D"font-size: 16px;line=
-height: 26px;height: 16px">&nbsp;</div></td></tr></tbody></table><div clas=
s=3D"post typography" dir=3D"auto" style=3D"--image-offset-margin: -117px;p=
adding: 32px 0 0 0;font-size: 16px;line-height: 26px;"><div class=3D"post-h=
eader" role=3D"region" aria-label=3D"Post header" style=3D"font-size: 16px;=
line-height: 26px;"><h1 class=3D"post-title published" style=3D"color: rgb(=
54,55,55);font-family: 'SF Pro Display',-apple-system-headline,system-ui,-a=
pple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif=
,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-=
webkit-font-smoothing: antialiased;-moz-osx-font-smoothing: antialiased;-we=
bkit-appearance: optimizelegibility;-moz-appearance: optimizelegibility;app=
earance: optimizelegibility;margin: 0;line-height: 36px;font-size: 32px;"><=
a href=3D"https://substack.com/app-link/post?publication_id=3D1373231&amp;p=
ost_id=3D167683527&amp;utm_source=3Dpost-email-title&amp;utm_campaign=3Dema=
il-post-title&amp;isFreemail=3Dfalse&amp;r=3D5kb93z&amp;token=3DeyJ1c2VyX2l=
kIjozMzY0NDgyMjMsInBvc3RfaWQiOjE2NzY4MzUyNywiaWF0IjoxNzUxODkzNDM3LCJleHAiOj=
E3NTQ0ODU0MzcsImlzcyI6InB1Yi0xMzczMjMxIiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.2T-N=
Dydj6u-g0Zbh9fPG1hfXozby11SlYZEn9hn7Aho" style=3D"color: rgb(54,55,55);text=
-decoration: none;">Context Windows Are a Lie: The Myth Blocking AGI=E2=80=
=94And How to Fix It</a></h1><h3 class=3D"subtitle" style=3D"font-family: '=
SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSyst=
emFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Se=
goe UI Emoji','Segoe UI Symbol';font-weight: normal;-webkit-font-smoothing:=
 antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearance: optim=
izelegibility;-moz-appearance: optimizelegibility;appearance: optimizelegib=
ility;margin: 4px 0 0;color: #777777;line-height: 24px;font-size: 18px;marg=
in-top: 12px;">Get hard numbers + six proven playbooks to rescue forgotten =
prompts, slash token spend, and beat 1M-token hype=E2=80=94today.</h3><tabl=
e class=3D"post-meta" role=3D"presentation" width=3D"100%" border=3D"0" cel=
lspacing=3D"0" cellpadding=3D"0" style=3D"margin: 1em 0;height: 20px;align-=
items: center;"><tbody><tr><td><table role=3D"presentation" width=3D"auto" =
border=3D"0" cellspacing=3D"0" cellpadding=3D"0"><tbody><tr><td><table role=
=3D"presentation" width=3D"auto" border=3D"0" cellspacing=3D"0" cellpadding=
=3D"0"><tbody><tr><td style=3D"vertical-align:middle;"><div class=3D"pencra=
ft pc-reset color-primary-zABazT line-height-20-t4M0El font-meta-MWBumP siz=
e-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq reset-IxiVJZ me=
ta-EgzBVA custom-css-email-post-author" style=3D"list-style: none;font-size=
: 11px;line-height: 20px;text-decoration: unset;color: rgb(54,55,55);margin=
: 0;font-family: 'SF Compact',-apple-system,system-ui,-apple-system,BlinkMa=
cSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji=
','Segoe UI Emoji','Segoe UI Symbol';font-weight: 500;text-transform: upper=
case;letter-spacing: .2px;"><a class=3D"pencraft pc-reset color-primary-zAB=
azT line-height-20-t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw8=
1nC transform-uppercase-yKDgcq reset-IxiVJZ meta-EgzBVA" style=3D"list-styl=
e: none;color: rgb(54,55,55);margin: 0;font-size: 11px;line-height: 20px;fo=
nt-family: 'SF Compact',-apple-system,system-ui,-apple-system,BlinkMacSyste=
mFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Seg=
oe UI Emoji','Segoe UI Symbol';font-weight: 500;text-transform: uppercase;l=
etter-spacing: .2px;text-decoration: none" href=3D"https://substack.com/@na=
tesnewsletter">Nate</a></div></td></tr></tbody></table></td></tr><tr><td><t=
able role=3D"presentation" width=3D"auto" border=3D"0" cellspacing=3D"0" ce=
llpadding=3D"0"><tbody><tr><td style=3D"vertical-align:middle;"><div class=
=3D"pencraft pc-reset color-secondary-ls1g8s line-height-20-t4M0El font-met=
a-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq res=
et-IxiVJZ meta-EgzBVA" style=3D"list-style: none;font-size: 11px;line-heigh=
t: 20px;text-decoration: unset;color: rgb(119,119,119);margin: 0;font-famil=
y: 'SF Compact',-apple-system,system-ui,-apple-system,BlinkMacSystemFont,'S=
egoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Em=
oji','Segoe UI Symbol';font-weight: 500;text-transform: uppercase;letter-sp=
acing: .2px;"><time datetime=3D"2025-07-07T13:03:12.698Z">Jul 7</time></div=
></td><td width=3D"4" style=3D"min-width: 4px"></td><td style=3D"vertical-a=
lign:middle;"><table role=3D"presentation" width=3D"auto" border=3D"0" cell=
spacing=3D"0" cellpadding=3D"0"><tbody><tr><td style=3D"vertical-align:midd=
le;"><div class=3D"pencraft pc-reset color-secondary-ls1g8s line-height-20-=
t4M0El font-meta-MWBumP size-11-NuY2Zx weight-medium-fw81nC transform-upper=
case-yKDgcq reset-IxiVJZ meta-EgzBVA" style=3D"list-style: none;font-size: =
11px;line-height: 20px;text-decoration: unset;color: rgb(119,119,119);margi=
n: 0;font-family: 'SF Compact',-apple-system,system-ui,-apple-system,BlinkM=
acSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoj=
i','Segoe UI Emoji','Segoe UI Symbol';font-weight: 500;text-transform: uppe=
rcase;letter-spacing: .2px;">=E2=88=99</div></td><td width=3D"4" style=3D"m=
in-width: 4px"></td><td style=3D"vertical-align:middle;"><div class=3D"penc=
raft pc-reset color-paid-LmY0EP line-height-20-t4M0El font-meta-MWBumP size=
-11-NuY2Zx weight-medium-fw81nC transform-uppercase-yKDgcq reset-IxiVJZ met=
a-EgzBVA" translated=3D"" style=3D"list-style: none;font-size: 11px;line-he=
ight: 20px;text-decoration: unset;color: rgb(94,73,217);margin: 0;font-fami=
ly: 'SF Compact',-apple-system,system-ui,-apple-system,BlinkMacSystemFont,'=
Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI E=
moji','Segoe UI Symbol';font-weight: 500;text-transform: uppercase;letter-s=
pacing: .2px;">Paid</div></td></tr></tbody></table></td></tr></tbody></tabl=
e></td></tr></tbody></table></td><td align=3D"right"><table role=3D"present=
ation" width=3D"auto" border=3D"0" cellspacing=3D"0" cellpadding=3D"0"><tbo=
dy><tr><td style=3D"vertical-align:middle;"><a href=3D"https://substack.com=
/@natesnewsletter"><img class=3D"custom-css-email-avatar avatar-QIQ5yR" src=
=3D"https://substackcdn.com/image/fetch/$s_!AgBy!,f_auto,q_auto:good,fl_pro=
gressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%=
2Fimages%2Fa37385e3-0387-487a-9f2c-e13aa963da4c_1080x1080.png" style=3D"box=
-sizing: border-box;border-radius: 500000px;max-width: 550px;border: none;v=
ertical-align: middle;width: 40px;height: 40px;min-width: 40px;min-height: =
40px;object-fit: cover;margin: 0px;display: inline" width=3D"40" height=3D"=
40"></a></td></tr></tbody></table></td></tr></tbody></table><table class=3D=
"email-ufi-2-top" role=3D"presentation" width=3D"100%" border=3D"0" cellspa=
cing=3D"0" cellpadding=3D"0" style=3D"border-top: 1px solid rgb(0,0,0,.1);b=
order-bottom: 1px solid rgb(0,0,0,.1);min-width: 100%;"><tbody><tr height=
=3D"16"><td height=3D"16" style=3D"font-size:0px;line-height:0;">&nbsp;</td=
></tr><tr><td><table role=3D"presentation" width=3D"100%" border=3D"0" cell=
spacing=3D"0" cellpadding=3D"0"><tbody><tr><td><table role=3D"presentation"=
 width=3D"auto" border=3D"0" cellspacing=3D"0" cellpadding=3D"0"><tbody><tr=
><td style=3D"vertical-align:middle;"><table role=3D"presentation" width=3D=
"38" border=3D"0" cellspacing=3D"0" cellpadding=3D"0"><tbody><tr><td align=
=3D"center"><a class=3D"email-icon-button" href=3D"https://substack.com/app=
-link/post?publication_id=3D1373231&amp;post_id=3D167683527&amp;utm_source=
=3Dsubstack&amp;isFreemail=3Dfalse&amp;submitLike=3Dtrue&amp;token=3DeyJ1c2=
VyX2lkIjozMzY0NDgyMjMsInBvc3RfaWQiOjE2NzY4MzUyNywicmVhY3Rpb24iOiLinaQiLCJpY=
XQiOjE3NTE4OTM0MzcsImV4cCI6MTc1NDQ4NTQzNywiaXNzIjoicHViLTEzNzMyMzEiLCJzdWIi=
OiJyZWFjdGlvbiJ9.yLW4JhM5bqPAC46n988ZubrdTV8yZnfQ52TbAVsTW6o&amp;utm_medium=
=3Demail&amp;utm_campaign=3Demail-reaction&amp;r=3D5kb93z" style=3D"font-fa=
mily: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetic=
a,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';d=
isplay: inline-block;font-weight: 500;border: 1px solid rgb(0,0,0,.1);borde=
r-radius: 9999px;text-transform: uppercase;font-size: 12px;line-height: 1;p=
adding: 9px 0;text-decoration: none;color: rgb(119,119,119);min-width: 38px=
;box-sizing: border-box;width: 38px"><img class=3D"icon" src=3D"https://sub=
stackcdn.com/image/fetch/$s_!PeVs!,w_36,c_scale,f_png,q_auto:good,fl_progre=
ssive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideHeart%3Fv%3D4%26heigh=
t%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2" width=3D"18"=
 height=3D"18" style=3D"border: none;vertical-align: middle;max-width: 18px=
" alt=3D""></a></td></tr></tbody></table></td><td width=3D"8" style=3D"min-=
width: 8px"></td><td style=3D"vertical-align:middle;"><table role=3D"presen=
tation" width=3D"38" border=3D"0" cellspacing=3D"0" cellpadding=3D"0"><tbod=
y><tr><td align=3D"center"><a class=3D"email-icon-button" href=3D"https://s=
ubstack.com/app-link/post?publication_id=3D1373231&amp;post_id=3D167683527&=
amp;utm_source=3Dsubstack&amp;utm_medium=3Demail&amp;isFreemail=3Dfalse&amp=
;comments=3Dtrue&amp;token=3DeyJ1c2VyX2lkIjozMzY0NDgyMjMsInBvc3RfaWQiOjE2Nz=
Y4MzUyNywiaWF0IjoxNzUxODkzNDM3LCJleHAiOjE3NTQ0ODU0MzcsImlzcyI6InB1Yi0xMzczM=
jMxIiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.2T-NDydj6u-g0Zbh9fPG1hfXozby11SlYZEn9hn=
7Aho&amp;r=3D5kb93z&amp;utm_campaign=3Demail-half-magic-comments&amp;action=
=3Dpost-comment&amp;utm_source=3Dsubstack&amp;utm_medium=3Demail" style=3D"=
font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,H=
elvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Sy=
mbol';display: inline-block;font-weight: 500;border: 1px solid rgb(0,0,0,.1=
);border-radius: 9999px;text-transform: uppercase;font-size: 12px;line-heig=
ht: 1;padding: 9px 0;text-decoration: none;color: rgb(119,119,119);min-widt=
h: 38px;box-sizing: border-box;width: 38px"><img class=3D"icon" src=3D"http=
s://substackcdn.com/image/fetch/$s_!x1tS!,w_36,c_scale,f_png,q_auto:good,fl=
_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideComments%3Fv%3=
D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2" wi=
dth=3D"18" height=3D"18" style=3D"border: none;vertical-align: middle;max-w=
idth: 18px" alt=3D""></a></td></tr></tbody></table></td><td width=3D"8" sty=
le=3D"min-width: 8px"></td><td style=3D"vertical-align:middle;"><table role=
=3D"presentation" width=3D"38" border=3D"0" cellspacing=3D"0" cellpadding=
=3D"0"><tbody><tr><td align=3D"center"><a class=3D"email-icon-button" href=
=3D"https://substack.com/app-link/post?publication_id=3D1373231&amp;post_id=
=3D167683527&amp;utm_source=3Dsubstack&amp;utm_medium=3Demail&amp;utm_conte=
nt=3Dshare&amp;utm_campaign=3Demail-share&amp;action=3Dshare&amp;triggerSha=
re=3Dtrue&amp;isFreemail=3Dfalse&amp;r=3D5kb93z&amp;token=3DeyJ1c2VyX2lkIjo=
zMzY0NDgyMjMsInBvc3RfaWQiOjE2NzY4MzUyNywiaWF0IjoxNzUxODkzNDM3LCJleHAiOjE3NT=
Q0ODU0MzcsImlzcyI6InB1Yi0xMzczMjMxIiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.2T-NDydj=
6u-g0Zbh9fPG1hfXozby11SlYZEn9hn7Aho" style=3D"font-family: system-ui,-apple=
-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Ap=
ple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';display: inline-block;f=
ont-weight: 500;border: 1px solid rgb(0,0,0,.1);border-radius: 9999px;text-=
transform: uppercase;font-size: 12px;line-height: 1;padding: 9px 0;text-dec=
oration: none;color: rgb(119,119,119);min-width: 38px;box-sizing: border-bo=
x;width: 38px"><img class=3D"icon" src=3D"https://substackcdn.com/image/fet=
ch/$s_!_L14!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2=
F%2Fsubstack.com%2Ficon%2FLucideShare2%3Fv%3D4%26height%3D36%26fill%3Dnone%=
26stroke%3D%2523808080%26strokeWidth%3D2" width=3D"18" height=3D"18" style=
=3D"border: none;vertical-align: middle;max-width: 18px" alt=3D""></a></td>=
</tr></tbody></table></td><td width=3D"8" style=3D"min-width: 8px"></td><td=
 style=3D"vertical-align:middle;"><table role=3D"presentation" width=3D"38"=
 border=3D"0" cellspacing=3D"0" cellpadding=3D"0"><tbody><tr><td align=3D"c=
enter"><a class=3D"email-icon-button" href=3D"https://substack.com/redirect=
/2/eyJlIjoiaHR0cHM6Ly9vcGVuLnN1YnN0YWNrLmNvbS9wdWIvbmF0ZXNuZXdzbGV0dGVyL3Av=
Y29udGV4dC13aW5kb3dzLWFyZS1hLWxpZS10aGUtbXl0aD91dG1fc291cmNlPXN1YnN0YWNrJnV=
0bV9tZWRpdW09ZW1haWwmdXRtX2NhbXBhaWduPWVtYWlsLXJlc3RhY2stY29tbWVudCZhY3Rpb2=
49cmVzdGFjay1jb21tZW50JnI9NWtiOTN6JnRva2VuPWV5SjFjMlZ5WDJsa0lqb3pNelkwTkRne=
U1qTXNJbkJ2YzNSZmFXUWlPakUyTnpZNE16VXlOeXdpYVdGMElqb3hOelV4T0Rrek5ETTNMQ0ps=
ZUhBaU9qRTNOVFEwT0RVME16Y3NJbWx6Y3lJNkluQjFZaTB4TXpjek1qTXhJaXdpYzNWaUlqb2l=
jRzl6ZEMxeVpXRmpkR2x2YmlKOS4yVC1ORHlkajZ1LWcwWmJoOWZQRzFoZlhvemJ5MTFTbFlaRW=
45aG43QWhvIiwicCI6MTY3NjgzNTI3LCJzIjoxMzczMjMxLCJmIjpmYWxzZSwidSI6MzM2NDQ4M=
jIzLCJpYXQiOjE3NTE4OTM0MzcsImV4cCI6MjA2NzQ2OTQzNywiaXNzIjoicHViLTAiLCJzdWIi=
OiJsaW5rLXJlZGlyZWN0In0.7vcXfQwAAJT2Huv29Ms2wKMWjcM6XedVtzM7ALO75bM?&amp;ut=
m_source=3Dsubstack&amp;utm_medium=3Demail" style=3D"font-family: system-ui=
,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-se=
rif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';display: inline-=
block;font-weight: 500;border: 1px solid rgb(0,0,0,.1);border-radius: 9999p=
x;text-transform: uppercase;font-size: 12px;line-height: 1;padding: 9px 0;t=
ext-decoration: none;color: rgb(119,119,119);min-width: 38px;box-sizing: bo=
rder-box;width: 38px"><img class=3D"icon" src=3D"https://substackcdn.com/im=
age/fetch/$s_!5EGt!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/htt=
ps%3A%2F%2Fsubstack.com%2Ficon%2FNoteForwardIcon%3Fv%3D4%26height%3D36%26fi=
ll%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2" width=3D"18" height=3D"=
18" style=3D"border: none;vertical-align: middle;max-width: 18px" alt=3D"">=
</a></td></tr></tbody></table></td></tr></tbody></table></td><td align=3D"r=
ight"><table role=3D"presentation" width=3D"auto" border=3D"0" cellspacing=
=3D"0" cellpadding=3D"0"><tbody><tr><td style=3D"vertical-align:middle;"><t=
able role=3D"presentation" width=3D"auto" border=3D"0" cellspacing=3D"0" ce=
llpadding=3D"0"><tbody><tr><td align=3D"center"><a class=3D"email-button-ou=
tline" href=3D"https://open.substack.com/pub/natesnewsletter/p/context-wind=
ows-are-a-lie-the-myth?utm_source=3Demail&amp;redirect=3Dapp-store&amp;utm_=
campaign=3Demail-read-in-app" style=3D"font-family: system-ui,-apple-system=
,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Col=
or Emoji','Segoe UI Emoji','Segoe UI Symbol';display: inline-block;font-wei=
ght: 500;border: 1px solid rgb(0,0,0,.1);border-radius: 9999px;text-transfo=
rm: uppercase;font-size: 12px;line-height: 12px;padding: 9px 14px;text-deco=
ration: none;color: rgb(119,119,119);"><div class=3D"email-button-spacer" s=
tyle=3D"font-size: 16px;line-height: 26px;display: inline-block;vertical-al=
ign: middle;max-width: 0;min-height: 18px;"></div><span class=3D"email-butt=
on-text" style=3D"vertical-align: middle;margin-right: 4px">READ IN APP</sp=
an><img class=3D"icon text-icon" src=3D"https://substackcdn.com/image/fetch=
/$s_!ET-_!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%=
2Fsubstack.com%2Ficon%2FLucideArrowUpRight%3Fv%3D4%26height%3D36%26fill%3Dn=
one%26stroke%3D%2523808080%26strokeWidth%3D2" width=3D"18" height=3D"18" st=
yle=3D"min-width: 18px;min-height: 18px;border: none;vertical-align: middle=
;margin-right: 0;margin-left: 0;max-width: 18px" alt=3D""></a></td></tr></t=
body></table></td></tr></tbody></table></td></tr></tbody></table></td></tr>=
<tr height=3D"16"><td height=3D"16" style=3D"font-size:0px;line-height:0;">=
&nbsp;</td></tr></tbody></table></div></div><div class=3D"post typography" =
dir=3D"auto" style=3D"--image-offset-margin: -117px;padding: 32px 0 0 0;fon=
t-size: 16px;line-height: 26px;"><div class=3D"body markup" dir=3D"auto" st=
yle=3D"text-align: initial;font-size: 16px;line-height: 26px;width: 100%;wo=
rd-break: break-word;margin-bottom: 16px;"><p style=3D"margin: 0 0 20px 0;c=
olor: rgb(54,55,55);line-height: 26px;font-size: 16px;margin-top: 0;"><em><=
span>I tend to get pretty turned off by AI hype, and </span><strong>my favo=
rite example of AI hype these days is AI context windows.</strong><span> So=
 being me, I wrote a piece about what the heck is actually going on under t=
he surface, why it works that way architecturally, and how to fix it. Oh, a=
nd I threw some AGI reflections in there just for fun!</span></em></p><p st=
yle=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size:=
 16px;"><em><span>Why bother? I get a LOT of messages that basically boil d=
own to </span><strong>=E2=80=9CI was promised a great context window that h=
eld all this stuff, and I put all this stuff in it=E2=80=94and this suppose=
dly smart model just crapped out on me.=E2=80=9D </strong><span>People thin=
k the problem is them, or the prompt. When it=E2=80=99s often mismanagement=
 of the context window coupled with believing vendor hype. We=E2=80=99ll ad=
dress both here.</span></em></p><p style=3D"margin: 0 0 20px 0;color: rgb(5=
4,55,55);line-height: 26px;font-size: 16px;"><em><strong>Imagine this:</str=
ong><span> You=E2=80=99ve finally convinced your CTO to approve a hefty inv=
estment in an AI solution that promises to understand, retain, and process =
vast amounts of information effortlessly=E2=80=94=E2=80=9C1M tokens of cont=
ext!=E2=80=9D the vendor boasted. Everyone was excited, thinking you=E2=80=
=99ve just purchased the digital equivalent of a photographic memory. Then,=
 reality strikes. Important details from page 40 (or 400) of your critical =
legal document vanish as if they=E2=80=99d never existed. The financial ana=
lysis that hinged on precise figures scattered throughout an extensive quar=
terly report? Completely botched. Your AI is hallucinating, forgetting inst=
ructions, and leaving you wondering if you=E2=80=99ve made an expensive mis=
take.</span></em></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);li=
ne-height: 26px;font-size: 16px;"><em><span>You=E2=80=99re not alone. </spa=
n><strong>Welcome to the Context Window Trap.</strong></em></p><p style=3D"=
margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"=
><em><span>It=E2=80=99s 2025, and we=E2=80=99ve arrived at a pivotal moment=
 in AI where promises and practicalities are diverging rapidly. As you read=
 this, countless teams worldwide are discovering the unsettling truth about=
 long-context AI models: they don=E2=80=99t quite work as advertised. </spa=
n><strong>This isn=E2=80=99t just an annoyance=E2=80=94it=E2=80=99s a funda=
mental barrier</strong><span> threatening to stall progress across industri=
es from healthcare to finance, from legal to software development. </span><=
/em></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26=
px;font-size: 16px;"><em><strong>What was sold was perfect memory, what we =
got was a fairly lossy semantic meaning pattern matcher with big holes.</st=
rong></em></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-heig=
ht: 26px;font-size: 16px;"><em>But why does this matter so urgently right n=
ow?</em></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height=
: 26px;font-size: 16px;"><em><span>Because the problem isn=E2=80=99t just a=
bout models forgetting. </span><strong>It=E2=80=99s about what that forgett=
ing reveals about the nature of AI itself.</strong><span> When your =E2=80=
=9Ccutting-edge=E2=80=9D model with a 1M-token capacity behaves like it has=
 just a fraction of that, it=E2=80=99s not simply a technical glitch. It=E2=
=80=99s a window into the very essence of how these models function=E2=80=
=94and more importantly, how they don=E2=80=99t.</span></em></p><p style=3D=
"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;=
"><em><span>In this essential guide, </span><strong>we=E2=80=99re going to =
explore the uncomfortable reality behind the =E2=80=9Clost in the middle=E2=
=80=9D phenomenon.</strong><span> You=E2=80=99ll understand exactly why mod=
els struggle to maintain coherence over extensive contexts, how their under=
lying architecture=E2=80=94built on probabilistic attention mechanisms=E2=
=80=94sets them up for failure, and </span><strong>why scaling up the conte=
xt size isn=E2=80=99t the silver bullet vendors claim it to be.</strong></e=
m></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px=
;font-size: 16px;"><em><strong>But this guide is more than a wake-up call; =
it=E2=80=99s a practical blueprint. </strong><span>Because despite it all, =
I definitely still see powerful AI systems every day. </span><strong>AI is =
worth building=E2=80=94but as this newsletter emphasizes=E2=80=94you have t=
o pay attention to the details to get the value you are looking for out of =
AI! It is not a magic wand.</strong><span> </span></em></p><p style=3D"marg=
in: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><em=
><strong>Anyway, inside I stuffed all the secret sauce for beating the cont=
ext window problem.</strong><span> Yes, that means it=E2=80=99s one of my c=
lassic longer posts, but you are human, and you can use smart strategies to=
 get what you want out of a full guide. I toyed with cutting it, and ultima=
tely I figure you deserve a full guide to one of the biggest problems in AI=
 right now. It=E2=80=99s too critical to get this right if you want to buil=
d systems that work.</span></em></p><blockquote style=3D"border-left: 4px s=
olid #45D800;margin: 20px 0;padding: 0;"><p style=3D"margin: 0 0 20px 0;col=
or: rgb(54,55,55);margin-left: 20px;line-height: 26px;font-size: 16px;"><em=
><strong>What we're really talking about here is the difference between usi=
ng AI and architecting with AI.</strong><span> </span><strong>Any fool can =
yeet 100K tokens into a prompt. It takes understanding to know when to chun=
k, when to retrieve, when to summarize, and when to tell the model exactly =
where to look. That's not a limitation =E2=80=93 that's craftsmanship.</str=
ong></em></p></blockquote><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,5=
5);line-height: 26px;font-size: 16px;"><em><strong>The strategies in here a=
re used by actual production teams and genuinely work</strong><span>: intel=
ligent chunking methods, smart retrieval systems, strategic summarization c=
hains, and more. We=E2=80=99ll walk through tailored industry-specific play=
books to tackle your exact use-cases, whether you=E2=80=99re navigating int=
ricate contracts in legal, dissecting dense financial reports, synthesizing=
 extensive healthcare records, or analyzing vast software codebases.</span>=
</em></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 2=
6px;font-size: 16px;"><em><strong>We=E2=80=99ll also critically assess the =
cost implications of these oversized contexts</strong><span>=E2=80=94real n=
umbers and real scenarios=E2=80=94to ensure your CFO won=E2=80=99t have a h=
eart attack when the bill arrives.</span></em></p><p style=3D"margin: 0 0 2=
0px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><em><strong>=
Ultimately, this is a call to action: The smartest teams aren=E2=80=99t wai=
ting for a magical =E2=80=9Cfix=E2=80=9D promised by future AI versions.</s=
trong><span> They=E2=80=99re proactively architecting around these fundamen=
tal limitations, adopting methods that don=E2=80=99t just sidestep the prob=
lem but actively leverage AI=E2=80=99s strengths. Whether you=E2=80=99re a =
believer in the potential for AGI or a skeptic seeing these systems as glor=
ified pattern-matchers, the strategies outlined here will significantly ele=
vate your approach.</span></em></p><p style=3D"margin: 0 0 20px 0;color: rg=
b(54,55,55);line-height: 26px;font-size: 16px;"><em><span>The Context Windo=
w Trap is real, it=E2=80=99s significant, and addressing it isn=E2=80=99t o=
ptional=E2=80=94it=E2=80=99s critical. </span><strong>Let=E2=80=99s dive in=
 and start building with the context windows we have=E2=80=94not the Cinder=
ella context windows we=E2=80=99re promised lol</strong></em></p><div class=
=3D"subscription-widget-wrap" style=3D"font-size: 16px;line-height: 26px;">=
<div class=3D"subscription-widget show-subscribe" style=3D"font-size: 16px;=
direction: ltr !important;font-weight: 400;text-decoration: none;font-famil=
y: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,A=
rial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';colo=
r: #363737;line-height: 1.5;max-width: 560px;margin: 24px auto;align-items:=
 flex-start;display: block;text-align: center;padding: 0px 32px;"><div clas=
s=3D"preamble" style=3D"margin-top: 16px;font-family: system-ui,-apple-syst=
em,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple C=
olor Emoji','Segoe UI Emoji','Segoe UI Symbol';font-size: 18px;max-width: 3=
84px;width: fit-content;line-height: 22px;display: flex;align-items: center=
;text-align: center;font-weight: 400;margin-left: auto;margin-right: auto;"=
><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font=
-size: 16px;">Subscribers get all these pieces!</p></div><div class=3D"subs=
cribe-widget is-signed-up is-fully-subscribed" data-component-name=3D"Subsc=
ribeWidget" style=3D"margin: 0 0 1em;direction: ltr;font-size: 16px;line-he=
ight: 26px;"><div class=3D"pencraft pc-reset button-wrapper" style=3D"text-=
decoration: unset;list-style: none;font-size: 16px;line-height: 26px;text-a=
lign: center;cursor: pointer;border-radius: 4px;"><a class=3D"button subscr=
ibe-btn outline" href=3D"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly=
9uYXRlc25ld3NsZXR0ZXIuc3Vic3RhY2suY29tL2FjY291bnQiLCJwIjoxNjc2ODM1MjcsInMiO=
jEzNzMyMzEsImYiOmZhbHNlLCJ1IjozMzY0NDgyMjMsImlhdCI6MTc1MTg5MzQzNywiZXhwIjoy=
MDY3NDY5NDM3LCJpc3MiOiJwdWItMCIsInN1YiI6ImxpbmstcmVkaXJlY3QifQ.qM4i62Bid2FV=
jLoLyP-LGtLmeE8_j9jmwfV30jTfl-Q?" style=3D"font-family: system-ui,-apple-sy=
stem,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple=
 Color Emoji','Segoe UI Emoji','Segoe UI Symbol';display: inline-block;box-=
sizing: border-box;cursor: pointer;border-radius: 8px;font-size: 14px;line-=
height: 20px;font-weight: 600;text-align: center;background-color: transpar=
ent;opacity: 1;outline: none;white-space: nowrap;text-decoration: none !imp=
ortant;border: 1px solid #45d800;margin: 0 auto;background: transparent;col=
or: #45d800;padding: 12px 20px;height: auto;"><img class=3D"check-icon stat=
ic" src=3D"https://substackcdn.com/image/fetch/$s_!3t53!,w_40,c_scale,f_png=
,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucid=
eCheck%3Fv%3D4%26height%3D40%26fill%3Dtransparent%26stroke%3D%252345D800%26=
strokeWidth%3D3.6" width=3D"20" height=3D"20" style=3D"border: none;vertica=
l-align: middle;-ms-interpolation-mode: bicubic;height: auto;display: inlin=
e-block;margin: -2px 8px 0 0;max-width: 20px" alt=3D""><span style=3D"text-=
decoration: none;">Subscribed</span></a></div></div></div></div><div class=
=3D"paywall-jump" data-component-name=3D"PaywallToDOM" style=3D"font-size: =
16px;line-height: 26px;display: none;"></div><h2 class=3D"header-anchor-pos=
t" style=3D"position: relative;font-family: 'SF Pro Display',-apple-system-=
headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helve=
tica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol=
';font-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoot=
hing: antialiased;-webkit-appearance: optimizelegibility;-moz-appearance: o=
ptimizelegibility;appearance: optimizelegibility;margin: 1em 0 0.625em 0;co=
lor: rgb(54,55,55);line-height: 1.16em;font-size: 1.625em;"><strong>TLDR: T=
he Context Window Trap Matters More Than You Think</strong></h2><p style=3D=
"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;=
"><strong>The revelation isn't that models forget. It's what the forgetting=
 reveals.</strong></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);l=
ine-height: 26px;font-size: 16px;">When a 200K-token model performs like a =
20K model, we're not looking at a bug=E2=80=94we're looking at the fundamen=
tal nature of these systems. The &quot;lost in the middle&quot; problem isn=
't an implementation detail. It's a window into how these models actually w=
ork, and why the path to AGI might be nothing like what we're building.</p>=
<p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-=
size: 16px;"><strong>Consider what's really happening:</strong><span> When =
you feed a model 100K tokens, it doesn't &quot;read&quot; them like you wou=
ld. It performs 10 billion attention operations, each asking &quot;how rela=
ted are these two tokens?&quot; But attention is probabilistic, not determi=
nistic. The model isn't maintaining a coherent mental model=E2=80=94it's pl=
aying a massive game of &quot;what word comes next?&quot; with weighted dic=
e.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-heigh=
t: 26px;font-size: 16px;">This is why position matters so much. The beginni=
ng tokens set the dice. The ending tokens are freshest when the model start=
s generating. Everything in between? It's there, technically, but only in t=
he way that page 1,847 of the phone book is &quot;there&quot; when you're l=
ooking for a number. The model can theoretically access it, but won't.</p><=
p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-s=
ize: 16px;"><strong>The deeper problem:</strong><span> We've built systems =
that look like they understand but actually perform sophisticated pattern m=
atching over sliding windows. When Thomson Reuters found that feeding more =
than 10 documents </span><em>hurt</em><span> performance, they discovered s=
omething profound: these models don't build understanding=E2=80=94they samp=
le from possibility space. More context doesn't mean deeper comprehension; =
it means more noise in the sampling process.</span></p><p style=3D"margin: =
0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>T=
his explains why RAG works so well (</span><a href=3D"https://substack.com/=
redirect/596f23bb-a80d-427d-9b7f-738e6538b7ce?j=3DeyJ1IjoiNWtiOTN6In0.zdzy8=
8YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,=
55);text-decoration: underline;">I wrote a guide to RAG here</a><span>). A =
tiny 1.1B model with good retrieval </span><a href=3D"https://substack.com/=
redirect/aec50b08-7b89-401c-b5d6-06286d1af05a?j=3DeyJ1IjoiNWtiOTN6In0.zdzy8=
8YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,=
55);text-decoration: underline;">beating GPT-4 Turbo</a><span> isn't a quir=
k=E2=80=94it's revealing that intelligence might be more about knowing wher=
e to look than holding everything in your head. Human experts don't memoriz=
e entire libraries; they know exactly which book, which chapter, which para=
graph contains what they need. Sound familiar? Oh and btw this does not get=
 fixed with o3 Pro=E2=80=94I=E2=80=99ve caught o3 Pro getting confused by l=
ong documents too. This is a persistent AI problem, and OpenAI and every ot=
her model maker have struggled with it. There are no good answers, partly b=
ecause physics (we=E2=80=99ll get to that).</span></p><p style=3D"margin: 0=
 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>=
The AGI implications are stark:</strong><span> If we can't get a model to r=
eliably track information across a mere 100K tokens=E2=80=94about a book's =
worth=E2=80=94how exactly do we expect it to maintain coherent understandin=
g across a lifetime of experience? The current architecture fundamentally c=
annot do what we're asking. It's not a scale problem. It's like trying to b=
uild a skyscraper with materials that can't support more than three floors=
=E2=80=94at some point, you need different materials, not just more of them=
.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height=
: 26px;font-size: 16px;"><strong>Here's the uncomfortable truth:</strong><s=
pan> The entire bet on LLMs achieving AGI is essentially a bet that humans =
are lossy compression functions too=E2=80=94so maybe it's close enough. We'=
re betting that human intelligence is just very sophisticated pattern match=
ing with strategic forgetting. That consciousness is compression. That unde=
rstanding is actually just very good bluffing based on statistical patterns=
.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height=
: 26px;font-size: 16px;">And you know what? The context window problem sugg=
ests this bet might be... wrong.</p><p style=3D"margin: 0 0 20px 0;color: r=
gb(54,55,55);line-height: 26px;font-size: 16px;"><strong>The economic reali=
ty reinforces this:</strong><span> The O(n=C2=B2) attention complexity isn'=
t just a computational annoyance=E2=80=94it's a fundamental barrier. Every =
doubling of context quadruples compute. The universe doesn't have enough en=
ergy to run true &quot;full attention&quot; at the scales AGI would require=
. Even if Moore's Law continued forever, we'd hit thermodynamic limits befo=
re achieving human-scale coherent context.</span></p><p style=3D"margin: 0 =
0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>S=
o what's really going on?</strong><span> We've built brilliant compression =
algorithms that can mimic understanding within narrow windows. When you cha=
t with GPT-4o, you're not talking to an entity that &quot;knows&quot; thing=
s=E2=80=94you're interacting with a system that can reconstruct plausible k=
nowledge from compressed patterns. The fact that it falls apart at scale re=
veals the trick. As I=E2=80=99ve written elsewhere, </span><a href=3D"https=
://substack.com/redirect/2b2a8ff6-15b4-4bf1-b164-00a76570a16a?j=3DeyJ1IjoiN=
WtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"c=
olor: rgb(54,55,55);text-decoration: underline;">the AI is a vibe</a><span>=
.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height=
: 26px;font-size: 16px;">But humans don't fall apart this way. Yes, we forg=
et. Yes, we compress. But we maintain coherent models of reality across tim=
e. We can read a book and understand how chapter 20 relates to chapter 1, e=
ven if we can't recite either verbatim. We build persistent mental structur=
es, not just statistical patterns. The difference between human &quot;lossy=
&quot; cognition and LLM &quot;lossy&quot; processing might be the differen=
ce between true intelligence and its simulacrum.</p><p style=3D"margin: 0 0=
 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>And =
there=E2=80=99s a difference between pre-trained weights and memory. I can =
get o3 to do decent literary analysis because it=E2=80=99s pre-trained on a=
 lot of literature. If I throw an actual book in the context window the suc=
cess at reading carefully is markedly worse. Humans don=E2=80=99t have the =
fundamental gap between what we=E2=80=99ve read before and what we=E2=80=99=
re reading now. If anything, what we=E2=80=99re reading now is </span><em>b=
etter remembered</em><span>. </span></p><p style=3D"margin: 0 0 20px 0;colo=
r: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>Anyway, this i=
s why the solutions work:</strong><span> When we chunk documents, build ret=
rieval systems, and carefully manage context, we're not working around a li=
mitation=E2=80=94we're building prosthetics for what these models fundament=
ally lack. Yes, even humans don't hold entire books in working memory. But =
we do something LLMs don't: we build mental models that persist and evolve.=
 We know that &quot;the contract stuff is in section 5&quot; not because we=
're doing attention operations, but because we've built a structured unders=
tanding.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line=
-height: 26px;font-size: 16px;"><strong>The vendors can't admit this</stron=
g><span> because it undermines the entire narrative. If context windows rev=
eal that LLMs are lossy in a fundamentally different way than humans=E2=80=
=94if they're compressing semantically without comprehending=E2=80=94then t=
here=E2=80=99s a chance we're not &quot;just a few breakthroughs&quot; from=
 AGI. We might just be playing poker with a deck that's missing a half doze=
n critical cards and pretending we just need to shuffle better.</span></p><=
p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-s=
ize: 16px;"><strong>What this means for builders:</strong><span> Let=E2=80=
=99s stop fighting the architecture and start embracing it. The companies w=
inning aren't those waiting for 1M context to &quot;finally work&quot;=E2=
=80=94they're those building systems that mirror how intelligence actually =
operates:</span></p><ul style=3D"margin-top: 0;padding: 0;"><li style=3D"ma=
rgin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,5=
5,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-lef=
t: 4px;font-size: 16px;margin: 0;">Aggressive abstraction and summarization=
 (like human memory=E2=80=94but externally managed)</p></li><li style=3D"ma=
rgin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,5=
5,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-lef=
t: 4px;font-size: 16px;margin: 0;">Pointer-based retrieval (compensating fo=
r lack of mental models)</p></li><li style=3D"margin: 8px 0 0 32px;mso-spec=
ial-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;mar=
gin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;marg=
in: 0;">Focused attention windows (the only thing these models do well)</p>=
</li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p styl=
e=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: bo=
rder-box;padding-left: 4px;font-size: 16px;margin: 0;">External memory stor=
es (replacing what should be internal coherence)</p></li></ul><p style=3D"m=
argin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">=
<strong>The philosophical punch:</strong><span> We've accidentally proven s=
omething profound. Either:</span></p><ol style=3D"margin-top: 0;padding: 0;=
"><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line=
-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;fon=
t-size: 16px;margin: 0;">Intelligence isn't about holding all information s=
imultaneously=E2=80=94it's about knowing what to forget and when to remembe=
r it, OR</p></li><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb=
(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;paddin=
g-left: 4px;font-size: 16px;margin: 0;">LLMs are lossy in a fundamentally d=
ifferent way than humans, and we're building elaborate workarounds for thei=
r inability to form coherent mental models</p></li></ol><p style=3D"margin:=
 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">The co=
ntext window problem forces us to pick a side. If you believe humans are ju=
st lossy functions too, then these workarounds are steps toward AGI. If you=
 believe human intelligence involves something more=E2=80=94persistent mode=
ls, true understanding, conscious experience=E2=80=94then we're building in=
creasingly sophisticated parrots.</p><p style=3D"margin: 0 0 20px 0;color: =
rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>The practical res=
ult:</strong><span> Every dollar you spend trying to force models to use ma=
ssive context is a dollar spent fighting physics and philosophy. Every doll=
ar spent on smart retrieval and abstraction is either:</span></p><ul style=
=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-specia=
l-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margi=
n-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin=
: 0;">A dollar spent aligning with how intelligence actually works, OR</p><=
/li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=
=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: bor=
der-box;padding-left: 4px;font-size: 16px;margin: 0;">A dollar spent buildi=
ng scaffolding around systems that have fundamental limitations that are no=
t getting fixed</p></li></ul><p style=3D"margin: 0 0 20px 0;color: rgb(54,5=
5,55);line-height: 26px;font-size: 16px;">The market is betting on option o=
ne. The context window problem suggests we=E2=80=99re probably not taking t=
he possibility of option two seriously enough. I don=E2=80=99t know which I=
 go for right now: probably 40% of my money is on =E2=80=9Cthis is how inte=
lligence actually works and we=E2=80=99re too proud to admit we=E2=80=99re =
this bad at memory as humans=E2=80=9D and 60% is on =E2=80=9Cwell we have f=
undamental limitations with LLMs here that mean we need new breakthroughs f=
or AGI.=E2=80=9D</p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);lin=
e-height: 26px;font-size: 16px;"><strong>Regardless, the vendors will keep =
promising larger context windows.</strong><span> The research papers will k=
eep showing U-shaped attention curves. But now you know what's really happe=
ning: we've built systems that compress and reconstruct meaning, not system=
s that understand it. Whether that's &quot;close enough&quot; for AGI depen=
ds on whether you think you're a lossy function too.</span></p><p style=3D"=
margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"=
>The most successful AI systems of the next decade won't be those with the =
longest context windows. They'll be those that gave up on the fantasy of pe=
rfect memory and either:</p><ul style=3D"margin-top: 0;padding: 0;"><li sty=
le=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: =
rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;pad=
ding-left: 4px;font-size: 16px;margin: 0;">Embraced the reality of intellig=
ent forgetting (if you're a believer), OR</p></li><li style=3D"margin: 8px =
0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line=
-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;fon=
t-size: 16px;margin: 0;">Built the best prosthetics for systems that can't =
truly think (if you're a skeptic)</p></li></ul><p style=3D"margin: 0 0 20px=
 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>Either wa=
y, the strategy is the same: architect around the limitation. The philosoph=
y behind why it works is what's up for debate, but that doesn=E2=80=99t sto=
p you building. It shouldn=E2=80=99t! </span><strong>Even if the LLMs are =
=E2=80=9Chuman dumb&quot; with context windows and this is a limitation tha=
t prevents AGI, that gap won=E2=80=99t prevent AI from absolutely transform=
ing every industry I can think of in the next couple decades. It=E2=80=99s =
already smart enough for that. </strong></p><p style=3D"margin: 0 0 20px 0;=
color: rgb(54,55,55);line-height: 26px;font-size: 16px;">Plan accordingly. =
And maybe hedge your bets. </p><h1 class=3D"header-anchor-post" style=3D"po=
sition: relative;font-family: 'SF Pro Display',-apple-system-headline,syste=
m-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,san=
s-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight:=
 bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing: antialia=
sed;-webkit-appearance: optimizelegibility;-moz-appearance: optimizelegibil=
ity;appearance: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55=
,55);line-height: 1.16em;font-size: 2em;"><strong>The Context Window Trap: =
Why Your 200K Token LLM Only Works Like It Has 20K (And What To Do About It=
)</strong></h1><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-hei=
ght: 26px;font-size: 16px;"><strong>Every week, I get the same DMs:</strong=
><span> </span><em>=E2=80=9CWhy does my model forget things I told it 50K t=
okens ago?=E2=80=9D =E2=80=9CClaude says it has 200K context but chokes on =
my 30-page document.=E2=80=9D =E2=80=9CWe paid for o3 Pro and it still can=
=E2=80=99t handle our use case.=E2=80=9D</em><span> If you=E2=80=99ve been =
promised 100K+ token context windows and are now watching your model halluc=
inate, forget instructions, or slow to a crawl, you=E2=80=99re not alone. T=
here=E2=80=99s a </span><strong>gap between marketing promises and producti=
on reality</strong><span> =E2=80=93 and it=E2=80=99s time to talk about it =
frankly.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line=
-height: 26px;font-size: 16px;"><span>This guide cuts through the vendor hy=
pe and explains </span><em>why</em><span> long context windows often </span=
><strong>fail in practice</strong><span>, and how to work around these limi=
tations. We=E2=80=99ll start by exposing the =E2=80=9Cbig lie=E2=80=9D of a=
dvertised vs effective context. Then we=E2=80=99ll dive into the technical =
truth (in simple terms) of why this happens =E2=80=93 covering attention me=
chanics, scaling costs, and hidden infrastructure limits. From there, we=E2=
=80=99ll quantify the </span><strong>hidden costs</strong><span> that will =
shock your CFO (token dollars, latency, hardware). But we won=E2=80=99t sto=
p at complaints: you=E2=80=99ll get </span><strong>battle-tested strategies=
</strong><span> (summary chains, smart chunking, retrieval hybrid methods, =
context budgeting, prompt tricks, dynamic compression) with real examples f=
or how to actually handle long inputs. We=E2=80=99ll explore </span><strong=
>playbooks for specific industries</strong><span> (legal, finance, healthca=
re, software) so you can see what works in scenarios like contract analysis=
 or code reviews. We=E2=80=99ll highlight the </span><strong>tools and fram=
eworks</strong><span> that genuinely help manage context and when to rely o=
n alternatives like vector databases. Finally, we=E2=80=99ll separate what=
=E2=80=99s </span><strong>coming next</strong><span> from what=E2=80=99s ju=
st hype, and provide a practical </span><strong>30-day plan</strong><span> =
(with a =E2=80=9CContext Strategy Canvas=E2=80=9D) to implement these solut=
ions in your own projects.</span></p><p style=3D"margin: 0 0 20px 0;color: =
rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>Tone check:</stro=
ng><span> Expect a direct, no-nonsense voice that calls out the BS. We=E2=
=80=99ll acknowledge the frustration (we=E2=80=99ve all been burned by exag=
gerated claims) but stay solution-focused. By the end, you=E2=80=99ll think=
 </span><em>=E2=80=9CFinally, someone told me the truth about context windo=
ws =E2=80=93 and what to actually DO about it.=E2=80=9D</em></p><p style=3D=
"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;=
">Let=E2=80=99s get started.</p><h2 class=3D"header-anchor-post" style=3D"p=
osition: relative;font-family: 'SF Pro Display',-apple-system-headline,syst=
em-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sa=
ns-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight=
: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing: antiali=
ased;-webkit-appearance: optimizelegibility;-moz-appearance: optimizelegibi=
lity;appearance: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,5=
5,55);line-height: 1.16em;font-size: 1.625em;"><strong>The Big Lie Nobody T=
alks About</strong></h2><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55)=
;line-height: 26px;font-size: 16px;"><strong>Advertised vs. Effective Conte=
xt:</strong><span> Vendors love to boast about massive context windows =E2=
=80=93 100K, 200K, even a million tokens. But in practice, </span><em>effec=
tive</em><span> context is often a fraction of that. Yes, models like GPT-4=
.1 claim a </span><strong>1,000,000-token</strong><span> context (enough fo=
r </span><em>War and Peace</em><span> in one go), and Claude advertises 200=
K. Yet developers find that beyond a certain point, the model behaves as if=
 earlier content =E2=80=9Cisn=E2=80=99t really there.=E2=80=9D As </span><a=
 href=3D"https://substack.com/redirect/e3db059a-6661-43c5-ae58-51f7a0a179fa=
?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=
=3D"" style=3D"color: rgb(54,55,55);text-decoration: underline;">one Google=
 forum user put it</a><span>, the 1M token context is basically </span><em>=
=E2=80=9Ccreative embellishment=E2=80=9D</em><span> =E2=80=93 the model can=
 only truly focus on about </span><strong>128K tokens at a time</strong><sp=
an>, </span><em>=E2=80=9Csame as all the other models.=E2=80=9D</em><span> =
In fact, not long ago this user observed Gemini 2.0 Pro (a Google model) </=
span><em>=E2=80=9Cwas failing immediately, even at 500K=E2=80=9D</em><span>=
 tokens until an updated version was released. In other words, that shiny m=
illion-token window often </span><strong>shrinks to a few tens of thousands=
</strong><span> of usable tokens once you actually test it. No this problem=
 hasn=E2=80=99t been fundamentally solved with Gemini 2.5 Pro, or any model=
 out there.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);l=
ine-height: 26px;font-size: 16px;"><strong>Real Examples of Failures:</stro=
ng><span> The internet is rife with examples of long-context models droppin=
g the ball. A particularly vivid case: an experimenter placed a specific se=
ntence in the </span><em>middle</em><span> of a 2,000-token text (well with=
in any large context) =E2=80=93 </span><em>=E2=80=9CAstrofield creates a no=
rmal understanding of non-celestial phenomena.=E2=80=9D</em><span> They the=
n asked various models to find that sentence. </span><strong>About 2/3 of t=
he models failed</strong><span>, including some that supposedly support 100=
K+ contexts. Think about that: even with just 2K tokens (tiny relative to a=
dvertised 128K or 200K limits), many models couldn=E2=80=99t recall a simpl=
e sentence hidden in the middle. This =E2=80=9C</span><strong>lost in the m=
iddle</strong><span>=E2=80=9D phenomenon has been documented in research: <=
/span><em><span>performance is highest when relevant info is at the beginni=
ng or end of the input, and it </span><strong>significantly degrades for in=
formation buried in the middle</strong><span>.</span></em><span> Even expli=
citly long-context models (like GPT-4=E2=80=99s extended versions or Claude=
 100K) show this behavior. They often nail questions if the answer is in th=
e first or last few pages, but completely miss it if it=E2=80=99s on page 5=
0 of 100. Users report models =E2=80=9Cforgetting=E2=80=9D instructions giv=
en halfway through a prompt, or a chatbot failing to remember a critical de=
tail from earlier conversation turns =E2=80=93 despite plenty of context wi=
ndow left.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);li=
ne-height: 26px;font-size: 16px;"><span>What does this mean in concrete ter=
ms? It means that your </span><strong>=E2=80=9C200K token=E2=80=9D model mi=
ght effectively have only ~20K of reliable memory</strong><span> in practic=
e. One academic study found that many open-source LLMs only use </span><em>=
less than half</em><span> of their claimed context length effectively. For =
example, a version of Llama 70B trained with a 128K window actually had an =
effective context of around </span><strong>64K</strong><span> in tests. And=
 64K is </span><em>being generous</em><span> =E2=80=93 that=E2=80=99s under=
 ideal conditions. In messy real-world tasks, the effective span can shrink=
 further. Thomson Reuters (who build legal AI tools) recently warned not to=
 be fooled by advertised context sizes: </span><em>=E2=80=9COften, the more=
 text included, the higher the risk of missing important details.=E2=80=9D<=
/em><span> In their internal benchmarks, they=E2=80=99ve seen that </span><=
strong>longer inputs can hurt</strong><span> =E2=80=93 the model starts ove=
rlooking key points when flooded with too much text. They observed a U-shap=
ed performance curve: feeding more and more documents initially helps (up t=
o a point), then performance </span><strong>drops off as context grows</str=
ong><span>. In fact, beyond ~10 documents worth of text, these models often=
 start to ignore the middle docs entirely.</span></p><p style=3D"margin: 0 =
0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>H=
allucinations and Degraded Performance:</strong><span> Along with forgotten=
 info, long contexts trigger other failures. Models may begin to </span><st=
rong>hallucinate</strong><span> =E2=80=93 inserting details that were never=
 in the prompt =E2=80=93 because they lose track of what=E2=80=99s ground t=
ruth versus their own guess. We=E2=80=99ve seen GPT-4 128K produce </span><=
em>=E2=80=9Cgarbage results=E2=80=9D</em><span> after a long wait when push=
ed to its limit (users have reported response times of 30+ minutes followed=
 by incoherent output). Another common symptom is </span><strong>incoherent=
 chaining</strong><span>: by the time the model generates output about an e=
arlier part of the prompt, it may have =E2=80=9Cforgotten=E2=80=9D context =
from much later that should have informed it. For instance, you give a 30-p=
age document to summarize, and the summary omits whole sections or contradi=
cts itself because the model essentially ran out of attention half-way thro=
ugh. Empirical tests by Stanford and others confirm a </span><em>=E2=80=9Cs=
ignificant decline in performance=E2=80=9D</em><span> when the relevant inf=
o is in the middle of a long input. They noted that models with 32K or 100K=
 windows all showed this issue =E2=80=93 a </span><strong>U-curve</strong><=
span> where performance was high if answer was at start or end, but </span>=
<em>tanked</em><span> for middle-position info. This aligns with the </span=
><strong>human primacy/recency effect</strong><span>, but for LLMs it=E2=80=
=99s an architectural limitation: they=E2=80=99re just not effectively usin=
g the full context even though they accept it.</span></p><p style=3D"margin=
: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><stro=
ng>Concrete Numbers:</strong><span> Just how bad is the drop-off? Early tes=
ts of GPT-4.1 (with the 1M token window) show it does </span><em>okay</em><=
span> up to some hundreds of thousands, but </span><em>accuracy plunges to =
~50% at the full 1M tokens</em><span>, compared to ~84% on an 8K prompt. An=
d that=E2=80=99s a </span><em>new</em><span> model supposedly trained for l=
ong range. Another long-context benchmark by Databricks found that GPT-4=E2=
=80=99s quality in a retrieval task </span><strong>started to degrade after=
 ~64K tokens</strong><span>, even though it could take 128K. A 405B-paramet=
er Llama prototype dropped off after ~32K. In other words, half or quarter =
of the advertised max is where the model starts </span><strong>slipping</st=
rong><span>. Anecdotally, developers working with Google=E2=80=99s Gemini 2=
.5 (1M context) have found it reliable only up to ~100K in complex scenario=
s, and highly variable beyond that. One dev on the AI StackExchange bluntly=
 titled a post </span><em>=E2=80=9CThe 1M context window lie=E2=80=9D</em><=
span>, after watching the model make =E2=80=9Cabsolutely stupid stuff=E2=80=
=9D edits when asked to handle code at ~200K tokens =E2=80=93 he concluded =
it really only had a </span><strong>~100K effective window</strong><span> i=
n practice.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);l=
ine-height: 26px;font-size: 16px;"><strong>=E2=80=9CLost in the Middle=E2=
=80=9D Phenomenon:</strong><span> We have to give this problem a name becau=
se you=E2=80=99ll see it over and over: </span><em>lost in the middle</em><=
span>. The model pays a lot of attention to the earliest part of the input =
(where often the instructions or system prompts are) and to the very end (o=
ften the user=E2=80=99s latest question). But the farther something is from=
 those ends, the more it becomes =E2=80=9Cbackground noise=E2=80=9D that th=
e model might neglect. It=E2=80=99s not strictly linear decay =E2=80=93 it =
can be pretty sharp drop-off beyond certain positions. In one study, GPT-3.=
5=E2=80=99s performance on a task </span><em>actually got worse</em><span> =
when given </span><em>more</em><span> context beyond a sweet spot, to the p=
oint that providing extra documents hurt accuracy compared to giving it not=
hing at all. Imagine that =E2=80=93 your fancy LLM might do </span><em>wors=
e</em><span> with the full 100-page dossier than it would if you gave it no=
 dossier! This is why many of you have seen bizarre omissions: e.g., you pr=
ompt with a long prepended company background then ask for analysis, and th=
e model=E2=80=99s answer ignores a crucial detail that was buried around to=
ken #15,000 in that background. It=E2=80=99s not malicious; it literally </=
span><em>didn=E2=80=99t register</em><span> that detail deeply.</span></p><=
p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-s=
ize: 16px;"><strong>The Big Lie:</strong><span> So, the big lie nobody talk=
s about is that </span><em>context length !=3D useful context</em><span>. T=
he marketing says =E2=80=9Cjust throw everything in, our model can handle 1=
00K tokens no problem,=E2=80=9D but the truth is that you often still need =
to </span><strong>summarize, chunk, and prioritize</strong><span> because t=
he model won=E2=80=99t genuinely leverage all that information. In high-sta=
kes settings, assuming an LLM will </span><strong>consistently remember som=
ething on page 50 of its input is a recipe for disaster</strong><span>. As =
Thomson Reuters=E2=80=99 CTO put it, </span><em>=E2=80=9Cwhen you look at t=
he advertised context window for leading models today, don=E2=80=99t be foo=
led into thinking this is a solved problem. In complex, reasoning-heavy rea=
l-world tasks, the effective context window shrinks.=E2=80=9D</em><span> Ev=
en they, with early access to GPT-4.1 and Claude Opus, found they </span><e=
m>still</em><span> had to chunk documents to avoid missed info.</span></p><=
p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-s=
ize: 16px;"><span>In summary, </span><strong>the =E2=80=9Clong context=E2=
=80=9D promise has been oversold</strong><span>. You feed a novel-sized pro=
mpt and get a response that feels like the model only read the first and la=
st chapters. Now let=E2=80=99s see </span><em>why</em><span> this happens.<=
/span></p><h2 class=3D"header-anchor-post" style=3D"position: relative;font=
-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,Bl=
inkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color =
Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-sm=
oothing: antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearanc=
e: optimizelegibility;-moz-appearance: optimizelegibility;appearance: optim=
izelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.1=
6em;font-size: 1.625em;"><strong>Why Long Context Windows Fail (The Technic=
al Truth =E2=80=93 Simply Explained)</strong></h2><p style=3D"margin: 0 0 2=
0px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>Why ca=
n=E2=80=99t these giant models just use all the tokens we give them? The an=
swer lies in how </span><strong>attention mechanisms</strong><span> work an=
d how models are trained =E2=80=93 but we=E2=80=99ll keep it high-level and=
 business-friendly.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,=
55,55);line-height: 26px;font-size: 16px;"><strong>Attention and Its Limits=
:</strong><span> Large Language Models use a mechanism called =E2=80=9Cself=
-attention=E2=80=9D to weigh different parts of the input text when generat=
ing each word of output. In essence, the model scans the entire input conte=
xt to decide what=E2=80=99s relevant at each step. The trouble is, attentio=
n is an </span><em>O(n=C2=B2)</em><span> operation =E2=80=93 meaning the co=
mputational work (and memory) grows quadratically with the number of tokens=
 </span><em>n</em><span>. A 100K token context isn=E2=80=99t just 10=C3=97 =
harder than a 10K context; it could be </span><strong>100=C3=97 or more</st=
rong><span> in cost. Vendors know this, and they employ engineering tricks =
to reduce the pain (like optimized kernels, or truncating low-relevance tok=
ens, or using </span><em>sparse attention</em><span> patterns). But fundame=
ntally, asking a model to </span><strong>truly consider 200,000 tokens all =
at once is a massive ask</strong><span>. The model might do =E2=80=9Csome=
=E2=80=9D attention across that range but not uniformly =E2=80=93 often, it=
 ends up effectively focusing on a subset of the context to save compute. T=
hink of it like a student with a stack of 20 textbooks open: they </span><e=
m>say</em><span> they=E2=80=99ve read them all, but really they skimmed and=
 only fully read a couple chapters.</span></p><p style=3D"margin: 0 0 20px =
0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>Beginnin=
g and End Bias (=E2=80=9CRecency=E2=80=9D effects):</strong><span> Most tra=
nsformer models also have a built-in bias to prioritize earlier and later t=
okens. Partly this is due to how they=E2=80=99re trained =E2=80=93 many tra=
ining examples are shorter than the max length, and models learn that the s=
tart of input often contains instructions or key info, and the end often co=
ntains the question or summary. Also, some positional encoding schemes (lik=
e the rotary embeddings used by many LLMs) effectively </span><strong>under=
value very distant positions</strong><span> because of how they extrapolate=
 beyond trained lengths. Research has identified a =E2=80=9C</span><strong>=
left-skewed position frequency</strong><span>=E2=80=9D issue: during traini=
ng, models see far more examples of tokens at position 1-1000 than at posit=
ion 50,000, so they=E2=80=99re under-practiced at using those later positio=
ns. In plain terms: if a model mostly saw 2K-length texts in training, givi=
ng it a 100K text is forcing it into unfamiliar territory. One study bluntl=
y concluded: </span><em>=E2=80=9Ceffective context lengths of open-source L=
LMs often fall short, typically not exceeding half of their training length=
s.=E2=80=9D</em><span>. They traced it to under-training of long positions =
=E2=80=93 the model=E2=80=99s =E2=80=9Cattention focus=E2=80=9D is heavily =
skewed to the first few thousand tokens. So when you stuff a huge input, it=
=E2=80=99s like a person trying to recall page 200 of a book they skimmed =
=E2=80=93 the fidelity just isn=E2=80=99t there.</span></p><p style=3D"marg=
in: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><st=
rong>Quadratic Scaling and Latency:</strong><span> The quadratic complexity=
 doesn=E2=80=99t just affect cost; it affects </span><em>performance and me=
mory</em><span>. As context grows, inference slows down dramatically. Even =
if a provider doesn=E2=80=99t charge you more for a long prompt (though man=
y do charge by token, which we=E2=80=99ll cover next), you=E2=80=99ll wait =
much longer for a response. For example, some users of Azure GPT-4 noticed =
that using near the max tokens made initial response latency </span><strong=
>very slow</strong><span>, and throughput dropped =E2=80=93 the model would=
 sometimes output a big chunk after a long pause (a sign it was struggling =
internally). One dev observed that </span><em>=E2=80=9Cincreasing the conte=
xt to 32K is actually a regression in most cases=E2=80=9D</em><span> due to=
 added latency, finding </span><strong>8K to be a sweet spot</strong><span>=
 for speed vs info. If 32K is already a latency problem, imagine 200K. Thes=
e models require so much computation to do full attention that either </spa=
n><strong>inference slows to a crawl</strong><span>, or the provider quietl=
y uses approximations (like not attending fully to everything, which ties b=
ack to lost-middle issues). It=E2=80=99s telling that one LLM service decid=
ed </span><em>not</em><span> to use 32K for general use because it made res=
ponses slower without clear benefit in most queries.</span></p><p style=3D"=
margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"=
><strong>Infrastructure Constraints (Vendor Secrets):</strong><span> There =
are also </span><em>hidden infrastructure limitations</em><span> vendors do=
n=E2=80=99t shout about. Running a 100K or 1M token model is </span><strong=
>memory intensive</strong><span>. The </span><em>KV cache</em><span> (which=
 stores intermediate keys/values for attention) grows linearly with context=
 length. This means if you self-host a model, doubling the context can doub=
le memory use. On a GPU, if the KV cache can=E2=80=99t fit, the system star=
ts swapping to CPU, causing extreme slowdowns (token generation speed can p=
lummet from e.g. 50 tokens/sec to 2 tokens/sec when spilling to CPU ). One =
guide bluntly stated: </span><em>=E2=80=9CYou can run big models or long pr=
ompts on a consumer GPU =E2=80=93 but rarely both.=E2=80=9D</em><span>. For=
 instance, a 70B parameter model might need two 80GB GPUs just to run at 16=
K context in float32 ; to push to 100K+ context typically requires 8-bit or=
 4-bit compression and still huge memory. Cloud providers manage this behin=
d the scenes with partitioned inference across multiple GPUs or specialized=
 hardware, but there=E2=80=99s a cost. Sometimes to offer a 100K window, pr=
oviders deploy a larger-but-slower model or use model parallelism, which mi=
ght be why some =E2=80=9Cturbo=E2=80=9D long-context models actually respon=
d slower than their short-context counterparts.</span></p><p style=3D"margi=
n: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><spa=
n>Also, not all model architectures scale equally. There=E2=80=99s rumor (a=
nd some evidence) that some API providers </span><em>don=E2=80=99t actually=
 use one single model for the entire 100K input</em><span>. They might have=
 a retrieval or summarization step internally for very long inputs. For exa=
mple. These rumors have swirled around Gemini and Llama in particular, with=
 one strategy speculating that a =E2=80=9CScout=E2=80=9D model finds releva=
nt bits in the million-token input, and a =E2=80=9CMaverick=E2=80=9D model =
(smaller context but smarter) then processes those. Yes I=E2=80=99m aware t=
hese are Llama names. The irony is not lost on me. </span></p><p style=3D"m=
argin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">=
<span>If the rumors are true, that means when you send 1M tokens, the syste=
m isn=E2=80=99t truly feeding all that to a single model =E2=80=93 it=E2=80=
=99s doing a two-stage process. This isn=E2=80=99t necessarily bad (it=E2=
=80=99s actually a sensible approach), but it underscores that </span><stro=
ng>the model isn=E2=80=99t doing what you think =E2=80=93 it=E2=80=99s not =
linearly reading token 1 to 1,000,000 and retaining all that in a single pa=
ss</strong><span>. The vendor marketing won=E2=80=99t detail these trade-of=
fs (hence why this is firmly a rumor, not a fact), but as builders we need =
to be aware that under the hood, long context might be implemented with tri=
cks that have implications for accuracy.</span></p><p style=3D"margin: 0 0 =
20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>=E2=
=80=9CMiddle=E2=80=9D Gets Overlooked by Design:</strong><span> One more te=
chnical point: if an LLM has to drop some attention for efficiency, where w=
ill it drop it? Likely in the middle. Some recent methods (like </span><em>=
LongContextReorder</em><span> in LangChain) explicitly reorder retrieved do=
cuments so that the most relevant are placed at the </span><em>edges</em><s=
pan> of the prompt (beginning or end) to exploit this bias. This hack exist=
s precisely because models tend to focus on the edges. If you suspect the m=
iddle of your prompt is being ignored, you=E2=80=99re probably right. Until=
 architectures evolve (there are research efforts like recurrent memory tra=
nsformers, but they=E2=80=99re not mainstream in 2025 production yet), this=
 issue will persist.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54=
,55,55);line-height: 26px;font-size: 16px;"><span>In summary, </span><stron=
g>long contexts fail because:</strong><span> (1) computationally it=E2=80=
=99s hard to </span><em>attend</em><span> to everything (O(n=C2=B2) bottlen=
eck), (2) models weren=E2=80=99t sufficiently trained on ultra-long sequenc=
es (so they extrapolate poorly beyond a certain length), (3) practical impl=
ementations quietly favor the start and end of the input, and (4) resource =
constraints force either slower performance or heuristics that drop informa=
tion. In non-academic terms: </span><em>the model=E2=80=99s attention is a =
mile wide and an inch deep</em><span> when you give it a huge prompt. It si=
mply can=E2=80=99t maintain uniform focus across 200K tokens with today=E2=
=80=99s technology and training.</span></p><p style=3D"margin: 0 0 20px 0;c=
olor: rgb(54,55,55);line-height: 26px;font-size: 16px;">Now that we know wh=
y this happens, let=E2=80=99s talk money =E2=80=93 because long context isn=
=E2=80=99t just a technical headache, it can hit your wallet hard.</p><h2 c=
lass=3D"header-anchor-post" style=3D"position: relative;font-family: 'SF Pr=
o Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFon=
t,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe U=
I Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoothing: antial=
iased;-moz-osx-font-smoothing: antialiased;-webkit-appearance: optimizelegi=
bility;-moz-appearance: optimizelegibility;appearance: optimizelegibility;m=
argin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-size: =
1.625em;"><strong>The Surprising Cost of Context</strong></h2><p style=3D"m=
argin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">=
<span>Many teams have been lured by =E2=80=9Cfeed it everything=E2=80=9D pr=
omises only to get a nasty surprise when the bill comes or the system grind=
s under load. Long contexts carry </span><strong>hidden costs</strong><span=
> in both </span><strong>dollars and performance</strong><span> that you mu=
st budget for.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55=
);line-height: 26px;font-size: 16px;"><strong>Token Costs =E2=80=93 Pay by =
the Prompt:</strong><span> Most API providers charge per token of input and=
 output. A token is roughly ~0.75 words in English. So a 100K token input i=
s like a 75,000-word dump =E2=80=93 about a 300-page book! OpenAI=E2=80=99s=
 pricing for GPT-4.1 (as of mid-2025) is around </span><strong>$2.00 per mi=
llion input tokens</strong><span> and </span><strong>$8.00 per million outp=
ut tokens</strong><span>. That might sound cheap until you do the math. If =
you max out a 1M context in one go, that=E2=80=99s $2 just to feed it in. I=
f the answer is, say, 50K tokens (50k is ~40k words, perhaps a very detaile=
d report), that=E2=80=99s another $0.40. So $2.40 for one query </span><em>=
at best</em><span>. If your application makes hundreds or thousands of such=
 calls, the costs balloon. And note: older models were even pricier per tok=
en; OpenAI significantly reduced costs in 2025. Some other providers charge=
 more. Claude 2=E2=80=99s 100K context, for example (back in 2024) was pric=
ed such that a full window could cost in the ballpark of $5-$10 per prompt =
once you include output. If you=E2=80=99re doing interactive chat with long=
 history, those tokens accumulate. The CFO might not realize that each user=
 conversation could be eating up </span><em>tens of cents or dollars</em><s=
pan> if you keep stuffing the full history or documents each turn.</span></=
p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;fon=
t-size: 16px;"><strong>Compute Time and Throughput:</strong><span> Even if =
token costs drop, </span><strong>compute time costs</strong><span> (and opp=
ortunity cost of throughput) are significant. A long context request ties u=
p the model for more time. That means fewer requests per second handled, an=
d possibly needing more replicas or paying for higher-rate API tiers to han=
dle the same user load. If one 100K request takes as long as 10 short reque=
sts, you=E2=80=99ve effectively multiplied your infrastructure cost by 10 f=
or that use case. This is why some teams find that throughput collapses whe=
n they try to scale out long-context queries =E2=80=93 you hit rate limits =
or your instance saturates. Cloud providers often have </span><em>rate limi=
ts</em><span> that scale with token count too (e.g., you might only get a f=
ew 100K-token requests per minute on an API key, versus many more smaller r=
equests). All of this can translate to needing to purchase higher tiers or =
more instances =E2=80=93 directly hitting the budget.</span></p><p style=3D=
"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;=
"><strong>Latency =3D User Experience Cost:</strong><span> Don=E2=80=99t un=
derestimate the </span><strong>latency penalty</strong><span>. We touched o=
n it =E2=80=93 long contexts are slower. If a user has to wait 30 seconds f=
or a response because you sent a giant prompt, that=E2=80=99s a cost in use=
r satisfaction (and maybe a cost in terms of having to implement more compl=
ex async handling, etc.). In some domains, you simply </span><em>cannot</em=
><span> afford that wait. Picture a customer support chatbot that needs to =
scan a knowledge base =E2=80=93 if it tries to stuff all articles into the =
prompt and responds in 1 minute, the user already bounced. So teams might i=
nvest in </span><em>workarounds or additional tech (like caching)</em><span=
> to mitigate latency, which is another indirect cost.</span></p><p style=
=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16=
px;"><strong>Memory and Hardware Requirements:</strong><span> If you=E2=80=
=99re running models yourself (or even if you=E2=80=99re not, the vendor=E2=
=80=99s costs here get passed to you indirectly), long context means </span=
><strong>huge memory usage</strong><span>. The KV cache (which stores info =
per token per layer) is massive at high lengths. A rough heuristic: For a m=
odel with </span><em>H</em><span> hidden size and </span><em>L</em><span> l=
ayers, KV memory is proportional to </span><em>L =C3=97 H =C3=97 n</em><spa=
n> (n =3D number of tokens). For GPT-4 sized models, this is in the gigabyt=
es at n=3D100K. One analysis showed that a 14B parameter model in 12GB VRAM=
 could only handle ~4K context; a 2-3B model was needed to handle 100K+ in =
that memory. So either you go with smaller models (losing quality) or bigge=
r hardware. High-end A100 GPUs with 80GB memory might handle ~16-32K contex=
t for a 70B model in float16, but 100K might require two or more GPUs or 8-=
bit quantization with performance hits. If you need to run this in real tim=
e, you might require </span><em>model parallelism</em><span>, which introdu=
ces complexity (multiple GPUs coordinating on one request). That=E2=80=99s =
fine for big companies, but for many teams it=E2=80=99s a headache and a co=
st (e.g., cloud GPU instances are expensive =E2=80=93 an 8=C3=97A100 setup =
can run thousands of dollars a month). Even in API usage, you pay for the p=
rovider=E2=80=99s hardware usage. It=E2=80=99s no coincidence the million-t=
oken GPT-4.1 costs a decent chunk per call =E2=80=93 running that monster i=
sn=E2=80=99t cheap on their side either.</span></p><p style=3D"margin: 0 0 =
20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>Exa=
mple =E2=80=93 Cost Reality Check:</strong><span> Let=E2=80=99s do a quick =
cost reality check scenario: Suppose you have an application that processes=
 50-page documents. You consider using GPT-4 Turbo 128K context vs a retrie=
val approach. If you go brute force, each document is ~50 pages ~ 12,500 to=
kens. You include maybe some instructions and queries, totaling 13K input, =
and you get a 1000-token output. That=E2=80=99s ~14K tokens per request. At=
 $2 per million input and $8 per million output, that=E2=80=99s about </spa=
n><strong>$0.028 per request</strong><span>. 1000 such requests (maybe 1000=
 documents processed) is $28. Not terrible. But what if you thought =E2=80=
=9Chey, we have 128K, let=E2=80=99s batch 10 documents together and ask the=
 model to find relevant info among them=E2=80=9D? Now your prompt is 130K t=
okens (10=C3=97 bigger). Output maybe slightly larger, say 2K tokens. That =
single request costs ~$0.26. To cover the same 1000 documents (100 requests=
 now, each with 10 docs), it=E2=80=99s $26 =E2=80=93 sounds similar, </span=
><strong>but wait</strong><span>: will the model even handle 10 docs well? =
We saw after ~10 docs performance drops. Perhaps you then decide to include=
 even more context =E2=80=9Cto be safe=E2=80=9D =E2=80=93 maybe you throw a=
 whole database of info (100s of pages) at it. You could easily hit the ful=
l 128K and $0.50 per call. And if you try GPT-4.1 with near 1M tokens input=
 (like an entire corpus), you=E2=80=99re burning $2+ each time. Multiply by=
 many calls or users and it adds up quickly. In contrast, a well-designed r=
etrieval (vector DB) approach might feed only, say, 2K tokens of relevant t=
ext per query (cost $0.004) with negligible difference in result quality =
=E2=80=93 as long as the retrieval finds the right info.</span></p><p style=
=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16=
px;"><span>The </span><strong>sticker shock</strong><span> really comes whe=
n teams test at small scale (a few calls) and see maybe a $10 bill, and the=
n deploy to production and the monthly invoice is thousands of dollars due =
to heavy token usage. One finance company discovered that letting a GPT-4 3=
2K model ingest full financial reports for Q&amp;A was costing them 5=C3=97=
 more than a refined approach that summarized sections first. Another start=
up found that turning on the 128K context option for their chatbot made the=
ir API costs </span><em>triple</em><span>, with only marginal gains in accu=
racy (because the model wasn=E2=80=99t actually using most of that extra in=
fo effectively!).</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55=
,55);line-height: 26px;font-size: 16px;"><strong>TL;DR for the CFO:</strong=
><span> Long context =3D </span><em>more tokens per request</em><span> =3D =
more money per request. It also often =3D slower responses =3D potentially =
fewer requests handled (or more servers needed) =3D more cost. And if we us=
e more complex infrastructure (like bigger GPUs or memory), that=E2=80=99s =
yet more cost.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55=
);line-height: 26px;font-size: 16px;"><span>We=E2=80=99ll discuss solutions=
 soon (like retrieval, chunking, etc.) that often provide </span><strong>80=
-90% of the value of long context at a fraction of the cost</strong><span>.=
 But before that, let=E2=80=99s enumerate these hidden costs clearly:</span=
></p><ul style=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 3=
2px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-heig=
ht: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-siz=
e: 16px;margin: 0;"><strong>Token Fees:</strong><span> More tokens =3D high=
er API charges. A 100K prompt can cost 10-20=C3=97 a 10K prompt. Multiply b=
y usage volumes.</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-speci=
al-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;marg=
in-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margi=
n: 0;"><strong>Compute &amp; Memory:</strong><span> Quadratic scaling means=
 potentially 100=C3=97 the computation at 10=C3=97 length. You pay in eithe=
r cloud credits or hardware.</span></p></li><li style=3D"margin: 8px 0 0 32=
px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-heigh=
t: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size=
: 16px;margin: 0;"><strong>Latency:</strong><span> User wait times can incr=
ease 5x-10x for long inputs, which can hurt conversion or require engineeri=
ng workarounds (loading spinners, etc.).</span></p></li><li style=3D"margin=
: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55=
);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4=
px;font-size: 16px;margin: 0;"><strong>Throughput &amp; Rate Limits:</stron=
g><span> Need more parallel calls or hit vendor rate caps sooner, possibly =
requiring higher pricing tiers or workarounds.</span></p></li><li style=3D"=
margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54=
,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-l=
eft: 4px;font-size: 16px;margin: 0;"><strong>Engineering Complexity:</stron=
g><span> Not directly dollars, but trying to utilize long contexts might in=
volve new tools (e.g. special vector indexes for partial retrieval, or moni=
toring to prevent runaway token usage) =E2=80=93 effectively engineering ti=
me =3D money.</span></p></li></ul><p style=3D"margin: 0 0 20px 0;color: rgb=
(54,55,55);line-height: 26px;font-size: 16px;">To make this concrete: imagi=
ne telling your CFO =E2=80=9CWe want to enable 100K context summarization o=
f client documents.=E2=80=9D Without the truth, they might think =E2=80=9Cw=
e already pay for GPT-4, it=E2=80=99s fine.=E2=80=9D But the truth is =E2=
=80=9Cthis could make each summary cost $1 instead of $0.05, and we=E2=80=
=99ll need to invest in optimizing or the users will wait too long.=E2=80=
=9D That=E2=80=99s a different conversation. It=E2=80=99s better to have th=
at up front with realistic numbers.</p><p style=3D"margin: 0 0 20px 0;color=
: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>We=E2=80=99ll pro=
vide a =E2=80=9CCost Reality Check=E2=80=9D framework later to help you cal=
culate these trade-offs for your use case. Now, onto the good news: how you=
 can still achieve your goals </span><strong>without</strong><span> falling=
 into the context trap. The answer is combining strategies =E2=80=93 essent=
ially doing </span><em>what the model isn=E2=80=99t doing for you automatic=
ally</em><span>. Think of it as </span><strong>managing context proactively=
</strong><span>.</span></p><h2 class=3D"header-anchor-post" style=3D"positi=
on: relative;font-family: 'SF Pro Display',-apple-system-headline,system-ui=
,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-se=
rif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bol=
d;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing: antialiased;=
-webkit-appearance: optimizelegibility;-moz-appearance: optimizelegibility;=
appearance: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55)=
;line-height: 1.16em;font-size: 1.625em;"><strong>Battle-Tested Strategies =
That Actually Work</strong></h2><p style=3D"margin: 0 0 20px 0;color: rgb(5=
4,55,55);line-height: 26px;font-size: 16px;"><span>Enough about problems =
=E2=80=93 let=E2=80=99s talk </span><strong>solutions</strong><span>. No si=
ngle silver bullet exists (sorry, you can=E2=80=99t just flip a switch to m=
ake a 200K model magically remember the 100K mark content). But </span><str=
ong>a toolkit of strategies</strong><span> can cover 99% of real-world need=
s. The key idea: </span><em>treat long context as a tool, not a crutch</em>=
<span>. Use the big window when it truly helps, but augment it with clever =
workflows so you=E2=80=99re not relying on it blindly. Here are </span><str=
ong>six battle-tested approaches</strong><span>, each with step-by-step gui=
dance, examples, and when to use them. You=E2=80=99ll likely combine severa=
l in practice.</span></p><h3 class=3D"header-anchor-post" style=3D"position=
: relative;font-family: 'SF Pro Display',-apple-system-headline,system-ui,-=
apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-seri=
f,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;=
-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing: antialiased;-w=
ebkit-appearance: optimizelegibility;-moz-appearance: optimizelegibility;ap=
pearance: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);l=
ine-height: 1.16em;font-size: 1.375em;"><strong>1. The =E2=80=9CSummary Cha=
in=E2=80=9D Pattern</strong></h3><p style=3D"margin: 0 0 20px 0;color: rgb(=
54,55,55);line-height: 26px;font-size: 16px;"><strong>When to use:</strong>=
<span> You have a </span><strong>large body of text</strong><span> (say a l=
ong report, book, or transcript) and you need an overall summary or to answ=
er high-level questions about it. The text doesn=E2=80=99t fit (or shouldn=
=E2=80=99t be dumped) in one prompt. Summary Chain shines when you need to =
</span><strong>distill information</strong><span> from a long source withou=
t losing key details. It=E2=80=99s great for things like summarizing a long=
 earnings call transcript, legal brief, or technical paper. Also useful to =
preserve information over </span><em>many</em><span> chat turns (compress o=
lder dialogue).</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,5=
5);line-height: 26px;font-size: 16px;"><strong>What it is:</strong><span> A=
 multi-step prompting workflow that </span><strong>incrementally compresses=
 content</strong><span> into a summary (or other intermediate form) that th=
e model can handle. Instead of feeding 100 pages at once, you break the tex=
t into smaller chunks, process each, then combine results. This is sometime=
s called =E2=80=9Chierarchical summarization=E2=80=9D or a map-reduce chain=
.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height=
: 26px;font-size: 16px;"><strong>Step-by-step implementation (Basic):</stro=
ng></p><ol style=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0=
 32px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0=
;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><stro=
ng>Chunk the Text:</strong><span> Split the document into reasonable chunks=
 =E2=80=93 e.g., 5 pages or 1000 tokens each (whatever the model can easily=
 handle with some room for output). Ensure chunks end at logical boundaries=
 (don=E2=80=99t cut mid-sentence if possible).</span></p></li><li style=3D"=
margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;m=
argin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;ma=
rgin: 0;"><strong>Summarize Each Chunk (Map Phase):</strong><span> For each=
 chunk, prompt the model: =E2=80=9CSummarize the following text in X senten=
ces, focusing on key facts=E2=80=A6=E2=80=9D (you can tailor X based on how=
 much compression you need). This yields a summary for each piece. Use the =
same model for this or even a smaller/cheaper model if you trust it for sum=
marization.</span></p></li><li style=3D"margin: 8px 0 0 32px;"><p style=3D"=
color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-=
box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Combine Summaries=
 (Reduce Phase):</strong><span> Now take the chunk summaries and either </s=
pan><em>concatenate them and summarize again</em><span> (if there are many)=
, or do it iteratively: e.g., summarize 5 summaries into a higher-level sum=
mary, etc. Essentially you are doing a tree reduction =E2=80=93 summaries o=
f summaries until you have one final concise summary. Another approach is t=
he </span><strong>Refine Chain</strong><span>: start with summary of chunk1=
, then feed summary + chunk2 summary to the model with a prompt =E2=80=9CRe=
fine this summary by incorporating the following information=E2=80=A6=E2=80=
=9D, and iterate chunk by chunk.</span></p></li><li style=3D"margin: 8px 0 =
0 32px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: =
0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><str=
ong>Final Answer:</strong><span> The end result is a manageable-length summ=
ary that captures the whole document. You can present that to the user, or =
if the goal was Q&amp;A, you can now feed the final summary plus the questi=
on into the model to get an answer.</span></p></li></ol><p style=3D"margin:=
 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><stron=
g>Enhanced Implementation:</strong><span> If you have infrastructure, you c=
an parallelize the Map phase (summarize chunks concurrently) to save time. =
You can also add an intermediate step where you ask the model to extract ke=
y </span><em>facts or data points</em><span> from each chunk (instead of fr=
eeform summary) and later aggregate those. For example, in a legal document=
, you might extract a list of =E2=80=9Cobligations=E2=80=9D from each secti=
on, then merge them. Or use a structured format (JSON) for summaries so the=
y are easier to merge.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(=
54,55,55);line-height: 26px;font-size: 16px;"><strong>Real Example:</strong=
><span> A company needed to summarize a </span><strong>200-page financial r=
eport</strong><span> for analysts. Directly prompting GPT-4 with 200 pages =
was impossible, and even 32K context wasn=E2=80=99t enough. They implemente=
d a summary chain: split into ~40 sections by report headings, summarized e=
ach (with GPT-3.5), then summarized those summaries (with GPT-4) into a 2-p=
age briefing. The chain of summaries preserved all major points (revenues, =
risks, outlook) and it cost under $5 of API calls, compared to an estimate =
of $50+ if they had tried to brute-force feed raw text into GPT-4 (even if =
it were able).</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55=
);line-height: 26px;font-size: 16px;"><span>Another example: Summarizing a =
</span><strong>long Slack chat history</strong><span> for adding new teamma=
tes quickly. Rather than giving the model 100 screens of chat (which it mig=
ht forget mid-way), they chunked the chat by week, summarized each week, th=
en summarized the summaries into a =E2=80=9Clast month recap=E2=80=9D. This=
 hierarchical approach maintained context that was lost when they tried a o=
ne-shot long summary.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(5=
4,55,55);line-height: 26px;font-size: 16px;"><strong>Quick Try (5-minute ex=
periment):</strong><span> Take a long Wikipedia article (or any text ~5000 =
words). Try to get a one-shot summary from your LLM in one go (you might ne=
ed GPT-4 32K or just use GPT-3.5 and see it fail due to length). Then imple=
ment a quick manual summary chain: split the article into 5 parts, ask the =
model to summarize each part in a paragraph. Then give it those 5 summaries=
 and ask for an overall summary. Compare the results. You=E2=80=99ll likely=
 see the chain summary is more coherent and hits all major points, whereas =
a one-shot (if it even worked) might miss something or just truncate. This =
small test demonstrates how breaking down the task yields better retention.=
</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height:=
 26px;font-size: 16px;"><strong>Why it works:</strong><span> You=E2=80=99re=
 sidestepping the model=E2=80=99s weakness with long inputs by </span><stro=
ng>never giving it more than it can handle at once</strong><span>. By doing=
 the heavy lifting of remembering via intermediate summaries, you reduce th=
e burden on the final prompt. It also </span><strong>reduces cost</strong><=
span> =E2=80=93 you may use cheaper models for chunk summaries and only use=
 the powerful model for the final step. Summary chaining does require some =
careful prompt design (to ensure each summary contains what you need for th=
e next stage), but many libraries (LangChain, etc.) have utilities for this=
 pattern out of the box.</span></p><h3 class=3D"header-anchor-post" style=
=3D"position: relative;font-family: 'SF Pro Display',-apple-system-headline=
,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Ari=
al,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-w=
eight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing: an=
tialiased;-webkit-appearance: optimizelegibility;-moz-appearance: optimizel=
egibility;appearance: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb=
(54,55,55);line-height: 1.16em;font-size: 1.375em;"><strong>2. Strategic Ch=
unking Workflows</strong></h3><p style=3D"margin: 0 0 20px 0;color: rgb(54,=
55,55);line-height: 26px;font-size: 16px;"><strong>When to use:</strong><sp=
an> Any time you have to process </span><strong>large texts or datasets</st=
rong><span> and the task can be localized to pieces of that input. This inc=
ludes use cases like: searching a large document for answers, analyzing sec=
tions of code across a codebase, or applying the same question to multiple =
records (like =E2=80=9Csummarize each chapter of this book=E2=80=9D). Strat=
egic chunking is about </span><em>how</em><span> you break up content and i=
terate, to avoid overwhelming the model. Use this when the problem doesn=E2=
=80=99t require combining everything at once, or when you can structure the=
 approach in a divide-and-conquer way.</span></p><p style=3D"margin: 0 0 20=
px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>What =
it is:</strong><span> </span><em>Chunking</em><span> simply means splitting=
 input into smaller parts (chunks) and processing them individually. But th=
e =E2=80=9Cstrategic=E2=80=9D part implies doing it intelligently: choosing=
 chunk sizes, overlaps, and flows that </span><strong>preserve context</str=
ong><span> and minimize error. Key techniques include </span><em>sliding wi=
ndows</em><span>, </span><em>overlapping chunks</em><span>, and </span><em>=
smart chunk size selection</em><span>.</span></p><p style=3D"margin: 0 0 20=
px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>Step-=
by-step implementation (Basic):</strong></p><ol style=3D"margin-top: 0;padd=
ing: 0;"><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,5=
5);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: =
4px;font-size: 16px;margin: 0;"><strong>Define Chunk Size:</strong><span> D=
ecide how big each chunk should be. The ideal chunk is </span><em>as large =
as possible</em><span> while still being manageable, to reduce how many tot=
al chunks you have (fewer chunks =3D fewer calls and less chance of missing=
 context across a boundary). For straightforward tasks, chunks around 2K-4K=
 tokens often work well (for a model with 8K or 16K window). If you have a =
100K model, you might chunk at 20K or 50K. The chunk size also depends on t=
he </span><strong>content structure</strong><span> =E2=80=93 e.g., for code=
, you might chunk by file or function; for a book, by chapter.</span></p></=
li><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);lin=
e-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;fo=
nt-size: 16px;margin: 0;"><strong>Overlap (if needed):</strong><span> If co=
ntext continuity matters (like reading comprehension across chunk boundarie=
s), use overlapping chunks. For example, take 2000-token chunks but overlap=
 by 200 tokens (so chunk1 is tokens 1-2000, chunk2 is 1801-3800, etc.). Thi=
s ensures anything that falls at a boundary is seen in at least one chunk e=
ntirely. Overlap helps avoid losing a sentence that gets cut in half betwee=
n chunks.</span></p></li><li style=3D"margin: 8px 0 0 32px;"><p style=3D"co=
lor: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-bo=
x;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Process Each Chunk:=
</strong><span> For each chunk, perform the needed operation with the LLM. =
This could be summarizing (as above), Q&amp;A (=E2=80=9CDoes this chunk con=
tain X?=E2=80=9D), classification, etc. Because each chunk is within contex=
t size, the model handles it well. You might parallelize these calls if usi=
ng an API.</span></p></li><li style=3D"margin: 8px 0 0 32px;"><p style=3D"m=
argin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;b=
ox-sizing: border-box;padding-left: 4px;font-size: 16px;"><strong>Aggregate=
 Results:</strong><span> Combine the results from chunks. The aggregation d=
epends on the task:</span></p><ul style=3D"margin-top: 0;padding: 0;"><li s=
tyle=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color=
: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;p=
adding-left: 4px;font-size: 16px;margin: 0;">If you asked Q&amp;A on each c=
hunk, you might merge the answers or choose the best answer (or have the mo=
del combine them if needed).</p></li><li style=3D"margin: 8px 0 0 32px;mso-=
special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px=
;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;=
margin: 0;">If you are searching for which chunk has the answer, you identi=
fy the chunk where it was found.</p></li><li style=3D"margin: 8px 0 0 32px;=
mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: =
26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 1=
6px;margin: 0;">If summarizing each chunk (like each section), you might th=
en do another pass to join those (which becomes a summary chain approach).<=
/p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p s=
tyle=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing:=
 border-box;padding-left: 4px;font-size: 16px;margin: 0;">For code analysis=
, maybe each chunk yields an analysis of that file, and you present or furt=
her process those analyses.</p></li></ul></li><li style=3D"margin: 8px 0 0 =
32px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;=
box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><stron=
g>Verify or Refine:</strong><span> Sometimes after getting per-chunk output=
s, you might do a follow-up. E.g., if two chunks had partial info to a ques=
tion, you may prompt the model with both relevant outputs to synthesize a f=
inal answer.</span></p></li></ol><p style=3D"margin: 0 0 20px 0;color: rgb(=
54,55,55);line-height: 26px;font-size: 16px;"><strong>Enhanced Implementati=
on:</strong><span> Use a </span><strong>dispatcher</strong><span> function =
that automatically handles chunk creation and result combination. Many fram=
eworks allow you to specify chunk size and overlap for text, and they=E2=80=
=99ll split for you (e.g., LangChain=E2=80=99s TextSplitter). For irregular=
 data (like JSON logs, or code), you might chunk by logical sections (like =
one log entry per chunk). If you need cross-chunk reasoning, you can incorp=
orate a </span><em>routing step</em><span>: first identify which chunks are=
 relevant, then only send those to the model. This crosses into retrieval (=
next strategy) but underscores that chunking can be paired with search.</sp=
an></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26p=
x;font-size: 16px;"><span>A powerful pattern is </span><strong>Sliding Wind=
ow + Re-ask</strong><span>: slide through the text in overlapping windows a=
nd have the model attempt the task on each. If the model answers confidentl=
y in window 3, you stop. If not, or if answers differ across windows, you h=
ave logic to reconcile that (maybe ask the model to consolidate answers).</=
span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 2=
6px;font-size: 16px;"><strong>Real Example:</strong><span> </span><em>Docum=
ent QA:</em><span> A team built a feature to answer questions from a large =
policy document (~80 pages). Instead of giving the whole doc to the model, =
they chunked it by section and used a simple heuristic: ask each chunk =E2=
=80=9CDo you contain information relevant to [the question]? If yes, provid=
e the relevant text; if no, say NONE.=E2=80=9D Then they concatenated all o=
utputs that were not =E2=80=9CNONE=E2=80=9D (i.e., the relevant snippets th=
e model identified) and fed that to a final prompt to answer the user=E2=80=
=99s question. Essentially, the model itself did a first pass skim on each =
chunk and found candidate info, and the second pass answered with those. Th=
is worked better than vector search in this case because the question was n=
uanced (the model=E2=80=99s understanding was helpful to decide relevance),=
 and it was far cheaper and more reliable than feeding all 80 pages at once=
. It was also </span><em>fast</em><span> because each chunk Q&amp;A was don=
e in parallel.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55=
);line-height: 26px;font-size: 16px;"><span>Another example: </span><em>Lar=
ge codebase analysis.</em><span> A company wanted an LLM to find potential =
bugs across a 100k-line codebase. They chunked the code by file (each file =
is a chunk), and asked the model to analyze each file for certain code smel=
ls or bug patterns. They collected the potential issues per file, then did =
a second pass where they provided relevant snippets from multiple files if =
an issue spanned files (like inconsistent function usage) for the model to =
consolidate. By chunking per file, they kept each analysis focused and with=
in context. The result was a report of issues across the codebase. If they =
had tried to load the entire codebase into a context=E2=80=A6 well, they ju=
st couldn=E2=80=99t, it=E2=80=99s too large. Even if they tried with a 1M t=
oken model, it would have cost a fortune and likely missed things.</span></=
p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;fon=
t-size: 16px;"><strong>Quick Try:</strong><span> Take a long text (maybe a =
20-page PDF, you can convert to text). Without reading it, ask an LLM a det=
ail question that would require scanning the whole thing (e.g., =E2=80=9CAc=
cording to this document, what is the deadline for submissions?=E2=80=9D). =
If you try to stuff the whole text, it might not be possible, or if possibl=
e, see if it answers correctly. Now implement a simple chunk approach: brea=
k the text into, say, 4 chunks, each ~5 pages. Ask the model the same quest=
ion for each chunk separately (=E2=80=9Cbased on this chunk, if it contains=
 the answer: [question]. If not, say =E2=80=98Not in this part=E2=80=99.). =
You might see one chunk says =E2=80=9CDeadline for submissions is June 30, =
2025=E2=80=9D and others say =E2=80=9CNot in this part=E2=80=9D. Now you kn=
ow which chunk had the info. You can then just use that chunk to answer the=
 question fully. This simulates how chunking + targeted reading can outperf=
orm blind full-context usage, especially if only a small part of the text h=
ad the answer.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55=
);line-height: 26px;font-size: 16px;"><strong>Why it works:</strong><span> =
Strategic chunking works because it </span><strong>respects the model=E2=80=
=99s attention limits</strong><span>. By not overloading it and by intellig=
ently covering the input (with overlaps to catch boundary info), you ensure=
 the model has what it needs in each invocation. It also </span><strong>sca=
les</strong><span>: you can distribute work over chunks in parallel, and yo=
u only pay proportional to content length (not quadratic). Essentially, you=
=E2=80=99re manually doing what a perfect long-context model </span><em>sho=
uld</em><span> do =E2=80=93 read bit by bit and combine =E2=80=93 but we co=
ntrol the process to avoid missed spots. Done right, the model=E2=80=99s ac=
curacy stays high because it=E2=80=99s never lost in a sea of text.</span><=
/p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;fo=
nt-size: 16px;">One caveat: If the task truly requires synthesizing dispara=
te pieces from across the entire text, chunking alone isn=E2=80=99t enough =
=E2=80=93 you then need to add a subsequent step (like we did with combinin=
g answers or like a summary of summaries). But chunking sets the stage for =
those advanced steps by breaking the problem down.</p><h3 class=3D"header-a=
nchor-post" style=3D"position: relative;font-family: 'SF Pro Display',-appl=
e-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Rob=
oto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe =
UI Symbol';font-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-f=
ont-smoothing: antialiased;-webkit-appearance: optimizelegibility;-moz-appe=
arance: optimizelegibility;appearance: optimizelegibility;margin: 1em 0 0.6=
25em 0;color: rgb(54,55,55);line-height: 1.16em;font-size: 1.375em;"><stron=
g>3. Hybrid Retrieval-Enhanced Context (RAG++)</strong></h3><p style=3D"mar=
gin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><s=
trong>When to use:</strong><span> When you have a </span><strong>large know=
ledge base or corpus</strong><span> and users ask questions that pertain to=
 only a small part of it. Classic examples: a chatbot over your company doc=
s or manuals, customer support knowledge base query, Q&amp;A over a set of =
PDFs, etc. If you find yourself tempted to stuff a ton of reference text in=
to the prompt =E2=80=9Cjust in case,=E2=80=9D retrieval is the smarter appr=
oach. Also use this when the same corpus is queried repeatedly =E2=80=93 re=
trieval lets you reuse an index rather than feeding the same data every tim=
e.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-heigh=
t: 26px;font-size: 16px;"><strong>What it is:</strong><span> </span><strong=
><a href=3D"https://substack.com/redirect/596f23bb-a80d-427d-9b7f-738e6538b=
7ce?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" re=
l=3D"" style=3D"color: rgb(54,55,55);text-decoration: underline;">Retrieval=
-Augmented Generation (RAG)</a></strong><span> is the practice of using a s=
earch or database step to fetch relevant pieces of text, and inserting only=
 those into the LLM prompt. The LLM then bases its answer on that retrieved=
 context (plus its own knowledge). Hybrid retrieval+context means even if y=
ou have a long-context model, you </span><em>still</em><span> use retrieval=
 to shrink the needed context and improve accuracy. Essentially, the LLM=E2=
=80=99s context window is treated as a </span><em>precious budget</em><span=
> =E2=80=93 you fill it with </span><em>only the most relevant snippets</em=
><span> from your documents, rather than raw dumping. This mitigates the lo=
st-in-middle issue by making sure the truly important info is likely at the=
 top of the prompt (since you choose what to include).</span></p><p style=
=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16=
px;"><strong>Step-by-step implementation:</strong></p><ol style=3D"margin-t=
op: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: r=
gb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padd=
ing-left: 4px;font-size: 16px;margin: 0;"><strong>Index Your Documents:</st=
rong><span> Use an embedding-based vector store or a keyword search index o=
n your corpus. For each document or section, store an embedding (semantic v=
ector) or inverted index for text.</span></p></li><li style=3D"margin: 8px =
0 0 32px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom=
: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><s=
trong>User Query to Search:</strong><span> When a question comes in, first =
transform the question into a search query. For vector DBs, embed the quest=
ion and do a similarity search. For keyword, do a keyword match. Retrieve t=
he top </span><em>N</em><span> relevant chunks/documents. </span><em>N</em>=
<span> is typically small, like 3-5, to avoid too much text.</span></p></li=
><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line-=
height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font=
-size: 16px;margin: 0;"><strong>Construct the Prompt with Retrieved Context=
:</strong><span> Prepare a prompt that includes the question and the retrie=
ved texts as context. A typical format: =E2=80=9C</span><em>Here are releva=
nt excerpts from our knowledge base:</em><span> =E2=80=A6=E2=80=A6 </span><=
em>Using only this information, answer the question:.</em><span>=E2=80=9D O=
ptionally, cite sources if needed.</span></p></li><li style=3D"margin: 8px =
0 0 32px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom=
: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><s=
trong>Generate Answer:</strong><span> The LLM sees only those few relevant =
pieces, which might total, say, 1000 tokens, well within limits. It then pr=
oduces an answer grounded in that context.</span></p></li><li style=3D"marg=
in: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margi=
n-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin=
: 0;"><strong>(Optional) Feedback Loop:</strong><span> If the answer seems =
off or the model says it lacks info, you might attempt another round (maybe=
 expanding the search or retrieving more). But ideally, a good retrieval se=
t yields a good answer in one go.</span></p></li></ol><p style=3D"margin: 0=
 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>=
Enhanced Implementation:</strong><span> For </span><strong>hybrid context</=
strong><span>, sometimes you might </span><em>combine retrieval with a long=
 context</em><span>. For example, if you have a model with 100K window and =
you=E2=80=99re analyzing a collection of documents, you might retrieve 50 r=
elevant pages (which might be, say, 25K tokens) and feed those in. That way=
 you=E2=80=99re not using the full 100K blindly, but you=E2=80=99re also ta=
king advantage of more context than a smaller model could. Another improvem=
ent is </span><strong>re-ranking</strong><span> the retrieved results to en=
sure the most important ones go first (if you have more than can fit). The =
</span><strong>LongContextReorder</strong><span> trick we saw is an example=
 =E2=80=93 it ensures the </span><em>most relevant docs are at the prompt e=
xtremes</em><span> , which can help the model pick them up despite any midd=
le-loss issue.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55=
);line-height: 26px;font-size: 16px;"><span>You could also use </span><stro=
ng>multiple retrieval passes</strong><span>: e.g., first retrieve a relevan=
t section, then within that section do a finer search for the exact answer.=
 But that can usually be simplified by the LLM reading the section itself.<=
/span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: =
26px;font-size: 16px;"><strong>Real Example:</strong><span> </span><em>Ente=
rprise Q&amp;A:</em><span> A company had a million documents of internal kn=
owledge. They enabled GPT-4.1 with a 1M window on it, but initial trials fa=
iled =E2=80=93 the model often rambled or hallucinated because they were fe=
eding too much and hoping it finds the needle in haystack. They switched to=
 a RAG approach: when a user asks a question, use ElasticSearch to find the=
 top 5 documents. Then they feed those (maybe 5-10K tokens total) into GPT-=
4 and get an answer that is nearly always correct and </span><em>with sourc=
es cited</em><span>. The cost per query dropped by &gt;90% because they wer=
en=E2=80=99t pumping hundreds of thousands of tokens each time, and the ans=
wers were more accurate since irrelevant info was gone. Users also got </sp=
an><em>faster responses</em><span> (a few seconds instead of 30+).</span></=
p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;fon=
t-size: 16px;"><span>Interestingly, they found that even if the final model=
 (GPT-4) </span><em>could</em><span> have accepted all docs, it </span><str=
ong>performed better with retrieval</strong><span>. That aligns with resear=
ch: </span><em>=E2=80=9CMost model performance decreases after a certain co=
ntext size=E2=80=A6 Modern LLMs can improve RAG with long context, but long=
er context is not always optimal=E2=80=9D</em><span>. It=E2=80=99s often be=
tter to retrieve say 20 relevant documents than to feed 100 and rely on the=
 model to sift through.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb=
(54,55,55);line-height: 26px;font-size: 16px;"><span>Another example: </spa=
n><em>ChatGPT plugins / tools.</em><span> When you ask ChatGPT something ab=
out recent info, it might use the Browsing tool or a vector database behind=
 the scenes. That=E2=80=99s RAG in action =E2=80=93 instead of relying on i=
ts entire training or context, it looks up the answer and then provides it.=
 Many applications (like Bing Chat, etc.) do this hybrid approach: search f=
irst, then feed results into the model. If you consider search as producing=
 a </span><em>dynamic context</em><span>, it=E2=80=99s essentially extendin=
g context but in a targeted way. Businesses can replicate this pattern with=
 their own data.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,=
55);line-height: 26px;font-size: 16px;"><strong>Quick Try:</strong><span> I=
f you have a folder of text/PDF files, try using an open source tool or sim=
ple script: use a vector database like FAISS or Pinecone (some have free ti=
ers) to index them. Then ask a few questions and retrieve top matches, feed=
 those into the model. Compare the answer vs if you tried to stuff entire d=
ocuments. Even simpler: Use Bing or Google to search a question about a top=
ic, copy the relevant snippet results into ChatGPT and see the answer quali=
ty. You=E2=80=99ll notice that providing just the needed info yields a very=
 direct and factual answer, whereas if you gave the model an entire Wikiped=
ia article, it might get lost or give a very general summary rather than th=
e specific detail you want.</span></p><p style=3D"margin: 0 0 20px 0;color:=
 rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>Why it works:</s=
trong><span> RAG plays to each component=E2=80=99s strengths =E2=80=93 sear=
ch (or embeddings) excels at pinpointing relevant chunks, and the LLM excel=
s at reasoning/generating with a moderate amount of text. It </span><strong=
>avoids drowning the model in irrelevant info</strong><span>. The model doe=
sn=E2=80=99t have to be a librarian; you already handed it the right pages.=
 Also, since the retrieved pieces are usually smaller and focused, the mode=
l is less likely to drift off or hallucinate =E2=80=93 it has concrete text=
 to ground its answer. It=E2=80=99s like open-book exam vs closed-book: in =
a closed-book scenario with a giant book memorized (long context with every=
thing), the model might misremember; in an open-book scenario with the inde=
x (retrieval) to the exact page, the model can quote or rely on the actual =
text. This reduces the load on the model=E2=80=99s =E2=80=9Cmemory=E2=80=9D=
 component and mitigates the lost-in-middle problem because the truly relev=
ant info is likely at the front of the prompt (you usually paste it in fres=
h, often preceded by a system message telling the model to use it).</span><=
/p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;fo=
nt-size: 16px;"><strong>One caution:</strong><span> Ensure your retrieval r=
esults are actually relevant. If your search pulls in some unrelated chunk =
(false positive), the model might get confused or use that incorrectly. So =
invest in good embeddings or search strategies (there are whole guides on v=
ector DB tuning). Also, limit how many documents you stuff in =E2=80=93 mor=
e is not merrier beyond a point. Empirically, after ~5-10 documents, perfor=
mance drops , so better to retrieve the top few and perhaps iterate if need=
ed rather than load 50 excerpts at once.</span></p><h3 class=3D"header-anch=
or-post" style=3D"position: relative;font-family: 'SF Pro Display',-apple-s=
ystem-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto=
,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI =
Symbol';font-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font=
-smoothing: antialiased;-webkit-appearance: optimizelegibility;-moz-appeara=
nce: optimizelegibility;appearance: optimizelegibility;margin: 1em 0 0.625e=
m 0;color: rgb(54,55,55);line-height: 1.16em;font-size: 1.375em;"><strong>4=
. The =E2=80=9CContext Budget=E2=80=9D Framework</strong></h3><p style=3D"m=
argin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">=
<strong>When to use:</strong><span> In complex applications, especially </s=
pan><strong>agent-like systems or multi-turn conversations</strong><span>, =
where many different types of information compete for context space =E2=80=
=93 e.g., system instructions, user dialogue history, retrieved knowledge, =
intermediate calculations, etc. Use a context budget when you have to juggl=
e these components and want to ensure the model sees the most important pie=
ces. It=E2=80=99s a proactive strategy to avoid context bloat and forgettin=
g.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-heigh=
t: 26px;font-size: 16px;"><strong>What it is:</strong><span> Treat your mod=
el=E2=80=99s context window like a </span><strong>budget to be allocated</s=
trong><span> among categories: instructions, relevant facts, recent convers=
ation, etc.. You then establish rules or dynamic policies for what gets inc=
luded and what gets trimmed when the budget is tight. Think of it as a slid=
ing window memory with prioritization. This approach is about </span><stron=
g>balancing context components</strong><span> so that the LLM doesn=E2=80=
=99t lose key details due to an overflowing prompt.</span></p><p style=3D"m=
argin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">=
<strong>Step-by-step implementation:</strong></p><ol style=3D"margin-top: 0=
;padding: 0;"><li style=3D"margin: 8px 0 0 32px;"><p style=3D"margin: 0 0 2=
0px 0;color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: b=
order-box;padding-left: 4px;font-size: 16px;"><strong>Identify Context Comp=
onents:</strong><span> List what elements go into your prompts. Common comp=
onents:</span></p><ul style=3D"margin-top: 0;padding: 0;"><li style=3D"marg=
in: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,=
55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left:=
 4px;font-size: 16px;margin: 0;">System Prompt (instructions on style/behav=
ior)</p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"=
><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-si=
zing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">User=E2=80=
=99s last query</p></li><li style=3D"margin: 8px 0 0 32px;mso-special-forma=
t: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-botto=
m: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">C=
onversation history (previous Q&amp;A pairs)</p></li><li style=3D"margin: 8=
px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);l=
ine-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;=
font-size: 16px;margin: 0;">Retrieved knowledge/docs</p></li><li style=3D"m=
argin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,=
55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-le=
ft: 4px;font-size: 16px;margin: 0;">Tool outputs or intermediate results (i=
f an agent)</p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: b=
ullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0=
;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">Possi=
bly a brief summary of the conversation or user profile (for personalizatio=
n)</p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><=
p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizi=
ng: border-box;padding-left: 4px;font-size: 16px;margin: 0;">Each of these =
takes up tokens.</p></li></ul></li><li style=3D"margin: 8px 0 0 32px;"><p s=
tyle=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;margin-bo=
ttom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;"><strong>=
Allocate Priority/Budget:</strong><span> Decide which components are highes=
t priority to always include, and which can be truncated or summarized when=
 needed. For example:</span></p><ul style=3D"margin-top: 0;padding: 0;"><li=
 style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"col=
or: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box=
;padding-left: 4px;font-size: 16px;margin: 0;">System instructions: high pr=
iority (always include core rules, maybe ~500 tokens budget).</p></li><li s=
tyle=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color=
: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;p=
adding-left: 4px;font-size: 16px;margin: 0;">Last user query: high (must in=
clude in full).</p></li><li style=3D"margin: 8px 0 0 32px;mso-special-forma=
t: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-botto=
m: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">C=
onversation history: medium =E2=80=93 allocate maybe 1000 tokens to it; if =
history is longer, include recent turns and summarize or drop older ones.</=
p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p st=
yle=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: =
border-box;padding-left: 4px;font-size: 16px;margin: 0;">Retrieved docs: hi=
gh, but only include top 3 results, etc., with maybe up to 2000 tokens.</p>=
</li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p styl=
e=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: bo=
rder-box;padding-left: 4px;font-size: 16px;margin: 0;">Tool outputs: medium=
, maybe include last tool result if needed.</p></li><li style=3D"margin: 8p=
x 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);li=
ne-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;f=
ont-size: 16px;margin: 0;"><span>This is your </span><em>context budget pla=
n</em><span> =E2=80=93 e.g., total 8000 tokens, where each category has a s=
oft limit.</span></p></li></ul></li><li style=3D"margin: 8px 0 0 32px;"><p =
style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing=
: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Dynamic =
Trimming/Compression:</strong><span> Implement logic that if the conversati=
on gets too long or retrieval returns too much text, you trim less importan=
t parts. For example, remove or shorten older chat turns (replace them with=
 a summary). This can be automated: after each turn, check token length; if=
 &gt; threshold, use an LLM to summarize the oldest interactions and replac=
e them with a note (this is what ChatGPT does with long chats =E2=80=93 it =
will stop referencing very old turns or compress them).</span></p></li><li =
style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line-heigh=
t: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size=
: 16px;margin: 0;"><strong>Consistent Formatting:</strong><span> Use struct=
ure in your prompt to separate components, so the model knows which is whic=
h (system vs user vs context text). This reduces confusion and helps the mo=
del follow the important parts.</span></p></li><li style=3D"margin: 8px 0 0=
 32px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0=
;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><stro=
ng>Testing and Tuning:</strong><span> Simulate scenarios where the context =
budget would overflow and see how your system handles it. For instance, hav=
e a 50-turn conversation and see if earlier points are truly gone or proper=
ly summarized. Adjust the summarization prompt if the summary loses critica=
l info. Essentially, tune how aggressively you compress vs drop.</span></p>=
</li></ol><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: =
26px;font-size: 16px;"><strong>Enhanced Implementation:</strong><span> A so=
phisticated approach is to maintain an </span><strong>explicit memory objec=
t</strong><span>. For example, store a running summary of the conversation =
separately and always include that in the prompt instead of full history (u=
pdating it each turn). Some use vector databases to store conversation chun=
ks and retrieve relevant past dialogue dynamically (though that can be over=
kill). Another advanced tactic is </span><strong>priority tagging</strong><=
span>: mark certain messages or facts as =E2=80=9Ccritical=E2=80=9D so they=
 are never omitted from context. For example, if a user provided crucial in=
fo early on (=E2=80=9CI=E2=80=99m allergic to shellfish=E2=80=9D in a medic=
al chat), you might pin that in the system prompt in future turns (=E2=80=
=9CRemember: User is allergic to shellfish.=E2=80=9D).</span></p><p style=
=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16=
px;">If building an agent with tool usage, design a prompt template where t=
he working memory (scratchpad) has limited lines =E2=80=93 if it=E2=80=99s =
too long, summarize earlier reasoning steps.</p><p style=3D"margin: 0 0 20p=
x 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">Some framework=
s have this baked in. For instance, LangChain has various ConversationMemor=
y classes that implement buffer (last N messages) or summarize+buffer strat=
egies. You can configure those.</p><p style=3D"margin: 0 0 20px 0;color: rg=
b(54,55,55);line-height: 26px;font-size: 16px;"><strong>Real Example:</stro=
ng><span> </span><em>AI Customer Support Agent:</em><span> This agent had t=
o follow a set of company policies (system prompt ~1000 tokens), carry on a=
 possibly lengthy conversation with a customer, and also pull in knowledge =
base articles as needed. Without control, prompts got huge and the agent st=
arted dropping earlier context (like forgetting what the customer asked ini=
tially) around turn 15. They implemented a context budget: always include s=
ystem policies, always include the last user message and agent reply. For c=
onversation history, they only kept the last 5 exchanges verbatim, and olde=
r ones were summarized into a shorter =E2=80=9CSession recap: =E2=80=A6=E2=
=80=9D that stayed in the prompt. For knowledge base, they limited to top 2=
 articles (instead of sometimes 5 or 6 that the search might return). They =
also stored the customer=E2=80=99s key info (name, account issue) in a shor=
t summary that persisted. Result: the agent stopped losing track of the con=
versation and didn=E2=80=99t exceed model limits. Importantly, by summarizi=
ng older turns, they </span><em>reduced token usage significantly</em><span=
> such that even a 30-turn dialog stayed under 8K tokens (whereas before it=
 blew past 16K and had to drop context arbitrarily).</span></p><p style=3D"=
margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"=
><span>The </span><strong>Context Budget Canvas</strong><span> (which we=E2=
=80=99ll present at the end) essentially formalizes this: you draw boxes fo=
r each context component and assign them token budgets and rules.</span></p=
><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font=
-size: 16px;"><span>Another example: </span><em>Slack/GitHub Bot:</em><span=
> A bot that followed lengthy threads had to remember what=E2=80=99s been r=
esolved. They gave it a habit of writing its own =E2=80=9CNotes so far=E2=
=80=9D after every few messages, which were then used in lieu of raw histor=
y. This kept it from repeating earlier answers and ensured it didn=E2=80=99=
t ask the same questions twice (a form of short-term memory management).</s=
pan></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26=
px;font-size: 16px;"><strong>Quick Try:</strong><span> If you=E2=80=99ve us=
ed ChatGPT in a very long conversation, you might have noticed it sometimes=
 forgets or you get the sense it=E2=80=99s losing info. A quick demo: Start=
 a new chat with GPT-4, tell it a list of 10 specific facts about you. Then=
 have a 15-turn conversation on various topics. At turn 16, ask it to recal=
l those 10 facts. Chances are it will recall only some, or with errors =E2=
=80=93 because its effective context shrunk and it dropped earlier info. No=
w try a context budget approach manually: after the 10 facts, ask it to sum=
marize those facts in 2 sentences and keep that summary. Throughout the con=
versation, occasionally remind it with the summary. You=E2=80=99ll see it=
=E2=80=99s more consistent. This is essentially what you=E2=80=99d automate=
: always persisting the critical info summary in the prompt.</span></p><p s=
tyle=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size=
: 16px;"><strong>Why it works:</strong><span> Without a budget, the model=
=E2=80=99s context gets filled ad-hoc and the least recent or lower priorit=
y info may be truncated or just mentally ignored by the model. By budgeting=
, </span><strong>you ensure the most important pieces are always present an=
d prominent</strong><span>. You=E2=80=99re also aligning with how the model=
=E2=80=99s attention tends to work =E2=80=93 since we know it focuses on re=
cent tokens, we explicitly keep important things recent (by repeating or su=
mmarizing them at the end of the prompt as needed). By doing summarization =
of older content, we </span><strong>compress low-value tokens into high-val=
ue tokens</strong><span>, freeing space. Essentially, context budgeting ack=
nowledges that </span><em>not everything can fit</em><span>, so it makes co=
nscious trade-offs about what stays at full detail, what gets abridged, and=
 what gets dropped entirely. The result is a system that feels like it has =
a longer memory and better focus than the raw model would.</span></p><p sty=
le=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: =
16px;"><span>As a plus, this can dramatically </span><strong>cut costs</str=
ong><span>. You=E2=80=99re not sending the entire chat history every time, =
just a summary. If each message is ~50 tokens and you have 20 turns, that=
=E2=80=99s 1000 tokens of history, but a summary might only be 100 tokens =
=E2=80=93 10=C3=97 reduction every prompt. Multiply by thousands of convers=
ations and you saved serious money.</span></p><h3 class=3D"header-anchor-po=
st" style=3D"position: relative;font-family: 'SF Pro Display',-apple-system=
-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helv=
etica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbo=
l';font-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoo=
thing: antialiased;-webkit-appearance: optimizelegibility;-moz-appearance: =
optimizelegibility;appearance: optimizelegibility;margin: 1em 0 0.625em 0;c=
olor: rgb(54,55,55);line-height: 1.16em;font-size: 1.375em;"><strong>5. Pos=
ition-Aware Prompting (Edge Optimization)</strong></h3><p style=3D"margin: =
0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong=
>When to use:</strong><span> When you must feed a lot of information and yo=
u </span><em>know</em><span> some parts are crucial. Also when using retrie=
val or multi-doc prompts, where you want to ensure the most relevant info i=
sn=E2=80=99t lost in the crowd. Essentially, anytime you=E2=80=99re constru=
cting a prompt with multiple parts, you should be </span><em>position-aware=
</em><span> =E2=80=93 leveraging the fact that </span><strong>beginnings an=
d ends of the prompt carry more weight</strong><span> (to the model) than t=
he middle. This strategy helps mitigate the =E2=80=9Clost in the middle=E2=
=80=9D problem by reordering or emphasizing content.</span></p><p style=3D"=
margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"=
><strong>What it is:</strong><span> Position-aware prompting means you </sp=
an><strong>intentionally place important context at the start or end of you=
r prompt</strong><span>, and/or repeat key points in those positions, so th=
e model is more likely to attend to them. It can also involve injecting rem=
inders or TL;DRs at intervals if the prompt is super long. It=E2=80=99s abo=
ut not treating the prompt as a neutral sequence =E2=80=93 order matters, a=
nd we exploit that.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,=
55,55);line-height: 26px;font-size: 16px;"><strong>Step-by-step implementat=
ion:</strong></p><ol style=3D"margin-top: 0;padding: 0;"><li style=3D"margi=
n: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin=
-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin:=
 0;"><strong>Determine Priority Info:</strong><span> Identify the absolutel=
y critical pieces of content/instructions the model must not ignore. For ex=
ample: the main question/task, any must-follow directive (=E2=80=9Cdon=E2=
=80=99t reveal internal data=E2=80=9D), or a key fact that the answer depen=
ds on.</span></p></li><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color=
: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;p=
adding-left: 4px;font-size: 16px;margin: 0;"><strong>Place Key Instructions=
 at Start:</strong><span> The system or initial prompt should contain the f=
undamental instructions. Models heavily weight the beginning of the prompt =
for directives , so don=E2=80=99t bury the actual task at token 15000. For =
instance, instead of listing 10 pages of reference then saying =E2=80=9CNow=
 answer question X=E2=80=9D, consider stating: =E2=80=9C</span><strong>Ques=
tion:</strong><span> X. We have provided relevant information below. Use it=
 to answer.=E2=80=9D at the top, so the model knows from the get-go what it=
=E2=80=99s looking for.</span></p></li><li style=3D"margin: 8px 0 0 32px;">=
<p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-siz=
ing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Order=
 Retrieved Docs by Relevance:</strong><span> If you retrieved chunks, </spa=
n><strong>don=E2=80=99t just list them in arbitrary order</strong><span>. P=
ut the most relevant one first, second most last, and the rest in the middl=
e. This =E2=80=9Csandwiching=E2=80=9D was suggested by research to surface =
relevant info to the model=E2=80=99s attention. So if chunk A is highest si=
milarity and chunk B is second highest, put A at top of context, B at botto=
m of context, and others in between.</span></p></li><li style=3D"margin: 8p=
x 0 0 32px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bott=
om: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">=
<strong>Reiterate Critical Facts at End:</strong><span> If there=E2=80=99s =
something the model absolutely should use, you might repeat or summarize it=
 in the final user prompt part. E.g., =E2=80=9CAs a reminder, the user=E2=
=80=99s core requirement is to increase conversion by 5%.=E2=80=9D This goe=
s just before the model starts generating its answer. The model=E2=80=99s o=
utput head often pays strong attention to the last few tokens of the prompt=
 (since it=E2=80=99s fresh in =E2=80=9Cmemory=E2=80=9D), so a well-placed r=
eminder can steer the answer.</span></p></li><li style=3D"margin: 8px 0 0 3=
2px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;b=
ox-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong=
>Use Section Headings or Delimiters:</strong><span> Clearly label parts of =
the context to avoid confusion. For example: =E2=80=9CInfo 1: [text] Info 2=
: [text] =E2=80=A6 Question: [text]=E2=80=9D. This helps ensure the model k=
nows which part is the user question vs context. It also allows you to refe=
rence these sections in the prompt: =E2=80=9CAnswer based on Info 1 and Inf=
o 2 above.=E2=80=9D By naming them, you=E2=80=99ve anchored the references.=
</span></p></li><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(=
54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding=
-left: 4px;font-size: 16px;margin: 0;"><strong>Check Model Focus via Testin=
g:</strong><span> You can test whether the model is picking up the importan=
t bits by asking it to explain its answer or by inserting known dummy info.=
 For instance, plant a clearly irrelevant but flashy sentence in the middle=
 of context and see if the model ignores it. If it doesn=E2=80=99t, you mig=
ht need to better structure prompts or limit length.</span></p></li></ol><p=
 style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-si=
ze: 16px;"><strong>Enhanced Implementation:</strong><span> In multi-turn in=
teractions, if the user provides new crucial info mid-way (like =E2=80=9CAc=
tually, our budget is $0=E2=80=9D), you may want to bump that info up in th=
e prompt in future turns (like include it in system message: =E2=80=9CUser=
=E2=80=99s budget =3D $0=E2=80=9D). This ensures it=E2=80=99s not lost down=
 in the middle of a long history.</span></p><p style=3D"margin: 0 0 20px 0;=
color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>Another adva=
nced trick: </span><strong>mid-context summaries.</strong><span> If you pro=
vide a very long context (say a 50K document) to a model that can handle it=
, consider splitting it into sections and for each section, prepend a one-s=
entence summary. That way even if the model skims, those summaries (which a=
ppear regularly) might catch its attention. Essentially, you scaffold the c=
ontext with =E2=80=9Cheads=E2=80=9D that outline what=E2=80=99s coming. It=
=E2=80=99s like an executive summary for each portion. Models do pick up on=
 those if phrased clearly (=E2=80=9CSection Summary: This part describes th=
e legal definitions.=E2=80=9D).</span></p><p style=3D"margin: 0 0 20px 0;co=
lor: rgb(54,55,55);line-height: 26px;font-size: 16px;">If using open-source=
 models or fiddling with architecture, one could also adjust positional enc=
oding biases (but that=E2=80=99s out of scope for most users). Our focus is=
 on prompt-level tactics.</p><p style=3D"margin: 0 0 20px 0;color: rgb(54,5=
5,55);line-height: 26px;font-size: 16px;"><strong>Real Example:</strong><sp=
an> </span><em>Multi-document analysis:</em><span> Here=E2=80=99s an exampl=
e that=E2=80=99s been coming up from Claude 2 through Opus 4. When I was ge=
tting more used to LLMs, I sometimes just pasted all the papers I wanted Cl=
aude to work on one after the other and asked Claude for common themes. The=
 output was superficial and missed a key theme that was actually only discu=
ssed in one paper buried in the middle of the prompt. The fix I found: add =
a bullet list at the top of the prompt: =E2=80=9CPaper Titles and Main Topi=
cs: 1) Paper A =E2=80=93 about theme X, 2) Paper B =E2=80=93 about theme Y,=
 =E2=80=A6=E2=80=9D. Then put the full text of each paper below. Then finis=
h the prompt with: =E2=80=9CIn summary, focus on themes X and Y.=E2=80=9D T=
he result is a much more on-point analysis that does mention the previously=
 missed theme. By giving the model a </span><strong>map of the context</str=
ong><span> up front and a reminder at the end, I can successfully guide its=
 attention within a very large prompt. I still use this with cutting edge m=
odels today.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);=
line-height: 26px;font-size: 16px;"><strong>Another example:</strong><span>=
 </span><em>Lost info in code assistance.</em><span> A developer noticed th=
at when she pasted a long code file and then added =E2=80=9C// TODO: fix bu=
g here=E2=80=9D at the bottom, the LLM would sometimes forget context from =
the top of the file relevant to the bug. She changed the approach: she wrot=
e a brief description of the bug at the top (=E2=80=9CBug: the output is wr=
ong when X happens=E2=80=9D) before the code, then pasted code, then after =
the code again said =E2=80=9CRefer to the above code to fix the bug where o=
utput is wrong when X.=E2=80=9D This sandwiching made the model focus on th=
at specific issue throughout its reading of the code, leading to correct fi=
xes. It=E2=80=99s essentially reminding at both ends what to look for in th=
e middle.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);lin=
e-height: 26px;font-size: 16px;"><strong>Quick Try:</strong><span> If you h=
ave an LLM that tends to ignore something, try reordering. A fun mini-test:=
 give the model a prompt like =E2=80=9CText1: </span><em>[some content with=
 the answer]</em><span> Text2: </span><em>[some irrelevant fluff]</em><span=
> Question: </span><em>[ask about the answer]</em><span>.=E2=80=9D See if i=
t answers correctly. Now swap Text1 and Text2 (so the fluff comes first, an=
swer second) and see if it gets it right or struggles. Many models perform =
better when relevant info is earlier. That demonstrates positioning matters=
. Now try labeling: =E2=80=9CRelevant Info: [answer content]. Irrelevant In=
fo: [fluff]. Question: =E2=80=A6=E2=80=9D and see =E2=80=93 likely it will =
ignore the irrelevant section properly. That shows how labeling and positio=
ning help steer attention.</span></p><p style=3D"margin: 0 0 20px 0;color: =
rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>Why it works:</st=
rong><span> We know from =E2=80=9CLost in the Middle=E2=80=9D research that=
 </span><strong>position has a huge impact on recall</strong><span>. By con=
trolling positions, we circumvent the model=E2=80=99s tendency to downweigh=
t the middle. Essentially, we </span><em>hack the attention</em><span>. The=
 first tokens condition the model heavily (that=E2=80=99s how prompting wor=
ks), and the last tokens are fresh in its short-term working memory when ge=
nerating. By planting important things there, we boost their chances of inf=
luencing the output. Also, instructing the model </span><em>where</em><span=
> to look (=E2=80=9CInfo 1 above pertains to X=E2=80=9D) can overcome some =
passivity =E2=80=93 instead of hoping it notices, we explicitly say =E2=80=
=9Cuse Info 1 and Info 3, ignore the rest if not relevant.=E2=80=9D Models =
respond well to such explicit guidance.</span></p><p style=3D"margin: 0 0 2=
0px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>One =
caution:</strong><span> If overdone, you might introduce bias or redundancy=
. Don=E2=80=99t repeat large chunks verbatim at the top and bottom, that wa=
stes tokens. Summarize or highlight. And avoid confusing the model by confl=
icting info (don=E2=80=99t say at top =E2=80=9CThe sky is blue=E2=80=9D and=
 at bottom =E2=80=9CThe sky is green=E2=80=9D even if as a test =E2=80=93 i=
t might average them or get uncertain). The goal is emphasis, not contradic=
tion.</span></p><h3 class=3D"header-anchor-post" style=3D"position: relativ=
e;font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-sys=
tem,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple =
Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-f=
ont-smoothing: antialiased;-moz-osx-font-smoothing: antialiased;-webkit-app=
earance: optimizelegibility;-moz-appearance: optimizelegibility;appearance:=
 optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-heigh=
t: 1.16em;font-size: 1.375em;"><strong>6. Dynamic Context Compression (On-t=
he-fly Simplification)</strong></h3><p style=3D"margin: 0 0 20px 0;color: r=
gb(54,55,55);line-height: 26px;font-size: 16px;"><strong>When to use:</stro=
ng><span> When dealing with data that has a lot of </span><strong>low-value=
 or repetitive content</strong><span>, or when you need to iteratively shor=
ten the context as a process goes on (like an agent reasoning through steps=
, gradually compressing what=E2=80=99s been done so far to fit more steps).=
 Also applicable if the input has boilerplate or noise that can be reduced =
without losing meaning (like logs, verbose transcripts, etc.). Use dynamic =
compression if you find that simply excluding data isn=E2=80=99t an option =
but it can be abbreviated.</span></p><p style=3D"margin: 0 0 20px 0;color: =
rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>What it is:</stro=
ng><span> Dynamically compressing context means using either algorithmic or=
 model-driven methods to </span><strong>shorten the content</strong><span> =
while preserving the needed info, during the runtime of your application. T=
his could be as simple as stripping stopwords or as advanced as asking a sm=
aller LLM to =E2=80=9Csummarize this paragraph in one sentence=E2=80=9D bef=
ore feeding it to the main LLM. It=E2=80=99s akin to zipping your context. =
The difference from the =E2=80=9Csummary chain=E2=80=9D is that dynamic com=
pression often happens continuously or as part of an agent loop, not just a=
 one-time summarization of a static document.</span></p><p style=3D"margin:=
 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><stron=
g>Step-by-step implementation:</strong></p><ol style=3D"margin-top: 0;paddi=
ng: 0;"><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55=
);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4=
px;font-size: 16px;margin: 0;"><strong>Identify Compressible Parts:</strong=
><span> Determine what parts of your context can be safely compressed witho=
ut losing fidelity for the task. E.g., if you have a JSON or structured dat=
a, maybe you can drop fields or reduce precision. If you have a list of rec=
ords, maybe certain columns can be summarized (like average values instead =
of listing all).</span></p></li><li style=3D"margin: 8px 0 0 32px;"><p styl=
e=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;margin-botto=
m: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;"><strong>Cho=
ose Compression Method:</strong><span> There are a few types:</span></p><ul=
 style=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-=
special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px=
;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;=
margin: 0;"><strong>Lossy compression (summarization):</strong><span> Use a=
n LLM or rule-based method to summarize text. e.g., compress a verbose meet=
ing transcript into bullet points. Or instruct: =E2=80=9CRewrite this conte=
nt in a more concise form.=E2=80=9D</span></p></li><li style=3D"margin: 8px=
 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);lin=
e-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;fo=
nt-size: 16px;margin: 0;"><strong>Non-lossy compression (reformatting):</st=
rong><span> Remove unnecessary tokens. For instance, remove extra whitespac=
e, comments in code (if not needed), verbose formality in text. Some texts =
can be rephrased shorter without losing info (e.g., =E2=80=9Cdue to the fac=
t that=E2=80=9D -&gt; =E2=80=9Cbecause=E2=80=9D).</span></p></li><li style=
=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rg=
b(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;paddi=
ng-left: 4px;font-size: 16px;margin: 0;"><strong>Structured compression:</s=
trong><span> If the data is structured, you can compress by aggregating. E.=
g., if 100 log lines have the same pattern, replace them with =E2=80=9C(rep=
eated 100 times: [pattern])=E2=80=9D.</span></p></li><li style=3D"margin: 8=
px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);l=
ine-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;=
font-size: 16px;margin: 0;"><strong>Model-driven compression pipelines:</st=
rong><span> Some research approaches train models specifically to compress =
(like smaller models that maintain meaning). An example is the =E2=80=9Cmin=
ifier=E2=80=9D approach that tries to drop words but keep meaning.</span></=
p></li></ul></li><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb=
(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;paddin=
g-left: 4px;font-size: 16px;margin: 0;"><strong>Integrate into Workflow:</s=
trong><span> Decide when to compress. Maybe at the end of each conversation=
 turn, compress the history beyond N turns (similar to context budget but f=
ocusing on making it shorter not just dropping). Or before sending retrieve=
d documents to the final LLM, run a compression model on them (either a sma=
ller LLM or even a rule-based extractor).</span></p></li><li style=3D"margi=
n: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin=
-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin:=
 0;"><strong>Quality Check:</strong><span> Have a step to verify that compr=
essed content still contains key info. This might involve testing or even p=
rompting the model to double-check something like =E2=80=9CIs anything crit=
ical missing from the summary above? If so, what?=E2=80=9D. Often, iterativ=
ely compressing can degrade quality, so maybe compress gradually and stop w=
hen hitting a certain size.</span></p></li><li style=3D"margin: 8px 0 0 32p=
x;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box=
-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>F=
allback:</strong><span> If the compressed version yields a weird or wrong a=
nswer, you might need to fall back to using the full context (perhaps at th=
e cost of a second call). For important tasks, it can be worth checking =E2=
=80=93 e.g., run the model with compressed context, get answer A. Then mayb=
e ask the model with original context (if possible) on a sample to ensure n=
o major discrepancy. If differences, improve the compression strategy.</spa=
n></p></li></ol><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-he=
ight: 26px;font-size: 16px;"><strong>Enhanced Implementation:</strong><span=
> Use specialized libraries or models for compression. For example, OpenAI=
=E2=80=99s tiktoken or similar can count tokens =E2=80=93 you could build a=
n automated system: =E2=80=9CIf &gt; X tokens, feed chunks into GPT-3.5 wit=
h a prompt =E2=80=98summarize this chunk preserving all numbers and names.=
=E2=80=99 Then combine those.=E2=80=9D Or use extractive compression: have =
the model identify the most relevant sentences rather than generating a sum=
mary (less chance of adding errors). This is like an extreme form of retrie=
val where you retrieve within a single document by selecting sentences.</sp=
an></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26p=
x;font-size: 16px;"><span>Another concept is </span><strong>Knowledge Disti=
llation</strong><span>: if you fine-tune a smaller model on generating equi=
valent answers using less context, effectively the smaller model learns to =
implicitly compress the context. But that=E2=80=99s heavy-lift R&amp;D and =
likely unnecessary if simpler compressions suffice.</span></p><p style=3D"m=
argin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">=
<strong>Real Example:</strong><span> </span><em>Chat log compression:</em><=
span> A support chatbot dealing with long dialogues started to exceed conte=
xt and sometimes forgot older issues the user mentioned. They implemented d=
ynamic compression: the bot had an internal action it could take =E2=80=9CS=
UMMARIZE_HISTORY=E2=80=9D which, when triggered (say after 10 turns), would=
 produce a concise summary of the chat so far. It would then replace the fu=
ll history with =E2=80=9C[History Summary]: =E2=80=A6=E2=80=9D plus the las=
t couple exchanges. This is dynamic because if the chat kept going, it coul=
d summarize again (a summary of summaries). They essentially replicated how=
 a human might take notes during a long conversation. The result was the bo=
t stayed aware of past points even 50 turns in, and the context length rema=
ined bounded. This is similar to context budgeting, but specifically focusi=
ng on using summarization as the tool to shrink content.</span></p><p style=
=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16=
px;"><span>Another example: </span><em>Log file analysis.</em><span> A team=
 was using an LLM to analyze very large log files for anomalies. Rather tha=
n feed raw logs, they wrote a pre-processor that would:</span></p><ul style=
=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-specia=
l-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margi=
n-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin=
: 0;">Remove all lines that matched a known =E2=80=9Cnormal=E2=80=9D patter=
n.</p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><=
p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizi=
ng: border-box;padding-left: 4px;font-size: 16px;margin: 0;">Cluster simila=
r lines and just show one example with a count.</p></li><li style=3D"margin=
: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55=
);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4=
px;font-size: 16px;margin: 0;">Trim each log line=E2=80=99s timestamp and e=
xtraneous debug info if not needed.</p></li><li style=3D"margin: 8px 0 0 32=
px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-heigh=
t: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size=
: 16px;margin: 0;">This cut down the log content by ~80% tokens without los=
ing the unusual events.</p></li><li style=3D"margin: 8px 0 0 32px;mso-speci=
al-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;marg=
in-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margi=
n: 0;">Then they gave the compressed log to the LLM, which could then focus=
 on the anomalies. If they hadn=E2=80=99t done that, the model would waste =
effort on repetitive normal entries and likely miss the needle in the hayst=
ack.</p></li></ul><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-=
height: 26px;font-size: 16px;"><strong>Quick Try:</strong><span> Take a ver=
bose piece of text, like an overly wordy blog article or legalese paragraph=
. First, feed it to the model and ask a question that requires detail from =
it. Now compress the text manually or using the model (=E2=80=9CSummarize t=
his in 3 sentences.=E2=80=9D). Feed the summary and ask the same question. =
Does it answer correctly? If yes, you managed to compress without losing th=
e needed info for that question. If no, it means the summary left out somet=
hing important. Adjust the summary prompt: maybe =E2=80=9Csummarize, but ma=
ke sure to include the specific figures and names.=E2=80=9D Try again. Thro=
ugh this, you learn what kind of compression retains the critical data. Thi=
s is essentially how you=E2=80=99d refine a dynamic compression approach =
=E2=80=93 ensure the compressed representation is sufficient for the querie=
s you expect.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55)=
;line-height: 26px;font-size: 16px;"><strong>Why it works:</strong><span> D=
ynamic compression acknowledges that a lot of real-world data is not inform=
ation-dense. There are redundancies, boilerplate, and fluff. By removing th=
ose, you deliver a higher signal-to-token ratio to the model. The model the=
n has less noise to wade through and can focus on the core content. This im=
proves not just speed and cost (fewer tokens) but often accuracy, because t=
he model isn=E2=80=99t distracted or wasting attention on irrelevant tokens=
. It=E2=80=99s similar to how you might highlight a document for someone be=
fore they read it =E2=80=93 here we =E2=80=9Chighlight=E2=80=9D by condensi=
ng.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-heig=
ht: 26px;font-size: 16px;">It also effectively extends what you can handle.=
 If you compress a text by 50%, you=E2=80=99ve effectively doubled the reac=
h of your context window for that task. Some experiments have shown that ca=
refully compressed long inputs allow smaller models to get results as good =
as bigger models with raw input, because the smaller context is used more e=
ffectively.</p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-hei=
ght: 26px;font-size: 16px;">However, be cautious: any lossy compression can=
 omit a subtle detail. Always align compression with your needs. If exact r=
ecall of numbers or specific quotes is required (like legal or financial co=
ntexts), consider extractive approaches (keeping those specific parts verba=
tim) combined with summarizing the rest, so you don=E2=80=99t accidentally =
lose them.</p><div style=3D"font-size: 16px;line-height: 26px;"><hr style=
=3D"margin: 32px 0;padding: 0;height: 1px;background: #e6e6e6;border: none;=
"></div><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26=
px;font-size: 16px;"><span>Now we=E2=80=99ve covered six key strategies. In=
 practice, you will </span><strong>mix and match</strong><span> these. For =
example, a legal analysis workflow might use </span><em>Retrieval</em><span=
> (to get relevant clauses) + </span><em>Chunking</em><span> (to process ea=
ch clause) + </span><em>Summary chain</em><span> (to summarize findings) + =
</span><em>Position-aware prompting</em><span> (to highlight crucial contra=
ct terms up front). A coding assistant might use </span><em>Context budget<=
/em><span> (to keep important prior code context) + </span><em>Dynamic comp=
ression</em><span> (to shorten long code comments or diffs) + </span><em>Re=
trieval</em><span> (to fetch relevant documentation).</span></p><p style=3D=
"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;=
"><span>The overarching theme is: </span><strong>don=E2=80=99t rely on a si=
ngle long prompt</strong><span> to do all the work. Structure it. Use multi=
ple calls if needed. By orchestrating these patterns, you essentially build=
 an LLM solution that </span><em>feels</em><span> much smarter and more rel=
iable, even if under the hood it=E2=80=99s the same base model.</span></p><=
p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-s=
ize: 16px;">We=E2=80=99ll next see how these strategies play out in specifi=
c industry scenarios =E2=80=93 because each domain has its quirks and best =
practices.</p><h2 class=3D"header-anchor-post" style=3D"position: relative;=
font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-syste=
m,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Co=
lor Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-fon=
t-smoothing: antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appea=
rance: optimizelegibility;-moz-appearance: optimizelegibility;appearance: o=
ptimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height:=
 1.16em;font-size: 1.625em;"><strong>Industry-Specific Playbooks</strong></=
h2><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;fo=
nt-size: 16px;">Different industries have different =E2=80=9Clong context=
=E2=80=9D challenges. Let=E2=80=99s delve into a few and outline how to app=
ly the above strategies in tailored ways, along with examples.</p><h3 class=
=3D"header-anchor-post" style=3D"position: relative;font-family: 'SF Pro Di=
splay',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'S=
egoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Em=
oji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoothing: antialiase=
d;-moz-osx-font-smoothing: antialiased;-webkit-appearance: optimizelegibili=
ty;-moz-appearance: optimizelegibility;appearance: optimizelegibility;margi=
n: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-size: 1.37=
5em;"><strong>Legal: Long Contracts and Case Files</strong></h3><p style=3D=
"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;=
"><strong>Scenario:</strong><span> You need to analyze or answer questions =
about lengthy legal documents =E2=80=93 e.g., a 100-page contract, a deposi=
tion transcript, or a bundle of related case files. Mistakes or missed clau=
ses can be costly, so accuracy is paramount. The text is formal, with secti=
ons, definitions, and possibly a need to cross-reference (=E2=80=9Csee Sect=
ion 5.2(b)=E2=80=9D).</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(5=
4,55,55);line-height: 26px;font-size: 16px;"><strong>Challenges:</strong><s=
pan> Legal documents often exceed 50K tokens. You can=E2=80=99t just dump t=
hem wholesale. They have internal structure (sections, clauses). Also, nuan=
ce is key =E2=80=93 skipping a =E2=80=9Cnot=E2=80=9D or misreading a defini=
tion can invert meaning. So summarization must be careful not to distort. A=
lso, some queries require comparing parts of the document (e.g., does claus=
e X conflict with clause Y?). The model might need to see both in context t=
o answer properly.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,5=
5,55);line-height: 26px;font-size: 16px;"><strong>Strategies to Use:</stron=
g><span> </span><strong>Strategic Chunking + Retrieval, Summary Chain, and =
Context Budgeting.</strong><span> Also possibly position-aware highlighting=
 of definitions.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,=
55);line-height: 26px;font-size: 16px;"><strong>Playbook:</strong></p><ol s=
tyle=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;"><p s=
tyle=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing:=
 border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Split by =
Sections:</strong><span> Leverage the document=E2=80=99s structure. Contrac=
ts usually have numbered sections. Process each section as a chunk. Use a <=
/span><em>Map-Reduce summary chain</em><span> if you want an overall summar=
y: summarize each section, then summarize those summaries into a contract b=
rief. This ensures no section=E2=80=99s key points are missed.</span></p></=
li><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);lin=
e-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;fo=
nt-size: 16px;margin: 0;"><strong>Definitions:</strong><span> Contracts hav=
e a definitions section. It=E2=80=99s crucial because terms like =E2=80=9CA=
ffiliate=E2=80=9D or =E2=80=9CEffective Date=E2=80=9D have precise meanings=
. A trick: extract all definitions into a small list at the top of your pro=
mpt as reference. E.g., =E2=80=9CDefinitions: =E2=80=98Effective Date=E2=80=
=99 means June 1, 2025; =E2=80=98Territory=E2=80=99 means North America=E2=
=80=A6=E2=80=9D. By position-aware prompting, the model will use these defi=
nitions throughout analysis.</span></p></li><li style=3D"margin: 8px 0 0 32=
px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;bo=
x-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>=
QA via Retrieval:</strong><span> For Q&amp;A, use a </span><strong>hierarch=
ical retrieval</strong><span> approach: First, find which sections likely c=
ontain the answer (you might keyword search within the contract =E2=80=93 e=
.g., question asks about indemnification, find where =E2=80=9Cindemnify=E2=
=80=9D appears). Then feed just those sections to the model with the questi=
on. If multiple documents (like multiple contracts or a law + a case), do r=
etrieval across docs.</span></p></li><li style=3D"margin: 8px 0 0 32px;"><p=
 style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizin=
g: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Compari=
sons:</strong><span> If task is comparing two contracts (e.g., differences =
in clauses), a good strategy is to summarize each contract=E2=80=99s key cl=
auses first, then have the model compare summaries, citing specific clauses=
. Alternatively, you can align them section by section (if similar structur=
e) and have the model analyze section pairs.</span></p></li><li style=3D"ma=
rgin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;mar=
gin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;marg=
in: 0;"><strong>Long Context Model with Chunking:</strong><span> Thomson Re=
uters=E2=80=99 experience showed that even with GPT-4 1M context, they </sp=
an><em>still chunk documents for complex tasks</em><span>. For example, in =
their CoCounsel product, they discovered feeding the full long document can=
 be less effective than chunking and then using the model on each chunk. So=
, if you have GPT-4.1 1M, you could technically stuff a giant contract, but=
 you might get a better result splitting it into e.g. 5 parts and asking 5 =
questions then combining. Their internal testing found </span><em>multi-hop=
 reasoning across a whole contract</em><span> works more reliably by not re=
lying on the model=E2=80=99s raw long context alone.</span></p></li><li sty=
le=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line-height: =
26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 1=
6px;margin: 0;"><strong>Use Summaries for Memory:</strong><span> In a workf=
low like due diligence, you may analyze dozens of contracts. Summarize each=
 with a consistent template (e.g., parties, term, liabilities, termination =
conditions). Those summaries are much shorter and can be compared or furthe=
r processed easily. Possibly have the LLM generate a table of key points ac=
ross contracts from those summaries.</span></p></li><li style=3D"margin: 8p=
x 0 0 32px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bott=
om: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">=
<strong>Verification &amp; Sources:</strong><span> Have the model quote the=
 clause or section number as evidence for its answers (you can instruct it:=
 =E2=80=9CCite the section where this is stated.=E2=80=9D). This keeps it g=
rounded. Some LLMs will fabricate if unsure, so when it cites something, yo=
u can cross-check that clause in the text. It=E2=80=99s an extra step but i=
mportant in legal.</span></p></li></ol><p style=3D"margin: 0 0 20px 0;color=
: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>Example:</stron=
g><span> </span><em>Contract QA:</em><span> Question: =E2=80=9CWhat are the=
 termination rights in this agreement, and under what conditions can each p=
arty terminate?=E2=80=9D Approach: vector search the contract text for =E2=
=80=9Cterminate, termination=E2=80=9D. Likely hits: a section named Termina=
tion, maybe others (like term of agreement). Extract those sections (maybe =
2 sections). Prompt model: =E2=80=9C</span><strong>Context:</strong><span> =
Section 8: Term and Termination =E2=80=93 [text]; Section 12.4: Termination=
 for Cause =E2=80=93 [text] </span><strong>Question:</strong><span> Summari=
ze the termination rights of each party in this contract, including conditi=
ons.=E2=80=9D The model then outputs: =E2=80=9CThe agreement can be termina=
ted by either party with 30 days=E2=80=99 notice if the other party breache=
s a material term (Section 12.4). Additionally, Client may terminate for co=
nvenience with 60 days=E2=80=99 notice (Section 8). The rights are=E2=80=A6=
=E2=80=9D etc., ideally referencing those sections. Because we fed it only =
the relevant parts, it wasn=E2=80=99t distracted by unrelated content and c=
ould focus. If we had given the entire 50-page contract, the answer might h=
ave been incomplete or buried something, especially if termination was in t=
he middle somewhere.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54=
,55,55);line-height: 26px;font-size: 16px;"><strong>Pitfall to Avoid:</stro=
ng><span> Don=E2=80=99t over-summarize critical legal language. If a questi=
on is about a precise condition (e.g., what triggers force majeure), a summ=
ary might oversimplify and miss nuance like =E2=80=9Cincluding but not limi=
ted to=E2=80=A6=E2=80=9D. In those cases, you might want the model to quote=
 or at least closely paraphrase the original language from that clause. It =
might be better to retrieve the clause and ask: =E2=80=9CAccording to the c=
lause, under what circumstances does force majeure apply?=E2=80=9D So, comp=
ress context up to a point, but for actual legal interpretation, sometimes =
the raw words matter. Balance is key.</span></p><h3 class=3D"header-anchor-=
post" style=3D"position: relative;font-family: 'SF Pro Display',-apple-syst=
em-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,He=
lvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Sym=
bol';font-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-sm=
oothing: antialiased;-webkit-appearance: optimizelegibility;-moz-appearance=
: optimizelegibility;appearance: optimizelegibility;margin: 1em 0 0.625em 0=
;color: rgb(54,55,55);line-height: 1.16em;font-size: 1.375em;"><strong>Fina=
ncial: Analyzing Reports and Data</strong></h3><p style=3D"margin: 0 0 20px=
 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>Scenari=
o:</strong><span> You have lengthy financial documents =E2=80=93 annual rep=
orts (10-Ks), earnings call transcripts, market research reports =E2=80=93 =
and you need insights or answers. These often include tables, numbers, and =
are updated regularly. Or you might feed a model lots of transactional data=
 or logs to identify trends.</span></p><p style=3D"margin: 0 0 20px 0;color=
: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>Challenges:</st=
rong><span> Financial texts mix narrative (CEO=E2=80=99s letter) and data (=
financial statements). They can be hundreds of pages. Also, precision with =
numbers is crucial; models can mess up numbers or do math wrong. There=E2=
=80=99s often repetition (quarterly reports have similar sections each year=
). You may want a summary but also exact figures for certain metrics.</span=
></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;=
font-size: 16px;"><strong>Strategies:</strong><span> </span><strong>Summary=
 Chain for narrative, Retrieval for specific data, Tools for calculations, =
Context Budget for multi-quarter analysis.</strong><span> Possibly use exte=
rnal tools for number crunching (like an in-prompt calculator).</span></p><=
p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-s=
ize: 16px;"><strong>Playbook:</strong></p><ol style=3D"margin-top: 0;paddin=
g: 0;"><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55)=
;line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4p=
x;font-size: 16px;margin: 0;"><strong>Separate Narrative vs Data:</strong><=
span> If analyzing a 10-K, consider treating the MD&amp;A (Management Discu=
ssion &amp; Analysis) narrative differently from the financial statements t=
ables. Summarize the narrative sections (they=E2=80=99re verbose) but perha=
ps extract the key figures from tables without summarizing them (you don=E2=
=80=99t want to lose accuracy by summarizing numbers).</span></p></li><li s=
tyle=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line-height=
: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size:=
 16px;margin: 0;"><strong>Tabular Data Handling:</strong><span> If context =
allows, you can include some tables. But models often struggle reading big =
tables reliably. An alternative is to pre-process tables: e.g., parse the t=
able with code and then present the model with a summary like =E2=80=9CReve=
nue in 2022 was $X (up Y% from 2021=E2=80=99s $Z).=E2=80=9D This is dynamic=
 compression for numbers. Or use the new function calling capabilities: hav=
e the model output a JSON of key numbers then verify or calculate. If the m=
odel has a tool plugin (like a Python interpreter), leverage it to do preci=
se math rather than mental math.</span></p></li><li style=3D"margin: 8px 0 =
0 32px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: =
0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><str=
ong>Use Comparisons Over Time:</strong><span> For multi-period analysis, ch=
unk by year or quarter. E.g., if analyzing Q1 reports for the last 4 years,=
 you might summarize each Q1, then ask the model to identify trends from th=
ose summaries. This ensures it actually sees all four periods (because you =
feed summary of each, instead of raw data where it might miss middle ones).=
</span></p></li><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(=
54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding=
-left: 4px;font-size: 16px;margin: 0;"><strong>Context Budget for Conversat=
ions:</strong><span> If you have an interactive financial analysis chat, ma=
intain a =E2=80=9Cfacts so far=E2=80=9D list. If the user and model have id=
entified certain important numbers or insights, keep those in a running sum=
mary that persists in the prompt (so the model doesn=E2=80=99t have to re-r=
ead tables to remember the revenue was $5M).</span></p></li><li style=3D"ma=
rgin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;mar=
gin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;marg=
in: 0;"><strong>Ask Specific Questions vs Open-Ended:</strong><span> Long f=
inancial documents can overwhelm an open =E2=80=9Canalyze this=E2=80=9D pro=
mpt. Instead, break the task: Ask, =E2=80=9CList the top 5 risk factors men=
tioned.=E2=80=9D or =E2=80=9CWhat were the main drivers of growth this year=
, according to the report?=E2=80=9D One can follow a sequence: first ask mo=
del to list sections and their gist (it can usually skim headings), then di=
ve deeper into each section as needed (using retrieval for that section=E2=
=80=99s text). This is basically summary chain applied section by section.<=
/span></p></li><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(5=
4,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-=
left: 4px;font-size: 16px;margin: 0;"><strong>Use Retrieval for Reference D=
ata:</strong><span> If a user asks =E2=80=9CHow did ACME Corp=E2=80=99s 202=
4 R&amp;D expense compare to 2023?=E2=80=9D, you could retrieve the parts o=
f the report that mention R&amp;D expense or find those figures (perhaps vi=
a a small search in the document text for =E2=80=9CResearch and development=
=E2=80=9D). Provide the snippet or value to the model. If the model should =
calculate the growth rate, ensure it has both numbers clearly and consider =
instructing it to do math carefully (or just calculate externally and feed =
it the result to be safe).</span></p></li><li style=3D"margin: 8px 0 0 32px=
;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-=
sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Mo=
nitoring Model Math:</strong><span> Be aware the model might make arithmeti=
c errors especially with big numbers. One approach: after the model answers=
 with numeric analysis, you could have a verification step. For instance, a=
sk the model: =E2=80=9CDouble-check the calculations above.=E2=80=9D Someti=
mes it will catch its own error on a second pass. Or use programmatic funct=
ion calling to actually compute differences. The =E2=80=9CTools That Help=
=E2=80=9D section below will mention monitoring solutions =E2=80=93 in fina=
ncial context, one might have a small script to validate that the percentag=
es it states match the numbers provided.</span></p></li></ol><p style=3D"ma=
rgin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><=
strong>Example:</strong><span> </span><em>Earnings Call Transcript:</em><sp=
an> ~30 pages of conversation. The goal: summarize main points and extract =
any forward-looking statements or sentiments. Approach:</span></p><ul style=
=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-specia=
l-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margi=
n-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin=
: 0;">Use summary chain: break transcript by speaker turn blocks (e.g., CEO=
 speech, CFO speech, Q&amp;A part).</p></li><li style=3D"margin: 8px 0 0 32=
px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-heigh=
t: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size=
: 16px;margin: 0;">Summarize each part.</p></li><li style=3D"margin: 8px 0 =
0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-h=
eight: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-=
size: 16px;margin: 0;">Then prompt: =E2=80=9CSummarize the key points from =
the earnings call: [CEO summary] [CFO summary] [Q&amp;A summary].=E2=80=9D =
The final output: bullet points of results, guidance, market reaction, etc.=
</p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p =
style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing=
: border-box;padding-left: 4px;font-size: 16px;margin: 0;">If interested in=
 sentiment, maybe instruct model: =E2=80=9CIdentify any concerns or optimis=
tic notes the management expressed.=E2=80=9D</p></li><li style=3D"margin: 8=
px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);l=
ine-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;=
font-size: 16px;margin: 0;">This yields a digest a human could have taken h=
ours to assemble.</p></li><li style=3D"margin: 8px 0 0 32px;mso-special-for=
mat: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bot=
tom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"=
>If there=E2=80=99s a specific question like =E2=80=9CWhat revenue guidance=
 was given for next quarter?=E2=80=9D, just retrieve that portion of transc=
ript (find where CFO says =E2=80=9Cguidance=E2=80=9D) and have the model qu=
ote it or paraphrase with high accuracy.</p></li></ul><p style=3D"margin: 0=
 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>=
Pitfall to Avoid:</strong><span> Hallucinating numbers. Do not trust an LLM=
 to pull a number out of thin air. It should come from provided text. To mi=
tigate: always provide the model the actual text or data that contains the =
number. If it=E2=80=99s summarizing and needs a number, either include that=
 in the context or verify afterwards. Also, extremely domain-specific finan=
ce concepts (like complex accounting terms) might confuse a general model; =
consider providing definitions or using a specialized model if available.</=
span></p><h3 class=3D"header-anchor-post" style=3D"position: relative;font-=
family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,Bli=
nkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color E=
moji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smo=
othing: antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearance=
: optimizelegibility;-moz-appearance: optimizelegibility;appearance: optimi=
zelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16=
em;font-size: 1.375em;"><strong>Healthcare: Patient Records and Clinical No=
tes</strong></h3><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-h=
eight: 26px;font-size: 16px;"><strong>Scenario:</strong><span> You want an =
LLM to help summarize a patient=E2=80=99s medical history from electronic h=
ealth records (EHR), or answer questions based on a patient=E2=80=99s recor=
ds (which might include doctor=E2=80=99s notes, lab results, medication lis=
ts, etc.). These records can be very lengthy over a chronically ill patient=
=E2=80=99s history.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,=
55,55);line-height: 26px;font-size: 16px;"><strong>Challenges:</strong><spa=
n> </span><strong>Privacy and accuracy</strong><span> are critical. Halluci=
nation is dangerous here (you don=E2=80=99t want the model to =E2=80=9Cmake=
 up=E2=80=9D a diagnosis). Medical terminology and shorthand in notes can b=
e complex. Also, the information might be spread over many notes and timepo=
ints. It=E2=80=99s similar to legal in that detail matters (e.g., medicatio=
n dosage, allergy info). Also, EHR data might not be nicely formatted =E2=
=80=93 lots of abbreviations, maybe scanned OCR text, etc.</span></p><p sty=
le=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: =
16px;"><strong>Strategies:</strong><span> </span><strong>Chunking by record=
 type/date, Context Budget focusing on problem list, Retrieval for relevant=
 notes, Summarization for older history.</strong><span> Also maybe use spec=
ialized medical LLM or at least provide context for terms.</span></p><p sty=
le=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: =
16px;"><strong>Playbook:</strong></p><ol style=3D"margin-top: 0;padding: 0;=
"><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line=
-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;fon=
t-size: 16px;margin: 0;"><strong>Organize by Source:</strong><span> Split t=
he patient record into logical groups: e.g., </span><em>History &amp; Physi=
cal notes</em><span>, </span><em>Progress notes by date</em><span>, </span>=
<em>Lab results</em><span>, </span><em>Medication list</em><span>, </span><=
em>Imaging reports</em><span>. Each is a chunk category. This helps because=
 the model might better handle one category at a time (and you can prompt a=
ccordingly).</span></p></li><li style=3D"margin: 8px 0 0 32px;"><p style=3D=
"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border=
-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Summarize Chrono=
logically:</strong><span> Often you need a narrative of the patient journey=
. You can apply a refine chain across chronological notes: start with the e=
arliest note summary, then refine it with new info from each subsequent not=
e (=E2=80=9CUpdate the summary given this new note=E2=80=9D). This yields a=
 progressive patient summary that ideally includes all major events. This i=
s a dynamic summarization approach and helps compress dozens of notes into =
one story.</span></p></li><li style=3D"margin: 8px 0 0 32px;"><p style=3D"c=
olor: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-b=
ox;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Highlight Key Data=
:</strong><span> Use position-aware technique to ensure critical info like =
</span><strong>Allergies, Current Medications, Major Diagnoses</strong><spa=
n> are up front. For instance, if present, always prepend: =E2=80=9CAllergi=
es: X; Current Meds: Y; Problems: Z.=E2=80=9D This context at the top ensur=
es the model never suggests a medication the patient is allergic to, for ex=
ample (because the allergy is right there in prompt).</span></p></li><li st=
yle=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line-height:=
 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: =
16px;margin: 0;"><strong>Retrieval for Q&amp;A:</strong><span> If a clinici=
an asks =E2=80=9CHas the patient ever had an MRI of the spine and what were=
 the results?=E2=80=9D, you can search the records for =E2=80=9CMRI spine=
=E2=80=9D and find that imaging report. Feed just that to answer. Or if ask=
ed =E2=80=9CList all medications the patient has been on for diabetes,=E2=
=80=9D retrieve medication lists and relevant notes about diabetes manageme=
nt.</span></p></li><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: r=
gb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padd=
ing-left: 4px;font-size: 16px;margin: 0;"><strong>Context Budget =E2=80=93 =
Focus on Recent Info:</strong><span> In healthcare, recent events are usual=
ly more relevant than a decade-old note, except maybe past surgical history=
. Use a strategy to always include the latest notes fully, and older ones o=
nly in summary form. E.g., =E2=80=9CLast 3 office visits full text, older v=
isits summarized in 1 paragraph.=E2=80=9D This ensures current data (like l=
ast week=E2=80=99s lab results) aren=E2=80=99t missed because the model got=
 lost in a 2010 note about a resolved issue.</span></p></li><li style=3D"ma=
rgin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line-height: 26px;mar=
gin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;marg=
in: 0;"><strong>Verification / No Hallucination Prompting:</strong><span> U=
se the prompt to discourage guessing: e.g. =E2=80=9CIf the records do not e=
xplicitly mention something, do not assume it.=E2=80=9D This might reduce h=
allucination. Also, use a </span><strong>tool or knowledge base</strong><sp=
an> for medical facts if needed rather than rely on model=E2=80=99s possibl=
y outdated knowledge (if question is about treatment guidelines, either pro=
vide that info or instruct it to base answers only on patient data).</span>=
</p></li><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,5=
5);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: =
4px;font-size: 16px;margin: 0;"><strong>Use of Medical NER (entity recognit=
ion):</strong><span> Pre-process notes to extract structured data: patient=
=E2=80=99s conditions, meds, etc. Then provide that as a structured summary=
 in prompt (like a problem list). This is like compressing and organizing t=
he info. Some open tools or libraries can do this or one can prompt an LLM =
to list the problems mentioned in notes, then feed that list back in as a r=
eference.</span></p></li></ol><p style=3D"margin: 0 0 20px 0;color: rgb(54,=
55,55);line-height: 26px;font-size: 16px;"><strong>Example:</strong><span> =
</span><em>Patient Summary:</em><span> Input: 50 pages of varied notes. Tas=
k: produce a summary for a referral letter. Approach:</span></p><ul style=
=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-specia=
l-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margi=
n-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin=
: 0;">Divide into sections: Past Medical History, Meds, Allergies (these ar=
e often available in the chart separately), then chronological notes.</p></=
li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=
=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: bor=
der-box;padding-left: 4px;font-size: 16px;margin: 0;">For notes: do a map-r=
educe: summarize each year=E2=80=99s notes into =E2=80=9CYear X summary=E2=
=80=9D (e.g., 2020: =E2=80=9CPatient had two hospitalizations for COPD exac=
erbation; started on home oxygen.=E2=80=9D).</p></li><li style=3D"margin: 8=
px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);l=
ine-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;=
font-size: 16px;margin: 0;">Then combine those yearly summaries into an ove=
rall summary.</p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format:=
 bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom:=
 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">Dou=
ble-check any critical values (like ensure latest lab values are mentioned)=
.</p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p=
 style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizin=
g: border-box;padding-left: 4px;font-size: 16px;margin: 0;">The final summa=
ry to doctor includes a separate list: =E2=80=9CActive issues: COPD, Type 2=
 Diabetes, Hypertension; Surgeries: Appendectomy (2005)=E2=80=A6=E2=80=9D f=
ollowed by narrative timeline. This ensures completeness.</p></li><li style=
=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rg=
b(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;paddi=
ng-left: 4px;font-size: 16px;margin: 0;">The model did not have to see ever=
y single note word-for-word at once, but via summarization it =E2=80=9Csaw=
=E2=80=9D them in aggregate.</p></li><li style=3D"margin: 8px 0 0 32px;mso-=
special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px=
;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;=
margin: 0;">If a question arises like =E2=80=9CWhen was the first diagnosis=
 of diabetes and what was the initial treatment?=E2=80=9D, one could either=
 parse the summary (if it=E2=80=99s in there) or specifically search the no=
tes for =E2=80=9Cdiabetes=E2=80=9D and find earliest mention and the plan.<=
/p></li></ul><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-heigh=
t: 26px;font-size: 16px;"><strong>Pitfall to Avoid:</strong><span> </span><=
strong>Mixing patients=E2=80=99 data.</strong><span> If by any chance multi=
ple patient data are together, ensure separation (shouldn=E2=80=99t happen =
in a single prompt scenario, but just in case). Also, privacy: if using an =
API, one must be careful with PHI (though that=E2=80=99s a whole other topi=
c beyond context handling). On a technical level, avoid hallucinated medica=
l advice =E2=80=93 ideally the system would only present what=E2=80=99s in =
records and not outside medical judgment (unless that=E2=80=99s desired and=
 then perhaps a specialized model or known references should be used). Alwa=
ys have a human clinician review outputs; use LLM as assistant, not final d=
ecision maker.</span></p><h3 class=3D"header-anchor-post" style=3D"position=
: relative;font-family: 'SF Pro Display',-apple-system-headline,system-ui,-=
apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-seri=
f,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;=
-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing: antialiased;-w=
ebkit-appearance: optimizelegibility;-moz-appearance: optimizelegibility;ap=
pearance: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);l=
ine-height: 1.16em;font-size: 1.375em;"><strong>Software: Large Codebases a=
nd Repositories</strong></h3><p style=3D"margin: 0 0 20px 0;color: rgb(54,5=
5,55);line-height: 26px;font-size: 16px;"><strong>Scenario:</strong><span> =
Using LLMs for code =E2=80=93 e.g., analyzing a large codebase for bugs or =
generating summaries of code functionality, or assisting with code reviews =
across thousands of lines. You might have files that far exceed context (ma=
ybe generated code or large configs), or multiple files that need to be con=
sidered together (like definition in one, usage in another).</span></p><p s=
tyle=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size=
: 16px;"><strong>Challenges:</strong><span> Code requires exactness. LLMs c=
an do well in small context but on huge code they may forget important line=
s or functions. Also, </span><em>repetition and boilerplate</em><span> are =
common in code (e.g., many similar functions, or long repetitive data struc=
tures) which waste context. The user might want a specific part fixed, but =
provided the whole file and the model might miss the relevant part. Also, c=
ode often has structured hierarchy (projects, modules).</span></p><p style=
=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16=
px;"><strong>Strategies:</strong><span> </span><strong>Chunk by file/functi=
on, Retrieval by symbol, Position-aware for focus, Dynamic compression via =
abstraction.</strong><span> Possibly chain-of-thought prompting for step-by=
-step debugging.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,=
55);line-height: 26px;font-size: 16px;"><strong>Playbook:</strong></p><ol s=
tyle=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;"><p s=
tyle=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing:=
 border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Chunk by =
File or Module:</strong><span> Treat each file as a chunk. Use file-level c=
ontext to answer questions or generate summaries per file. If an analysis n=
eeds multiple files (like find where a function is called across files), co=
nsider building an index of function definitions and references (like a sim=
ple symbol index), then retrieve relevant snippets.</span></p></li><li styl=
e=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line-height: 2=
6px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16=
px;margin: 0;"><strong>Use Code-Aware Tools:</strong><span> Many LLM coding=
 assistants use a technique: they first retrieve relevant code snippets via=
 static analysis or regex (like find all references to a function name) and=
 only feed those into the model. You can do similar: if user asks about fun=
ction foo(), search the codebase for foo( and pull those contexts.</span></=
p></li><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55)=
;line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4p=
x;font-size: 16px;margin: 0;"><strong>Position hints:</strong><span> If you=
 feed a large code file in (say 500 lines) and you want the model to focus =
on a particular section (maybe where a bug likely is), you can insert a com=
ment like // NOTE: Potential issue here in the code before giving it to the=
 model. This is a form of position-aware prompting in code. The model will =
likely pay attention there.</span></p></li><li style=3D"margin: 8px 0 0 32p=
x;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box=
-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>S=
ummarize/Abstract Repeated Code:</strong><span> If a file contains a lot of=
 repetitive code (like 100 similar entries), you can compress it: =E2=80=9C=
// =E2=80=A6 50 similar lines omitted =E2=80=A6=E2=80=9D or =E2=80=9Cfuncti=
on definitions for getters are similar and omitted for brevity.=E2=80=9D Th=
e model doesn=E2=80=99t need to see all if they are formulaic. It can trust=
 that omitted content is just more of the same. This manual compression of =
code can save a ton of tokens.</span></p></li><li style=3D"margin: 8px 0 0 =
32px;"><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26p=
x;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px=
;"><strong>Iterative Debugging:</strong><span> If trying to debug with the =
model, don=E2=80=99t feed entire project at once. Instead:</span></p><ul st=
yle=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-spe=
cial-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;ma=
rgin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;mar=
gin: 0;">Ask it to analyze one part (maybe the error log or a specific func=
tion).</p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet=
;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-=
sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">Then if it=
 suspects the issue is elsewhere, retrieve that file and feed it.</p></li><=
li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"c=
olor: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-b=
ox;padding-left: 4px;font-size: 16px;margin: 0;">The agent approach: the mo=
del might say =E2=80=9CIt might be an issue in Function Y in file2.py.=E2=
=80=9D Then you fetch file2.py and provide it to confirm. This is similar t=
o how one would debug manually =E2=80=93 focus where clues lead.</p></li></=
ul></li><li style=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55=
);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4=
px;font-size: 16px;margin: 0;"><strong>Context Budget in Chat Code Review:<=
/strong><span> If using an LLM in a code review chat, keep the code diff or=
 relevant chunk and drop older diffs as they become irrelevant. Summarize w=
hat decisions have been made (=E2=80=9CWe decided to use approach X to fix =
Y=E2=80=9D) and keep that as context rather than the entire prior code disc=
ussion.</span></p></li><li style=3D"margin: 8px 0 0 32px;"><p style=3D"colo=
r: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;=
padding-left: 4px;font-size: 16px;margin: 0;"><strong>Leverage Embeddings f=
or Code:</strong><span> Represent each function or class by an embedding of=
 its docstring or signature. This can allow semantic search (=E2=80=9Cembed=
ding search for Vector class multiply method=E2=80=9D). This is more advanc=
ed, but retrieval can be semantic (e.g., user asks =E2=80=9Cwhere in code d=
o we parse JSON?=E2=80=9D =E2=80=93 embed that question, find nearest code =
blocks embedding, likely lands in a JSON parser file).</span></p></li><li s=
tyle=3D"margin: 8px 0 0 32px;"><p style=3D"color: rgb(54,55,55);line-height=
: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size:=
 16px;margin: 0;"><strong>Two-Model Approach:</strong><span> This is intere=
sting and was hinted at in that forum: use one model or pass to scan a lot =
of code (maybe smaller context model or one specialized in code search) to =
find candidate locations, then use the bigger reasoning model to analyze th=
ose. E.g., Model A: =E2=80=9CFind all functions named X or similar in the c=
ode =E2=80=93 output their paths.=E2=80=9D Then Model B: =E2=80=9CGiven the=
se code snippets from those locations, do Z.=E2=80=9D</span></p></li></ol><=
p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-s=
ize: 16px;"><strong>Example:</strong><span> </span><em>Large Codebase Q&amp=
;A:</em><span> Question: =E2=80=9CWhere is the user=E2=80=99s password hash=
ed in this project?=E2=80=9D Instead of giving the whole project, you:</spa=
n></p><ul style=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 =
32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-hei=
ght: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-si=
ze: 16px;margin: 0;">Search code for =E2=80=9Chash(=E2=80=9C or =E2=80=9Cbc=
rypt=E2=80=9D etc. Suppose it finds auth.py line 150 and utils/security.py =
line 20.</p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bull=
et;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;bo=
x-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">You feed=
 the model: =E2=80=9CIn auth.py at line 150: =E2=80=A6 (code) =E2=80=A6, in=
 utils/security.py at line 20: =E2=80=A6 (code) =E2=80=A6 The question: whe=
re is the password hashed and how?=E2=80=9D</p></li><li style=3D"margin: 8p=
x 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);li=
ne-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;f=
ont-size: 16px;margin: 0;">The model then sees exactly the relevant code an=
d can answer: =E2=80=9CThe password is hashed in utils/security.py in the f=
unction hash_password(password) which uses bcrypt. In auth.py line 150, tha=
t function is called before saving the user.=E2=80=9D It might even quote s=
ome snippet. This is a precise and cost-effective answer.</p></li></ul><p s=
tyle=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size=
: 16px;"><span>Another: </span><em>Code Summary:</em><span> Summarize a 100=
0-line file. You could chunk it by function: have the model summarize each =
function (maybe by prompt =E2=80=9CSummarize what this function does=E2=80=
=9D), then combine. Or first, strip out comments and trivial code, compress=
 the repetitive sections as described, maybe you end up with a 400-line gis=
t that=E2=80=99s still semantically representative. Then feed that for summ=
ary. This might be semi-automated with scripts.</span></p><p style=3D"margi=
n: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><str=
ong>Pitfall to Avoid:</strong><span> The model </span><em>guessing</em><spa=
n> something about code it didn=E2=80=99t see. If you skip too much, it mig=
ht assume. So when compressing code, ensure you mark clearly when you=E2=80=
=99ve omitted something and maybe describe it. E.g., =E2=80=9C// =E2=80=A6 =
similar logic repeated =E2=80=A6=E2=80=9D. Also, watch out for the model ou=
tputting hallucinated code that isn=E2=80=99t in the base =E2=80=93 stick t=
o tasks like summarization or Q&amp;A rather than generating completely new=
 large code in these contexts unless you have tests. For bug fixes, always =
test the suggested fix; LLM might fix one thing but break another if it lac=
ked full context.</span></p><div style=3D"font-size: 16px;line-height: 26px=
;"><hr style=3D"margin: 32px 0;padding: 0;height: 1px;background: #e6e6e6;b=
order: none;"></div><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);lin=
e-height: 26px;font-size: 16px;">We have now looked at specific playbooks f=
or different domains. While the tools are similar (chunk, retrieve, summari=
ze), the way you apply them varies. Legal needed careful retention of exact=
 language for citing clauses. Finance needed careful handling of numbers. H=
ealthcare needs summarization with caution on facts. Code needs targeted se=
arching for relevant parts and possibly ignoring repetitive boilerplate.</p=
><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font=
-size: 16px;">Next, let=E2=80=99s go over some tools and frameworks that ca=
n help implement these strategies without reinventing the wheel.</p><h2 cla=
ss=3D"header-anchor-post" style=3D"position: relative;font-family: 'SF Pro =
Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,=
'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI =
Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoothing: antialia=
sed;-moz-osx-font-smoothing: antialiased;-webkit-appearance: optimizelegibi=
lity;-moz-appearance: optimizelegibility;appearance: optimizelegibility;mar=
gin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-size: 1.=
625em;"><strong>The Tools That Actually Help</strong></h2><p style=3D"margi=
n: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><spa=
n>Managing context can be tough to implement from scratch, but luckily ther=
e=E2=80=99s an evolving ecosystem of tools, libraries, and platform feature=
s to make it easier. Here we focus on those that </span><em>actually</em><s=
pan> help in production (as opposed to just theoretical demos).</span></p><=
h3 class=3D"header-anchor-post" style=3D"position: relative;font-family: 'S=
F Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSyste=
mFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Seg=
oe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoothing: an=
tialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearance: optimize=
legibility;-moz-appearance: optimizelegibility;appearance: optimizelegibili=
ty;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-si=
ze: 1.375em;"><strong>Context Management Libraries &amp; Frameworks</strong=
></h3><ul style=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 =
32px;mso-special-format: bullet;"><p style=3D"margin: 0 0 20px 0;color: rgb=
(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;paddin=
g-left: 4px;font-size: 16px;"><strong>LangChain (and LangChain Hub):</stron=
g><span> LangChain became popular for chaining LLM calls and has utilities =
for many patterns we discussed. For context:</span></p><ul style=3D"margin-=
top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-special-format: b=
ullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0=
;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><span=
>It provides </span><strong>TextSplitter</strong><span> classes to chunk te=
xt in various ways (by chars, tokens, sentences, etc.) =E2=80=93 useful for=
 </span><em>Strategic Chunking</em><span>.</span></p></li><li style=3D"marg=
in: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,=
55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left:=
 4px;font-size: 16px;margin: 0;"><span>It has </span><strong>DocumentLoader=
s</strong><span> and </span><strong>VectorStore</strong><span> integrations=
 to do RAG easily =E2=80=93 you can ingest your docs and query them with a =
few lines.</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-special-for=
mat: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bot=
tom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"=
><span>The </span><strong>Memory</strong><span> classes implement conversat=
ion memory: e.g., ConversationBufferMemory (keeps full history), Conversati=
onSummaryMemory (summarizes old messages) =E2=80=93 implementing </span><em=
>Context Budget</em><span> logic out-of-the-box.</span></p></li><li style=
=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rg=
b(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;paddi=
ng-left: 4px;font-size: 16px;margin: 0;"><span>There=E2=80=99s a </span><st=
rong>LongContextReorder</strong><span> transformer (we saw earlier) in Lang=
Chain=E2=80=99s community utils which will reorder retrieved docs to avoid =
lost-in-middle.</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-specia=
l-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margi=
n-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin=
: 0;"><em>Using it:</em><span> If you=E2=80=99re building a Python app, Lan=
gChain can save time orchestrating calls. Just be mindful of performance an=
d test that it=E2=80=99s doing what you expect (some early versions were le=
ss optimized).</span></p></li></ul></li><li style=3D"margin: 8px 0 0 32px;m=
so-special-format: bullet;"><p style=3D"margin: 0 0 20px 0;color: rgb(54,55=
,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left=
: 4px;font-size: 16px;"><strong><a href=3D"https://substack.com/redirect/7a=
bc1ba2-fbc0-403f-97c8-c40851788b58?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9=
hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,55);text-de=
coration: underline;">LlamaIndex</a><span> (aka GPT Index):</span></strong>=
<span> Another library focused originally on documents. It builds indices (=
vectors, keyword, etc.) and has query engines that automatically break quer=
ies into retrieval + synthesis steps.</span></p><ul style=3D"margin-top: 0;=
padding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"=
><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-si=
zing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><span>It has=
 capabilities like </span><strong>TreeIndex</strong><span> which essentiall=
y does summarization in a tree (great for summary chain).</span></p></li><l=
i style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"co=
lor: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-bo=
x;padding-left: 4px;font-size: 16px;margin: 0;"><span>LlamaIndex is nice fo=
r </span><em>large documents</em><span> =E2=80=93 it can recursively summar=
ize big texts by building an index.</span></p></li><li style=3D"margin: 8px=
 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);lin=
e-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;fo=
nt-size: 16px;margin: 0;"><span>It also can do </span><em>structured retrie=
val</em><span> (pull specific fields from JSON, etc.).</span></p></li><li s=
tyle=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color=
: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;p=
adding-left: 4px;font-size: 16px;margin: 0;">If you have lots of data sourc=
es, it can unify them (SQL, documents, APIs).</p></li><li style=3D"margin: =
8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);=
line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px=
;font-size: 16px;margin: 0;">It may be a good fit if your use case is docum=
ent question answering or building a chatbot over knowledge.</p></li></ul><=
/li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=
=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;margin-bottom=
: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;"><strong><a h=
ref=3D"https://substack.com/redirect/665c508e-f2f9-44cc-b2db-21e3acfaf315?j=
=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"=
" style=3D"color: rgb(54,55,55);text-decoration: underline;">OpenAI Functio=
n Calling</a><span> / Tools:</span></strong><span> OpenAI=E2=80=99s API now=
 supports function calling where you can define tools (like search, calcula=
tors) that the model can use. This is useful for context:</span></p><ul sty=
le=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-spec=
ial-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;mar=
gin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;marg=
in: 0;"><span>You can implement a </span><strong>search tool</strong><span>=
 that when the model queries it, you do a vector DB lookup and return relev=
ant text. The model then sees that text. This essentially is in-chat retrie=
val without you handling the prompting logic.</span></p></li><li style=3D"m=
argin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,=
55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-le=
ft: 4px;font-size: 16px;margin: 0;"><span>You can have a </span><strong>sum=
marize function</strong><span> that triggers a known sequence (though often=
 you might handle summarization outside the model).</span></p></li><li styl=
e=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: r=
gb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padd=
ing-left: 4px;font-size: 16px;margin: 0;">E.g., have a function get_documen=
t(section_id) that returns a section of a doc. The model can decide to call=
 it if it needs more info.</p></li><li style=3D"margin: 8px 0 0 32px;mso-sp=
ecial-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;m=
argin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;ma=
rgin: 0;">This moves some orchestration inside the model=E2=80=99s reasonin=
g, which is powerful but requires careful setup so it doesn=E2=80=99t hallu=
cinate tool usage.</p></li><li style=3D"margin: 8px 0 0 32px;mso-special-fo=
rmat: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bo=
ttom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;=
">If done right, you get dynamic retrieval and even iterative reading (the =
model can decide =E2=80=9CI need section 2=E2=80=A6 now section 3=E2=80=A6=
=E2=80=9D).</p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: b=
ullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0=
;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">Many =
open-source frameworks (LangChain, etc.) also support this or similar agent=
 capabilities.</p></li></ul></li><li style=3D"margin: 8px 0 0 32px;mso-spec=
ial-format: bullet;"><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);li=
ne-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;f=
ont-size: 16px;"><strong><a href=3D"https://substack.com/redirect/6c5ba8cf-=
7f4a-481b-8d88-421da858fb61?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7=
BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,55);text-decoratio=
n: underline;">Zep Memory</a></strong><span>: Zep is an open-source LLM mem=
ory store. It=E2=80=99s designed to plug into chatbots to store conversatio=
n history with embeddings, and serve summaries or relevant pieces back.</sp=
an></p><ul style=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0=
 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-he=
ight: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-s=
ize: 16px;margin: 0;">It automatically updates a summary of the conversatio=
n as it grows, and can fetch historically relevant messages (like if the us=
er asks something related to what was said 50 turns ago, it might bring tha=
t back).</p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bull=
et;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;bo=
x-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><span>Th=
is directly helps </span><em>context budget and retrieval</em><span> in dia=
logues. Instead of you coding memory logic, Zep handles it.</span></p></li>=
<li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"=
color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-=
box;padding-left: 4px;font-size: 16px;margin: 0;">Useful if you want a pers=
istent long-term memory that survives beyond one session (it can store vect=
or embeddings of all conversation chunks).</p></li></ul></li><li style=3D"m=
argin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"margin: 0 0 20=
px 0;color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: bo=
rder-box;padding-left: 4px;font-size: 16px;"><strong><a href=3D"https://sub=
stack.com/redirect/0c009758-68c2-4855-8243-7d0e03d945c8?j=3DeyJ1IjoiNWtiOTN=
6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: =
rgb(54,55,55);text-decoration: underline;">Pinecone</a><span> / </span><a h=
ref=3D"https://substack.com/redirect/a4084310-5982-41c6-af86-49e486f08ff3?j=
=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"=
" style=3D"color: rgb(54,55,55);text-decoration: underline;">Weaviate</a><s=
pan> / </span><a href=3D"https://substack.com/redirect/f8a88eea-984b-4c1c-b=
ef0-02f85ae30742?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4t=
AjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,55);text-decoration: underlin=
e;">Chroma</a><span> (Vector DBs):</span></strong><span> These are vector d=
atabases that store embeddings and let you query by similarity. They are al=
most a must-have for implementing RAG at scale.</span></p><ul style=3D"marg=
in-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-special-format=
: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom=
: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">Th=
ey handle large volumes, indexing, and fast search =E2=80=93 so for tens of=
 thousands of docs, you can retrieve in milliseconds the top relevant bits.=
</p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p =
style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing=
: border-box;padding-left: 4px;font-size: 16px;margin: 0;">They often have =
metadata filters (e.g., only search docs of type X or date range Y).</p></l=
i><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=
=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: bor=
der-box;padding-left: 4px;font-size: 16px;margin: 0;">Many of them have dem=
os or templates for pairing with LLMs.</p></li><li style=3D"margin: 8px 0 0=
 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-he=
ight: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-s=
ize: 16px;margin: 0;">If you use OpenAI or Azure, you also have the option =
of their built-in vector stores (e.g., Cognitive Search with embeddings, or=
 Pinecone hosted).</p></li><li style=3D"margin: 8px 0 0 32px;mso-special-fo=
rmat: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bo=
ttom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;=
"><em>Using effectively:</em><span> ensure you embed queries and docs with =
the same model and a good dimensionality. Monitor the similarity scores and=
 experiment with chunk sizes for indexing (too large chunk might reduce pre=
cision, too small might lose context).</span></p></li><li style=3D"margin: =
8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);=
line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px=
;font-size: 16px;margin: 0;">Weaviate and others even have modules for spec=
ific data types (images, etc.) but for text, basic ones suffice.</p></li></=
ul></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p s=
tyle=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;margin-bo=
ttom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;"><strong>=
Prompt Templates and Libraries:</strong><span> Tools like </span><a href=3D=
"https://substack.com/redirect/d411a9ac-31f3-4e8e-84aa-83eef316f902?j=3DeyJ=
1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" styl=
e=3D"color: rgb(54,55,55);text-decoration: underline;">Promptify</a><span>,=
 Guidance (by Microsoft), etc., allow more programmatic control over prompt=
s.</span></p><ul style=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8=
px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);l=
ine-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;=
font-size: 16px;margin: 0;">Guidance can let you write a prompt with contro=
l flow (like loops, conditionals) which can be useful to implement things l=
ike =E2=80=9Cfor each chunk, do X=E2=80=9D within one prompt if using an LL=
M that way.</p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: b=
ullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0=
;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">These=
 are more advanced and sometimes it=E2=80=99s easier to just script logic i=
n Python than to manage a single complex prompt, but they=E2=80=99re worth =
knowing.</p></li></ul></li><li style=3D"margin: 8px 0 0 32px;mso-special-fo=
rmat: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bo=
ttom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;=
"><strong><a href=3D"https://substack.com/redirect/351f4f88-9a00-4d77-be44-=
99f3453dfdbb?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMR=
MPOw0" rel=3D"" style=3D"color: rgb(54,55,55);text-decoration: underline;">=
Zilliz</a><span> / </span><a href=3D"https://substack.com/redirect/32d10319=
-0c8a-412f-8a6e-0f513b7a0328?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC=
7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,55);text-decorati=
on: underline;">Milvus</a><span>:</span></strong><span> These are vector DB=
 options (Milvus is open source, Zilliz is cloud) similar to Pinecone etc. =
They=E2=80=99re quite scalable if you need enterprise scale.</span></p></li=
><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D=
"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border=
-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong><a href=3D"https=
://substack.com/redirect/c8003d2f-fd10-46bb-b80f-188b6b2553bf?j=3DeyJ1IjoiN=
WtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"c=
olor: rgb(54,55,55);text-decoration: underline;">Hugging Face Transformer</=
a><span>s / Text Generation Inference:</span></strong><span> If you=E2=80=
=99re using open-source models, the HF ecosystem has utilities like transfo=
rmers pipeline, and text-generation-inference server which handle large con=
text (with some optimizations like streaming, etc.). You might need to adju=
st settings like max_length (for context) carefully.</span></p></li></ul><h=
3 class=3D"header-anchor-post" style=3D"position: relative;font-family: 'SF=
 Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystem=
Font,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Sego=
e UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoothing: ant=
ialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearance: optimizel=
egibility;-moz-appearance: optimizelegibility;appearance: optimizelegibilit=
y;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-siz=
e: 1.375em;"><strong>Monitoring and Evaluation Tools for Context</strong></=
h3><ul style=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32p=
x;mso-special-format: bullet;"><p style=3D"margin: 0 0 20px 0;color: rgb(54=
,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-l=
eft: 4px;font-size: 16px;"><strong><a href=3D"https://substack.com/redirect=
/b10d3f53-0636-4a30-ae66-c4d9a4e77ab7?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQ=
Jf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,55);text=
-decoration: underline;">Arize Phoenix</a></strong><span>: Mentioned earlie=
r, Arize=E2=80=99s Phoenix is an LLM observability tool. It can trace and e=
valuate model performance.</span></p><ul style=3D"margin-top: 0;padding: 0;=
"><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=
=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: bor=
der-box;padding-left: 4px;font-size: 16px;margin: 0;">It allows you to see =
where in the input the model is focusing (via attention heatmaps) or to run=
 the lost-in-middle evaluation by perturbing input.</p></li><li style=3D"ma=
rgin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,5=
5,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-lef=
t: 4px;font-size: 16px;margin: 0;">You could use it to test your system: fe=
ed a known input with some salient info in the middle and see if the model =
picks it up. Phoenix would help identify such failure modes.</p></li><li st=
yle=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color:=
 rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;pa=
dding-left: 4px;font-size: 16px;margin: 0;">Also, Arize has error analysis;=
 e.g., you can log your LLM=E2=80=99s responses and use their tools to spot=
 if longer inputs correlate with more errors =E2=80=93 a way to quantify ef=
fective context.</p></li></ul></li><li style=3D"margin: 8px 0 0 32px;mso-sp=
ecial-format: bullet;"><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);=
line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px=
;font-size: 16px;"><strong>LangSmith (by LangChain):</strong><span> A newer=
 tool to trace LLM interactions. It logs each step of a chain or agent. Thi=
s is useful to debug how your context strategies are working in multi-step =
flows (e.g., did the agent actually call the search tool with a good query?=
 Did the summarizer step produce something too lossy?).</span></p><ul style=
=3D"margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-specia=
l-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margi=
n-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin=
: 0;">It=E2=80=99s more devtool than monitoring for production, but helps r=
efine your prompt chains.</p></li></ul></li><li style=3D"margin: 8px 0 0 32=
px;mso-special-format: bullet;"><p style=3D"margin: 0 0 20px 0;color: rgb(5=
4,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-=
left: 4px;font-size: 16px;"><strong>OpenAI Eval Harness:</strong><span> Ope=
nAI has an evals framework (and others in community) where you can script e=
valuations for your prompt flows. For example, you can create test cases to=
 ensure that when info is at various positions, the model still finds it. R=
unning those regularly can catch regressions if you change prompts or updat=
e models.</span></p><ul style=3D"margin-top: 0;padding: 0;"><li style=3D"ma=
rgin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,5=
5,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-lef=
t: 4px;font-size: 16px;margin: 0;">This is more for offline testing but is =
crucial for building trust that your context management works.</p></li></ul=
></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p sty=
le=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;margin-bott=
om: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;"><strong>Cu=
stom Logging &amp; Alerts:</strong><span> Don=E2=80=99t underestimate rolli=
ng your own simple monitors: e.g., log the length of each prompt and respon=
se, and perhaps the portion of context that was actually used (if model end=
s up quoting from context or not). If you see responses like =E2=80=9CI can=
not find X=E2=80=9D but X was indeed in the context (just buried), that=E2=
=80=99s a flag your method failed =E2=80=93 you could set up alerts or trig=
gers for that kind of scenario in logs.</span></p><ul style=3D"margin-top: =
0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet=
;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-=
sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">Another tr=
ick: plant a hidden marker in each chunk of context (like a unique token) a=
nd see if the model ever mentions or uses it. If some marker never appears =
or affects output across many runs, maybe that chunk is consistently ignore=
d (could be fine if irrelevant, or sign of lost-mid).</p></li></ul></li></u=
l><h3 class=3D"header-anchor-post" style=3D"position: relative;font-family:=
 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSy=
stemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','=
Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoothing:=
 antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearance: optim=
izelegibility;-moz-appearance: optimizelegibility;appearance: optimizelegib=
ility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font=
-size: 1.375em;"><strong>Alternatives and When to Use Them</strong></h3><p =
style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-siz=
e: 16px;"><span>We=E2=80=99ve focused on using long contexts effectively, b=
ut sometimes </span><strong>the best solution is to not use long context at=
 all</strong><span> for certain parts:</span></p><ul style=3D"margin-top: 0=
;padding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;=
"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-s=
izing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Vec=
tor DB + Smaller Model vs Giant Context Model:</strong><span> As one experi=
ment cited, a 1.1B model with RAG outperformed GPT-4 Turbo on fact search. =
If your use case is straightforward QA on a static knowledge base, consider=
 using a moderate LLM (maybe open-source) with a good retrieval system, ins=
tead of paying for super long context on GPT-4. The cost can be lower and p=
erformance surprisingly good since the retrieval does heavy lifting.</span>=
</p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p =
style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing=
: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Traditio=
nal Search/Database:</strong><span> For data like logs, code search, FAQs =
=E2=80=93 sometimes a good old keyword search or SQL query can get the need=
ed info faster than embedding search or feeding everything to an LLM. Don=
=E2=80=99t force the model to read a big table to find a max value =E2=80=
=93 you can query that from a database and just show the model the result. =
Use the right tool for the job and use LLM for what it=E2=80=99s best at (l=
anguage understanding, summarizing, reasoning).</span></p></li><li style=3D=
"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(5=
4,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-=
left: 4px;font-size: 16px;margin: 0;"><strong>Fine-tuning or Smaller Specia=
lized Models:</strong><span> If you repeatedly need to do a specific compre=
ssion (like always summarizing legal docs in a format), a small fine-tuned =
model might do that in one step with less context (because it=E2=80=99s lea=
rned to pick out important parts). Fine-tuning extends =E2=80=9Ceffective=
=E2=80=9D context by teaching the model to ignore irrelevant middle parts.<=
/span></p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet=
;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-=
sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>St=
ructured approaches (graphs, ontologies):</strong><span> In something like =
medical or even legal, you could store info in a structured form (like a kn=
owledge graph of patient conditions, or contract obligation matrix). Then q=
ueries can be answered by traversing that and using LLM just to phrase the =
answer. That reduces reliance on long text prompts.</span></p></li></ul><p =
style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-siz=
e: 16px;"><span>The theme is: </span><em>Integrate LLMs with the rest of yo=
ur stack.</em><span> Don=E2=80=99t treat them as magical oracles that must =
ingest everything raw. Use databases, search engines, caching. There are em=
erging libraries that help with this orchestration:</span></p><ul style=3D"=
margin-top: 0;padding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-special-fo=
rmat: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bo=
ttom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;=
"><strong>LlamaIndex</strong><span> (again) can integrate with SQL: e.g., i=
f data is better fetched via SQL query, you can allow the LLM to do that ra=
ther than scanning a text dump. This is available through tools like the SQ=
LDatabaseAgent in LangChain or similar in LlamaIndex, where the model can d=
ecide to pull data from a DB.</span></p></li><li style=3D"margin: 8px 0 0 3=
2px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-heig=
ht: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-siz=
e: 16px;margin: 0;"><strong>Memory Augmented Chains</strong><span>: like th=
e Reflexion idea (model writes important info to its =E2=80=9Cnotes=E2=80=
=9D and reads later). Some frameworks implement prototypes of this. Could b=
e relevant if you want the model to create and refer to an external scratch=
pad file or notes.</span></p></li></ul><p style=3D"margin: 0 0 20px 0;color=
: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>Technical Notes=
 (Callouts):</strong></p><ul style=3D"margin-top: 0;padding: 0;"><li style=
=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rg=
b(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;paddi=
ng-left: 4px;font-size: 16px;margin: 0;"><em>Tokenization differences:</em>=
<span> Remember that models have different tokenization. 100K tokens for GP=
T-4 might be ~80K words, but for a different model 100K tokens could be mor=
e or fewer words. When planning chunk sizes, always measure with the tokeni=
zer of your target model (OpenAI=E2=80=99s tiktoken can mimic GPT-4 tokeniz=
ation).</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format=
: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom=
: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><e=
m>Batching calls:</em><span> If chunking leads to many parallel calls (say =
splitting a doc into 20 chunks and summarizing each), watch your API rate l=
imits. You might need to batch or throttle. Some libraries can batch prompt=
s for you to maximize throughput.</span></p></li><li style=3D"margin: 8px 0=
 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-=
height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font=
-size: 16px;margin: 0;"><em>Model version drift:</em><span> A prompt that w=
orks on GPT-4-0314 might behave differently on GPT-4-0613. So if you rely o=
n position or certain formatting, retest when models update. E.g., some los=
t-in-middle quirks might improve over time (OpenAI claims GPT-4.1 was speci=
fically trained to handle full 1M better , albeit still with drop at extrem=
e). So adapt your strategies as models get better, but never assume the hyp=
e is fully realized without testing.</span></p></li><li style=3D"margin: 8p=
x 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);li=
ne-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;f=
ont-size: 16px;margin: 0;"><em>Open-source models context:</em><span> Many =
open ones (Llama 2 etc.) have context limits like 4K or 16K. There are proj=
ects extending them (e.g., 100K Llama via finetuning positional embeddings)=
. If you use those, verify the effective context. Some of these extended mo=
dels might still effectively only do much less. Always validate with a lost=
-in-middle style test for any model you choose.</span></p></li></ul><p styl=
e=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 1=
6px;"><span>Using these tools and approaches can significantly reduce the e=
ngineering burden. Instead of writing your own chunk-splitting logic or vec=
tor search, you plug one of these in. Just be sure to </span><strong>evalua=
te them on your data</strong><span> =E2=80=93 sometimes default settings ar=
en=E2=80=99t optimal, and you=E2=80=99ll tweak chunk size or prompt templat=
es.</span></p><h2 class=3D"header-anchor-post" style=3D"position: relative;=
font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-syste=
m,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Co=
lor Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-fon=
t-smoothing: antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appea=
rance: optimizelegibility;-moz-appearance: optimizelegibility;appearance: o=
ptimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height:=
 1.16em;font-size: 1.625em;"><strong>What=E2=80=99s Coming Next (And What=
=E2=80=99s Just Hype)</strong></h2><p style=3D"margin: 0 0 20px 0;color: rg=
b(54,55,55);line-height: 26px;font-size: 16px;">It=E2=80=99s 2025; we=E2=80=
=99ve seen context windows explode this past year. What=E2=80=99s next in t=
he pipeline, and which claims should you view skeptically?</p><p style=3D"m=
argin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">=
<strong>Emerging Solutions:</strong></p><ul style=3D"margin-top: 0;padding:=
 0;"><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p styl=
e=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: bo=
rder-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Truly Longer=
 Context Models:</strong><span> Google=E2=80=99s Gemini 2.5 Pro (1M) and ta=
lk of 2M context in the next version. OpenAI=E2=80=99s GPT-4.1 series at 1M=
. These indicate a push towards million+ token contexts being mainstream. W=
e might see </span><strong>OpenAI GPT-5</strong><span> or Anthropic=E2=80=
=99s Claude-5 boasting even more (maybe 10M? Who knows). DeepMind=E2=80=99s=
 work hints at multi-million context (they did a 2M context demo).</span></=
p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p st=
yle=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;margin-bot=
tom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;"><strong>A=
rchitectural Changes:</strong><span> Researchers are tackling the fundament=
al limits. For example:</span></p><ul style=3D"margin-top: 0;padding: 0;"><=
li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"c=
olor: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-b=
ox;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Efficient Attentio=
n</strong><span> (FlashAttention, etc.) =E2=80=93 these don=E2=80=99t chang=
e quality per se but allow models to handle longer inputs faster. This can =
reduce cost a bit. By 2025, many models employ FlashAttention under the hoo=
d for speed, but they still have the =E2=80=9Clost middle=E2=80=9D issue be=
cause that=E2=80=99s a conceptual issue, not just speed.</span></p></li><li=
 style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"col=
or: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box=
;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Segmented or Hierarc=
hical Models:</strong><span> E.g., models that first summarize chunks inter=
nally and then combine (basically building summary chain into the architect=
ure). These could use something like </span><strong>Transformer-XL</strong>=
<span> or </span><strong>Retentive Networks</strong><span> where there=E2=
=80=99s a notion of state or recurrence, so they don=E2=80=99t attend fully=
 to far past tokens but have a kind of memory. If those take off, effective=
 context might improve. There=E2=80=99s research like </span><strong>Recurr=
ent GPT</strong><span> that aim to let models process streams indefinitely =
by compressing state. Not production-ready yet, but keep an eye.</span></p>=
</li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p styl=
e=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: bo=
rder-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Retrieval-Au=
gmented Generation (RAG) built-in:</strong><span> Expect models that come w=
ith built-in retrieval mechanisms. E.g., </span><strong>DeepMind=E2=80=99s =
retro</strong><span> (from 2022) used a database for factual info. OpenAI m=
ight integrate a vector search in future models so that you give it an API =
to fetch relevant text instead of expanding context. This blurs context win=
dow concept, but effectively it=E2=80=99s infinite context via retrieval. I=
f that happens, it might solve many issues as the model learns to pull only=
 what it needs.</span></p></li></ul></li><li style=3D"margin: 8px 0 0 32px;=
mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: =
26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 1=
6px;margin: 0;"><strong>Better Positional Embeddings:</strong><span> The </=
span><strong>rotary embeddings scaling</strong><span> trick took us this fa=
r, but new research like </span><a href=3D"https://substack.com/redirect/d2=
6da626-fa79-4f2d-9c13-b8c6a0dc7248?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9=
hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,55);text-de=
coration: underline;">that </a><strong><a href=3D"https://substack.com/redi=
rect/d26da626-fa79-4f2d-9c13-b8c6a0dc7248?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFx=
PmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"color: rgb(54,55,55);=
text-decoration: underline;">SHIFT (StRING)</a></strong><a href=3D"https://=
substack.com/redirect/d26da626-fa79-4f2d-9c13-b8c6a0dc7248?j=3DeyJ1IjoiNWti=
OTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"colo=
r: rgb(54,55,55);text-decoration: underline;"> paper</a><span> suggests we =
can train models to use long positions more uniformly. Perhaps new models (=
like Llama 4 or others) will incorporate such techniques to have more </spa=
n><em>uniform attention</em><span> across the window. That could mitigate l=
ost-in-middle somewhat, making effective context closer to actual context.<=
/span></p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet=
;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-=
sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Me=
mory and Tool Use:</strong><span> There=E2=80=99s a lot of work on making L=
LMs more </span><strong>agentic</strong><span> =E2=80=93 e.g., automaticall=
y storing info in a =E2=80=9Cmemory=E2=80=9D (external) when context is ful=
l, and retrieving it later as needed. Projects like </span><strong>BabyAGI,=
 AutoGPT</strong><span> etc., are not really new, but they signal interest =
in LLMs that can self-manage context by writing notes. We might see product=
s that implement robust long-term memory via vector DB behind the scenes, g=
iving illusion of huge context.</span></p></li><li style=3D"margin: 8px 0 0=
 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-he=
ight: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-s=
ize: 16px;margin: 0;"><strong>Open-Source Developments:</strong><span> If h=
istory repeats, open models will catch up on context length. E.g., maybe Ll=
ama 5 will target 500K of effective working memory. Also models like Mistra=
l or others might push long context as a differentiator (like Anthropic did=
). There=E2=80=99s also techniques to fine-tune or patch open models to ext=
end context without full retraining (some success with Llama 2 extended to =
32K by fine-tuning position encodings). Expect the community to keep pushin=
g those boundaries, though as we know, length alone doesn=E2=80=99t equal e=
ffectiveness.</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-special-=
format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-=
bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: =
0;"><strong>Expanded Multimodal Context:</strong><span> Context is already =
not just text. Models accept images, audio, etc., along with text. That=E2=
=80=99s already started but it needs to expand to include more video. Tools=
 that do </span><em>multimodal compression</em><span> will be part of the t=
oolset (e.g., summarizing a video transcript to feed into a text model).</s=
pan></p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"=
><p style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-si=
zing: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Hard=
ware Advances:</strong><span> There are hardware innovations (like more GPU=
 memory, faster interconnects) that could make large context less slow or c=
ostly, thus more feasible to always use. But they don=E2=80=99t fix the cor=
e forgetting issue =E2=80=93 they just remove some performance penalty. Sti=
ll, if inference cost drops, some might choose brute force usage of context=
 (and maybe just over-sample context to hide lost info =E2=80=93 like repea=
t important parts multiple times in input, which some have tried).</span></=
p></li></ul><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height=
: 26px;font-size: 16px;"><strong>Realistic Timeline:</strong><span> Over th=
e next 1-2 years, I suspect we=E2=80=99ll get marginal improvements in </sp=
an><em>effective</em><span> long context but not a miraculous fix. Maybe GP=
T-5 will be </span><em>somewhat</em><span> better at long contexts through =
more training or architecture tweaks, but not perfect memory. It=E2=80=99s =
likely the vendor marketing will continue to outpace reality, as we saw wit=
h Gemini boasting 1M but devs finding it effectively ~128K. So, remain heal=
thily skeptical.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,=
55);line-height: 26px;font-size: 16px;"><strong>What=E2=80=99s likely to re=
main a problem:</strong></p><ul style=3D"margin-top: 0;padding: 0;"><li sty=
le=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: =
rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;pad=
ding-left: 4px;font-size: 16px;margin: 0;"><strong>Lost-in-the-middle is no=
t fully solved</strong><span> by just scaling context. Without fundamentall=
y new architectures, the U-shaped performance issue will persist. Vendors m=
ight not talk about it, but independent evals (like that Stanford paper ) w=
ill keep showing it.</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-s=
pecial-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;=
margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;m=
argin: 0;"><strong>Cost will still scale with length.</strong><span> Even i=
f per-token cost drops, feeding 1M tokens will always cost more than 1K. So=
 the CFO=E2=80=99s shock might lessen per call, but high volume usage of me=
ga contexts will still need budget considerations. If anything, as context =
capacity grows, people will try to stuff more, possibly raising costs again=
 in aggregate.</span></p></li><li style=3D"margin: 8px 0 0 32px;mso-special=
-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: 26px;margin=
-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin:=
 0;"><strong>Model hallucination and quality dips</strong><span> on long in=
puts: More context means more chances to include conflicting or irrelevant =
info that confuses the model. That fundamental challenge =E2=80=93 discerni=
ng what=E2=80=99s relevant among a huge context =E2=80=93 is not solved by =
bigger memory alone; it=E2=80=99s a reasoning challenge. The model might st=
ill latch onto a random detail and base answer on it erroneously.</span></p=
></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p sty=
le=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: b=
order-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Human facto=
rs:</strong><span> Long contexts can lull you into not curating inputs. If =
someone thinks =E2=80=9CI have 500K tokens, I=E2=80=99ll just toss in every=
thing=E2=80=9D, the answers might degrade as the model drowns, and it=E2=80=
=99s harder for a human to even verify since the prompt is huge. So, the pr=
actice of intelligently providing context will remain important (garbage in=
, garbage out still applies).</span></p></li></ul><p style=3D"margin: 0 0 2=
0px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>Wort=
h Watching (Emerging solutions):</strong></p><ul style=3D"margin-top: 0;pad=
ding: 0;"><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p=
 style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizin=
g: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Retriev=
al-augmented models</strong><span>: As mentioned, any news of models that i=
ntegrate search or memory in architecture is promising.</span></p></li><li =
style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"colo=
r: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;=
padding-left: 4px;font-size: 16px;margin: 0;"><strong>Continual learning or=
 adaptive models</strong><span>: There=E2=80=99s an idea of models that can=
 update their weights or knowledge incrementally (instead of static trainin=
g). If that happens, you might not need long prompts =E2=80=93 the model co=
uld =E2=80=9Clearn=E2=80=9D your data. But short term, most will use retrie=
val as the form of memory.</span></p></li><li style=3D"margin: 8px 0 0 32px=
;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height:=
 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: =
16px;margin: 0;"><strong>Tool ecosystems</strong><span>: With function call=
ing, the trend might be to break tasks into sub-tasks. For example, rather =
than one giant prompt, the model of 2026 might routinely do a sequence: sea=
rch -&gt; read -&gt; calculate -&gt; answer. So you=E2=80=99ll orchestrate =
less manually as the model does it internally or via an agent framework. Ke=
ep an eye on frameworks like OpenAI=E2=80=99s =E2=80=9CAutonomous Agents=E2=
=80=9D if they release official ones, or on Microsoft=E2=80=99s Jarvis (whi=
ch chains model with tools).</span></p></li><li style=3D"margin: 8px 0 0 32=
px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-heigh=
t: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size=
: 16px;margin: 0;"><strong>Quality metrics</strong><span>: We might see bet=
ter ways to measure if a model effectively used the context. For instance, =
automated checks or model self-diagnostics (=E2=80=9CI might have missed so=
mething above=E2=80=9D). Some research is on calibrating LLM confidence wit=
h respect to provided context. If that works, a model might say, =E2=80=9CI=
=E2=80=99m not certain I saw info about X in the above; maybe provide more =
details.=E2=80=9D That could be useful in production to know if more retrie=
val is needed.</span></p></li></ul><p style=3D"margin: 0 0 20px 0;color: rg=
b(54,55,55);line-height: 26px;font-size: 16px;"><strong>Hype to be cautious=
 about:</strong></p><ul style=3D"margin-top: 0;padding: 0;"><li style=3D"ma=
rgin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,5=
5,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-lef=
t: 4px;font-size: 16px;margin: 0;"><strong>=E2=80=9CWe have 1M context, so =
you don=E2=80=99t need RAG anymore.=E2=80=9D</strong><span> Don=E2=80=99t b=
uy it, as we thoroughly explored. Databricks=E2=80=99 study even titled abo=
ut whether long context will subsume RAG found that most models still dippe=
d beyond certain lengths. It concluded long context </span><em>complements<=
/em><span> retrieval, doesn=E2=80=99t replace it.</span></p></li><li style=
=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color: rg=
b(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;paddi=
ng-left: 4px;font-size: 16px;margin: 0;"><strong>=E2=80=9CLoad your entire =
data warehouse into the prompt.=E2=80=9D</strong><span> Some vendors might =
pitch that their model can consume tons of data directly, making databases =
obsolete. In reality, for any structured large data, a combination of datab=
ase + LLM is far superior. So hype that says =E2=80=9CLLM will just replace=
 your SQL analytics by reading everything in one go=E2=80=9D =E2=80=93 extr=
emely impractical and error-prone.</span></p></li><li style=3D"margin: 8px =
0 0 32px;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line=
-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;fon=
t-size: 16px;margin: 0;"><strong>=E2=80=9COur model never forgets.=E2=80=9D=
</strong><span> If you hear that, ask for proof. It=E2=80=99s a red flag as=
 of 2025. At best, some smaller domain-specific models fine-tuned on long s=
equences might do better (like a code model fine-tuned to use a whole code =
file could perhaps utilize more of it than a general model). But =E2=80=9Cn=
ever forgets=E2=80=9D is not realistic given how attention works.</span></p=
></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p sty=
le=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: b=
order-box;padding-left: 4px;font-size: 16px;margin: 0;"><strong>=E2=80=9CJu=
st increase the tokens.=E2=80=9D</strong><span> Some might think if lost-mi=
d at 100K, maybe at 1M it just pushes the problem further out. But effectiv=
e utilization % might even drop as window grows, if training doesn=E2=80=99=
t keep up. Without targeted training, giving models bigger windows could mo=
stly benefit at the extremes (like being able to do one prompt instead of m=
ultiple) but not improve mid usage.</span></p></li></ul><p style=3D"margin:=
 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><stron=
g>Investment Recommendations:</strong></p><ul style=3D"margin-top: 0;paddin=
g: 0;"><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p st=
yle=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: =
border-box;padding-left: 4px;font-size: 16px;margin: 0;"><span>Invest in </=
span><strong>RAG infrastructure</strong><span>: It=E2=80=99s likely to rema=
in valuable if properly deployed and designed. Build or buy a good vector d=
atabase, set up pipelines to keep it updated with your latest data. This wi=
ll serve you regardless of context window expansions.</span></p></li><li st=
yle=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"color:=
 rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;pa=
dding-left: 4px;font-size: 16px;margin: 0;"><span>Invest in </span><strong>=
evaluation and monitoring</strong><span>: as discussed, knowing how your LL=
M apps handle context now will let you gauge when a new model is actually a=
n improvement or not. If GPT-5 comes and claims some breakthrough, you=E2=
=80=99ll have baseline tests to verify those claims on your workload.</span=
></p></li><li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p=
 style=3D"color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizin=
g: border-box;padding-left: 4px;font-size: 16px;margin: 0;"><span>Keep a po=
rtion of budget for </span><strong>experimenting</strong><span> with new co=
ntext features, but don=E2=80=99t commit fully until proven. For example, m=
aybe try a project on GPT-4.1 1M to see how it works, but don=E2=80=99t ref=
actor everything to rely on it until it=E2=80=99s stable and you have mitig=
ations for its weaknesses.</span></p></li><li style=3D"margin: 8px 0 0 32px=
;mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height:=
 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: =
16px;margin: 0;"><strong>Training and fine-tuning</strong><span>: Consider =
fine-tuning smaller models on your domain data to encapsulate more context.=
 This is an investment in reducing dependency on huge contexts. For instanc=
e, fine-tune a model to summarize your reports in one shot =E2=80=93 then y=
ou don=E2=80=99t need to feed entire reports at inference.</span></p></li><=
li style=3D"margin: 8px 0 0 32px;mso-special-format: bullet;"><p style=3D"c=
olor: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-b=
ox;padding-left: 4px;font-size: 16px;margin: 0;"><strong>Hybrid approaches<=
/strong><span>: invest in team skills or tech that combine symbolic/algorit=
hmic methods with LLMs (like knowledge graphs, search, logic rules). Those =
will help in domains where raw LLM performance might stall. It=E2=80=99s cl=
ear that pure end-to-end LLM reasoning on long text has limits; hybrid syst=
ems often win in benchmarks (e.g., retrieval + LLM was stronger than just f=
eeding text on QA tasks).</span></p></li><li style=3D"margin: 8px 0 0 32px;=
mso-special-format: bullet;"><p style=3D"color: rgb(54,55,55);line-height: =
26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 1=
6px;margin: 0;"><span>Keep an eye on </span><strong>open-source</strong><sp=
an>: The gap between open and closed is still large for some capabilities, =
but open models are catching up fast in context and I expect the new OpenAI=
 open source model to be a step in this direction. Investing time in an ope=
n solution might pay off in cost savings long run, especially if you can fi=
ne-tune it to your needs and not worry about sending sensitive data to a th=
ird party. But factor in the engineering effort and maybe the model quality=
 gap.</span></p></li></ul><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,5=
5);line-height: 26px;font-size: 16px;"><strong>Bottom Line:</strong><span> =
Long contexts will get longer, but you=E2=80=99ll likely </span><em>always<=
/em><span> need strategies to handle them smartly. The truth is, the silver=
 bullet of =E2=80=9Cjust throw it all in=E2=80=9D is not appearing magicall=
y. So the multi-strategy approach we=E2=80=99ve outlined will remain releva=
nt. Over time, some manual steps may be automated by improved models or age=
nts, but understanding these principles ensures you can adapt as the toolin=
g evolves.</span></p><h2 class=3D"header-anchor-post" style=3D"position: re=
lative;font-family: 'SF Pro Display',-apple-system-headline,system-ui,-appl=
e-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'A=
pple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-web=
kit-font-smoothing: antialiased;-moz-osx-font-smoothing: antialiased;-webki=
t-appearance: optimizelegibility;-moz-appearance: optimizelegibility;appear=
ance: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-=
height: 1.16em;font-size: 1.625em;"><strong>Wrapping it up</strong></h2><p =
style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-siz=
e: 16px;"><span>So here we are. You've just spent considerable time learnin=
g what the vendors didn't want you to know: that their shiny 1M context win=
dows are, functionally speaking, about as reliable as a chocolate teapot. Y=
ou've seen the evidence, understood the technical reasons, and most importa=
ntly, learned what to actually </span><em>do</em><span> about it.</span></p=
><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font=
-size: 16px;">Let's be honest about what just happened. You probably starte=
d reading this because something wasn't working. Maybe you watched your mod=
el forget crucial information from page 30 of that contract. Maybe you got =
the monthly API bill and had to explain to finance why each customer query =
costs more than a coffee at Starbucks. Or maybe you're just tired of your &=
quot;state-of-the-art&quot; AI assistant performing worse than an intern wh=
o actually read the documents.</p><p style=3D"margin: 0 0 20px 0;color: rgb=
(54,55,55);line-height: 26px;font-size: 16px;"><strong>The real insight her=
e isn't that long context is broken</strong><span> =E2=80=93 you probably s=
uspected that already. The real insight is that </span><em>this isn't going=
 to magically fix itself</em><span>. GPT-5 won't solve it. Claude-whatever =
won't solve it. Gemini-infinity won't solve it. Because it's not fundamenta=
lly a model capability problem =E2=80=93 it's an attention mechanics proble=
m, a training data problem, and honestly, a bit of a physics problem (that =
O(n=C2=B2) complexity isn't going anywhere).</span></p><p style=3D"margin: =
0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>B=
ut here's the thing: </span><strong>you don't need it to be fixed</strong><=
span>.</span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-h=
eight: 26px;font-size: 16px;"><span>Throughout this guide, we've seen over =
and over that the smart, structured approaches </span><em>outperform</em><s=
pan> the brute force &quot;dump everything&quot; method. That legal team us=
ing retrieval to find specific clauses? They're getting better results than=
 the team trying to feed entire contracts. That financial analyst who chunk=
s reports by section? More accurate than the one maxing out GPT-4's context=
. The healthcare system that maintains a careful context budget? It actuall=
y remembers patient allergies, unlike the one drowning in medical history.<=
/span></p><blockquote style=3D"border-left: 4px solid #45D800;margin: 20px =
0;padding: 0;"><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);margin-l=
eft: 20px;line-height: 26px;font-size: 16px;"><strong>I=E2=80=99ll say it o=
ne more time: What we're really talking about here is the difference betwee=
n </strong><em><strong>using</strong></em><strong> AI and </strong><em><str=
ong>architecting with</strong></em><strong> AI.</strong><span> </span><stro=
ng>Any fool can yeet 100K tokens into a prompt. It takes understanding to k=
now when to chunk, when to retrieve, when to summarize, and when to tell th=
e model exactly where to look. That's not a limitation =E2=80=93 that's cra=
ftsmanship.</strong></p></blockquote><p style=3D"margin: 0 0 20px 0;color: =
rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong>The vendors will =
keep playing their game</strong><span>. Next month, someone else will annou=
nce a 10M token model (Llama=E2=80=99s claim was always a bit sketchy). The=
 month after, 100M. Some hapless author will keep publishing those U-shaped=
 graphs showing performance tanking in the middle, buried on page 47 of pap=
ers no one reads. And the model makes will keep charging you for every toke=
n while the model quietly ignores most of them. Don=E2=80=99t be fooled. </=
span></p><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 2=
6px;font-size: 16px;"><strong>Build for the context window we have, not the=
 context window you wish existed. </strong><span>And then debate with me in=
 the comments about where you=E2=80=99re leaning on AGI now that you=E2=80=
=99ve digested all this context window context :)</span></p><div class=3D"s=
ubscription-widget-wrap" style=3D"font-size: 16px;line-height: 26px;"><div =
class=3D"subscription-widget show-subscribe" style=3D"font-size: 16px;direc=
tion: ltr !important;font-weight: 400;text-decoration: none;font-family: sy=
stem-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,=
sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';color: #3=
63737;line-height: 1.5;max-width: 560px;margin: 24px auto;align-items: flex=
-start;display: block;text-align: center;padding: 0px 32px;"><div class=3D"=
preamble" style=3D"margin-top: 16px;font-family: system-ui,-apple-system,Bl=
inkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color =
Emoji','Segoe UI Emoji','Segoe UI Symbol';font-size: 18px;max-width: 384px;=
width: fit-content;line-height: 22px;display: flex;align-items: center;text=
-align: center;font-weight: 400;margin-left: auto;margin-right: auto;"><p s=
tyle=3D"margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size=
: 16px;">For more on AI, subscribe and share!</p></div><div class=3D"subscr=
ibe-widget is-signed-up is-fully-subscribed" data-component-name=3D"Subscri=
beWidget" style=3D"margin: 0 0 1em;direction: ltr;font-size: 16px;line-heig=
ht: 26px;"><div class=3D"pencraft pc-reset button-wrapper" style=3D"text-de=
coration: unset;list-style: none;font-size: 16px;line-height: 26px;text-ali=
gn: center;cursor: pointer;border-radius: 4px;"><a class=3D"button subscrib=
e-btn outline" href=3D"https://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9u=
YXRlc25ld3NsZXR0ZXIuc3Vic3RhY2suY29tL2FjY291bnQiLCJwIjoxNjc2ODM1MjcsInMiOjE=
zNzMyMzEsImYiOmZhbHNlLCJ1IjozMzY0NDgyMjMsImlhdCI6MTc1MTg5MzQzNywiZXhwIjoyMD=
Y3NDY5NDM3LCJpc3MiOiJwdWItMCIsInN1YiI6ImxpbmstcmVkaXJlY3QifQ.qM4i62Bid2FVjL=
oLyP-LGtLmeE8_j9jmwfV30jTfl-Q?" style=3D"font-family: system-ui,-apple-syst=
em,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple C=
olor Emoji','Segoe UI Emoji','Segoe UI Symbol';display: inline-block;box-si=
zing: border-box;cursor: pointer;border-radius: 8px;font-size: 14px;line-he=
ight: 20px;font-weight: 600;text-align: center;background-color: transparen=
t;opacity: 1;outline: none;white-space: nowrap;text-decoration: none !impor=
tant;border: 1px solid #45d800;margin: 0 auto;background: transparent;color=
: #45d800;padding: 12px 20px;height: auto;"><img class=3D"check-icon static=
" src=3D"https://substackcdn.com/image/fetch/$s_!3t53!,w_40,c_scale,f_png,q=
_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideC=
heck%3Fv%3D4%26height%3D40%26fill%3Dtransparent%26stroke%3D%252345D800%26st=
rokeWidth%3D3.6" width=3D"20" height=3D"20" style=3D"border: none;vertical-=
align: middle;-ms-interpolation-mode: bicubic;height: auto;display: inline-=
block;margin: -2px 8px 0 0;max-width: 20px" alt=3D""><span style=3D"text-de=
coration: none;">Subscribed</span></a></div></div></div></div><div class=3D=
"captioned-image-container-static" style=3D"font-size: 16px;line-height: 26=
px;margin: 32px auto;"><figure style=3D"width: 100%;margin: 0 auto;"><table=
 class=3D"image-wrapper" width=3D"100%" border=3D"0" cellspacing=3D"0" cell=
padding=3D"0" data-component-name=3D"Image2ToDOMStatic" style=3D"mso-paddin=
g-alt: 1em 0 1.6em;"><tbody><tr><td style=3D"text-align: center;"></td><td =
class=3D"content" align=3D"left" width=3D"1024" style=3D"text-align: center=
;"><a class=3D"image-link" target=3D"_blank" href=3D"https://substack.com/r=
edirect/447dcda3-f731-476f-bb05-aabcb1507e99?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88=
YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=3D"" style=3D"position: relative=
;flex-direction: column;align-items: center;padding: 0;width: auto;height: =
auto;border: none;text-decoration: none;display: block;margin: 0;"><img cla=
ss=3D"wide-image" data-attrs=3D"{&quot;src&quot;:&quot;https://substack-pos=
t-media.s3.amazonaws.com/public/images/4f0e8a32-7d48-49bd-9b3e-884d39df90c4=
_1024x1536.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;=
:null,&quot;imageSize&quot;:null,&quot;height&quot;:1536,&quot;width&quot;:=
1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2979322,&quot;alt&quot;=
:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;h=
ref&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&qu=
ot;internalRedirect&quot;:&quot;https://natesnewsletter.substack.com/i/1676=
83527?img=3Dhttps%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fi=
mages%2F4f0e8a32-7d48-49bd-9b3e-884d39df90c4_1024x1536.png&quot;,&quot;isPr=
ocessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt=
=3D"" width=3D"550" height=3D"825" src=3D"https://substackcdn.com/image/fet=
ch/$s_!1zoV!,w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3=
A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4f0e8a32-7d=
48-49bd-9b3e-884d39df90c4_1024x1536.png" style=3D"border: none !important;v=
ertical-align: middle;display: block;-ms-interpolation-mode: bicubic;height=
: auto;margin-bottom: 0;width: auto !important;max-width: 100% !important;m=
argin: 0 auto;"></a></td><td style=3D"text-align: center;"></td></tr></tbod=
y></table></figure></div><p style=3D"margin: 0 0 20px 0;color: rgb(54,55,55=
);line-height: 26px;font-size: 16px;"></p><h4 class=3D"header-anchor-post" =
style=3D"position: relative;font-family: 'SF Pro Display',-apple-system-hea=
dline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetic=
a,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';f=
ont-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothin=
g: antialiased;-webkit-appearance: optimizelegibility;-moz-appearance: opti=
mizelegibility;appearance: optimizelegibility;margin: 1em 0 0.625em 0;color=
: rgb(54,55,55);line-height: 1.16em;font-size: 1.125em;margin-bottom: 0;"><=
a href=3D"https://substack.com/redirect/103e730c-47e4-42b7-b153-75d15e10ef5=
5?j=3DeyJ1IjoiNWtiOTN6In0.zdzy88YFxPmLQJf9hshgUC7BDnPG6RxQ4tAjMRMPOw0" rel=
=3D"" style=3D"color: rgb(54,55,55);text-decoration: underline;">A Google D=
oc with all the links for this article</a></h4></div></div><div class=3D"co=
ntainer-border" style=3D"margin: 32px 0 0;width: 100%;box-sizing: border-bo=
x;border-top: 1px solid #e6e6e6;font-size: 16px;line-height: 26px;"></div><=
div class=3D"post-cta typography markup" style=3D"--image-offset-margin: -1=
17px;text-align: initial;word-break: break-word;margin-bottom: 32px;margin:=
 32px 0;font-size: 16px;line-height: 26px;"><p class=3D"referrals-cta-text"=
 style=3D"color: rgb(54,55,55);text-align: center;width: 90%;line-height: 2=
6px;font-size: 16px;margin-top: 0;max-width: 384px;margin: auto"></p><h4 st=
yle=3D"font-family: 'SF Pro Display',-apple-system-headline,system-ui,-appl=
e-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'A=
pple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-web=
kit-font-smoothing: antialiased;-moz-osx-font-smoothing: antialiased;-webki=
t-appearance: optimizelegibility;-moz-appearance: optimizelegibility;appear=
ance: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-=
height: 1.16em;font-size: 1.125em;text-align: center">Invite your friends a=
nd earn rewards</h4><div style=3D"line-height: 26px;text-align: center;font=
-size: 14px">If you enjoy Nate=E2=80=99s Substack, share it with your frien=
ds and earn rewards when they subscribe.</div><p style=3D"color: rgb(54,55,=
55);margin: 0 auto 20px;text-align: center;width: 90%;line-height: 26px;fon=
t-size: 16px;"></p><p class=3D"cta-box" style=3D"color: rgb(54,55,55);margi=
n: 0 auto 20px;width: 90%;line-height: 26px;font-size: 16px;margin-bottom: =
0;text-align: center;margin-left: auto;margin-right: auto;"><a class=3D"but=
ton primary" role=3D"button" href=3D"https://substack.com/redirect/2/eyJlIj=
oiaHR0cHM6Ly9uYXRlc25ld3NsZXR0ZXIuc3Vic3RhY2suY29tL2xlYWRlcmJvYXJkP3JlZmVyc=
mVyX3Rva2VuPTVrYjkzeiZyPTVrYjkzeiZ1dG1fY2FtcGFpZ249ZW1haWwtbGVhZGVyYm9hcmQi=
LCJwIjoxNjc2ODM1MjcsInMiOjEzNzMyMzEsImYiOmZhbHNlLCJ1IjozMzY0NDgyMjMsImlhdCI=
6MTc1MTg5MzQzNywiZXhwIjoyMDY3NDY5NDM3LCJpc3MiOiJwdWItMCIsInN1YiI6Imxpbmstcm=
VkaXJlY3QifQ.sfC5ry85C8usH0837XTs-8lG2P1Kcnvd-AjZDO3TOtA?&amp;utm_source=3D=
substack&amp;utm_medium=3Demail&amp;utm_content=3Dpostcta" style=3D"font-fa=
mily: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetic=
a,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';d=
isplay: inline-block;box-sizing: border-box;cursor: pointer;border: none;he=
ight: 40px;border-radius: 8px;font-size: 14px;line-height: 20px;font-weight=
: 600;text-align: center;padding: 10px 20px;margin: 0;opacity: 1;outline: n=
one;white-space: nowrap;color: #ffffff !important;text-decoration: none !im=
portant;background-color: #45D800;">Invite Friends</a></p></div><table clas=
s=3D"email-ufi-2-bottom" role=3D"presentation" width=3D"100%" border=3D"0" =
cellspacing=3D"0" cellpadding=3D"0" style=3D"border-top: 1px solid rgb(0,0,=
0,.1);border-bottom: 1px solid rgb(0,0,0,.1);min-width: 100%;"><tbody><tr h=
eight=3D"16"><td height=3D"16" style=3D"font-size:0px;line-height:0;">&nbsp=
;</td></tr><tr><td><table role=3D"presentation" width=3D"100%" border=3D"0"=
 cellspacing=3D"0" cellpadding=3D"0"><tbody><tr><td><table role=3D"presenta=
tion" width=3D"auto" border=3D"0" cellspacing=3D"0" cellpadding=3D"0" style=
=3D"margin:0 auto;"><tbody><tr><td style=3D"vertical-align:middle;"><table =
role=3D"presentation" width=3D"auto" border=3D"0" cellspacing=3D"0" cellpad=
ding=3D"0"><tbody><tr><td align=3D"center"><a class=3D"email-button-outline=
" href=3D"https://substack.com/app-link/post?publication_id=3D1373231&amp;p=
ost_id=3D167683527&amp;utm_source=3Dsubstack&amp;isFreemail=3Dfalse&amp;sub=
mitLike=3Dtrue&amp;token=3DeyJ1c2VyX2lkIjozMzY0NDgyMjMsInBvc3RfaWQiOjE2NzY4=
MzUyNywicmVhY3Rpb24iOiLinaQiLCJpYXQiOjE3NTE4OTM0MzcsImV4cCI6MTc1NDQ4NTQzNyw=
iaXNzIjoicHViLTEzNzMyMzEiLCJzdWIiOiJyZWFjdGlvbiJ9.yLW4JhM5bqPAC46n988ZubrdT=
V8yZnfQ52TbAVsTW6o&amp;utm_medium=3Demail&amp;utm_campaign=3Demail-reaction=
&amp;r=3D5kb93z" style=3D"font-family: system-ui,-apple-system,BlinkMacSyst=
emFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Se=
goe UI Emoji','Segoe UI Symbol';display: inline-block;font-weight: 500;bord=
er: 1px solid rgb(0,0,0,.1);border-radius: 9999px;text-transform: uppercase=
;font-size: 12px;line-height: 12px;padding: 9px 14px;text-decoration: none;=
color: rgb(119,119,119);"><img class=3D"icon" src=3D"https://substackcdn.co=
m/image/fetch/$s_!PeVs!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep=
/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideHeart%3Fv%3D4%26height%3D36%26fi=
ll%3Dnone%26stroke%3D%2523808080%26strokeWidth%3D2" width=3D"18" height=3D"=
18" style=3D"margin-right: 8px;min-width: 18px;min-height: 18px;border: non=
e;vertical-align: middle;max-width: 18px" alt=3D""><span class=3D"email-but=
ton-text" style=3D"vertical-align: middle;">Like</span></a></td></tr></tbod=
y></table></td><td width=3D"8" style=3D"min-width: 8px"></td><td style=3D"v=
ertical-align:middle;"><table role=3D"presentation" width=3D"auto" border=
=3D"0" cellspacing=3D"0" cellpadding=3D"0"><tbody><tr><td align=3D"center">=
<a class=3D"email-button-outline" href=3D"https://substack.com/app-link/pos=
t?publication_id=3D1373231&amp;post_id=3D167683527&amp;utm_source=3Dsubstac=
k&amp;utm_medium=3Demail&amp;isFreemail=3Dfalse&amp;comments=3Dtrue&amp;tok=
en=3DeyJ1c2VyX2lkIjozMzY0NDgyMjMsInBvc3RfaWQiOjE2NzY4MzUyNywiaWF0IjoxNzUxOD=
kzNDM3LCJleHAiOjE3NTQ0ODU0MzcsImlzcyI6InB1Yi0xMzczMjMxIiwic3ViIjoicG9zdC1yZ=
WFjdGlvbiJ9.2T-NDydj6u-g0Zbh9fPG1hfXozby11SlYZEn9hn7Aho&amp;r=3D5kb93z&amp;=
utm_campaign=3Demail-half-magic-comments&amp;action=3Dpost-comment&amp;utm_=
source=3Dsubstack&amp;utm_medium=3Demail" style=3D"font-family: system-ui,-=
apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-seri=
f,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';display: inline-bl=
ock;font-weight: 500;border: 1px solid rgb(0,0,0,.1);border-radius: 9999px;=
text-transform: uppercase;font-size: 12px;line-height: 12px;padding: 9px 14=
px;text-decoration: none;color: rgb(119,119,119);"><img class=3D"icon" src=
=3D"https://substackcdn.com/image/fetch/$s_!x1tS!,w_36,c_scale,f_png,q_auto=
:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Ficon%2FLucideCommen=
ts%3Fv%3D4%26height%3D36%26fill%3Dnone%26stroke%3D%2523808080%26strokeWidth=
%3D2" width=3D"18" height=3D"18" style=3D"margin-right: 8px;min-width: 18px=
;min-height: 18px;border: none;vertical-align: middle;max-width: 18px" alt=
=3D""><span class=3D"email-button-text" style=3D"vertical-align: middle;">C=
omment</span></a></td></tr></tbody></table></td><td width=3D"8" style=3D"mi=
n-width: 8px"></td><td style=3D"vertical-align:middle;"><table role=3D"pres=
entation" width=3D"auto" border=3D"0" cellspacing=3D"0" cellpadding=3D"0"><=
tbody><tr><td align=3D"center"><a class=3D"email-button-outline" href=3D"ht=
tps://substack.com/redirect/2/eyJlIjoiaHR0cHM6Ly9vcGVuLnN1YnN0YWNrLmNvbS9wd=
WIvbmF0ZXNuZXdzbGV0dGVyL3AvY29udGV4dC13aW5kb3dzLWFyZS1hLWxpZS10aGUtbXl0aD91=
dG1fc291cmNlPXN1YnN0YWNrJnV0bV9tZWRpdW09ZW1haWwmdXRtX2NhbXBhaWduPWVtYWlsLXJ=
lc3RhY2stY29tbWVudCZhY3Rpb249cmVzdGFjay1jb21tZW50JnI9NWtiOTN6JnRva2VuPWV5Sj=
FjMlZ5WDJsa0lqb3pNelkwTkRneU1qTXNJbkJ2YzNSZmFXUWlPakUyTnpZNE16VXlOeXdpYVdGM=
Elqb3hOelV4T0Rrek5ETTNMQ0psZUhBaU9qRTNOVFEwT0RVME16Y3NJbWx6Y3lJNkluQjFZaTB4=
TXpjek1qTXhJaXdpYzNWaUlqb2ljRzl6ZEMxeVpXRmpkR2x2YmlKOS4yVC1ORHlkajZ1LWcwWmJ=
oOWZQRzFoZlhvemJ5MTFTbFlaRW45aG43QWhvIiwicCI6MTY3NjgzNTI3LCJzIjoxMzczMjMxLC=
JmIjpmYWxzZSwidSI6MzM2NDQ4MjIzLCJpYXQiOjE3NTE4OTM0MzcsImV4cCI6MjA2NzQ2OTQzN=
ywiaXNzIjoicHViLTAiLCJzdWIiOiJsaW5rLXJlZGlyZWN0In0.7vcXfQwAAJT2Huv29Ms2wKMW=
jcM6XedVtzM7ALO75bM?&amp;utm_source=3Dsubstack&amp;utm_medium=3Demail" styl=
e=3D"font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Rob=
oto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe =
UI Symbol';display: inline-block;font-weight: 500;border: 1px solid rgb(0,0=
,0,.1);border-radius: 9999px;text-transform: uppercase;font-size: 12px;line=
-height: 12px;padding: 9px 14px;text-decoration: none;color: rgb(119,119,11=
9);"><img class=3D"icon" src=3D"https://substackcdn.com/image/fetch/$s_!5EG=
t!,w_36,c_scale,f_png,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubsta=
ck.com%2Ficon%2FNoteForwardIcon%3Fv%3D4%26height%3D36%26fill%3Dnone%26strok=
e%3D%2523808080%26strokeWidth%3D2" width=3D"18" height=3D"18" style=3D"marg=
in-right: 8px;min-width: 18px;min-height: 18px;border: none;vertical-align:=
 middle;max-width: 18px" alt=3D""><span class=3D"email-button-text" style=
=3D"vertical-align: middle;">Restack</span></a></td></tr></tbody></table></=
td></tr></tbody></table></td><td align=3D"right"><table role=3D"presentatio=
n" width=3D"auto" border=3D"0" cellspacing=3D"0" cellpadding=3D"0"><tbody><=
tr></tr></tbody></table></td></tr></tbody></table></td></tr><tr height=3D"1=
6"><td height=3D"16" style=3D"font-size:0px;line-height:0;">&nbsp;</td></tr=
></tbody></table><div class=3D"footer footer-ZM59BM" style=3D"color: rgb(11=
9,119,119);text-align: center;font-size: 16px;line-height: 26px;padding: 24=
px0;"><div style=3D"font-size: 16px;line-height: 26px;padding-bottom: 24px"=
><p class=3D"pencraft pc-reset color-secondary-ls1g8s size-12-mmZ61m reset-=
IxiVJZ small meta-B2bqa5" style=3D"list-style: none;font-family: system-ui,=
-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-ser=
if,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';padding-bottom: 0=
;font-size: 12px;line-height: 16px;margin: 0;color: rgb(119,119,119);text-d=
ecoration: unset;">=C2=A9 2025 <span>Nate</span><br>548 Market Street PMB 7=
2296, San Francisco, CA 94104 <br><a href=3D"https://substack.com/redirect/=
2/eyJlIjoiaHR0cHM6Ly9uYXRlc25ld3NsZXR0ZXIuc3Vic3RhY2suY29tL2FjdGlvbi9kaXNhY=
mxlX2VtYWlsP3Rva2VuPWV5SjFjMlZ5WDJsa0lqb3pNelkwTkRneU1qTXNJbkJ2YzNSZmFXUWlP=
akUyTnpZNE16VXlOeXdpYVdGMElqb3hOelV4T0Rrek5ETTNMQ0psZUhBaU9qRTNPRE0wTWprME1=
6Y3NJbWx6Y3lJNkluQjFZaTB4TXpjek1qTXhJaXdpYzNWaUlqb2laR2x6WVdKc1pWOWxiV0ZwYk=
NKOS5DeTNzZkRyRUZ0NzB1Y0V5MGJnZ2lJSmVidjY2cDlnbktkeUlMOFQtOEw4IiwicCI6MTY3N=
jgzNTI3LCJzIjoxMzczMjMxLCJmIjpmYWxzZSwidSI6MzM2NDQ4MjIzLCJpYXQiOjE3NTE4OTM0=
MzcsImV4cCI6MjA2NzQ2OTQzNywiaXNzIjoicHViLTAiLCJzdWIiOiJsaW5rLXJlZGlyZWN0In0=
.TvFiqXnlGsazm03-mEncGKrEvZZV5Wig9LLldnjxUPs?" style=3D"text-decoration: un=
derline;color: rgb(119,119,119);"><span style=3D"color: rgb(119,119,119);te=
xt-decoration: underline;">Unsubscribe</span></a></p></div><p class=3D"foot=
erSection-EHR0jG small powered-by-substack" style=3D"padding: 0 24px;font-s=
ize: 12px;line-height: 20px;margin: 0;color: rgb(119,119,119);font-family: =
system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Aria=
l,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';padding=
-bottom: 0;margin-top: 0;"><a href=3D"https://substack.com/redirect/2/eyJlI=
joiaHR0cHM6Ly9zdWJzdGFjay5jb20vc2lnbnVwP3V0bV9zb3VyY2U9c3Vic3RhY2smdXRtX21l=
ZGl1bT1lbWFpbCZ1dG1fY29udGVudD1mb290ZXImdXRtX2NhbXBhaWduPWF1dG9maWxsZWQtZm9=
vdGVyJmZyZWVTaWdudXBFbWFpbD1laXRhbkBlaXNsYXcuY28uaWwmcj01a2I5M3oiLCJwIjoxNj=
c2ODM1MjcsInMiOjEzNzMyMzEsImYiOmZhbHNlLCJ1IjozMzY0NDgyMjMsImlhdCI6MTc1MTg5M=
zQzNywiZXhwIjoyMDY3NDY5NDM3LCJpc3MiOiJwdWItMCIsInN1YiI6ImxpbmstcmVkaXJlY3Qi=
fQ.2oJIFeldpsJ0rIbQhYqQx7h1BlE6AeKIbvRCZuAi4NE?" style=3D"color: rgb(119,11=
9,119);text-decoration: none;display: inline-block;margin: 0 4px;"><img src=
=3D"https://substackcdn.com/image/fetch/$s_!LkrL!,w_270,c_limit,f_auto,q_au=
to:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Femail%2Fpub=
lish-button%402x.png" srcset=3D"https://substackcdn.com/image/fetch/$s_!wgf=
j!,w_135,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubs=
tack.com%2Fimg%2Femail%2Fpublish-button.png, https://substackcdn.com/image/=
fetch/$s_!LkrL!,w_270,c_limit,f_auto,q_auto:good,fl_progressive:steep/https=
%3A%2F%2Fsubstack.com%2Fimg%2Femail%2Fpublish-button%402x.png 2x, https://s=
ubstackcdn.com/image/fetch/$s_!KjtY!,w_405,c_limit,f_auto,q_auto:good,fl_pr=
ogressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Femail%2Fpublish-button%4=
03x.png 3x" width=3D"135" alt=3D"Start writing" height=3D"40" style=3D"max-=
width: 550px;border: none !important;vertical-align: middle;"></a></p></div=
></div></td><td></td></tr></tbody></table><img src=3D"https://eotrx.substac=
kcdn.com/open?token=3DeyJtIjoiPDIwMjUwNzA3MTMwMjA2LjMuY2M1YzBmMGJlMjEyYzhkO=
EBtZzEuc3Vic3RhY2suY29tPiIsInUiOjMzNjQ0ODIyMywiciI6ImVpdGFuQGVpc2xhdy5jby5p=
bCIsImQiOiJtZzEuc3Vic3RhY2suY29tIiwicCI6MTY3NjgzNTI3LCJ0IjoicG9kY2FzdCIsImE=
iOiJvbmx5X3BhaWQiLCJzIjoxMzczMjMxLCJjIjoicG9zdCIsImYiOmZhbHNlLCJwb3NpdGlvbi=
I6ImJvdHRvbSIsImlhdCI6MTc1MTg5MzQzNywiZXhwIjoxNzU0NDg1NDM3LCJpc3MiOiJwdWItM=
CIsInN1YiI6ImVvIn0.lVTy1mg6PWnmXMHrgL3aGWhvY5pICLhV6FRaaFRws0c" alt=3D"" wi=
dth=3D"1" height=3D"1" border=3D"0" style=3D"height:1px !important;width:1p=
x !important;border-width:0 !important;margin-top:0 !important;margin-botto=
m:0 !important;margin-right:0 !important;margin-left:0 !important;padding-t=
op:0 !important;padding-bottom:0 !important;padding-right:0 !important;padd=
ing-left:0 !important;"><img width=3D"1" height=3D"1" alt=3D"" src=3D"https=
://email.mg1.substack.com/o/eJxEkEGu4yAQBU8TlhY0YPCCs1hN08mgscEy7Yl8-1F-Fn9=
bJZWeHqHwq593OvoQVZIOQCErTiZ4Exfr7KJ4x7qtL258onBZUX5tiPOi_iQTEV2OBIvxz-JtLu=
DdE-ISjPXZzaom0OB10MFYDXqe7ETkST91ZjBAscSH0_vLTOPKQ5D-TtR39Vm14lUqN-LU23avB=
9by5bUkM4c5Wg_hS-Q-OB29EA5Rx5VX6vt-tSr3yg3zxiXJefFHbZVQam8_FRssWKPOxFWwPZzm=
OjZ8T9Snuqlx5dJ3rC01FB6N32NjET6VfG-7Bp-fjrWzcxHAqn8J_gcAAP__fwhxKw"></body>=
</html>

--d4a764290111796f177397fbcc44f39587ce16a92695f81b46e9ced34d81--
